{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON1C7q24y_FP"
   },
   "source": [
    "## Overview\n",
    "Our project focuses on analyzing academic papers from arXiv, a leading repository of research papers across various disciplines. By extracting and analyzing the text from summary, we aim to develop a system that automatically assigns relevant labels to each paper, categorizing them into appropriate research topics. The dataset for this project includes publicly available academic papers, with key attributes such as paper titles, summary, and their associated labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h027an4zf5A"
   },
   "source": [
    "## Prerequisites\n",
    "Before running the project, ensure that you have installed these packages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2ekS3_hewIN"
   },
   "outputs": [],
   "source": [
    "# Fetching data through ArXiv API\n",
    "!pip install feedparser\n",
    "\n",
    "# Word Cloud\n",
    "!pip install wordcloud\n",
    "\n",
    "# Text Preprocessing and Encoding\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "\n",
    "# Traditional ML models\n",
    "!pip install scikit-learn\n",
    "!pip install xgboost\n",
    "\n",
    "# Deep Learning/ Transformers model\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install nltk\n",
    "!pip install pytorch_lightning\n",
    "!pip install torchmetrics\n",
    "!pip install torchtext\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnG_mA0x001A"
   },
   "source": [
    "## Usage\n",
    "\n",
    "Once the libraries metioned above are installed, you are ready to explore our project!\n",
    "\n",
    "**Data Acquisition and Preprocessing**\n",
    "\n",
    "You can begin data acquisition by running the code provided in the Data_Acquisition_Preprocessing.ipynb. Ensure the correct search term is selected for scraping by updating the search query in the designated section of the code, labeled search_query within the function call.\n",
    "\n",
    "*   For single-word queries (e.g., \"physics\"), simply input the search term as-is.\n",
    "\n",
    "*  For multi-word queries (e.g., \"machine learning\"), replace the space with \"+\" (e.g., \"machine+learning\").\n",
    "\n",
    "No other modifications are required to the code.\n",
    "Once the search query is specified, the code will begin scraping research papers matching the given query from arXiv.org. It will collect papers in batches (default: 100 papers per batch) until the specified total number of papers is retrieved. The data will be saved and downloaded as a .csv file containing paper details such as title, authors, summary, publication date, and category.\n",
    "\n",
    "After that, go to the preprocess section, where the code will clean, format, and preprocess the merged data. This step ensures the dataset is clear, consistent, and ready for analysis.\n",
    "\n",
    "**Exploratory Data Analysis**\n",
    "\n",
    "We will begin the first phase with EDA. Navigate to the EDA.ipynb and run all the code to see our visualizations! These include: Number of Papers per Year, Distribution of Papers by Number of Categories, and Word Cloud for two time periods: 2005-2014 and 2015-2024.\n",
    "\n",
    "**Traditional Machine Learning Models**\n",
    "\n",
    "Please open the Naïve Bayes, XGBoost, Logistic Regression, and Random Forest notebooks for our base models. Run all the code cells sequentially.\n",
    "\n",
    "The notebook will first preprocess the text data and perform the necessary encoding. Next, it will train the model and conduct hyperparameter tuning. Once training is complete, the model will be tested on the testing set using the best parameters.\n",
    "\n",
    "After execution, you will see the test set metrics, including F1-score, precision, recall, Hamming loss, and Jaccard score. A classification report will also be displayed, summarizing the model's performance across different categories.\n",
    "\n",
    "**Deep Learning and Transformer Models**\n",
    "\n",
    "Open the notebook ( whatever you pick )\n",
    "\n",
    "The notebook will first preprocess the text data and perform the necessary encoding. Next, it will train the model and conduct hyperparameter tuning ( Only on LSTM ) . Once training is complete, the model will be tested on the testing set using the best parameters ( only for LSTM ) .\n",
    "\n",
    "After execution, you will see the test set metrics, including F1-score, precision, recall, Hamming loss, and Jaccard score. A classification report will also be displayed, summarizing the model's performance across different categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaJotrGlEkUE"
   },
   "source": [
    "## Additional notes\n",
    "* **Search Term-Specific Adjustments**: Ensure the correct search term is entered in the code. For multi-word queries (e.g., \"machine learning\"), replace spaces with \"+\" (e.g., \"machine+learning\") to avoid issues with retrieval.\n",
    "* **Training Time Considerations**: Model training time may vary depending on the dataset size and model complexity. Deep learning models and transformer-based models may take significantly longer compared to traditional machine learning models like Naïve Bayes or Random Forest.\n",
    "* **Hyperparameter Tuning Impact**: The hyperparameter tuning process, especially for models like XGBoost and deep learning models, may require extended processing time. Consider adjusting tuning parameters if computational resources are limited.\n",
    "* **Execution Order**: Run all code cells sequentially without interruption to ensure smooth execution, proper data preprocessing, and accurate results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
