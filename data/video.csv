title,authors,summary,published,updated,link,pdf_url,categories
VDPVE: VQA Dataset for Perceptual Video Enhancement,"Yixuan Gao, Yuqin Cao, Tengchuan Kou, Wei Sun, Yunlong Dong, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai","Recently, many video enhancement methods have been proposed to improve video quality from different aspects such as color, brightness, contrast, and stability. Therefore, how to evaluate the quality of the enhanced video in a way consistent with human visual perception is an important research topic. However, most video quality assessment methods mainly calculate video quality by estimating the distortion degrees of videos from an overall perspective. Few researchers have specifically proposed a video quality assessment method for video enhancement, and there is also no comprehensive video quality assessment dataset available in public. Therefore, we construct a Video quality assessment dataset for Perceptual Video Enhancement (VDPVE) in this paper. The VDPVE has 1211 videos with different enhancements, which can be divided into three sub-datasets: the first sub-dataset has 600 videos with color, brightness, and contrast enhancements; the second sub-dataset has 310 videos with deblurring; and the third sub-dataset has 301 deshaked videos. We invited 21 subjects (20 valid subjects) to rate all enhanced videos in the VDPVE. After normalizing and averaging the subjective opinion scores, the mean opinion score of each video can be obtained. Furthermore, we split the VDPVE into a training set, a validation set, and a test set, and verify the performance of several state-of-the-art video quality assessment methods on the test set of the VDPVE.",2023-03-16T13:11:16Z,2023-03-16T13:11:16Z,http://arxiv.org/abs/2303.09290v1,http://arxiv.org/pdf/2303.09290v1,eess.IV
Coding Standards as Anchors for the CVPR CLIC video track,"Théo Ladune, Pierrick Philippe","In 2021, a new track has been initiated in the Challenge for Learned Image Compression~: the video track. This category proposes to explore technologies for the compression of short video clips at 1 Mbit/s. This paper proposes to generate coded videos using the latest standardized video coders, especially Versatile Video Coding (VVC). The objective is not only to measure the progress made by learning techniques compared to the state of the art video coders, but also to quantify their progress from years to years. With this in mind, this paper documents how to generate the video sequences fulfilling the requirements of this challenge, in a reproducible way, targeting the maximum performance for VVC.",2021-05-20T15:33:36Z,2021-05-20T15:33:36Z,http://arxiv.org/abs/2105.09833v1,http://arxiv.org/pdf/2105.09833v1,eess.IV
PRNU-Based Source Device Attribution for YouTube Videos,"Emmanuel Kiegaing Kouokam, Ahmet Emir Dirik","Photo Response Non-Uniformity (PRNU) is a camera imaging sensor imperfection which has earned a great interest for source device attribution of digital videos. A majority of recent researches about PRNU-based source device attribution for digital videos do not take into consideration the effects of video compression on the PRNU noise in video frames, but rather consider video frames as isolated images of equal importance. As a result, these methods perform poorly on re-compressed or low bit-rate videos. This paper proposes a novel method for PRNU fingerprint estimation from video frames taking into account the effects of video compression on the PRNU noise in these frames. With this method, we aim to determine whether two videos from unknown sources originate from the same device or not. Experimental results on a large set of videos show that the method we propose is more effective than existing frame-based methods that use either only I frames or all (I-B-P) frames, especially on YouTube videos.",2019-03-21T17:52:22Z,2019-04-01T15:51:12Z,http://arxiv.org/abs/1903.09141v2,http://arxiv.org/pdf/1903.09141v2,"eess.IV, eess.SP"
Tencent Video Dataset (TVD): A Video Dataset for Learning-based Visual   Data Compression and Analysis,"Xiaozhong Xu, Shan Liu, Zeqiang Li","Learning-based visual data compression and analysis have attracted great interest from both academia and industry recently. More training as well as testing datasets, especially good quality video datasets are highly desirable for related research and standardization activities. Tencent Video Dataset (TVD) is established to serve various purposes such as training neural network-based coding tools and testing machine vision tasks including object detection and tracking. TVD contains 86 video sequences with a variety of content coverage. Each video sequence consists of 65 frames at 4K (3840x2160) spatial resolution. In this paper, the details of this dataset, as well as its performance when compressed by VVC and HEVC video codecs, are introduced.",2021-05-12T20:46:56Z,2021-05-12T20:46:56Z,http://arxiv.org/abs/2105.05961v1,http://arxiv.org/pdf/2105.05961v1,eess.IV
Predicting the Quality of Compressed Videos with Pre-Existing   Distortions,"Xiangxu Yu, Neil Birkbeck, Yilin Wang, Christos G. Bampis, Balu Adsumilli, Alan C. Bovik","Over the past decade, the online video industry has greatly expanded the volume of visual data that is streamed and shared over the Internet. Moreover, because of the increasing ease of video capture, many millions of consumers create and upload large volumes of User-Generated-Content (UGC) videos. Unlike streaming television or cinematic content produced by professional videographers and cinemagraphers, UGC videos are most commonly captured by naive users having limited skills and imperfect technique, and often are afflicted by highly diverse and mixed in-capture distortions. These UGC videos are then often uploaded for sharing onto cloud servers, where they further compressed for storage and transmission. Our paper tackles the highly practical problem of predicting the quality of compressed videos (perhaps during the process of compression, to help guide it), with only (possibly severely) distorted UGC videos as references. To address this problem, we have developed a novel Video Quality Assessment (VQA) framework that we call 1stepVQA (to distinguish it from two-step methods that we discuss). 1stepVQA overcomes limitations of Full-Reference, Reduced-Reference and No-Reference VQA models by exploiting the statistical regularities of both natural videos and distorted videos. We show that 1stepVQA is able to more accurately predict the quality of compressed videos, given imperfect reference videos. We also describe a new dedicated video database which includes (typically distorted) UGC reference videos, and a large number of compressed versions of them. We show that the 1stepVQA model outperforms other VQA models in this scenario. We are providing the dedicated new database free of charge at https://live.ece.utexas.edu/research/onestep/index.html",2020-04-06T19:06:58Z,2020-04-06T19:06:58Z,http://arxiv.org/abs/2004.02943v1,http://arxiv.org/pdf/2004.02943v1,eess.IV
Video Compression Coding via Colorization: A Generative Adversarial   Network (GAN)-Based Approach,"Zhaoqing Pan, Feng Yuan, Jianjun Lei, Sam Kwong","Under the limited storage, computing and network bandwidth resources, the video compression coding technology plays an important role for visual communication. To efficiently compress raw video data, a colorization-based video compression coding method is proposed in this paper. In the proposed encoder, only the video luminance components are encoded and transmitted. To restore the video chrominance information, a generative adversarial network (GAN) model is adopted in the proposed decoder. In order to make the GAN work efficiently for video colorization, the generator of the proposed GAN model adopts an optimized MultiResUNet, an attention module, and a mixed loss function. Experimental results show that when compared with the H.265/HEVC video compression coding standard using all-intra coding structure, the proposed video compression coding method achieves an average of 72.05% BDBR reduction, and an average of 4.758 dB BDPSNR increase. Moreover, to our knowledge, this is the first work which compresses videos by using GAN-based colorization, and it provides a new way for addressing the video compression coding problems.",2019-12-23T07:24:22Z,2019-12-23T07:24:22Z,http://arxiv.org/abs/1912.10653v1,http://arxiv.org/pdf/1912.10653v1,eess.IV
Flexible Architecture for Real-time Processing of Multiple Video Signals,"Mohamed Awad, Islam T. Abougindia, Ahmed Elliethy, Hussein A. Aly","Simultaneous processing of multiple video sources requires each pixel in a frame from a video source to be processed synchronously with the pixels at the same spatial positions in corresponding frames from the other video sources. However, simultaneous processing is challenging as corresponding frames from different video signals provided by multiple sources have time-varying delay because of the electrical and mechanical restrictions inside the video sources hardware that cause deviation in the corresponding frame rates. Researchers overcome the aforementioned challenges either by utilizing ready-made video processing systems or designing and implementing a custom system tailored to their specific application. These video processing systems lack flexibility in handling different applications requirements such as the required number of video sources and outputs, video standards, or frame rates of the input/output videos. In this paper, we present a design for a flexible simultaneous video processing architecture that is suitable for various applications. The proposed architecture is upgradeable to deal with multiple video standards, scalable to process/produce a variable number of input/output videos, and compatible with most video processors. Moreover, we present in details the analog/digital mixed-signals and power distribution considerations used in designing the proposed architecture. As a case study application of the proposed flexible architecture, we utilized the architecture for a realization of a simultaneous video processing system that performs video fusion from visible and near-infrared video sources in real time. We make available the source files of the hardware design along with the bill of material (BOM) of the case study to be a reference for researchers who intend to design and implement simultaneous multi-video processing systems.",2019-12-29T23:08:08Z,2019-12-29T23:08:08Z,http://arxiv.org/abs/2001.02048v1,http://arxiv.org/pdf/2001.02048v1,"eess.IV, eess.SP"
Assessment of Subjective and Objective Quality of Live Streaming Sports   Videos,"Zaixi Shang, Joshua P. Ebenezer, Alan C. Bovik, Yongjun Wu, Hai Wei, Sriram Sethuraman","Video live streaming is gaining prevalence among video streaming services, especially for the delivery of popular sporting events. Many objective Video Quality Assessment (VQA) models have been developed to predict the perceptual quality of videos. Appropriate databases that exemplify the distortions encountered in live streaming videos are important to designing and learning objective VQA models. Towards making progress in this direction, we built a video quality database specifically designed for live streaming VQA research. The new video database is called the Laboratory for Image and Video Engineering (LIVE) Live stream Database. The LIVE Livestream Database includes 315 videos of 45 contents impaired by 6 types of distortions. We also performed a subjective quality study using the new database, whereby more than 12,000 human opinions were gathered from 40 subjects. We demonstrate the usefulness of the new resource by performing a holistic evaluation of the performance of current state-of-the-art (SOTA) VQA models. The LIVE Livestream database is being made publicly available for these purposes at https://live.ece.utexas.edu/research/LIVE_APV_Study/apv_index.html.",2021-06-15T20:44:38Z,2021-06-15T20:44:38Z,http://arxiv.org/abs/2106.08431v1,http://arxiv.org/pdf/2106.08431v1,eess.IV
An End-Cloud Computing Enabled Surveillance Video Transmission System,"Dingxi Yang, Zhijin Qin, Liting Wang, Xiaoming Tao, Fang Cui, Hengjiang Wang","The enormous data volume of video poses a significant burden on the network. Particularly, transferring high-definition surveillance videos to the cloud consumes a significant amount of spectrum resources. To address these issues, we propose a surveillance video transmission system enabled by end-cloud computing. Specifically, the cameras actively down-sample the original video and then a redundant frame elimination module is employed to further reduce the data volume of surveillance videos. Then we develop a key-frame assisted video super-resolution model to reconstruct the high-quality video at the cloud side. Moreover, we propose a strategy of extracting key frames from source videos for better reconstruction performance by utilizing the peak signal-to-noise ratio (PSNR) of adjacent frames to measure the propagation distance of key frame information. Simulation results show that the developed system can effectively reduce the data volume by the end-cloud collaboration and outperforms existing video super-resolution models significantly in terms of PSNR and structural similarity index (SSIM).",2023-11-08T13:45:06Z,2023-11-08T13:45:06Z,http://arxiv.org/abs/2311.04685v1,http://arxiv.org/pdf/2311.04685v1,eess.IV
A Subjective and Objective Study of Space-Time Subsampled Video Quality,"Dae Yeol Lee, Somdyuti Paul, Christos G. Bampis, Hyunsuk Ko, Jongho Kim, Se Yoon Jeong, Blake Homan, Alan C. Bovik","Video dimensions are continuously increasing to provide more realistic and immersive experiences to global streaming and social media viewers. However, increments in video parameters such as spatial resolution and frame rate are inevitably associated with larger data volumes. Transmitting increasingly voluminous videos through limited bandwidth networks in a perceptually optimal way is a current challenge affecting billions of viewers. One recent practice adopted by video service providers is space-time resolution adaptation in conjunction with video compression. Consequently, it is important to understand how different levels of space-time subsampling and compression affect the perceptual quality of videos. Towards making progress in this direction, we constructed a large new resource, called the ETRI-LIVE Space-Time Subsampled Video Quality (ETRI-LIVE STSVQ) database, containing 437 videos generated by applying various levels of combined space-time subsampling and video compression on 15 diverse video contents. We also conducted a large-scale human study on the new dataset, collecting about 15,000 subjective judgments of video quality. We provide a rate-distortion analysis of the collected subjective scores, enabling us to investigate the perceptual impact of space-time subsampling at different bit rates. We also evaluated and compared the performance of leading video quality models on the new database.",2021-01-29T22:05:57Z,2021-01-29T22:05:57Z,http://arxiv.org/abs/2102.00088v1,http://arxiv.org/pdf/2102.00088v1,eess.IV
Compressed Video Action Recognition with Refined Motion Vector,"Haoyuan Cao, Shining Yu, Jiashi Feng","Although CNN has reached satisfactory performance in image-related tasks, using CNN to process videos is much more challenging due to the enormous size of raw video streams. In this work, we propose to use motion vectors and residuals from modern video compression techniques to effectively learn the representation of the raw frames and greatly remove the temporal redundancy, giving a faster video processing model. Compressed Video Action Recognition(CoViAR) has explored to directly use compressed video to train the deep neural network, where the motion vectors were utilized to present temporal information. However, motion vector is designed for minimizing video size where precise motion information is not obligatory. Compared with optical flow, motion vectors contain noisy and unreliable motion information. Inspired by the mechanism of video compression codecs, we propose an approach to refine the motion vectors where unreliable movement will be removed while temporal information is largely reserved. We prove that replacing the original motion vector with refined one and using the same network as CoViAR has achieved state-of-art performance on the UCF-101 and HMDB-51 with negligible efficiency degrades comparing with original CoViAR.",2019-10-06T21:34:42Z,2019-10-06T21:34:42Z,http://arxiv.org/abs/1910.02533v1,http://arxiv.org/pdf/1910.02533v1,eess.IV
Low Light Video Enhancement using Synthetic Data Produced with an   Intermediate Domain Mapping,"Danai Triantafyllidou, Sean Moran, Steven McDonagh, Sarah Parisot, Gregory Slabaugh","Advances in low-light video RAW-to-RGB translation are opening up the possibility of fast low-light imaging on commodity devices (e.g. smartphone cameras) without the need for a tripod. However, it is challenging to collect the required paired short-long exposure frames to learn a supervised mapping. Current approaches require a specialised rig or the use of static videos with no subject or object motion, resulting in datasets that are limited in size, diversity, and motion. We address the data collection bottleneck for low-light video RAW-to-RGB by proposing a data synthesis mechanism, dubbed SIDGAN, that can generate abundant dynamic video training pairs. SIDGAN maps videos found 'in the wild' (e.g. internet videos) into a low-light (short, long exposure) domain. By generating dynamic video data synthetically, we enable a recently proposed state-of-the-art RAW-to-RGB model to attain higher image quality (improved colour, reduced artifacts) and improved temporal consistency, compared to the same model trained with only static real video data.",2020-07-17T18:42:08Z,2020-07-17T18:42:08Z,http://arxiv.org/abs/2007.09187v1,http://arxiv.org/pdf/2007.09187v1,eess.IV
On the benefit of parameter-driven approaches for the modeling and the   prediction of Satisfied User Ratio for compressed video,"Jingwen Zhu, Patrick Le Callet, Anne-Flore Perrin, Sriram Sethuraman, Kumar Rahul","The human eye cannot perceive small pixel changes in images or videos until a certain threshold of distortion. In the context of video compression, Just Noticeable Difference (JND) is the smallest distortion level from which the human eye can perceive the difference between reference video and the distorted/compressed one. Satisfied-User-Ratio (SUR) curve is the complementary cumulative distribution function of the individual JNDs of a viewer group. However, most of the previous works predict each point in SUR curve by using features both from source video and from compressed videos with assumption that the group-based JND annotations follow Gaussian distribution, which is neither practical nor accurate. In this work, we firstly compared various common functions for SUR curve modeling. Afterwards, we proposed a novel parameter-driven method to predict the video-wise SUR from video features. Besides, we compared the prediction results of source-only features based (SRC-based) models and source plus compressed videos features (SRC+PVS-based) models.",2022-06-20T15:33:57Z,2022-06-20T15:33:57Z,http://arxiv.org/abs/2206.09854v1,http://arxiv.org/pdf/2206.09854v1,eess.IV
Energy-Rate-Quality Tradeoffs of State-of-the-Art Video Codecs,"Angeliki Katsenou, Jongwewi Mao, Ioannis Mavromatis","The adoption of video conferencing and video communication services, accelerated by COVID-19, has driven a rapid increase in video data traffic. The demand for higher resolutions and quality, the need for immersive video formats, and the newest, more complex video codecs increase the energy consumption in data centers and display devices. In this paper, we explore and compare the energy consumption across optimized state-of-the-art video codecs, SVT-AV1, VVenC/VVdeC, VP9, and x.265. Furthermore, we align the energy usage with various objective quality metrics and the compression performance for a set of video sequences across different resolutions. The results indicate that from the tested codecs and configurations, SVT-AV1 provides the best tradeoff between energy consumption and quality. The reported results aim to serve as a guide towards sustainable video streaming while not compromising the quality of experience of the end user.",2022-10-02T20:39:25Z,2022-10-02T20:39:25Z,http://arxiv.org/abs/2210.00618v1,http://arxiv.org/pdf/2210.00618v1,eess.IV
Sweet Streams are Made of This: The System Engineer's View on Energy   Efficiency in Video Communications,"Christian Herglotz, Matthias Kränzler, Robert Schober, André Kaup","In recent years, the global use of online video services has increased rapidly. Today, a manifold of applications, such as video streaming, video conferencing, live broadcasting, and social networks, make use of this technology. A recent study found that the development and the success of these services had as a consequence that, nowadays, more than 1% of the global greenhouse-gas emissions are related to online video, with growth rates close to 10% per year. This article reviews the latest findings concerning energy consumption of online video from the system engineer's perspective, where the system engineer is the designer and operator of a typical online video service. We discuss all relevant energy sinks, highlight dependencies with quality-of-service variables as well as video properties, review energy consumption models for different devices from the literature, and aggregate these existing models into a global model for the overall energy consumption of a generic online video service. Analyzing this model and its implications, we find that end-user devices and video encoding have the largest potential for energy savings. Finally, we provide an overview of recent advances in energy efficiency improvement for video streaming and propose future research directions for energy-efficient video streaming services.",2022-09-30T12:04:30Z,2022-09-30T12:04:30Z,http://arxiv.org/abs/2209.15405v1,http://arxiv.org/pdf/2209.15405v1,eess.IV
"UAV Immersive Video Streaming: A Comprehensive Survey, Benchmarking, and   Open Challenges","Mohit K. Sharma, Chen-Feng Liu, Ibrahim Farhat, Nassim Sehad, Wassim Hamidouche, Merouane Debbah","Over the past decade, the utilization of UAVs has witnessed significant growth, owing to their agility, rapid deployment, and maneuverability. In particular, the use of UAV-mounted 360-degree cameras to capture omnidirectional videos has enabled truly immersive viewing experiences with up to 6DoF. However, achieving this immersive experience necessitates encoding omnidirectional videos in high resolution, leading to increased bitrates. Consequently, new challenges arise in terms of latency, throughput, perceived quality, and energy consumption for real-time streaming of such content. This paper presents a comprehensive survey of research efforts in UAV-based immersive video streaming, benchmarks popular video encoding schemes, and identifies open research challenges. Initially, we review the literature on 360-degree video coding, packaging, and streaming, with a particular focus on standardization efforts to ensure interoperability of immersive video streaming devices and services. Subsequently, we provide a comprehensive review of research efforts focused on optimizing video streaming for timevarying UAV wireless channels. Additionally, we introduce a high resolution 360-degree video dataset captured from UAVs under different flying conditions. This dataset facilitates the evaluation of complexity and coding efficiency of software and hardware video encoders based on popular video coding standards and formats, including AVC/H.264, HEVC/H.265, VVC/H.266, VP9, and AV1. Our results demonstrate that HEVC achieves the best trade-off between coding efficiency and complexity through its hardware implementation, while AV1 format excels in coding efficiency through its software implementation, specifically using the libsvt-av1 encoder. Furthermore, we present a real testbed showcasing 360-degree video streaming over a UAV, enabling remote control of the drone via a 5G cellular network.",2023-10-31T18:45:27Z,2023-10-31T18:45:27Z,http://arxiv.org/abs/2311.00082v1,http://arxiv.org/pdf/2311.00082v1,eess.IV
Effect of High Frame Rates on 3D Video Quality of Experience,"Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos","In this paper, we study the effect of 3D videos with increased frame rates on the viewers quality of experience. We performed a series of subjective tests to seek the subjects preferences among videos of the same scene at four different frame rates: 24, 30, 48, and 60 frames per second (fps). Results revealed that subjects clearly prefer higher frame rates. In particular, Mean Opinion Score (MOS) values associated with the 60 fps 3D videos were 55% greater than MOS values of the 24 fps 3D videos.",2018-03-13T06:44:03Z,2018-03-13T06:44:03Z,http://arxiv.org/abs/1803.04653v1,http://arxiv.org/pdf/1803.04653v1,eess.IV
ViSTRA3: Video Coding with Deep Parameter Adaptation and Post Processing,"Chen Feng, Duolikun Danier, Charlie Tan, Fan Zhang, David Bull","This paper presents a deep learning-based video compression framework (ViSTRA3). The proposed framework intelligently adapts video format parameters of the input video before encoding, subsequently employing a CNN at the decoder to restore their original format and enhance reconstruction quality. ViSTRA3 has been integrated with the H.266/VVC Test Model VTM 14.0, and evaluated under the Joint Video Exploration Team Common Test Conditions. Bj{\o}negaard Delta (BD) measurement results show that the proposed framework consistently outperforms the original VVC VTM, with average BD-rate savings of 1.8% and 3.7% based on the assessment of PSNR and VMAF.",2021-11-30T16:26:42Z,2021-11-30T16:26:42Z,http://arxiv.org/abs/2111.15536v1,http://arxiv.org/pdf/2111.15536v1,eess.IV
Comparing H.265/HEVC and VP9: Impact of High Frame Rates on the   Perceptual Quality of Compressed Videos,"Tariq Rahim, Muhammad Arslan Usman, Soo Young Shin","High frame rates have been known to enhance the perceived visual quality of specific video content. However, the lack of investigation of high frame rates has restricted the expansion of this research field particularly in the context of full-high-definition (FHD) and 4K ultra-high-definition video formats. This study involves a subjective and objective quality assessment of compressed FHD videos. First, we compress the FHD videos by employing high-efficiency video coding, and VP9 at five quantization parameter levels for multiple frame rates, i.e., 15fps, 30fps, and 60fps. The FHD videos are obtained from a high frame-rate video database BVI-HFR, spanning various scenes, colors, and motions, and are shown to be representative of the BBC broadcast content. Second, a detailed subjective quality assessment of compressed videos for both encoders and individual frame rates is conducted, resulting in subjective measurements in the form of the differential mean opinion score reflecting the quality of experience. In particular, the aim is to investigate the impact of compression on the perceptual quality of compressed FHD videos and compare the performance of both encoders for each frame rate. Finally, 11 state-of-the-art objective quality assessment metrics are benchmarked using the subjective measurements, to investigate the correlation as a statistical evaluation between the two models in terms of correlation coefficients. A recommendation for enhancing the quality estimation of full-reference (FR) video quality measurements (VQMs) is presented after the extensive investigation.",2020-06-04T07:20:05Z,2020-06-04T07:20:05Z,http://arxiv.org/abs/2006.02671v1,http://arxiv.org/pdf/2006.02671v1,eess.IV
Perceptual Video Quality Prediction Emphasizing Chroma Distortions,"Li-Heng Chen, Christos G. Bampis, Zhi Li, Joel Sole, Alan C. Bovik","Measuring the quality of digital videos viewed by human observers has become a common practice in numerous multimedia applications, such as adaptive video streaming, quality monitoring, and other digital TV applications. Here we explore a significant, yet relatively unexplored problem: measuring perceptual quality on videos arising from both luma and chroma distortions from compression. Toward investigating this problem, it is important to understand the kinds of chroma distortions that arise, how they relate to luma compression distortions, and how they can affect perceived quality. We designed and carried out a subjective experiment to measure subjective video quality on both luma and chroma distortions, introduced both in isolation as well as together. Specifically, the new subjective dataset comprises a total of $210$ videos afflicted by distortions caused by varying levels of luma quantization commingled with different amounts of chroma quantization. The subjective scores were evaluated by $34$ subjects in a controlled environmental setting. Using the newly collected subjective data, we were able to demonstrate important shortcomings of existing video quality models, especially in regards to chroma distortions. Further, we designed an objective video quality model which builds on existing video quality algorithms, by considering the fidelity of chroma channels in a principled way. We also found that this quality analysis implies that there is room for reducing bitrate consumption in modern video codecs by creatively increasing the compression factor on chroma channels. We believe that this work will both encourage further research in this direction, as well as advance progress on the ultimate goal of jointly optimizing luma and chroma compression in modern video encoders.",2020-09-23T15:11:55Z,2020-09-24T17:24:08Z,http://arxiv.org/abs/2009.11203v2,http://arxiv.org/pdf/2009.11203v2,eess.IV
An In-router Identification Scheme for Selective Discard of Video   Packets,"Ashkan Moharrami, Mohammad Ghasempour, Mohammad Ghanbari","High quality (HQ) video services occupy large portions of the total bandwidth and are among the main causes of congestion at network bottlenecks. Since video is resilient to data loss, throwing away less important video packets can ease network congestion with minimal damage to video quality and free up bandwidth for other data flows. Frame type is one of the features that can be used to determine the importance of video packets, but this information is stored in the packet payload. Due to limited processing power of devices in high throughput/speed networks, data encryption and user credibility issues, it is costly for the network to find the frame type of each packet. Therefore, a fast and reliable standalone method to recognize video packet types at network level is desired. This paper proposes a method to model the structure of live video streams in a network node which results in determining the frame type of each packet. It enables the network nodes to mark and if need be to discard less important video packets ahead of congestion, and therefore preserve video quality and free up bandwidth for more important packet types. The method does not need to read the IP layer payload and uses only the packet header data for decisions. Experimental results indicate while dropping packets under packet type prediction degrades video quality with respect to its true type by 0.5-3 dB, it has 7-20 dB improvement over when packets are dropped randomly.",2021-04-27T07:29:38Z,2021-04-27T07:29:38Z,http://arxiv.org/abs/2104.13013v1,http://arxiv.org/pdf/2104.13013v1,eess.IV
Large-Scale Study of Perceptual Video Quality,"Zeina Sinno, Alan C. Bovik","The great variations of videographic skills, camera designs, compression and processing protocols, and displays lead to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores, resulting in an average of 240 recorded human opinions per video. We demonstrate the value of the new resource, which we call the LIVE Video Quality Challenge Database (LIVE-VQC), by conducting a comparison of leading NR video quality predictors on it. This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores. The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html .",2018-03-05T16:38:03Z,2018-11-04T15:42:11Z,http://arxiv.org/abs/1803.01761v2,http://arxiv.org/pdf/1803.01761v2,eess.IV
Towards a Video Quality Assessment based Framework for Enhancement of   Laparoscopic Videos,"Zohaib Amjad Khan, Azeddine Beghdadi, Faouzi Alaya Cheikh, Mounir Kaaniche, Egidijus Pelanis, Rafael Palomar, Åsmund Avdem Fretland, Bjørn Edwin, Ole Jakob Elle","Laparoscopic videos can be affected by different distortions which may impact the performance of surgery and introduce surgical errors. In this work, we propose a framework for automatically detecting and identifying such distortions and their severity using video quality assessment. There are three major contributions presented in this work (i) a proposal for a novel video enhancement framework for laparoscopic surgery; (ii) a publicly available database for quality assessment of laparoscopic videos evaluated by expert as well as non-expert observers and (iii) objective video quality assessment of laparoscopic videos including their correlations with expert and non-expert scores.",2020-03-28T01:40:13Z,2020-03-28T01:40:13Z,http://arxiv.org/abs/2003.12679v1,http://arxiv.org/pdf/2003.12679v1,eess.IV
Assessing Visual Quality of Omnidirectional Videos,"Mai Xu, Chen Li, Zulin Wang, Zhenzhong Chen, Zhenyu Guan","In contrast with traditional video, omnidirectional video enables spherical viewing direction with support for head-mounted displays, providing an interactive and immersive experience. Unfortunately, to the best of our knowledge, there are few visual quality assessment (VQA) methods, either subjective or objective, for omnidirectional video coding. This paper proposes both subjective and objective methods for assessing quality loss in encoding omnidirectional video. Specifically, we first present a new database, which includes the viewing direction data from several subjects watching omnidirectional video sequences. Then, from our database, we find a high consistency in viewing directions across different subjects. The viewing directions are normally distributed in the center of the front regions, but they sometimes fall into other regions, related to video content. Given this finding, we present a subjective VQA method for measuring difference mean opinion score (DMOS) of the whole and regional omnidirectional video, in terms of overall DMOS (O-DMOS) and vectorized DMOS (V-DMOS), respectively. Moreover, we propose two objective VQA methods for encoded omnidirectional video, in light of human perception characteristics of omnidirectional video. One method weighs the distortion of pixels with regard to their distances to the center of front regions, which considers human preference in a panorama. The other method predicts viewing directions according to video content, and then the predicted viewing directions are leveraged to allocate weights to the distortion of each pixel in our objective VQA method. Finally, our experimental results verify that both the subjective and objective methods proposed in this paper advance state-of-the-art VQA for omnidirectional video.",2017-09-19T10:54:24Z,2019-07-14T02:30:32Z,http://arxiv.org/abs/1709.06342v4,http://arxiv.org/pdf/1709.06342v4,eess.IV
BVI-CC: A Dataset for Research on Video Compression and Quality   Assessment,"Angeliki V. Katsenou, Fan Zhang, Mariana Afonso, Goce Dimitrov, David R. Bull","The video technology scenery has been very vivid over the past years, with novel video coding technologies introduced that promise improved compression performance over state-of-the-art technologies. Despite the fact that a lot of video datasets are available, representative content of the wide parameter space along with subjective evaluations of variations of encoded content from an unpartial end is required. In response to this requirement, this paper features a dataset, the BVI-CC. Three video codecs were deployed to create the variations of the encoded sequences: High Efficiency Video Coding (HEVC) Test Model (HM), AOMedia Video 1 (AV1), and Versatile Video Coding (VVC) Test Model (VTM). Nine source video sequences were carefully selected to offer both diversity and representativeness in the spatio-temporal domain. Different spatial resolution versions of the sequences were created and encoded by all three codecs at pre-defined target bit rates. The compression efficiency of the codecs was evaluated with commonly used objective quality metrics, and the subjective quality of their reconstructed content was also evaluated through psychophysical experiments. Additionally, an adaptive bit rate (convex hull rate-distortion optimization across spatial resolutions) test case was assessed using both objective and subjective evaluations. Finally, the computational complexities of the tested codecs were examined. All data have been made publicly available as part of the dataset, which can be used for coding performance evaluation and video quality metric development.",2020-03-23T13:54:47Z,2022-02-21T13:00:44Z,http://arxiv.org/abs/2003.10282v2,http://arxiv.org/pdf/2003.10282v2,eess.IV
An Interactive Annotation Tool for Perceptual Video Compression,"Evgenya Pergament, Pulkit Tandon, Kedar Tatwawadi, Oren Rippel, Lubomir Bourdev, Bruno Olshausen, Tsachy Weissman, Sachin Katti, Alexander G. Anderson","Human perception is at the core of lossy video compression and yet, it is challenging to collect data that is sufficiently dense to drive compression. In perceptual quality assessment, human feedback is typically collected as a single scalar quality score indicating preference of one distorted video over another. In reality, some videos may be better in some parts but not in others. We propose an approach to collecting finer-grained feedback by asking users to use an interactive tool to directly optimize for perceptual quality given a fixed bitrate. To this end, we built a novel web-tool which allows users to paint these spatio-temporal importance maps over videos. The tool allows for interactive successive refinement: we iteratively re-encode the original video according to the painted importance maps, while maintaining the same bitrate, thus allowing the user to visually see the trade-off of assigning higher importance to one spatio-temporal part of the video at the cost of others. We use this tool to collect data in-the-wild (10 videos, 17 users) and utilize the obtained importance maps in the context of x264 coding to demonstrate that the tool can indeed be used to generate videos which, at the same bitrate, look perceptually better through a subjective study - and are 1.9 times more likely to be preferred by viewers. The code for the tool and dataset can be found at https://github.com/jenyap/video-annotation-tool.git",2022-05-08T23:14:05Z,2022-05-08T23:14:05Z,http://arxiv.org/abs/2205.03969v1,http://arxiv.org/pdf/2205.03969v1,eess.IV
Comparison and Analysis of Cognitive Load under 2D/3D Visual Stimuli,"Yu Liu, Chen Song, Yunpeng Yin, Herui Shi, Jinglin Sun, Han Wang, Peiguang Jing","With the increasing prevalence of 3D videos, investigating the differences of viewing experiences between 2D and 3D videos has become an important issue. In this study, we explored the cognitive load induced by 2D and 3D video stimuli under various cognitive tasks utilizing electroencephalogram (EEG) data. We also introduced the Cognitive Load Index (CLI), a metric which combines {\theta} and {\alpha} oscillations to evaluate the cognitive differences. Four video stimuli, each associated with typical cognitive tasks were adopted in our experiments. Subjects were exposed to both 2D and 3D video stimuli, and the corresponding EEG data were recorded. Then, we analyzed the power within the 0.5-45 Hz frequency of EEG data, and CLI was utilized to evaluate the brain activity of different subjects. According to our experiments and analysis, videos that involve simple observational tasks (P <0.05) consistently induced a higher cognitive load in subjects when they were viewing 3D videos. However, for videos that involve calculation tasks (P >0.05), the differences in cognitive load induced by 2D and 3D video were not obvious. Thus, we concluded that 3D videos could generally induce a higher cognitive load, but the extent of the differences also depended on the contents of the video stimuli and the viewing purpose.",2023-02-25T03:11:49Z,2024-05-13T09:10:03Z,http://arxiv.org/abs/2302.12968v4,http://arxiv.org/pdf/2302.12968v4,q-bio.NC
Evaluating the Performance of Existing Full-Reference Quality Metrics on   High Dynamic Range (HDR) Video Content,"Maryam Azimi, Amin Banitalebi-Dehkordi, Yuanyuan Dong, Mahsa T. Pourazad, Panos Nasiopoulos","While there exists a wide variety of Low Dynamic Range (LDR) quality metrics, only a limited number of metrics are designed specifically for the High Dynamic Range (HDR) content. With the introduction of HDR video compression standardization effort by international standardization bodies, the need for an efficient video quality metric for HDR applications has become more pronounced. The objective of this study is to compare the performance of the existing full-reference LDR and HDR video quality metrics on HDR content and identify the most effective one for HDR applications. To this end, a new HDR video dataset is created, which consists of representative indoor and outdoor video sequences with different brightness, motion levels and different representing types of distortions. The quality of each distorted video in this dataset is evaluated both subjectively and objectively. The correlation between the subjective and objective results confirm that VIF quality metric outperforms all to ther tested metrics in the presence of the tested types of distortions.",2018-03-13T14:02:40Z,2018-03-13T14:02:40Z,http://arxiv.org/abs/1803.04815v1,http://arxiv.org/pdf/1803.04815v1,eess.IV
Compression of High Dynamic Range Video Using the HEVC and H.264/AVC   Standards,"Amin Banitalebi-Dehkordi, Maryam Azimi, Mahsa T. Pourazad, Panos Nasiopoulos","The existing video coding standards such as H.264/AVC and High Efficiency Video Coding (HEVC) have been designed based on the statistical properties of Low Dynamic Range (LDR) videos and are not accustomed to the characteristics of High Dynamic Range (HDR) content. In this study, we investigate the performance of the latest LDR video compression standard, HEVC, as well as the recent widely commercially used video compression standard, H.264/AVC, on HDR content. Subjective evaluations of results on an HDR display show that viewers clearly prefer the videos coded via an HEVC-based encoder to the ones encoded using an H.264/AVC encoder. In particular, HEVC outperforms H.264/AVC by an average of 10.18% in terms of mean opinion score and 25.08% in terms of bit rate savings.",2018-03-13T14:13:40Z,2018-03-13T14:13:40Z,http://arxiv.org/abs/1803.04823v1,http://arxiv.org/pdf/1803.04823v1,eess.IV
Benchmark 3D eye-tracking dataset for visual saliency prediction on   stereoscopic 3D video,"Amin Banitalebi-Dehkordi, Eleni Nasiopoulos, Mahsa T. Pourazad, Panos Nasiopoulos","Visual Attention Models (VAMs) predict the location of an image or video regions that are most likely to attract human attention. Although saliency detection is well explored for 2D image and video content, there are only few attempts made to design 3D saliency prediction models. Newly proposed 3D visual attention models have to be validated over large-scale video saliency prediction datasets, which also contain results of eye-tracking information. There are several publicly available eye-tracking datasets for 2D image and video content. In the case of 3D, however, there is still a need for large-scale video saliency datasets for the research community for validating different 3D-VAMs. In this paper, we introduce a large-scale dataset containing eye-tracking data collected from 61 stereoscopic 3D videos (and also 2D versions of those) and 24 subjects participated in a free-viewing test. We evaluate the performance of the existing saliency detection methods over the proposed dataset. In addition, we created an online benchmark for validating the performance of the existing 2D and 3D visual attention models and facilitate addition of new VAMs to the benchmark. Our benchmark currently contains 50 different VAMs.",2018-03-13T14:40:40Z,2018-03-13T14:40:40Z,http://arxiv.org/abs/1803.04845v1,http://arxiv.org/pdf/1803.04845v1,eess.IV
OpenVVC: a Lightweight Software Decoder for the Versatile Video Coding   Standard,"Thomas Amestoy, Pierre-loup Cabarat, Guillaume Gautier, Wassim Hamidouche, Daniel Menard","In the recent years, users requirements for higher resolution, coupled with the apparition of new multimedia applications, have created the need for a new video coding standard. The new generation video coding standard, called Versatile Video Coding (VVC), has been developed by the Joint Video Experts Team, and offers coding capability beyond the previous generation High Efficiency Video Coding (HEVC) standard. Due to the incorporation of more advanced and complex tools, the decoding complexity of VVC standard compared to HEVC has approximately doubled. This complexity increase raises new research challenges to achieve live software decoding. In this context, we developed OpenVVC, an open-source software decoder that supports a broad range of VVC functionalities. This paper presents the OpenVVC software architecture, its parallelism strategy as well as a detailed set of experimental results. By combining extensive data level parallelism with frame level parallelism, OpenVVC achieves real-time decoding of UHD video content. Moreover, the memory required by OpenVVC is remarkably low, which presents a great advantage for its integration on embedded platforms with low memory resources. The code of the OpenVVC decoder is publicly available at https://github.com/OpenVVC/OpenVVC",2022-05-24T17:23:44Z,2022-05-24T17:23:44Z,http://arxiv.org/abs/2205.12217v1,http://arxiv.org/pdf/2205.12217v1,eess.IV
Subjective and Objective Quality Assessment of High-Motion Sports Videos   at Low-Bitrates,"Joshua P. Ebenezer, Yixu Chen, Yongjun Wu, Hai Wei, Sriram Sethuraman","Videos often have to be transmitted and stored at low bitrates due to poor network connectivity during adaptive bitrate streaming. Designing optimal bitrate ladders that would select the perceptually-optimized resolution, frame-rate, and compression level for low-bitrate videos for adaptive streaming across the internet is therefore a task of great interest. Towards that end, we conducted the first large-scale study of medium and low-bitrate videos from live sports for two codecs (Elemental AVC and HEVC) and created the Amazon Prime Video Low-Bitrate Sports (APV LBS) dataset. The study involved 94 participants and 742 videos, with more than 23,000 human opinion scores collected in total. We analyzed the data obtained and we also conducted an extensive evaluation of objective Video Quality Assessment (VQA) algorithms and benchmarked their performance, and make recommendations on bitrate ladder design. We're making the metadata and VQA features available at https://github.com/JoshuaEbenezer/lbmfr-public.",2022-07-12T19:26:53Z,2022-07-12T19:26:53Z,http://arxiv.org/abs/2207.05798v1,http://arxiv.org/pdf/2207.05798v1,eess.IV
LCCM-VC: Learned Conditional Coding Modes for Video Compression,"Hadi Hadizadeh, Ivan V. Bajić","End-to-end learning-based video compression has made steady progress over the last several years. However, unlike learning-based image coding, which has already surpassed its handcrafted counterparts, learning-based video coding still has some ways to go. In this paper we present learned conditional coding modes for video coding (LCCM-VC), a video coding model that achieves state-of-the-art results among learning-based video coding methods. Our model utilizes conditional coding engines from the recent conditional augmented normalizing flows (CANF) pipeline, and introduces additional coding modes to improve compression performance. The compression efficiency is especially good in the high-quality/high-bitrate range, which is important for broadcast and video-on-demand streaming applications. The implementation of LCCM-VC is available at https://github.com/hadihdz/lccm_vc",2022-10-28T04:10:17Z,2023-04-19T06:53:20Z,http://arxiv.org/abs/2210.15883v2,http://arxiv.org/pdf/2210.15883v2,eess.IV
Rate-Distortion Optimization With Alternative References For UGC Video   Compression,"Xin Xiong, Eduardo Pavez, Antonio Ortega, Balu Adsumilli","User generated content (UGC) refers to videos that are uploaded by users and shared over the Internet. UGC may have low quality due to noise and previous compression. When re-encoding UGC for streaming or downloading, a traditional video coding pipeline will perform rate-distortion (RD) optimization to choose coding parameters. However, in the UGC video coding case, since the input is not pristine, quality ``saturation'' (or even degradation) can be observed, i.e., increased bitrate only leads to improved representation of coding artifacts and noise present in the UGC input. In this paper, we study the saturation problem in UGC compression, where the goal is to identify and avoid during encoding, the coding parameters and rates that lead to quality saturation. We proposed a geometric criterion for saturation detection that works with rate-distortion optimization, and only requires a few frames from the UGC video. In addition, we show how to combine the proposed saturation detection method with existing video coding systems that implement rate-distortion optimization for efficient compression of UGC videos.",2023-03-11T00:30:08Z,2023-03-11T00:30:08Z,http://arxiv.org/abs/2303.06254v1,http://arxiv.org/pdf/2303.06254v1,eess.IV
Duration-adaptive Video Highlight Pre-caching for Vehicular   Communication Network,"Liang Xu, Deshi Li, Kaitao Meng, Mingliu Liu, Shuya Zhu","Video traffic in vehicular communication networks (VCNs) faces exponential growth. However, different segments of most videos reveal various attractiveness for viewers, and the pre-caching decision is greatly affected by the dynamic service duration that edge nodes can provide services for mobile vehicles driving along a road. In this paper, we propose an efficient video highlight pre-caching scheme in the vehicular communication network, adapting to the service duration. Specifically, a highlight entropy model is devised with the consideration of the segments' popularity and continuity between segments within a period of time, based on which, an optimization problem of video highlight pre-caching is formulated. As this problem is non-convex and lacks a closed-form expression of the objective function, we decouple multiple variables by deriving candidate highlight segmentations of videos through wavelet transform, which can significantly reduce the complexity of highlight pre-caching. Then the problem is solved iteratively by a highlight-direction trimming algorithm, which is proven to be locally optimal. Simulation results based on real-world video datasets demonstrate significant improvement in highlight entropy and jitter compared to benchmark schemes.",2023-09-05T04:35:55Z,2023-09-05T04:35:55Z,http://arxiv.org/abs/2309.01944v1,http://arxiv.org/pdf/2309.01944v1,eess.IV
PIM: Video Coding using Perceptual Importance Maps,"Evgenya Pergament, Pulkit Tandon, Oren Rippel, Lubomir Bourdev, Alexander G. Anderson, Bruno Olshausen, Tsachy Weissman, Sachin Katti, Kedar Tatwawadi","Human perception is at the core of lossy video compression, with numerous approaches developed for perceptual quality assessment and improvement over the past two decades. In the determination of perceptual quality, different spatio-temporal regions of the video differ in their relative importance to the human viewer. However, since it is challenging to infer or even collect such fine-grained information, it is often not used during compression beyond low-level heuristics. We present a framework which facilitates research into fine-grained subjective importance in compressed videos, which we then utilize to improve the rate-distortion performance of an existing video codec (x264). The contributions of this work are threefold: (1) we introduce a web-tool which allows scalable collection of fine-grained perceptual importance, by having users interactively paint spatio-temporal maps over encoded videos; (2) we use this tool to collect a dataset with 178 videos with a total of 14443 frames of human annotated spatio-temporal importance maps over the videos; and (3) we use our curated dataset to train a lightweight machine learning model which can predict these spatio-temporal importance regions. We demonstrate via a subjective study that encoding the videos in our dataset while taking into account the importance maps leads to higher perceptual quality at the same bitrate, with the videos encoded with importance maps preferred $1.8 \times$ over the baseline videos. Similarly, we show that for the 18 videos in test set, the importance maps predicted by our model lead to higher perceptual quality videos, $2 \times$ preferred over the baseline at the same bitrate.",2022-12-20T22:22:35Z,2023-04-09T04:57:09Z,http://arxiv.org/abs/2212.10674v2,http://arxiv.org/pdf/2212.10674v2,eess.IV
Hierarchical Reinforcement Learning Based Video Semantic Coding for   Segmentation,"Guangqi Xie, Xin Li, Shiqi Lin, Li Zhang, Kai Zhang, Yue Li, Zhibo Chen","The rapid development of intelligent tasks, e.g., segmentation, detection, classification, etc, has brought an urgent need for semantic compression, which aims to reduce the compression cost while maintaining the original semantic information. However, it is impractical to directly integrate the semantic metric into the traditional codecs since they cannot be optimized in an end-to-end manner. To solve this problem, some pioneering works have applied reinforcement learning to implement image-wise semantic compression. Nevertheless, video semantic compression has not been explored since its complex reference architectures and compression modes. In this paper, we take a step forward to video semantic compression and propose the Hierarchical Reinforcement Learning based task-driven Video Semantic Coding, named as HRLVSC. Specifically, to simplify the complex mode decision of video semantic coding, we divided the action space into frame-level and CTU-level spaces in a hierarchical manner, and then explore the best mode selection for them progressively with the cooperation of frame-level and CTU-level agents. Moreover, since the modes of video semantic coding will exponentially increase with the number of frames in a Group of Pictures (GOP), we carefully investigate the effects of different mode selections for video semantic coding and design a simple but effective mode simplification strategy for it. We have validated our HRLVSC on the video segmentation task with HEVC reference software HM16.19. Extensive experimental results demonstrated that our HRLVSC can achieve over 39% BD-rate saving for video semantic coding under the Low Delay P configuration.",2022-08-24T13:22:22Z,2022-08-24T13:22:22Z,http://arxiv.org/abs/2208.11529v1,http://arxiv.org/pdf/2208.11529v1,eess.IV
SDRTV-to-HDRTV Conversion via Spatial-Temporal Feature Fusion,"Kepeng Xu, Li Xu, Gang He, Chang Wu, Zijia Ma, Ming Sun, Yu-Wing Tai","HDR(High Dynamic Range) video can reproduce realistic scenes more realistically, with a wider gamut and broader brightness range. HDR video resources are still scarce, and most videos are still stored in SDR (Standard Dynamic Range) format. Therefore, SDRTV-to-HDRTV Conversion (SDR video to HDR video) can significantly enhance the user's video viewing experience. Since the correlation between adjacent video frames is very high, the method utilizing the information of multiple frames can improve the quality of the converted HDRTV. Therefore, we propose a multi-frame fusion neural network \textbf{DSLNet} for SDRTV to HDRTV conversion. We first propose a dynamic spatial-temporal feature alignment module \textbf{DMFA}, which can align and fuse multi-frame. Then a novel spatial-temporal feature modulation module \textbf{STFM}, STFM extracts spatial-temporal information of adjacent frames for more accurate feature modulation. Finally, we design a quality enhancement module \textbf{LKQE} with large kernels, which can enhance the quality of generated HDR videos. To evaluate the performance of the proposed method, we construct a corresponding multi-frame dataset using HDR video of the HDR10 standard to conduct a comprehensive evaluation of different methods. The experimental results show that our method obtains state-of-the-art performance. The dataset and code will be released.",2022-11-04T07:40:01Z,2022-11-04T07:40:01Z,http://arxiv.org/abs/2211.02297v1,http://arxiv.org/pdf/2211.02297v1,eess.IV
Semi-supervised Learning of Perceptual Video Quality by Generating   Consistent Pairwise Pseudo-Ranks,"Shankhanil Mitra, Saiyam Jogani, Rajiv Soundararajan","Designing learning-based no-reference (NR) video quality assessment (VQA) algorithms for camera-captured videos is cumbersome due to the requirement of a large number of human annotations of quality. In this work, we propose a semi-supervised learning (SSL) framework exploiting many unlabelled and very limited amounts of labelled authentically distorted videos. Our main contributions are two-fold. Leveraging the benefits of consistency regularization and pseudo-labelling, our SSL model generates pairwise pseudo-ranks for the unlabelled videos using a student-teacher model on strongweak augmented videos. We design the strong-weak augmentations to be quality invariant to use the unlabelled videos effectively in SSL. The generated pseudo-ranks are used along with the limited labels to train our SSL model. Our primary focus in SSL for NR VQA is to learn the mapping from video feature representations to the quality scores. We compare various feature extraction methods and show that our SSL framework can lead to improved performance on these features. In addition to the existing features, we present a spatial and temporal feature extraction method based on predicting spatial and temporal entropic differences. We show that these features help achieve a robust performance when trained with limited data providing a better baseline to apply SSL. Extensive experiments on three popular VQA datasets demonstrate that a combination of our novel SSL approach and features achieves an impressive performance in terms of correlation with human perception, even though the number of human-annotated videos may be limited.",2022-11-30T15:34:27Z,2022-11-30T15:34:27Z,http://arxiv.org/abs/2211.17075v1,http://arxiv.org/pdf/2211.17075v1,eess.IV
Cuboid-Net: A Multi-Branch Convolutional Neural Network for Joint   Space-Time Video Super Resolution,"Congrui Fu, Hui Yuan, Hongji Xu, Hao Zhang, Liquan Shen","The demand for high-resolution videos has been consistently rising across various domains, propelled by continuous advancements in science, technology, and societal. Nonetheless, challenges arising from limitations in imaging equipment capabilities, imaging conditions, as well as economic and temporal factors often result in obtaining low-resolution images in particular situations. Space-time video super-resolution aims to enhance the spatial and temporal resolutions of low-resolution and low-frame-rate videos. The currently available space-time video super-resolution methods often fail to fully exploit the abundant information existing within the spatio-temporal domain. To address this problem, we tackle the issue by conceptualizing the input low-resolution video as a cuboid structure. Drawing on this perspective, we introduce an innovative methodology called ""Cuboid-Net,"" which incorporates a multi-branch convolutional neural network. Cuboid-Net is designed to collectively enhance the spatial and temporal resolutions of videos, enabling the extraction of rich and meaningful information across both spatial and temporal dimensions. Specifically, we take the input video as a cuboid to generate different directional slices as input for different branches of the network. The proposed network contains four modules, i.e., a multi-branch-based hybrid feature extraction (MBFE) module, a multi-branch-based reconstruction (MBR) module, a first stage quality enhancement (QE) module, and a second stage cross frame quality enhancement (CFQE) module for interpolated frames only. Experimental results demonstrate that the proposed method is not only effective for spatial and temporal super-resolution of video but also for spatial and angular super-resolution of light field.",2024-07-24T04:05:20Z,2024-07-24T04:05:20Z,http://arxiv.org/abs/2407.16986v1,http://arxiv.org/pdf/2407.16986v1,eess.IV
360-Degree Video Super Resolution and Quality Enhancement Challenge:   Methods and Results,"Ahmed Telili, Wassim Hamidouche, Ibrahim Farhat, Hadi Amirpour, Christian Timmerer, Ibrahim Khadraoui, Jiajie Lu, The Van Le, Jeonneung Baek, Jin Young Lee, Yiying Wei, Xiaopeng Sun, Yu Gao, JianCheng Huangl, Yujie Zhong","Omnidirectional (360-degree) video is rapidly gaining popularity due to advancements in immersive technologies like virtual reality (VR) and extended reality (XR). However, real-time streaming of such videos, especially in live mobile scenarios like unmanned aerial vehicles (UAVs), is challenged by limited bandwidth and strict latency constraints. Traditional methods, such as compression and adaptive resolution, help but often compromise video quality and introduce artifacts that degrade the viewer experience. Additionally, the unique spherical geometry of 360-degree video presents challenges not encountered in traditional 2D video. To address these issues, we initiated the 360-degree Video Super Resolution and Quality Enhancement Challenge. This competition encourages participants to develop efficient machine learning solutions to enhance the quality of low-bitrate compressed 360-degree videos, with two tracks focusing on 2x and 4x super-resolution (SR). In this paper, we outline the challenge framework, detailing the two competition tracks and highlighting the SR solutions proposed by the top-performing models. We assess these models within a unified framework, considering quality enhancement, bitrate gain, and computational efficiency. This challenge aims to drive innovation in real-time 360-degree video streaming, improving the quality and accessibility of immersive visual experiences.",2024-11-11T06:16:03Z,2024-11-11T06:16:03Z,http://arxiv.org/abs/2411.06738v1,http://arxiv.org/pdf/2411.06738v1,eess.IV
Synthesizing Light Field Video from Monocular Video,"Shrisudhan Govindarajan, Prasan Shedligeri, Sarah, Kaushik Mitra","The hardware challenges associated with light-field(LF) imaging has made it difficult for consumers to access its benefits like applications in post-capture focus and aperture control. Learning-based techniques which solve the ill-posed problem of LF reconstruction from sparse (1, 2 or 4) views have significantly reduced the requirement for complex hardware. LF video reconstruction from sparse views poses a special challenge as acquiring ground-truth for training these models is hard. Hence, we propose a self-supervised learning-based algorithm for LF video reconstruction from monocular videos. We use self-supervised geometric, photometric and temporal consistency constraints inspired from a recent self-supervised technique for LF video reconstruction from stereo video. Additionally, we propose three key techniques that are relevant to our monocular video input. We propose an explicit disocclusion handling technique that encourages the network to inpaint disoccluded regions in a LF frame, using information from adjacent input temporal frames. This is crucial for a self-supervised technique as a single input frame does not contain any information about the disoccluded regions. We also propose an adaptive low-rank representation that provides a significant boost in performance by tailoring the representation to each input scene. Finally, we also propose a novel refinement block that is able to exploit the available LF image data using supervised learning to further refine the reconstruction quality. Our qualitative and quantitative analysis demonstrates the significance of each of the proposed building blocks and also the superior results compared to previous state-of-the-art monocular LF reconstruction techniques. We further validate our algorithm by reconstructing LF videos from monocular videos acquired using a commercial GoPro camera.",2022-07-21T08:23:27Z,2022-07-21T08:23:27Z,http://arxiv.org/abs/2207.10357v1,http://arxiv.org/pdf/2207.10357v1,eess.IV
Saliency Inspired Quality Assessment of Stereoscopic 3D Video,"Amin Banitalebi-Dehkordi, Panos Nasiopoulos","To study the visual attentional behavior of Human Visual System (HVS) on 3D content, eye tracking experiments are performed and Visual Attention Models (VAMs) are designed. One of the main applications of these VAMs is in quality assessment of 3D video. The usage of 2D VAMs in designing 2D quality metrics is already well explored. This paper investigates the added value of incorporating 3D VAMs into Full-Reference (FR) and No-Reference (NR) quality assessment metrics for stereoscopic 3D video. To this end, state-of-the-art 3D VAMs are integrated to quality assessment pipeline of various existing FR and NR stereoscopic video quality metrics. Performance evaluations using a large scale database of stereoscopic videos with various types of distortions demonstrated that using saliency maps generally improves the performance of the quality assessment task for stereoscopic video. However, depending on the type of distortion, utilized metric, and VAM, the amount of improvement will change.",2018-03-12T02:46:51Z,2018-03-12T02:46:51Z,http://arxiv.org/abs/1803.04096v1,http://arxiv.org/pdf/1803.04096v1,eess.IV
Motion estimation for fisheye video sequences combining perspective   projection with camera calibration information,"Andrea Eichenseer, Michel Bätz, André Kaup","Fisheye cameras prove a convenient means in surveillance and automotive applications as they provide a very wide field of view for capturing their surroundings. Contrary to typical rectilinear imagery, however, fisheye video sequences follow a different mapping from the world coordinates to the image plane which is not considered in standard video processing techniques. In this paper, we present a motion estimation method for real-world fisheye videos by combining perspective projection with knowledge about the underlying fisheye projection. The latter is obtained by camera calibration since actual lenses rarely follow exact models. Furthermore, we introduce a re-mapping for ultra-wide angles which would otherwise lead to wrong motion compensation results for the fisheye boundary. Both concepts extend an existing hybrid motion estimation method for equisolid fisheye video sequences that decides between traditional and fisheye block matching in a block-based manner. Compared to that method, the proposed calibration and re-mapping extensions yield gains of up to 0.58 dB in luminance PSNR for real-world fisheye video sequences. Overall gains amount to up to 3.32 dB compared to traditional block matching.",2022-12-02T13:39:32Z,2022-12-02T13:39:32Z,http://arxiv.org/abs/2212.01164v1,http://arxiv.org/pdf/2212.01164v1,eess.IV
Synthetic Hyperspectral Array Video Database with Applications to   Cross-Spectral Reconstruction and Hyperspectral Video Coding,"Frank Sippel, Jürgen Seiler, André Kaup","In this paper, a synthetic hyperspectral video database is introduced. Since it is impossible to record ground truth hyperspectral videos, this database offers the possibility to leverage the evaluation of algorithms in diverse applications. For all scenes, depth maps are provided as well to yield the position of a pixel in all spatial dimensions as well as the reflectance in spectral dimension. Two novel algorithms for two different applications are proposed to prove the diversity of applications that can be addressed by this novel database. First, a cross-spectral image reconstruction algorithm is extended to exploit the temporal correlation between two consecutive frames. The evaluation using this hyperspectral database shows an increase in PSNR of up to 5.6 dB dependent on the scene. Second, a hyperspectral video coder is introduced which extends an existing hyperspectral image coder by exploiting temporal correlation. The evaluation shows rate savings of up to 10% depending on the scene. The novel hyperspectral video database and source code is available at https:// github.com/ FAU-LMS/ HyViD for use by the research community.",2023-01-18T14:11:59Z,2023-02-20T07:53:11Z,http://arxiv.org/abs/2301.07551v3,http://arxiv.org/pdf/2301.07551v3,eess.IV
Multiple description video coding for real-time applications using HEVC,"Trung Hieu Le, Marc Antonini, Marc Lambert, Karima Alioua","Remote control vehicles require the transmission of large amounts of data, and video is one of the most important sources for the driver. To ensure reliable video transmission, the encoded video stream is transmitted simultaneously over multiple channels. However, this solution incurs a high transmission cost due to the wireless channel's unreliable and random bit loss characteristics. To address this issue, it is necessary to use more efficient video encoding methods that can make the video stream robust to noise. In this paper, we propose a low-complexity, low-latency 2-channel Multiple Description Coding (MDC) solution with an adaptive Instantaneous Decoder Refresh (IDR) frame period, which is compatible with the HEVC standard. This method shows better resistance to high packet loss rates with lower complexity.",2023-03-10T10:31:19Z,2023-08-07T08:49:43Z,http://arxiv.org/abs/2303.05843v2,http://arxiv.org/pdf/2303.05843v2,eess.IV
Motion Plane Adaptive Motion Modeling for Spherical Video Coding in   H.266/VVC,"Andy Regensky, Christian Herglotz, André Kaup","Motion compensation is one of the key technologies enabling the high compression efficiency of modern video coding standards. To allow compression of spherical video content, special mapping functions are required to project the video to the 2D image plane. Distortions inevitably occurring in these mappings impair the performance of classical motion models. In this paper, we propose a novel motion plane adaptive motion modeling technique (MPA) for spherical video that allows to perform motion compensation on different motion planes in 3D space instead of having to work on the - in theory arbitrarily mapped - 2D image representation directly. The integration of MPA into the state-of-the-art H.266/VVC video coding standard shows average Bj{\o}ntegaard Delta rate savings of 1.72\% with a peak of 3.37\% based on PSNR and 1.55\% with a peak of 2.92\% based on WS-PSNR compared to VTM-14.2.",2023-06-23T11:59:39Z,2023-06-23T11:59:39Z,http://arxiv.org/abs/2306.13694v1,http://arxiv.org/pdf/2306.13694v1,eess.IV
Extended Signaling Methods for Reduced Video Decoder Power Consumption   Using Green Metadata,"Christian Herglotz, Matthias Kränzler, Xixue Chu, Edouard Francois, Yong He, André Kaup","In this paper, we discuss one aspect of the latest MPEG standard edition on energy-efficient media consumption, also known as Green Metadata (ISO/IEC 232001-11), which is the interactive signaling for remote decoder-power reduction for peer-to-peer video conferencing. In this scenario, the receiver of a video, e.g., a battery-driven portable device, can send a dedicated request to the sender which asks for a video bitstream representation that is less complex to decode and process. Consequently, the receiver saves energy and extends operating times. We provide an overview on latest studies from the literature dealing with energy-saving aspects, which motivate the extension of the legacy Green Metadata standard. Furthermore, we explain the newly introduced syntax elements and verify their effectiveness by performing dedicated experiments. We show that the integration of these syntax elements can lead to dynamic energy savings of up to 90% for software video decoding and 80% for hardware video decoding, respectively.",2023-10-26T12:26:13Z,2023-10-26T12:26:13Z,http://arxiv.org/abs/2310.17346v1,http://arxiv.org/pdf/2310.17346v1,eess.IV
Encoder-Quantization-Motion-based Video Quality Metrics,"Yixu Chen, Zaixi Shang, Hai Wei, Yongjun Wu, Sriram Sethuraman","In an adaptive bitrate streaming application, the efficiency of video compression and the encoded video quality depend on both the video codec and the quality metric used to perform encoding optimization. The development of such a quality metric need large scale subjective datasets. In this work we merge several datasets into one to support the creation of a metric tailored for video compression and scaling. We proposed a set of HEVC lightweight features to boost performance of the metrics. Our metrics can be computed from tightly coupled encoding process with 4% compute overhead or from the decoding process in real-time. The proposed method can achieve better correlation than VMAF and P.1204.3. It can extrapolate to different dynamic ranges, and is suitable for real-time video quality metrics delivery in the bitstream. The performance is verified by in-distribution and cross-dataset tests. This work paves the way for adaptive client-side heuristics, real-time segment optimization, dynamic bitrate capping, and quality-dependent post-processing neural network switching, etc.",2024-04-09T21:10:17Z,2024-04-09T21:10:17Z,http://arxiv.org/abs/2404.06620v1,http://arxiv.org/pdf/2404.06620v1,eess.IV
Selective Encryption of VVC Encoded Video Streams for the Internet of   Video Things,"Amir Fotovvat, Khan A. Wahid","Visual sensors serve as a critical component of the Internet of Things (IoT). There is an ever-increasing demand for broad applications and higher resolutions of videos and cameras in smart homes and smart cities, such as in security cameras. To utilize this large volume of video data generated from networks of visual sensors for various machine vision applications, it needs to be compressed and securely transmitted over the Internet. H.266/VVC, as the new compression standard, brings the highest compression for visual data. To provide security along with high compression, a selective encryption method for hiding information of videos is presented for this new compression standard. Selective encryption methods can lower the computation overhead of the encryption while keeping the video bitstream format which is useful when the video goes into untrusted blocks such as transcoding or watermarking. Syntax elements that represent considerable information are selected for the encryption, i.e., luma Intra Prediction Modes (IPMs), Motion Vector Difference (MVD), and residual signs., then the results of the proposed method are investigated in terms of visual security and bit rate change. Our experiments show that the encrypted videos provide higher visual security compared to other similar works in previous standards, and integration of the presented encryption scheme into the VVC encoder has little impact on the bit rate efficiency (results in 2% to 3% bit rate increase).",2021-03-27T08:40:39Z,2021-03-27T08:40:39Z,http://arxiv.org/abs/2103.14844v1,http://arxiv.org/pdf/2103.14844v1,eess.IV
"Space-Time Video Regularity and Visual Fidelity: Compression, Resolution   and Frame Rate Adaptation","Dae Yeol Lee, Hyunsuk Ko, Jongho Kim, Alan C. Bovik","In order to be able to deliver today's voluminous amount of video contents through limited bandwidth channels in a perceptually optimal way, it is important to consider perceptual trade-offs of compression and space-time downsampling protocols. In this direction, we have studied and developed new models of natural video statistics (NVS), which are useful because high-quality videos contain statistical regularities that are disturbed by distortions. Specifically, we model the statistics of divisively normalized difference between neighboring frames that are relatively displaced. In an extensive empirical study, we found that those paths of space-time displaced frame differences that provide maximal regularity against our NVS model generally align best with motion trajectories. Motivated by this, we build a new video quality prediction engine that extracts NVS features from displaced frame differences, and combines them in a learned regressor that can accurately predict perceptual quality. As a stringent test of the new model, we apply it to the difficult problem of predicting the quality of videos subjected not only to compression, but also to downsampling in space and/or time. We show that the new quality model achieves state-of-the-art (SOTA) prediction performance compared on the new ETRI-LIVE Space-Time Subsampled Video Quality (STSVQ) database, which is dedicated to this problem. Downsampling protocols are of high interest to the streaming video industry, given rapid increases in frame resolutions and frame rates.",2021-03-31T02:29:01Z,2021-03-31T02:29:01Z,http://arxiv.org/abs/2103.16771v1,http://arxiv.org/pdf/2103.16771v1,eess.IV
A Dual Sensor Computational Camera for High Quality Dark Videography,"Yuxiao Cheng, Runzhao Yang, Zhihong Zhang, Jinli Suo, Qionghai Dai","Videos captured under low light conditions suffer from severe noise. A variety of efforts have been devoted to image/video noise suppression and made large progress. However, in extremely dark scenarios, extensive photon starvation would hamper precise noise modeling. Instead, developing an imaging system collecting more photons is a more effective way for high-quality video capture under low illuminations. In this paper, we propose to build a dual-sensor camera to additionally collect the photons in NIR wavelength, and make use of the correlation between RGB and near-infrared (NIR) spectrum to perform high-quality reconstruction from noisy dark video pairs. In hardware, we build a compact dual-sensor camera capturing RGB and NIR videos simultaneously. Computationally, we propose a dual-channel multi-frame attention network (DCMAN) utilizing spatial-temporal-spectral priors to reconstruct the low-light RGB and NIR videos. In addition, we build a high-quality paired RGB and NIR video dataset, based on which the approach can be applied to different sensors easily by training the DCMAN model with simulated noisy input following a physical-process-based CMOS noise model. Both experiments on synthetic and real videos validate the performance of this compact dual-sensor camera design and the corresponding reconstruction algorithm in dark videography.",2022-04-11T10:04:32Z,2022-04-11T10:04:32Z,http://arxiv.org/abs/2204.04987v1,http://arxiv.org/pdf/2204.04987v1,eess.IV
No-Reference Video Quality Assessment Using Space-Time Chips,"Joshua P. Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, Alan C. Bovik","We propose a new prototype model for no-reference video quality assessment (VQA) based on the natural statistics of space-time chips of videos. Space-time chips (ST-chips) are a new, quality-aware feature space which we define as space-time localized cuts of video data in directions that are determined by the local motion flow. We use parametrized distribution fits to the bandpass histograms of space-time chips to characterize quality, and show that the parameters from these models are affected by distortion and can hence be used to objectively predict the quality of videos. Our prototype method, which we call ChipQA-0, is agnostic to the types of distortion affecting the video, and is based on identifying and quantifying deviations from the expected statistics of natural, undistorted ST-chips in order to predict video quality. We train and test our resulting model on several large VQA databases and show that our model achieves high correlation against human judgments of video quality and is competitive with state-of-the-art models.",2020-07-31T18:40:57Z,2020-08-23T20:54:13Z,http://arxiv.org/abs/2008.00031v3,http://arxiv.org/pdf/2008.00031v3,eess.IV
Reducing latency and bandwidth for video streaming using keypoint   extraction and digital puppetry,"Roshan Prabhakar, Shubham Chandak, Carina Chiu, Renee Liang, Huong Nguyen, Kedar Tatwawadi, Tsachy Weissman","COVID-19 has made video communication one of the most important modes of information exchange. While extensive research has been conducted on the optimization of the video streaming pipeline, in particular the development of novel video codecs, further improvement in the video quality and latency is required, especially under poor network conditions. This paper proposes an alternative to the conventional codec through the implementation of a keypoint-centric encoder relying on the transmission of keypoint information from within a video feed. The decoder uses the streamed keypoints to generate a reconstruction preserving the semantic features in the input feed. Focusing on video calling applications, we detect and transmit the body pose and face mesh information through the network, which are displayed at the receiver in the form of animated puppets. Using efficient pose and face mesh detection in conjunction with skeleton-based animation, we demonstrate a prototype requiring lower than 35 kbps bandwidth, an order of magnitude reduction over typical video calling systems. The added computational latency due to the mesh extraction and animation is below 120ms on a standard laptop, showcasing the potential of this framework for real-time applications. The code for this work is available at https://github.com/shubhamchandak94/digital-puppetry/.",2020-11-07T16:12:23Z,2021-01-08T11:10:58Z,http://arxiv.org/abs/2011.03800v2,http://arxiv.org/pdf/2011.03800v2,eess.IV
Advances In Video Compression System Using Deep Neural Network: A Review   And Case Studies,"Dandan Ding, Zhan Ma, Di Chen, Qingshuang Chen, Zoe Liu, Fengqing Zhu","Significant advances in video compression system have been made in the past several decades to satisfy the nearly exponential growth of Internet-scale video traffic. From the application perspective, we have identified three major functional blocks including pre-processing, coding, and post-processing, that have been continuously investigated to maximize the end-user quality of experience (QoE) under a limited bit rate budget. Recently, artificial intelligence (AI) powered techniques have shown great potential to further increase the efficiency of the aforementioned functional blocks, both individually and jointly. In this article, we review extensively recent technical advances in video compression system, with an emphasis on deep neural network (DNN)-based approaches; and then present three comprehensive case studies. On pre-processing, we show a switchable texture-based video coding example that leverages DNN-based scene understanding to extract semantic areas for the improvement of subsequent video coder. On coding, we present an end-to-end neural video coding framework that takes advantage of the stacked DNNs to efficiently and compactly code input raw videos via fully data-driven learning. On post-processing, we demonstrate two neural adaptive filters to respectively facilitate the in-loop and post filtering for the enhancement of compressed frames. Finally, a companion website hosting the contents developed in this work can be accessed publicly at https://purdueviper.github.io/dnn-coding/.",2021-01-16T01:25:04Z,2021-01-16T01:25:04Z,http://arxiv.org/abs/2101.06341v1,http://arxiv.org/pdf/2101.06341v1,eess.IV
Perceptual Quality Assessment of HEVC and VVC Standards for 8K Video,"Charles Bonnineau, Wassim Hamidouche, Jerome Fournier, Naty Sidaty, Jean-Francois Travers, Olivier Deforges","With the growing data consumption of emerging video applications and users requirement for higher resolutions, up to 8K, a huge effort has been made in video compression technologies. Recently, versatile video coding (VVC) has been standardized by the moving picture expert group (MPEG), providing a significant improvement in compression performance over its predecessor high efficiency video coding (HEVC). In this paper, we provide a comparative subjective quality evaluation between VVC and HEVC standards for 8K resolution videos. In addition, we evaluate the perceived quality improvement offered by 8K over UHD 4K resolution. The compression performance of both VVC and HEVC standards has been conducted in random access (RA) coding configuration, using their respective reference software, VVC test model (VTM-11) and HEVC test model (HM-16.20). Objective measurements, using PSNR, MS-SSIM and VMAF metrics have shown that the bitrate gains offered by VVC over HEVC for 8K video content are around 31%, 26% and 35%, respectively. Subjectively, VVC offers an average of 40% of bitrate reduction over HEVC for the same visual quality. A compression gain of 50% has been reached for some tested video sequences regarding a Student t-test analysis. In addition, for most tested scenes, a significant visual difference between uncompressed 4K and 8K has been noticed.",2021-09-14T09:53:58Z,2021-12-20T15:32:31Z,http://arxiv.org/abs/2109.06555v3,http://arxiv.org/pdf/2109.06555v3,eess.IV
Video Quality Assessment and Coding Complexity of the Versatile Video   Coding Standard,"Thomas Amestoy, Naty Sidaty, Wassim Hamidouche, Pierrick Philippe, Daniel Menard","In recent years, the proliferation of multimedia applications and formats, such as IPTV, Virtual Reality (VR, 360-degree), and point cloud videos, has presented new challenges to the video compression research community. Simultaneously, there has been a growing demand from users for higher resolutions and improved visual quality. To further enhance coding efficiency, a new video coding standard, Versatile Video Coding (VVC), was introduced in July 2020. This paper conducts a comprehensive analysis of coding performance and complexity for the latest VVC standard in comparison to its predecessor, High Efficiency Video Coding (HEVC). The study employs a diverse set of test sequences, covering both High Definition (HD) and Ultra High Definition (UHD) resolutions, and spans a wide range of bit-rates. These sequences are encoded using the reference software encoders of HEVC (HM) and VVC (VTM). The results consistently demonstrate that VVC outperforms HEVC, achieving bit-rate savings of up to 40% on the subjective quality scale, particularly at realistic bit-rates and quality levels. Objective quality metrics, including PSNR, SSIM, and VMAF, support these findings, revealing bit-rate savings ranging from 31% to 40%, depending on the video content, spatial resolution, and the selected quality metric. However, these improvements in coding efficiency come at the cost of significantly increased computational complexity. On average, our results indicate that the VVC decoding process is 1.5 times more complex, while the encoding process becomes at least eight times more complex than that of the HEVC reference encoder. Our simultaneous profiling of the two standards sheds light on the primary evolutionary differences between them and highlights the specific stages responsible for the observed increase in complexity.",2023-10-19T18:48:02Z,2023-10-19T18:48:02Z,http://arxiv.org/abs/2310.13093v1,http://arxiv.org/pdf/2310.13093v1,eess.IV
Analysis of Neural Video Compression Networks for 360-Degree Video   Coding,"Andy Regensky, Fabian Brand, André Kaup","With the increasing efforts of bringing high-quality virtual reality technologies into the market, efficient 360-degree video compression gains in importance. As such, the state-of-the-art H.266/VVC video coding standard integrates dedicated tools for 360-degree video, and considerable efforts have been put into designing 360-degree projection formats with improved compression efficiency. For the fast-evolving field of neural video compression networks (NVCs), the effects of different 360-degree projection formats on the overall compression performance have not yet been investigated. It is thus unclear, whether a resampling from the conventional equirectangular projection (ERP) to other projection formats yields similar gains for NVCs as for hybrid video codecs, and which formats perform best. In this paper, we analyze several generations of NVCs and an extensive set of 360-degree projection formats with respect to their compression performance for 360-degree video. Based on our analysis, we find that projection format resampling yields significant improvements in compression performance also for NVCs. The adjusted cubemap projection (ACP) and equatorial cylindrical projection (ECP) show to perform best and achieve rate savings of more than 55% compared to ERP based on WS-PSNR for the most recent NVC. Remarkably, the observed rate savings are higher than for H.266/VVC, emphasizing the importance of projection format resampling for NVCs.",2024-02-15T17:15:54Z,2024-02-15T17:15:54Z,http://arxiv.org/abs/2402.10257v1,http://arxiv.org/pdf/2402.10257v1,eess.IV
Parameter-Efficient Instance-Adaptive Neural Video Compression,"Hyunmo Yang, Seungjun Oh, Eunbyung Park","Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines. However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data. The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video. However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training. In this work, we propose a parameter-efficient instance-adaptive video compression framework. Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences. The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms. Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings. We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR) and BD rates compared to the baselines NVC. Our code is available on https://github.com/ohsngjun/PEVC.",2024-05-14T12:14:58Z,2024-11-28T08:06:54Z,http://arxiv.org/abs/2405.08530v3,http://arxiv.org/pdf/2405.08530v3,eess.IV
2BiVQA: Double Bi-LSTM based Video Quality Assessment of UGC Videos,"Ahmed Telili, Sid Ahmed Fezza, Wassim Hamidouche, Hanene F. Z. Brachemi Meftah","Recently, with the growing popularity of mobile devices as well as video sharing platforms (e.g., YouTube, Facebook, TikTok, and Twitch), User-Generated Content (UGC) videos have become increasingly common and now account for a large portion of multimedia traffic on the internet. Unlike professionally generated videos produced by filmmakers and videographers, typically, UGC videos contain multiple authentic distortions, generally introduced during capture and processing by naive users. Quality prediction of UGC videos is of paramount importance to optimize and monitor their processing in hosting platforms, such as their coding, transcoding, and streaming. However, blind quality prediction of UGC is quite challenging because the degradations of UGC videos are unknown and very diverse, in addition to the unavailability of pristine reference. Therefore, in this paper, we propose an accurate and efficient Blind Video Quality Assessment (BVQA) model for UGC videos, which we name 2BiVQA for double Bi-LSTM Video Quality Assessment. 2BiVQA metric consists of three main blocks, including a pre-trained Convolutional Neural Network (CNN) to extract discriminative features from image patches, which are then fed into two Recurrent Neural Networks (RNNs) for spatial and temporal pooling. Specifically, we use two Bi-directional Long Short Term Memory (Bi-LSTM) networks, the first is used to capture short-range dependencies between image patches, while the second allows capturing longrange dependencies between frames to account for the temporal memory effect. Experimental results on recent large-scale UGC VQA datasets show that 2BiVQA achieves high performance at lower computational cost than most state-of-the-art VQA models. The source code of our 2BiVQA metric is made publicly available at: https://github.com/atelili/2BiVQA",2022-08-31T11:20:20Z,2023-10-27T19:54:45Z,http://arxiv.org/abs/2208.14774v3,http://arxiv.org/pdf/2208.14774v3,eess.IV
A Human Visual System-Based 3D Video Quality Metric,"Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos","Although several 2D quality metrics have been proposed for images and videos, in the case of 3D efforts are only at the initial stages. In this paper, we propose a new full-reference quality metric for 3D content. Our method is modeled around the HVS, fusing the information of both left and right channels, considering color components, the cyclopean views of the two videos and disparity. Performance evaluations showed that our 3D quality metric successfully monitors the degradation of quality caused by several representative types of distortion and it has 86% correlation with the results of subjective evaluations.",2018-03-13T05:15:00Z,2018-03-13T05:15:00Z,http://arxiv.org/abs/1803.04624v1,http://arxiv.org/pdf/1803.04624v1,eess.IV
Quality assessment methods for perceptual video compression,"Fan Zhang, David R. Bull","This paper describes a quality assessment model for perceptual video compression applications (PVM), which stimulates visual masking and distortion-artefact perception using an adaptive combination of noticeable distortions and blurring artefacts. The method shows significant improvement over existing quality metrics based on the VQEG database, and provides compatibility with in-loop rate-quality optimisation for next generation video codecs due to its latency and complexity attributes. Performance comparison are validated against a range of different distortion types.",2021-06-15T13:32:58Z,2021-06-15T13:32:58Z,http://arxiv.org/abs/2106.08124v1,http://arxiv.org/pdf/2106.08124v1,eess.IV
An adaptive Lagrange multiplier determination method for rate-distortion   optimisation in hybrid video codecs,"Fan Zhang, David R. Bull","This paper describes an adaptive Lagrange multiplier determination method for rate-quality optimisation in video compression. Inspired by the experimental results of a Lagrange multiplier selection test, the presented approach adaptively estimates the optimum Lagrange multiplier for different video content, based on distortion statistics of recently encoded frames. The proposed algorithm has been fully integrated into both the H.264 and HEVC reference codecs, and is used in rate-distortion optimisation for encoding B frames. The results show promising (up to 11% on the sequences tested) overall bitrate savings, for a minimal increase in complexity, on various types of test content based on Bjontegaard delta measurements.",2021-06-15T13:41:55Z,2021-06-15T13:41:55Z,http://arxiv.org/abs/2106.08141v1,http://arxiv.org/pdf/2106.08141v1,eess.IV
Person Detection in Collaborative Group Learning Environments Using   Multiple Representations,"Wenjing Shi, Marios S. Pattichis, Sylvia Celedón-Pattichis, Carlos LópezLeiva","We introduce the problem of detecting a group of students from classroom videos. The problem requires the detection of students from different angles and the separation of the group from other groups in long videos (one to one and a half hours). We use multiple image representations to solve the problem. We use FM components to separate each group from background groups, AM-FM components for detecting the back-of-the-head, and YOLO for face detection. We use classroom videos from four different groups to validate our approach. Our use of multiple representations is shown to be significantly more accurate than the use of YOLO alone.",2021-12-22T20:30:44Z,2021-12-22T20:30:44Z,http://arxiv.org/abs/2112.12217v1,http://arxiv.org/pdf/2112.12217v1,eess.IV
Artificial Intelligence based Video Codec (AIVC) for CLIC 2022,"Théo Ladune, Gordon Clare, Pierrick Philippe, Félix Henri","This paper presents the AIVC submission to the CLIC 2022 video track. AIVC is a fully-learned video codec based on conditional autoencoders. The flexibility of the AIVC models is leveraged to implement rate allocation and frame structure competition to select the optimal coding configuration per-sequence. This competition yields compelling compression performance, offering a rate reduction of -26 % compared with the absence of competition.",2022-06-28T12:07:14Z,2022-06-28T12:07:14Z,http://arxiv.org/abs/2206.13934v1,http://arxiv.org/pdf/2206.13934v1,"eess.IV, eess.SP"
Transmission of high-definition video signals underwater using surface   electromagnetic waves,"Igor I. Smolyaninov, Quirino Balzano, Mark Barry","A portable radio communication system operating in the 30 MHz band and capable of transmitting high-definition live underwater video images is presented. The system operation is based on launching electromagnetic surface waves propagating along water-air interface using specially designed surface wave antennas. Since the propagation length of the surface electromagnetic waves far exceeds the skin depth of bulk radio waves at the same frequency, this technique is useful for video communication underwater over distances of several meters. Also, this system appears to be efficient at communicating through the water-air interface.",2022-10-02T18:16:20Z,2022-10-02T18:16:20Z,http://arxiv.org/abs/2210.06296v1,http://arxiv.org/pdf/2210.06296v1,"eess.SP, eess.IV"
Video Decoding Energy Estimation Using Processor Events,"Christian Herglotz, André Kaup","In this paper, we show that processor events like instruction counts or cache misses can be used to accurately estimate the processing energy of software video decoders. Therefore, we perform energy measurements on an ARM-based evaluation platform and count processor level events using a dedicated profiling software. Measurements are performed for various codecs and decoder implementations to prove the general viability of our observations. Using the estimation method proposed in this paper, the true decoding energy for various recent video coding standards including HEVC and VP9 can be estimated with a mean estimation error that is smaller than 6%.",2023-07-26T07:19:23Z,2023-07-26T07:19:23Z,http://arxiv.org/abs/2307.14000v1,http://arxiv.org/pdf/2307.14000v1,eess.IV
Subjective and Objective Quality Assessment of Rendered Human Avatar   Videos in Virtual Reality,"Yu-Chih Chen, Avinab Saha, Alexandre Chapiro, Christian Häne, Jean-Charles Bazin, Bo Qiu, Stefano Zanetti, Ioannis Katsavounidis, Alan C. Bovik","We study the visual quality judgments of human subjects on digital human avatars (sometimes referred to as ""holograms"" in the parlance of virtual reality [VR] and augmented reality [AR] systems) that have been subjected to distortions. We also study the ability of video quality models to predict human judgments. As streaming human avatar videos in VR or AR become increasingly common, the need for more advanced human avatar video compression protocols will be required to address the tradeoffs between faithfully transmitting high-quality visual representations while adjusting to changeable bandwidth scenarios. During transmission over the internet, the perceived quality of compressed human avatar videos can be severely impaired by visual artifacts. To optimize trade-offs between perceptual quality and data volume in practical workflows, video quality assessment (VQA) models are essential tools. However, there are very few VQA algorithms developed specifically to analyze human body avatar videos, due, at least in part, to the dearth of appropriate and comprehensive datasets of adequate size. Towards filling this gap, we introduce the LIVE-Meta Rendered Human Avatar VQA Database, which contains 720 human avatar videos processed using 20 different combinations of encoding parameters, labeled by corresponding human perceptual quality judgments that were collected in six degrees of freedom VR headsets. To demonstrate the usefulness of this new and unique video resource, we use it to study and compare the performances of a variety of state-of-the-art Full Reference and No Reference video quality prediction models, including a new model called HoloQA. As a service to the research community, we publicly releases the metadata of the new database at https://live.ece.utexas.edu/research/LIVE-Meta-rendered-human-avatar/index.html.",2024-08-13T17:11:54Z,2024-10-02T21:23:19Z,http://arxiv.org/abs/2408.07041v2,http://arxiv.org/pdf/2408.07041v2,eess.IV
Exploring the Distributed Video Coding in a Quality Assessment Context,"A. Banitalebi, H. R. Tohidypour","In the popular video coding trend, the encoder has the task to exploit both spatial and temporal redundancies present in the video sequence, which is a complex procedure. As a result almost all video encoders have five to ten times more complexity than their decoders. In a video compression process, one of the main tasks at the encoder side is motion estimation which is to extract the temporal correlation between frames. Distributed video coding (DVC) proposed the idea that can lead to low complexity encoders and higher complexity decoders. DVC is a new paradigm in video compression based on the information theoretic ideas of Slepian-Wolf and Wyner-Ziv theorems. Wyner-Ziv coding is naturally robust against transmission errors and can be used for joint source and channel coding. Side Information is one of the key components of the Wyner-Ziv decoder. Better side information generation will result in better functionality of Wyner-Ziv coder. In this paper we proposed a new method that can generate side information with a better quality and thus better compression. We have used HVS (human visual system) based image quality metrics as our quality criterion. The motion estimation we used in the decoder is modified due to these metrics such that we could obtain finer side information. The motion compensation is optimized for perceptual quality metrics and leads to better side information generation compared to con- ventional MSE (mean squared error) or SAD (sum of absolute difference) based motion compensation currently used in the literature. Better motion compensation means better compression.",2018-03-13T04:32:05Z,2018-03-13T04:32:05Z,http://arxiv.org/abs/1803.04614v1,http://arxiv.org/pdf/1803.04614v1,eess.IV
Lightweight Hardware Transform Design for the Versatile Video Coding 4K   ASIC Decoders,"Ibrahim Farhat, Wassim Hamidouche, Adrien Grill, Daniel Ménard, Olivier Déforges","Versatile Video Coding (VVC) is the next generation video coding standard finalized in July 2020. VVC introduces new coding tools enhancing the coding efficiency compared to its predecessor High Efficiency Video Coding (HEVC). These new tools have a significant impact on the VVC software decoder complexity estimated to 2 times HEVC decoder complexity. In particular, the transform module includes in VVC separable and non-separable transforms named Multiple Transform Selection (MTS) and Low Frequency Non-Separable Transform (LFNST) tools, respectively. In this paper, we present an area-efficient hardware architecture of the inverse transform module for a VVC decoder. The proposed design uses a total of 64 regular multipliers in a pipelined architecture targeting Application-Specific Integrated Circuit (ASIC) platforms. It consists in a multi-standard architecture that supports the transform modules of recent MPEG standards including Advanced Video Coding (AVC), HEVC and VVC. The architecture leverages all primary and secondary transforms optimisations including butterfly de-composition, coefficients zeroing and the inherent linear relationship between the transforms. The synthesized results show that the proposed method sustains a constant throughput of 1sample per cycle and a constant latency for all block sizes. The proposed hardware inverse transform module operates at 600MHz frequency enabling to decode in real-time 4K video at 30 frames per second in 4:2:2 chroma sub-sampling format. The proposed module has been integrated in an ASIC UHD decoder targeting energy-aware decoding of VVC videos on consumer devices.",2021-07-24T17:40:18Z,2021-11-06T09:24:35Z,http://arxiv.org/abs/2107.11659v2,http://arxiv.org/pdf/2107.11659v2,eess.IV
Performance Analysis of Optimized Versatile Video Coding Software   Decoders on Embedded Platforms,"Anup Saha, Wassim Hamidouche, Miguel Chavarrías, Guillaume Gautier, Fernando Pescador, Ibrahim Farhat","In recent years, the global demand for high-resolution videos and the emergence of new multimedia applications have created the need for a new video coding standard. Hence, in July 2020 the Versatile Video Coding (VVC) standard was released providing up to 50% bit-rate saving for the same video quality compared to its predecessor High Efficiency Video Coding (HEVC). However, this bit-rate saving comes at the cost of a high computational complexity, particularly for live applications and on resource-constraint embedded devices. This paper presents two optimized VVC software decoders, named OpenVVC and Versatile Video deCoder (VVdeC), designed for low resources platforms. They exploit optimization techniques such as data level parallelism using Single Instruction Multiple Data (SIMD) instructions and functional level parallelism using frame, tile and slice-based parallelisms. Furthermore, a comparison in terms of decoding run time, energy and memory consumption between the two decoders is presented while targeting two different resource-constraint embedded devices. The results showed that both decoders achieve real-time decoding of Full High definition (FHD) resolution over the first platform using 8 cores and High-definition (HD) real-time decoding for the second platform using only 4 cores with comparable results in terms of average consumed energy: around 26 J and 15 J for the 8 cores and 4 cores embedded platforms, respectively. Regarding the memory usage, OpenVVC showed better results with less average maximum memory consumed during run time compared to VVdeC.",2022-06-30T14:27:48Z,2022-06-30T14:27:48Z,http://arxiv.org/abs/2206.15311v1,http://arxiv.org/pdf/2206.15311v1,eess.IV
Weakly Semi-Supervised Detection in Lung Ultrasound Videos,"Jiahong Ouyang, Li Chen, Gary Y. Li, Naveen Balaraju, Shubham Patil, Courosh Mehanian, Sourabh Kulhare, Rachel Millin, Kenton W. Gregory, Cynthia R. Gregory, Meihua Zhu, David O. Kessler, Laurie Malia, Almaz Dessie, Joni Rabiner, Di Coneybeare, Bo Shopsin, Andrew Hersh, Cristian Madar, Jeffrey Shupp, Laura S. Johnson, Jacob Avila, Kristin Dwyer, Peter Weimersheimer, Balasundar Raju, Jochen Kruecker, Alvin Chen","Frame-by-frame annotation of bounding boxes by clinical experts is often required to train fully supervised object detection models on medical video data. We propose a method for improving object detection in medical videos through weak supervision from video-level labels. More concretely, we aggregate individual detection predictions into video-level predictions and extend a teacher-student training strategy to provide additional supervision via a video-level loss. We also introduce improvements to the underlying teacher-student framework, including methods to improve the quality of pseudo-labels based on weak supervision and adaptive schemes to optimize knowledge transfer between the student and teacher networks. We apply this approach to the clinically important task of detecting lung consolidations (seen in respiratory infections such as COVID-19 pneumonia) in medical ultrasound videos. Experiments reveal that our framework improves detection accuracy and robustness compared to baseline semi-supervised models, and improves efficiency in data and annotation usage.",2023-08-08T02:36:41Z,2023-08-08T02:36:41Z,http://arxiv.org/abs/2308.04463v1,http://arxiv.org/pdf/2308.04463v1,eess.IV
Designs and Implementations in Neural Network-based Video Coding,"Yue Li, Junru Li, Chaoyi Lin, Kai Zhang, Li Zhang, Franck Galpin, Thierry Dumas, Hongtao Wang, Muhammed Coban, Jacob Ström, Du Liu, Kenneth Andersson","The past decade has witnessed the huge success of deep learning in well-known artificial intelligence applications such as face recognition, autonomous driving, and large language model like ChatGPT. Recently, the application of deep learning has been extended to a much wider range, with neural network-based video coding being one of them. Neural network-based video coding can be performed at two different levels: embedding neural network-based (NN-based) coding tools into a classical video compression framework or building the entire compression framework upon neural networks. This paper elaborates some of the recent exploration efforts of JVET (Joint Video Experts Team of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC29) in the name of neural network-based video coding (NNVC), falling in the former category. Specifically, this paper discusses two major NN-based video coding technologies, i.e. neural network-based intra prediction and neural network-based in-loop filtering, which have been investigated for several meeting cycles in JVET and finally adopted into the reference software of NNVC. Extensive experiments on top of the NNVC have been conducted to evaluate the effectiveness of the proposed techniques. Compared with VTM-11.0_nnvc, the proposed NN-based coding tools in NNVC-4.0 could achieve {11.94%, 21.86%, 22.59%}, {9.18%, 19.76%, 20.92%}, and {10.63%, 21.56%, 23.02%} BD-rate reductions on average for {Y, Cb, Cr} under random-access, low-delay, and all-intra configurations respectively.",2023-09-11T22:12:41Z,2023-09-13T18:41:44Z,http://arxiv.org/abs/2309.05846v2,http://arxiv.org/pdf/2309.05846v2,eess.IV
Energy Demand Prediction for Hardware Video Decoders Using Software   Profiling,"Matthias Kränzler, Christian Herglotz, André Kaup","Energy efficiency for video communications is essential for mobile devices with a limited battery capacity. Therefore, hardware decoder implementations are commonly used to significantly reduce the energetic load of video playback. The energy consumption of such a hardware implementation largely depends on a previously published specification of a video coding standard that defines which coding tools and methods are included. However, during the standardization of a video coding standard, the energy demand of a hardware implementation is unknown. Hence, the hardware complexity of coding tools is judged subjectively by experts from the field of hardware programming without using standardized assessment procedures. To solve this problem, we propose a method that accurately models the energy demand of existing hardware decoders with an average error of 1.79% by exploiting information from software decoder profiling. Motivated by the low estimation error, we propose a hardware decoding energy metric that can predict and estimate the energy demand of an unknown hardware implementation using information from existing hardware decoder implementations and available software implementations of the future video decoder. By using multiple video coding standards for model training, we can predict the relative energy demand of an unknown hardware decoder with a minimum error of 4.54% without using the corresponding hardware decoder for training.",2024-02-15T13:14:38Z,2024-12-12T14:23:36Z,http://arxiv.org/abs/2402.09926v3,http://arxiv.org/pdf/2402.09926v3,eess.IV
ODVista: An Omnidirectional Video Dataset for super-resolution and   Quality Enhancement Tasks,"Ahmed Telili, Ibrahim Farhat, Wassim Hamidouche, Hadi Amirpour","Omnidirectional or 360-degree video is being increasingly deployed, largely due to the latest advancements in immersive virtual reality (VR) and extended reality (XR) technology. However, the adoption of these videos in streaming encounters challenges related to bandwidth and latency, particularly in mobility conditions such as with unmanned aerial vehicles (UAVs). Adaptive resolution and compression aim to preserve quality while maintaining low latency under these constraints, yet downscaling and encoding can still degrade quality and introduce artifacts. Machine learning (ML)-based super-resolution (SR) and quality enhancement techniques offer a promising solution by enhancing detail recovery and reducing compression artifacts. However, current publicly available 360-degree video SR datasets lack compression artifacts, which limit research in this field. To bridge this gap, this paper introduces omnidirectional video streaming dataset (ODVista), which comprises 200 high-resolution and high quality videos downscaled and encoded at four bitrate ranges using the high-efficiency video coding (HEVC)/H.265 standard. Evaluations show that the dataset not only features a wide variety of scenes but also spans different levels of content complexity, which is crucial for robust solutions that perform well in real-world scenarios and generalize across diverse visual environments. Additionally, we evaluate the performance, considering both quality enhancement and runtime, of two handcrafted and two ML-based SR models on the validation and testing sets of ODVista.",2024-03-01T15:30:04Z,2024-03-07T15:48:54Z,http://arxiv.org/abs/2403.00604v2,http://arxiv.org/pdf/2403.00604v2,eess.IV
Explore Cross-Codec Quality-Rate Convex Hulls Relation for Adaptive   Streaming,Masoumeh Farhadi Nia,"With the ongoing advancement of video technology and the emergence of new video platforms, suppliers of video contents are striving to ensure that the video quality meets the desire of consumers. Accessing a limited amount of channel bandwidth, they are often looking for a novel approach to decrease the use of data and thus the required energy and cost. This study evaluates the Quality Rate performance of H.264, H.265, and VP9 codecs across resolutions (960*544, 1920*1080, 3840*2160) to optimize video quality while minimizing bitrate, crucial for energy and cost efficiency. At this approach, original videos at native resolutions were encoded, decoded, and rescaled using FFmpeg. For each resolution, encoding and decoding were performed at various quantization levels. Quality Rate (QR) curves were generated using PSNR and VMAF metric against bitrate. Convex Hull curves were then derived and mathematically modelled for each resolution. The procedure was systematically applied to H.264, H.265, and VP9 codecs. Results indicate that increasing CRF values reduce bitrate, PSNR, and VMAF, with PSNR ranging between 20-40 dB. Logarithmic polynomial modelling of convex hulls demonstrated high accuracy, with low RMSE and high R-Squared values. These findings suggest that the convex hull of one codec can predict the performance of others, aiding future content-driven prediction methodologies and enhancing adaptive streaming efficiency. Keywords: Video Codecs, Adaptive Streaming, Compression, Bitrate, PSNR, VMAF, H.264, H.265, VP9",2024-08-16T22:52:00Z,2024-08-16T22:52:00Z,http://arxiv.org/abs/2408.09044v1,http://arxiv.org/pdf/2408.09044v1,eess.IV
Mitigation of H.264 and H.265 Video Compression for Reliable PRNU   Estimation,"Enes Altınışık, Kasım Taşdemir, Hüsrev Taha Sencar","The photo-response non-uniformity (PRNU) is a distinctive image sensor characteristic, and an imaging device inadvertently introduces its sensor's PRNU into all media it captures. Therefore, the PRNU can be regarded as a camera fingerprint and used for source attribution. The imaging pipeline in a camera, however, involves various processing steps that are detrimental to PRNU estimation. In the context of photographic images, these challenges are successfully addressed and the method for estimating a sensor's PRNU pattern is well established. However, various additional challenges related to generation of videos remain largely untackled. With this perspective, this work introduces methods to mitigate disruptive effects of widely deployed H.264 and H.265 video compression standards on PRNU estimation. Our approach involves an intervention in the decoding process to eliminate a filtering procedure applied at the decoder to reduce blockiness. It also utilizes decoding parameters to develop a weighting scheme and adjust the contribution of video frames at the macroblock level to PRNU estimation process. Results obtained on videos captured by 28 cameras show that our approach increases the PRNU matching metric up to more than five times over the conventional estimation method tailored for photos.",2019-05-23T12:12:14Z,2019-05-23T12:12:14Z,http://arxiv.org/abs/1905.09611v1,http://arxiv.org/pdf/1905.09611v1,eess.IV
Study of Compression Statistics and Prediction of Rate-Distortion Curves   for Video Texture,"Angeliki V. Katsenou, Mariana Afonso, David R. Bull","Encoding textural content remains a challenge for current standardised video codecs. It is therefore beneficial to understand video textures in terms of both their spatio-temporal characteristics and their encoding statistics in order to optimize encoding performance. In this paper, we analyse the spatio-temporal features and statistics of video textures, explore the rate-quality performance of different texture types and investigate models to mathematically describe them. For all considered theoretical models, we employ machine-learning regression to predict the rate-quality curves based solely on selected spatio-temporal features extracted from uncompressed content. All experiments were performed on homogeneous video textures to ensure validity of the observations. The results of the regression indicate that using an exponential model we can more accurately predict the expected rate-quality curve (with a mean Bj{\o}ntegaard Delta rate of 0.46% over the considered dataset) while maintaining a low relative complexity. This is expected to be adopted by in the loop processes for faster encoding decisions such as rate-distortion optimisation, adaptive quantization, partitioning, etc.",2021-02-08T12:41:08Z,2021-02-08T12:41:08Z,http://arxiv.org/abs/2102.04167v1,http://arxiv.org/pdf/2102.04167v1,eess.IV
A Perceptual Based Motion Compensation Technique for Video Coding,"Amin Banitalebi, Said Nader-Esfahani, Alireza Nasiri Avanaki","Motion estimation is one of the important procedures in the all video encoders. Most of the complexity of the video coder depends on the complexity of the motion estimation step. The original motion estimation algorithm has a remarkable complexity and therefore many improvements were proposed to enhance the crude version of the motion estimation. The basic idea of many of these works were to optimize some distortion function for mean squared error (MSE) or sum of absolute difference (SAD) in block matching But it is shown that these metrics do not conclude the quality as it is, on the other hand, they are not compatible with the human visual system (HVS). In this paper we explored the usage of the image quality metrics in the video coding and more specific in the motion estimation. We have utilized the perceptual image quality metrics instead of MSE or SAD in the block based motion estimation. Three different metrics have used: structural similarity or SSIM, complex wavelet structural similarity or CW-SSIM, visual information fidelity or VIF. Experimental results showed that usage of the quality criterions can improve the compression rate while the quality remains fix and thus better quality in coded video at the same bit budget.",2018-03-13T03:43:51Z,2018-03-13T03:43:51Z,http://arxiv.org/abs/1803.04607v1,http://arxiv.org/pdf/1803.04607v1,eess.IV
3D Video Quality Metric for 3D Video Compression,"Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos","As the evolution of multiview display technology is bringing glasses-free 3DTV closer to reality, MPEG and VCEG are preparing an extension to HEVC to encode multiview video content. View synthesis in the current version of the 3D video codec is performed using PSNR as a quality metric measure. In this paper, we propose a full- reference Human-Visual-System based 3D video quality metric to be used in multiview encoding as an alternative to PSNR. Performance of our metric is tested in a 2-view case scenario. The quality of the compressed stereo pair, formed from a decoded view and a synthesized view, is evaluated at the encoder side. The performance is verified through a series of subjective tests and compared with that of PSNR, SSIM, MS-SSIM, VIFp, and VQM metrics. Experimental results showed that our 3D quality metric has the highest correlation with Mean Opinion Scores (MOS) compared to the other tested metrics.",2018-03-13T05:35:26Z,2018-03-13T05:35:26Z,http://arxiv.org/abs/1803.04629v1,http://arxiv.org/pdf/1803.04629v1,eess.IV
Introducing A Public Stereoscopic 3D High Dynamic Range (SHDR) Video   Database,Amin Banitalebi-Dehkordi,"High Dynamic Range (HDR) displays and cameras are paving their ways through the consumer market at a rapid growth rate. Thanks to TV and camera manufacturers, HDR systems are now becoming available commercially to end users. This is taking place only a few years after the blooming of 3D video technologies. MPEG/ITU are also actively working towards the standardization of these technologies. However, preliminary research efforts in these video technologies are hammered by the lack of sufficient experimental data. In this paper, we introduce a Stereoscopic 3D HDR (SHDR) database of videos that is made publicly available to the research community. We explain the procedure taken to capture, calibrate, and post-process the videos. In addition, we provide insights on potential use-cases, challenges, and research opportunities, implied by the combination of higher dynamic range of the HDR aspect, and depth impression of the 3D aspect.",2018-03-13T14:42:46Z,2018-03-13T14:42:46Z,http://arxiv.org/abs/1803.04847v1,http://arxiv.org/pdf/1803.04847v1,eess.IV
Versatile video coding and super-resolution for efficient delivery of 8K   video with 4K backward-compatibility,"Charles Bonnineau, Wassim Hamidouche, Jean-Francois Travers, Olivier Deforges","In this paper, we propose, through an objective study, to compare and evaluate the performance of different coding approaches allowing the delivery of an 8K video signal with 4K backward-compatibility on broadcast networks. Presented approaches include simulcast of 8K and 4K single-layer signals encoded using High-Efficiency Video Coding (HEVC) and Versatile Video Coding (VVC) standards, spatial scalability using SHVC with 4K base layer (BL) and 8K enhancement-layer (EL), and super-resolution applied on 4K VVC signal after decoding to reach 8K resolution. For up-scaling, we selected the deep-learning-based super-resolution method called Super-Resolution with Feedback Network (SRFBN) and the Lanczos interpolation filter. We show that the deep-learning-based approach achieves visual quality gain over simulcast, especially on bit-rates lower than 30Mb/s with average gain of 0.77dB, 0.015, and 7.97 for PSNR, SSIM, and VMAF, respectively and out-performs the Lanczos filter in average by 29% of BD-rate savings.",2020-02-17T12:51:10Z,2020-02-17T12:51:10Z,http://arxiv.org/abs/2002.06922v1,http://arxiv.org/pdf/2002.06922v1,"eess.IV, eess.SP"
Energy Efficient Video Decoding for VVC Using a Greedy Strategy Based   Design Space Exploration,"Matthias Kränzler, Christian Herglotz, André Kaup","IP traffic has increased significantly in recent years, and it is expected that this progress will continue. Recent studies report that the viewing of online video content accounts for a share of 1% of the global greenhouse gas emissions. To reduce the data traffic of video streaming, the new standard Versatile Video Coding (VVC) has been finalized in 2020. In this paper, the energy efficiency of two different VVC decoders is analyzed in detail. Furthermore, we propose a design space exploration that uses an algorithm based on a greedy strategy to derive coding tool profiles that optimize the energy demand of the decoder. We show that the algorithm derives optimal coding tool profiles for a subset of coding tools. Additionally, we propose profiles that reduce the energy demand of VVC decoders and provide energy savings of more than 50% for sequences with 4K resolution. Thereby, we will also show that the proposed profiles can have a lower decoding energy demand than comparable HEVC-encoded bit streams while also having a significantly lower bit rate.",2021-11-23T23:25:16Z,2021-11-23T23:25:16Z,http://arxiv.org/abs/2111.12194v1,http://arxiv.org/pdf/2111.12194v1,eess.IV
Generative Compression for Face Video: A Hybrid Scheme,"Anni Tang, Yan Huang, Jun Ling, Zhiyu Zhang, Yiwei Zhang, Rong Xie, Li Song","As the latest video coding standard, versatile video coding (VVC) has shown its ability in retaining pixel quality. To excavate more compression potential for video conference scenarios under ultra-low bitrate, this paper proposes a bitrate adjustable hybrid compression scheme for face video. This hybrid scheme combines the pixel-level precise recovery capability of traditional coding with the generation capability of deep learning based on abridged information, where Pixel wise Bi-Prediction, Low-Bitrate-FOM and Lossless Keypoint Encoder collaborate to achieve PSNR up to 36.23 dB at a low bitrate of 1.47 KB/s. Without introducing any additional bitrate, our method has a clear advantage over VVC under a completely fair comparative experiment, which proves the effectiveness of our proposed scheme. Moreover, our scheme can adapt to any existing encoder / configuration to deal with different encoding requirements, and the bitrate can be dynamically adjusted according to the network condition.",2022-04-21T12:39:53Z,2023-03-20T13:14:06Z,http://arxiv.org/abs/2204.10055v3,http://arxiv.org/pdf/2204.10055v3,eess.IV
Towards Perceptually Optimized End-to-end Adaptive Video Streaming,"Christos G. Bampis, Zhi Li, Ioannis Katsavounidis, Te-Yuan Huang, Chaitanya Ekanadham, Alan C. Bovik","Measuring Quality of Experience (QoE) and integrating these measurements into video streaming algorithms is a multi-faceted problem that fundamentally requires the design of comprehensive subjective QoE databases and metrics. To achieve this goal, we have recently designed the LIVE-NFLX-II database, a highly-realistic database which contains subjective QoE responses to various design dimensions, such as bitrate adaptation algorithms, network conditions and video content. Our database builds on recent advancements in content-adaptive encoding and incorporates actual network traces to capture realistic network variations on the client device. Using our database, we study the effects of multiple streaming dimensions on user experience and evaluate video quality and quality of experience models. We believe that the tools introduced here will help inspire further progress on the development of perceptually-optimized client adaptation and video streaming strategies. The database is publicly available at http://live.ece.utexas.edu/research/LIVE_NFLX_II/live_nflx_plus.html.",2018-08-12T06:17:38Z,2018-08-12T06:17:38Z,http://arxiv.org/abs/1808.03898v1,http://arxiv.org/pdf/1808.03898v1,eess.IV
Inpainting-based Video Compression in FullHD,"Sarah Andris, Pascal Peter, Rahul Mohideen Kaja Mohideen, Joachim Weickert, Sebastian Hoffmann","Compression methods based on inpainting are an evolving alternative to classical transform-based codecs for still images. Attempts to apply these ideas to video compression are rare, since reaching real-time performance is very challenging. Therefore, current approaches focus on simplified frame-by-frame reconstructions that ignore temporal redundancies. As a remedy, we propose a highly efficient, real-time capable prediction and correction approach that fully relies on partial differential equations (PDEs) in all steps of the codec: Dense variational optic flow fields yield accurate motion-compensated predictions, while homogeneous diffusion inpainting is applied for intra prediction. To compress residuals, we introduce a new highly efficient block-based variant of pseudodifferential inpainting. Our novel architecture outperforms other inpainting-based video codecs in terms of both quality and speed. For the first time in inpainting-based video compression, we can decompress FullHD (1080p) videos in real-time with a fully CPU-based implementation, outperforming previous approaches by roughly one order of magnitude.",2020-08-24T09:07:41Z,2021-05-04T09:42:49Z,http://arxiv.org/abs/2008.10273v3,http://arxiv.org/pdf/2008.10273v3,eess.IV
Explainable Machine Learning based Transform Coding for High Efficiency   Intra Prediction,"Na Li, Yun Zhang, C. -C. Jay Kuo","Machine learning techniques provide a chance to explore the coding performance potential of transform. In this work, we propose an explainable transform based intra video coding to improve the coding efficiency. Firstly, we model machine learning based transform design as an optimization problem of maximizing the energy compaction or decorrelation capability. The explainable machine learning based transform, i.e., Subspace Approximation with Adjusted Bias (Saab) transform, is analyzed and compared with the mainstream Discrete Cosine Transform (DCT) on their energy compaction and decorrelation capabilities. Secondly, we propose a Saab transform based intra video coding framework with off-line Saab transform learning. Meanwhile, intra mode dependent Saab transform is developed. Then, Rate Distortion (RD) gain of Saab transform based intra video coding is theoretically and experimentally analyzed in detail. Finally, three strategies on integrating the Saab transform and DCT in intra video coding are developed to improve the coding efficiency. Experimental results demonstrate that the proposed 8$\times$8 Saab transform based intra video coding can achieve Bj{\o}nteggard Delta Bit Rate (BDBR) from -1.19% to -10.00% and -3.07% on average as compared with the mainstream 8$\times$8 DCT based coding scheme.",2020-12-21T07:02:33Z,2020-12-21T07:02:33Z,http://arxiv.org/abs/2012.11152v1,http://arxiv.org/pdf/2012.11152v1,eess.IV
Versatile Video Coding Standard: A Review from Coding Tools to Consumers   Deployment,"Wassim Hamidouche, Thibaud Biatek, Mohsen Abdoli, Edouard François, Fernando Pescador, Miloš Radosavljević, Daniel Menard, Mickael Raulet","The amount of video content and the number of applications based on multimedia information increase each day. The development of new video coding standards is a challenge to increase the compression rate and other important features with a reasonable increase in the computational load. Video Experts Team (JVET) of ITU-T and the JCT group within ISO/IEC have worked together to standardize the Versatile Video Coding, approved finally in July 2020 as ITU-T H.266 | MPEG-I - Part 3 (ISO/IEC 23090-3) standard. This paper overviews some interesting consumer electronic use cases, the compression tools described in the standard, the current available real time implementations and the first industrial trials done with this standard.",2021-06-27T14:31:35Z,2021-11-06T09:34:19Z,http://arxiv.org/abs/2106.14245v2,http://arxiv.org/pdf/2106.14245v2,eess.IV
Optimized processing order for 3D hole filling in video sequences using   frequency selective extrapolation,"Jürgen Seiler, Susanne Schöll, Wolfgang Schnurrer, André Kaup","A problem often arising in video communication is the reconstruction of missing or distorted areas in a video sequence. Such holes of unavailable pixels may be caused for example by transmission errors of coded video data or undesired objects like logos. In order to close the holes given neighboring available content, a signal extrapolation has to be performed. The best quality can be achieved, if spatial as well as temporal information is used for the reconstruction. However, the question always is in which order to process the extrapolation to obtain the best result. In this paper, an optimized processing order is introduced for improving the extrapolation quality of Three-dimensional Frequency Selective Extrapolation. Using the proposed optimized order, holes in video sequences can be closed from the outer margin to the center, leading to a higher reconstruction quality, and visually noticeable gains of more than 0.5 dB PSNR are possible.",2022-07-20T08:30:38Z,2022-07-20T08:30:38Z,http://arxiv.org/abs/2207.09737v1,http://arxiv.org/pdf/2207.09737v1,eess.IV
A Multi-scale Video Denoising Algorithm for Raw Image,"Bin Ma, Yueli Hu, Xianxian Lv, Kai Li","Video denoising for raw image has always been the difficulty of camera image processing. On the one hand, image denoising performance largely determines the image quality, moreover denoising effect in raw image will affect the accuracy of the following operations of ISP processing flow. On the other hand, compared with image, video have motion information in time sequence, thus motion estimation which is complex and computationally expensive is needed in video denoising. In view of the above problems, this paper proposes a video denoising algorithm for raw image, performing multiple cascading processing stages on raw-RGB image based on convolutional neural network, and carries out implicit motion estimation in the network. The denoising performance is far superior to that of traditional algorithms with minimal computation and bandwidth, and has computational advantages compared with most deep learning algorithms.",2022-09-05T03:19:58Z,2022-09-05T03:19:58Z,http://arxiv.org/abs/2209.01740v1,http://arxiv.org/pdf/2209.01740v1,eess.IV
Motion Magnification Algorithms for Video-Based Breathing Monitoring,"Veronica Mattioli, Davide Alinovi, Gianluigi Ferrari, Francesco Pisani, Riccardo Raheli","In this paper, we present two video processing techniques for contact-less estimation of the Respiratory Rate (RR) of framed subjects. Due to the modest extent of movements related to respiration in both infants and adults, specific algorithms to efficiently detect breathing are needed. For this reason, motion-related variations in video signals are exploited to identify respiration of the monitored patient and simultaneously estimate the RR over time. Our estimation methods rely on two motion magnification algorithms that are exploited to enhance the subtle respiration-related movements. In particular, amplitude- and phase-based algorithms for motion magnification are considered to extract reliable motion signals. The proposed estimation systems perform both spatial decomposition of the video frames combined with proper temporal filtering to extract breathing information. After periodic (or quasi-periodic) respiratory signals are extracted and jointly analysed, we apply the Maximum Likelihood (ML) criterion to estimate the fundamental frequency, corresponding to the RR. The performance of the presented methods is first assessed by comparison with reference data. Videos framing different subjects, i.e., newborns and adults, are tested. Finally, the RR estimation accuracy of both methods is measured in terms of normalized Root Mean Squared Error (RMSE).",2022-11-29T09:33:24Z,2022-11-29T09:33:24Z,http://arxiv.org/abs/2211.16046v1,http://arxiv.org/pdf/2211.16046v1,eess.IV
Coding of distortion-corrected fisheye video sequences using H.265/HEVC,"Andrea Eichenseer, André Kaup","Images and videos captured by fisheye cameras exhibit strong radial distortions due to their large field of view. Conventional intra-frame as well as inter-frame prediction techniques as employed in hybrid video coding schemes are not designed to cope with such distortions, however. So far, captured fish-eye data has been coded and stored without consideration to any loss in efficiency resulting from radial distortion. This paper investigates the effects on the coding efficiency when applying distortion correction as a pre-processing step as opposed to the state-of-the-art method of post-processing. Both methods make use of the latest video coding standard H.265/HEVC and are compared with regard to objective as well as subjective video quality. It is shown that a maximum PSNR gain of 1.91 dB for intra-frame and 1.37 dB for inter-frame coding is achieved when using the pre-processing method. Average gains amount to 1.16 dB and 0.95 dB for intra-frame and inter-frame coding, respectively.",2022-11-30T13:20:54Z,2022-11-30T13:20:54Z,http://arxiv.org/abs/2211.16967v1,http://arxiv.org/pdf/2211.16967v1,eess.IV
A Bit Stream Feature-Based Energy Estimator for HEVC Software Encoding,"Geetha Ramasubbu, André Kaup, Christian Herglotz","The total energy consumption of today's video coding systems is globally significant and emphasizes the need for sustainable video coder applications. To develop such sustainable video coders, the knowledge of the energy consumption of state-of-the-art video coders is necessary. For that purpose, we need a dedicated setup that measures the energy of the encoding and decoding system. However, such measurements are costly and laborious. To this end, this paper presents an energy estimator that uses a subset of bit stream features to accurately estimate the energy consumption of the HEVC software encoding process. The proposed model reaches a mean estimation error of 4.88% when averaged over presets of the x265 encoder implementation. The results from this work help to identify properties of encoding energy-saving bit streams and, in turn, are useful for developing new energy-efficient video coding algorithms.",2022-12-11T21:48:55Z,2024-10-02T01:35:45Z,http://arxiv.org/abs/2212.05609v3,http://arxiv.org/pdf/2212.05609v3,eess.IV
Learned Wavelet Video Coding using Motion Compensated Temporal Filtering,"Anna Meyer, Fabian Brand, André Kaup","We present an end-to-end trainable wavelet video coder based on motion-compensated temporal filtering (MCTF). Thereby, we introduce a different coding scheme for learned video compression, which is currently dominated by residual and conditional coding approaches. By performing discrete wavelet transforms in temporal, horizontal, and vertical dimension, we obtain an explainable framework with spatial and temporal scalability. We focus on investigating a novel trainable MCTF module that is implemented using the lifting scheme. We show how multiple temporal decomposition levels in MCTF can be considered during training and how larger temporal displacements due to the MCTF coding order can be handled. Further, we present a content adaptive extension to MCTF which adapts to different motion strengths during inference. In our experiments, we compare our MCTF-based approach to learning-based conditional coders and traditional hybrid video coding. Especially at high rates, our approach has promising rate-distortion performance. Our method achieves average Bj{\o}ntegaard Delta savings of up to 21% over HEVC on the UVG data set and thereby outperforms state-of-the-art learned video coders.",2023-05-25T16:16:38Z,2023-10-12T12:04:43Z,http://arxiv.org/abs/2305.16211v2,http://arxiv.org/pdf/2305.16211v2,eess.IV
MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions   for Continuous Space-Time Video Super-Resolution,"Yi-Hsin Chen, Si-Cun Chen, Yi-Hsin Chen, Yen-Yu Lin, Wen-Hsiao Peng","This work addresses continuous space-time video super-resolution (C-STVSR) that aims to up-scale an input video both spatially and temporally by any scaling factors. One key challenge of C-STVSR is to propagate information temporally among the input video frames. To this end, we introduce a space-time local implicit neural function. It has the striking feature of learning forward motion for a continuum of pixels. We motivate the use of forward motion from the perspective of learning individual motion trajectories, as opposed to learning a mixture of motion trajectories with backward motion. To ease motion interpolation, we encode sparsely sampled forward motion extracted from the input video as the contextual input. Along with a reliability-aware splatting and decoding scheme, our framework, termed MoTIF, achieves the state-of-the-art performance on C-STVSR. The source code of MoTIF is available at https://github.com/sichun233746/MoTIF.",2023-07-16T09:43:07Z,2023-09-21T07:42:23Z,http://arxiv.org/abs/2307.07988v2,http://arxiv.org/pdf/2307.07988v2,eess.IV
Deep Hierarchical Video Compression,"Ming Lu, Zhihao Duan, Fengqing Zhu, Zhan Ma","Recently, probabilistic predictive coding that directly models the conditional distribution of latent features across successive frames for temporal redundancy removal has yielded promising results. Existing methods using a single-scale Variational AutoEncoder (VAE) must devise complex networks for conditional probability estimation in latent space, neglecting multiscale characteristics of video frames. Instead, this work proposes hierarchical probabilistic predictive coding, for which hierarchal VAEs are carefully designed to characterize multiscale latent features as a family of flexible priors and posteriors to predict the probabilities of future frames. Under such a hierarchical structure, lightweight networks are sufficient for prediction. The proposed method outperforms representative learned video compression models on common testing videos and demonstrates computational friendliness with much less memory footprint and faster encoding/decoding. Extensive experiments on adaptation to temporal patterns also indicate the better generalization of our hierarchical predictive mechanism. Furthermore, our solution is the first to enable progressive decoding that is favored in networked video applications with packet loss.",2023-12-12T09:59:51Z,2023-12-12T09:59:51Z,http://arxiv.org/abs/2312.07126v1,http://arxiv.org/pdf/2312.07126v1,eess.IV
A Video-Aware FEC-Based Unequal Loss Protection System for Video   Streaming over RTP,"César Díaz, Julián Cabrera, Fernando Jaureguizar, Narciso García","A video-aware unequal loss protection (ULP) system for protecting RTP video streaming in bursty packet loss networks is proposed. Considering the relevance of the frame, the state of the channel, and the bitrate constraints of the protection bitstream, our algorithm selects in real time the most suitable frames to be protected through forward error protection (FEC) techniques. It benefits from a wise RTP encapsulation that allows working at a frame level without requiring any further process than that of parsing RTP headers. This makes our system straightforward and fast, perfectly suitable to be included in commercial video streaming servers. Simulation results show how our technique outperforms other proposed ULP schemes.",2024-02-07T10:32:13Z,2024-02-07T10:32:13Z,http://arxiv.org/abs/2402.04729v1,http://arxiv.org/pdf/2402.04729v1,eess.IV
A Comprehensive Review of Software and Hardware Energy Efficiency of   Video Decoders,"Matthias Kränzler, Christian Herglotz, André Kaup","Energy and compression efficiency are two essential parts of modern video decoder implementations that have to be considered. This work comprehensively studies the following six video coding formats regarding compression and decoding energy efficiency: AVC, VP9, HEVC, AV1, VVC, and AVM. We first evaluate the energy demand of reference and optimized software decoder implementations. Furthermore, we consider the influence of the usage of SIMD instructions on those decoder implementations. We find that AV1 is a sweet spot for optimized software decoder implementations with an additional energy demand of 16.55% and bitrate savings of -43.95% compared to VP9. We furthermore evaluate the hardware decoding energy demand of four video coding formats. Thereby, we show that AV1 has energy demand increases by 117.50% compared to VP9. For HEVC, we found a sweet spot in terms of energy demand with an increase of 6.06% with respect to VP9. Relative to their optimized software counterparts, hardware video decoders reduce the energy consumption to less than 9% compared to software decoders.",2024-02-14T08:02:50Z,2024-02-14T08:02:50Z,http://arxiv.org/abs/2402.09001v1,http://arxiv.org/pdf/2402.09001v1,eess.IV
A New Multi-Picture Architecture for Learned Video Deinterlacing and   Demosaicing with Parallel Deformable Convolution and Self-Attention Blocks,"Ronglei Ji, A. Murat Tekalp","Despite the fact real-world video deinterlacing and demosaicing are well-suited to supervised learning from synthetically degraded data because the degradation models are known and fixed, learned video deinterlacing and demosaicing have received much less attention compared to denoising and super-resolution tasks. We propose a new multi-picture architecture for video deinterlacing or demosaicing by aligning multiple supporting pictures with missing data to a reference picture to be reconstructed, benefiting from both local and global spatio-temporal correlations in the feature space using modified deformable convolution blocks and a novel residual efficient top-$k$ self-attention (kSA) block, respectively. Separate reconstruction blocks are used to estimate different types of missing data. Our extensive experimental results, on synthetic or real-world datasets, demonstrate that the proposed novel architecture provides superior results that significantly exceed the state-of-the-art for both tasks in terms of PSNR, SSIM, and perceptual quality. Ablation studies are provided to justify and show the benefit of each novel modification made to the deformable convolution and residual efficient kSA blocks. Code is available: https://github.com/KUIS-AI-Tekalp-Research-Group/Video-Deinterlacing.",2024-04-19T17:26:43Z,2024-04-19T17:26:43Z,http://arxiv.org/abs/2404.13018v1,http://arxiv.org/pdf/2404.13018v1,eess.IV
Efficient Learned Wavelet Image and Video Coding,"Anna Meyer, Srivatsa Prativadibhayankaram, André Kaup","Learned wavelet image and video coding approaches provide an explainable framework with a latent space corresponding to a wavelet decomposition. The wavelet image coder iWave++ achieves state-of-the-art performance and has been employed for various compression tasks, including lossy as well as lossless image, video, and medical data compression. However, the approaches suffer from slow decoding speed due to the autoregressive context model used in iWave++. In this paper, we show how a parallelized context model can be integrated into the iWave++ framework. Our experimental results demonstrate a speedup factor of over 350 and 240 for image and video compression, respectively. At the same time, the rate-distortion performance in terms of Bj{\o}ntegaard delta bitrate is slightly worse by 1.5\% for image coding and 1\% for video coding. In addition, we analyze the learned wavelet decomposition by visualizing its subband impulse responses.",2024-05-21T09:33:49Z,2024-11-06T09:21:34Z,http://arxiv.org/abs/2405.12631v2,http://arxiv.org/pdf/2405.12631v2,eess.IV
On Annotation-free Optimization of Video Coding for Machines,"Marc Windsheimer, Fabian Brand, André Kaup","Today, image and video data is not only viewed by humans, but also automatically analyzed by computer vision algorithms. However, current coding standards are optimized for human perception. Emerging from this, research on video coding for machines tries to develop coding methods designed for machines as information sink. Since many of these algorithms are based on neural networks, most proposals for video coding for machines build upon neural compression. So far, optimizing the compression by applying the task loss of the analysis network, for which ground truth data is needed, is achieving the best coding performance. But ground truth data is difficult to obtain and thus an optimization without ground truth is preferred. In this paper, we present an annotation-free optimization strategy for video coding for machines. We measure the distortion by calculating the task loss of the analysis network. Therefore, the predictions on the compressed image are compared with the predictions on the original image, instead of the ground truth data. Our results show that this strategy can even outperform training with ground truth data with rate savings of up to 7.5 %. By using the non-annotated training data, the rate gains can be further increased up to 8.2 %.",2024-06-12T07:00:49Z,2024-06-12T07:00:49Z,http://arxiv.org/abs/2406.07938v1,http://arxiv.org/pdf/2406.07938v1,eess.IV
Deep Unfolding-Aided Parameter Tuning for Plug-and-Play Based Video   Snapshot Compressive Imaging,"Takashi Matsuda, Ryo Hayakawa, Youji Iiguni","Snapshot compressive imaging (SCI) captures high-dimensional data efficiently by compressing it into two-dimensional observations and reconstructing high-dimensional data from two-dimensional observations with various algorithms. Plug-and-play (PnP) is a promising approach for the video SCI reconstruction because it can leverage both the observation model and denoising methods for videos. This paper proposes a deep unfolding-based method for tuning noise level parameters in PnP-based video SCI, which significantly affects the reconstruction accuracy. For the training of the parameters, we prepare training data from the densely annotated video segmentation (DAVIS) dataset, reparametrize the noise level parameters, and apply the checkpointing technique to reduce the required memory. Simulation results show that the trained noise level parameters significantly improve the reconstruction accuracy and exhibit a non-monotonic pattern, which is different from the assumptions in the conventional convergence analyses of PnP-based algorithms.",2024-06-28T12:22:31Z,2024-10-29T05:49:37Z,http://arxiv.org/abs/2406.19870v2,http://arxiv.org/pdf/2406.19870v2,eess.IV
A Multi-Scale Spatial-Temporal Network for Wireless Video Transmission,"Xinyi Zhou, Danlan Huang, Zhixin Qi, Liang Zhang, Ting Jiang","Deep joint source-channel coding (DeepJSCC) has shown promise in wireless transmission of text, speech, and images within the realm of semantic communication. However, wireless video transmission presents greater challenges due to the difficulty of extracting and compactly representing both spatial and temporal features, as well as its significant bandwidth and computational resource requirements. In response, we propose a novel video DeepJSCC (VDJSCC) approach to enable end-to-end video transmission over a wireless channel. Our approach involves the design of a multi-scale vision Transformer encoder and decoder to effectively capture spatial-temporal representations over long-term frames. Additionally, we propose a dynamic token selection module to mask less semantically important tokens from spatial or temporal dimensions, allowing for content-adaptive variable-length video coding by adjusting the token keep ratio. Experimental results demonstrate the effectiveness of our VDJSCC approach compared to digital schemes that use separate source and channel codes, as well as other DeepJSCC schemes, in terms of reconstruction quality and bandwidth reduction.",2024-11-15T04:21:58Z,2024-11-15T04:21:58Z,http://arxiv.org/abs/2411.09936v1,http://arxiv.org/pdf/2411.09936v1,eess.IV
Towards Hybrid-Optimization Video Coding,"Shuai Huo, Dong Liu, Li Li, Siwei Ma, Feng Wu, Wen Gao","Video coding is a mathematical optimization problem of rate and distortion essentially. To solve this complex optimization problem, two popular video coding frameworks have been developed: block-based hybrid video coding and end-to-end learned video coding. If we rethink video coding from the perspective of optimization, we find that the existing two frameworks represent two directions of optimization solutions. Block-based hybrid coding represents the discrete optimization solution because those irrelevant coding modes are discrete in mathematics. It searches for the best one among multiple starting points (i.e. modes). However, the search is not efficient enough. On the other hand, end-to-end learned coding represents the continuous optimization solution because the gradient descent is based on a continuous function. It optimizes a group of model parameters efficiently by the numerical algorithm. However, limited by only one starting point, it is easy to fall into the local optimum. To better solve the optimization problem, we propose to regard video coding as a hybrid of the discrete and continuous optimization problem, and use both search and numerical algorithm to solve it. Our idea is to provide multiple discrete starting points in the global space and optimize the local optimum around each point by numerical algorithm efficiently. Finally, we search for the global optimum among those local optimums. Guided by the hybrid optimization idea, we design a hybrid optimization video coding framework, which is built on continuous deep networks entirely and also contains some discrete modes. We conduct a comprehensive set of experiments. Compared to the continuous optimization framework, our method outperforms pure learned video coding methods. Meanwhile, compared to the discrete optimization framework, our method achieves comparable performance to HEVC reference software HM16.10 in PSNR.",2022-07-12T14:36:52Z,2022-07-12T14:36:52Z,http://arxiv.org/abs/2207.05565v1,http://arxiv.org/pdf/2207.05565v1,eess.IV
Real-time video streaming in vivo using ultrasound as the communication   channel,"Zhengchang Kou, Rita J. Miller, Andrew C. Singer, Michael L. Oelze","The emergence of capsule endoscopy has provided a means of capturing video of the small intestines without having to resort to an invasive procedure involving intubation. However, real-time video streaming to a receiver outside the body remains challenging for capsule endoscopy. Traditional electromagnetic-based solutions are limited in their data rates and available power. Recently, ultrasound was investigated as a communication channel for through-tissue data transmission. To achieve real-time video streaming through tissue, data rates of ultrasound need to exceed 1 Mbps. In a previous study, we demonstrated ultrasound communications with data rates greater than 30 Mbps with two focused ultrasound transducers using a large footprint laboratory system through slabs of lossy tissues [1]. While the form factor of the transmitter is also crucial for capsule endoscopy, it is obvious that a large, focused transducer cannot fit within the size of a capsule. Several other challenges for achieving high-speed ultrasonic communication through tissue include strong reflections leading to multipath effects and attenuation. In this work, we demonstrate ultrasonic video communications using a mm-scale microcrystal transmitter with video streaming supplied by a camera connected to a Field Programmable Gate Array (FPGA). The signals were transmitted through a tissue-mimicking phantom and through the abdomen of a rabbit in vivo. The ultrasound signal was recorded by an array probe connected to a Verasonics Vantage system and decoded back to video. To improve the received signal quality, we combined the signal from multiple channels of the array probe. Orthogonal frequency division multiplexing (OFDM) modulation was used to reduce the receiver complexity under a strong multipath environment.",2020-09-28T23:20:17Z,2020-09-28T23:20:17Z,http://arxiv.org/abs/2009.13683v1,http://arxiv.org/pdf/2009.13683v1,eess.SP
Correlation-aware Cooperative Multigroup Broadcast 360° Video   Delivery Network: A Hierarchical Deep Reinforcement Learning Approach,"Fenghe Hu, Yansha Deng, A. Hamid Aghvami","With the stringent requirement of receiving video from unmanned aerial vehicle (UAV) from anywhere in the stadium of sports events and the significant-high per-cell throughput for video transmission to virtual reality (VR) users, a promising solution is a cell-free multi-group broadcast (CF-MB) network with cooperative reception and broadcast access points (AP). To explore the benefit of broadcasting user-correlated decode-dependent video resources to spatially correlated VR users, the network should dynamically schedule the video and cluster APs into virtual cells for a different group of VR users with overlapped video requests. By decomposition the problem into scheduling and association sub-problems, we first introduce the conventional non-learning-based scheduling and association algorithms, and a centralized deep reinforcement learning (DRL) association approach based on the rainbow agent with a convolutional neural network (CNN) to generate decisions from observation. To reduce its complexity, we then decompose the association problem into multiple sub-problems, resulting in a networked-distributed Partially Observable Markov decision process (ND-POMDP). To solve it, we propose a multi-agent deep DRL algorithm. To jointly solve the coupled association and scheduling problems, we further develop a hierarchical federated DRL algorithm with scheduler as meta-controller, and association as the controller. Our simulation results shown that our CF-MB network can effectively handle real-time video transmission from UAVs to VR users. Our proposed learning architectures is effective and scalable for a high-dimensional cooperative association problem with increasing APs and VR users. Also, our proposed algorithms outperform non-learning based methods with significant performance improvement.",2020-10-21T23:31:35Z,2021-11-05T00:47:21Z,http://arxiv.org/abs/2010.11347v3,http://arxiv.org/pdf/2010.11347v3,eess.SP
Fast Video-based Face Recognition in Collaborative Learning Environments,Phuong Tran,"Face recognition is a classical problem in Computer Vision that has experienced significant progress. Yet, in digital videos, face recognition is complicated by occlusion, pose and lighting variations, and persons entering/leaving the scene. The thesis's goal is to develop a fast method for face recognition in digital videos that is applicable to large datasets. The thesis introduces several methods to address the problems associated with video face recognition. First, to address issues associated with pose and lighting variations, a collection of face prototypes is associated with each student. Second, to speed up the process, sampling, K-means Clustering, and a combination of both are used to reduce the number of face prototypes per student. Third, the videos are processed at different frame rates. Fourth, the thesis proposes the use of active sets to address occlusion and to eliminate face recognition application on video frames with slow face motions. Fifth, the thesis develops a group face detector that recognizes students within a collaborative learning group, while rejecting out-of-group face detections. Sixth, the thesis introduces a face DeID for protecting the students' identities. Seventh, the thesis uses data augmentation to increase the training set's size. The different methods are combined using multi-objective optimization to guarantee that the full method remains fast without sacrificing accuracy. To test the approach, the thesis develops the AOLME dataset of 138 student faces (81 boys and 57 girls) of ages 10 to 14, who are predominantly Latina/o students. Compared to the baseline method, the final optimized method resulted in fast recognition times with significant improvements in face recognition accuracy. Using face prototype sampling only, the proposed method achieved an accuracy of 71.8% compared to 62.3% for the baseline system, while running 11.6 times faster.",2021-10-26T01:54:38Z,2021-10-26T01:54:38Z,http://arxiv.org/abs/2110.14720v1,http://arxiv.org/pdf/2110.14720v1,eess.IV
Motion Estimation for Fisheye Video With an Application to Temporal   Resolution Enhancement,"Andrea Eichenseer, Michel Bätz, André Kaup","Surveying wide areas with only one camera is a typical scenario in surveillance and automotive applications. Ultra wide-angle fisheye cameras employed to that end produce video data with characteristics that differ significantly from conventional rectilinear imagery as obtained by perspective pinhole cameras. Those characteristics are not considered in typical image and video processing algorithms such as motion estimation, where translation is assumed to be the predominant kind of motion. This contribution introduces an adapted technique for use in block-based motion estimation that takes into the account the projection function of fisheye cameras and thus compensates for the non-perspective properties of fisheye videos. By including suitable projections, the translational motion model that would otherwise only hold for perspective material is exploited, leading to improved motion estimation results without altering the source material. In addition, we discuss extensions that allow for a better prediction of the peripheral image areas, where motion estimation falters due to spatial constraints, and further include calibration information to account for lens properties deviating from the theoretical function. Simulations and experiments are conducted on synthetic as well as real-world fisheye video sequences that are part of a data set created in the context of this paper. Average synthetic and real-world gains of 1.45 and 1.51 dB in luminance PSNR are achieved compared against conventional block matching. Furthermore, the proposed fisheye motion estimation method is successfully applied to motion compensated temporal resolution enhancement, where average gains amount to 0.79 and 0.76 dB.",2023-03-01T11:44:46Z,2023-03-01T11:44:46Z,http://arxiv.org/abs/2303.00433v1,http://arxiv.org/pdf/2303.00433v1,eess.IV
Bronchoscopic video synchronization for interactive multimodal   inspection of bronchial lesions,"Qi Chang, Patrick D. Byrnes, Danish Ahmad, Jennifer Toth, Rebecca Bascom, William E. Higgins","With lung cancer being the most fatal cancer worldwide, it is important to detect the disease early. A potentially effective way of detecting early cancer lesions developing along the airway walls (epithelium) is bronchoscopy. To this end, developments in bronchoscopy offer three promising noninvasive modalities for imaging bronchial lesions: white-light bronchoscopy (WLB), autofluorescence bronchoscopy (AFB), and narrow-band imaging (NBI). While these modalities give complementary views of the airway epithelium, the physician must manually inspect each video stream produced by a given modality to locate the suspect cancer lesions. Unfortunately, no effort has been made to rectify this situation by providing efficient quantitative and visual tools for analyzing these video streams. This makes the lesion search process extremely time-consuming and error-prone, thereby making it impractical to utilize these rich data sources effectively. We propose a framework for synchronizing multiple bronchoscopic videos to enable an interactive multimodal analysis of bronchial lesions. Our methods first register the video streams to a reference 3D chest computed-tomography (CT) scan to produce multimodal linkages to the airway tree. Our methods then temporally correlate the videos to one another to enable synchronous visualization of the resulting multimodal data set. Pictorial and quantitative results illustrate the potential of the methods.",2023-03-20T16:44:00Z,2023-03-20T16:44:00Z,http://arxiv.org/abs/2303.11258v1,http://arxiv.org/pdf/2303.11258v1,eess.IV
Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity,"Krishna Srikar Durbha, Alan C. Bovik","Adaptive video streaming allows for the construction of bitrate ladders that deliver perceptually optimized visual quality to viewers under bandwidth constraints. Two common approaches to adaptation are per-title encoding and per-shot encoding. The former involves encoding each program, movie, or other content in a manner that is perceptually- and bandwidth-optimized for that content but is otherwise fixed. The latter is a more granular approach that optimizes the encoding parameters for each scene or shot (however defined) of a video content. Per-shot video encoding, as pioneered by Netflix, encodes on a per-shot basis using the Dynamic Optimizer (DO). Under the control of the VMAF perceptual video quality prediction engine, the DO delivers high-quality videos to millions of viewers at considerably reduced bitrates than per-title or fixed bitrate ladder encoding. A variety of per-title and per-shot encoding techniques have been recently proposed that seek to reduce computational overhead and to construct optimal bitrate ladders more efficiently using low-level features extracted from source videos. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features extracted from different scales and subbands. We compare the performance of our model, which we call VIF-ladder, against other content-adaptive bitrate ladder prediction methods, counterparts of them that we designed to construct quality ladders, a fixed bitrate ladder, and bitrate ladders constructed via exhaustive encoding using Bjontegaard delta metrics.",2024-08-04T05:12:21Z,2024-08-04T05:12:21Z,http://arxiv.org/abs/2408.01932v1,http://arxiv.org/pdf/2408.01932v1,eess.IV
Performance and Non-adversarial Robustness of the Segment Anything Model   2 in Surgical Video Segmentation,"Yiqing Shen, Hao Ding, Xinyuan Shao, Mathias Unberath","Fully supervised deep learning (DL) models for surgical video segmentation have been shown to struggle with non-adversarial, real-world corruptions of image quality including smoke, bleeding, and low illumination. Foundation models for image segmentation, such as the segment anything model (SAM) that focuses on interactive prompt-based segmentation, move away from semantic classes and thus can be trained on larger and more diverse data, which offers outstanding zero-shot generalization with appropriate user prompts. Recently, building upon this success, SAM-2 has been proposed to further extend the zero-shot interactive segmentation capabilities from independent frame-by-frame to video segmentation. In this paper, we present a first experimental study evaluating SAM-2's performance on surgical video data. Leveraging the SegSTRONG-C MICCAI EndoVIS 2024 sub-challenge dataset, we assess SAM-2's effectiveness on uncorrupted endoscopic sequences and evaluate its non-adversarial robustness on videos with corrupted image quality simulating smoke, bleeding, and low brightness conditions under various prompt strategies. Our experiments demonstrate that SAM-2, in zero-shot manner, can achieve competitive or even superior performance compared to fully-supervised deep learning models on surgical video data, including under non-adversarial corruptions of image quality. Additionally, SAM-2 consistently outperforms the original SAM and its medical variants across all conditions. Finally, frame-sparse prompting can consistently outperform frame-wise prompting for SAM-2, suggesting that allowing SAM-2 to leverage its temporal modeling capabilities leads to more coherent and accurate segmentation compared to frequent prompting.",2024-08-07T21:33:07Z,2024-08-16T12:51:05Z,http://arxiv.org/abs/2408.04098v2,http://arxiv.org/pdf/2408.04098v2,eess.IV
Joint Source-Channel Optimization for UAV Video Coding and Transmission,"Kesong Wu, Xianbin Cao, Peng Yang, Haijun Zhang, Tony Q. S. Quek, Dapeng Oliver Wu","This paper is concerned with unmanned aerial vehicle (UAV) video coding and transmission in scenarios such as emergency rescue and environmental monitoring. Unlike existing methods of modeling UAV video source coding and channel transmission separately, we investigate the joint source-channel optimization issue for video coding and transmission. Particularly, we design eight-dimensional delay-power-rate-distortion models in terms of source coding and channel transmission and characterize the correlation between video coding and transmission, with which a joint source-channel optimization problem is formulated. Its objective is to minimize end-to-end distortion and UAV power consumption by optimizing fine-grained parameters related to UAV video coding and transmission. This problem is confirmed to be a challenging sequential-decision and non-convex optimization problem. We therefore decompose it into a family of repeated optimization problems by Lyapunov optimization and design an approximate convex optimization scheme with provable performance guarantees to tackle these problems. Based on the theoretical transformation, we propose a Lyapunov repeated iteration (LyaRI) algorithm. Both objective and subjective experiments are conducted to comprehensively evaluate the performance of LyaRI. The results indicate that, compared to its counterparts, LyaRI achieves better video quality and stability performance, with a 47.74% reduction in the variance of the obtained encoding bitrate.",2024-08-13T06:38:45Z,2024-12-24T13:16:17Z,http://arxiv.org/abs/2408.06667v5,http://arxiv.org/pdf/2408.06667v5,eess.SP
Region of Interest (ROI) Coding for Aerial Surveillance Video using AVC   & HEVC,"Holger Meuel, Florian Kluger, Jörn Ostermann","Aerial surveillance from Unmanned Aerial Vehicles (UAVs), i.e. with moving cameras, is of growing interest for police as well as disaster area monitoring. For more detailed ground images the camera resolutions are steadily increasing. Simultaneously the amount of video data to transmit is increasing significantly, too. To reduce the amount of data, Region of Interest (ROI) coding systems were introduced which mainly encode some regions in higher quality at the cost of the remaining image regions. We employ an existing ROI coding system relying on global motion compensation to retain full image resolution over the entire image. Different ROI detectors are used to automatically classify a video image on board of the UAV in ROI and non-ROI. We propose to replace the modified Advanced Video Coding (AVC) video encoder by a modified High Efficiency Video Coding (HEVC) encoder. Without any change of the detection system itself, but by replacing the video coding back-end we are able to improve the coding efficiency by 32% on average although regular HEVC provides coding gains of 12-30% only for the same test sequences and similar PSNR compared to regular AVC coding. Since the employed ROI coding mainly relies on intra mode coding of new emerging image areas, gains of HEVC-ROI coding over AVC-ROI coding compared to regular coding of the entire frames including predictive modes (inter) depend on sequence characteristics. We present a detailed analysis of bit distribution within the frames to explain the gains. In total we can provide coding data rates of 0.7-1.0 Mbit/s for full HDTV video sequences at 30 fps at reasonable quality of more than 37 dB.",2018-01-19T15:13:22Z,2018-01-19T15:13:22Z,http://arxiv.org/abs/1801.06442v1,http://arxiv.org/pdf/1801.06442v1,eess.IV
Framework for High-performance Video Acquisition and Processing in   MTCA.4 Form Factor,"Aleksander Mielczarek, Dariusz Makowski, Piotr Perek, Andrzej Napieralski","The video acquisition and processing systems are commonly used in industrial and scientific applications. Many of them utilize Camera Link interface for the transmission of a video stream from the camera to the host system. The framework presented in the paper enables capturing such data, processing it and transmitting to the host CPU. It consist of MTCA.4-compliant frame grabber and a set of software libraries supporting several different cameras. It is designed for use in large scale physics experiments such as ITER tokamak or European X-Ray Free-Electron Laser (E-XFEL), as well as in the Centre for Free-Electron Laser Science (CFEL). The proposed video acquisition solution features the worlds first Camera Link frame grabber for the MTCA.4 architecture. Thanks to the modern FPGA circuit architecture, the deserialization is done using only the built-in ISERDES primitives, which reduces the costs and complexity of the required hardware.",2018-06-27T13:32:57Z,2018-06-27T13:32:57Z,http://arxiv.org/abs/1806.10993v1,http://arxiv.org/pdf/1806.10993v1,"eess.IV, eess.SP"
Spectral Reflectance based Heart Rate Measurement from Facial Video,"Arvind Subramaniam, Rajitha K","Remote detection of the cardiac pulse has a number of applications in sports and medicine, and can be used to determine the physiological state of the subject. Previous approaches to estimate Heart Rate from video require the subject to remain stationary and employ background information to eliminate illumination interferences. The present research proposes a spectral reflectance based novel illumination rectification method to eliminate illumination variations in the video. Our method does not rely on the background of the video and is robust to extreme motion interferences (head movements). Furthermore, in order to tackle extreme motion artifacts, the present framework introduces a novel feature point recovery system which recovers the feature tracking points lost during extreme head movements of the subject. Finally, the individual HR estimates from multiple feature points are combined to produce an average HR. We evaluate the efficacy of our framework on the MAHNOB HCI dataset, a publicly available dataset employed by previous methods. Our HR measurement framework outperformed previous methods and had a root mean square error of 5.21%.",2019-01-02T06:02:58Z,2019-01-02T06:02:58Z,http://arxiv.org/abs/1901.00273v1,http://arxiv.org/pdf/1901.00273v1,eess.IV
Neural Video Compression using Spatio-Temporal Priors,"Haojie Liu, Tong Chen, Ming Lu, Qiu Shen, Zhan Ma","The pursuit of higher compression efficiency continuously drives the advances of video coding technologies. Fundamentally, we wish to find better ""predictions"" or ""priors"" that are reconstructed previously to remove the signal dependency efficiently and to accurately model the signal distribution for entropy coding. In this work, we propose a neural video compression framework, leveraging the spatial and temporal priors, independently and jointly to exploit the correlations in intra texture, optical flow based temporal motion and residuals. Spatial priors are generated using downscaled low-resolution features, while temporal priors (from previous reference frames and residuals) are captured using a convolutional neural network based long-short term memory (ConvLSTM) structure in a temporal recurrent fashion. All of these parts are connected and trained jointly towards the optimal rate-distortion performance. Compared with the High-Efficiency Video Coding (HEVC) Main Profile (MP), our method has demonstrated averaged 38% Bjontegaard-Delta Rate (BD-Rate) improvement using standard common test sequences, where the distortion is multi-scale structural similarity (MS-SSIM).",2019-02-20T02:49:50Z,2019-02-21T03:47:41Z,http://arxiv.org/abs/1902.07383v2,http://arxiv.org/pdf/1902.07383v2,eess.IV
Using modern motion estimation algorithms in existing video codecs,"Daniel J. Ringis, Davinder Singh, Francois Pitie, Anil Kokaram",Motion estimation is a key component of any modern video codec. Our understanding of motion and the estimation of motion from video has come a very long way since 2000. More than 135 different algorithms have been recently reviewed by Scharstein et al http://vision.middlebury.edu/flow/. These new algorithms differ markedly from Block Matching which has been the mainstay of video compression for some time. This paper presents comparisons of H.264 and MP4 compression using different motion estimation methods. In so doing we present as well methods for adapting pre-computed motion fields for use within a codec. We do not observe significant gains to be had with the methods chosen w.r.t. Rate Distortion tradeoffs but the results reflect a significantly more complex interrelationship between motion and compression than would be expected. There remains much more to be done to improve the coverage of this comparison to the emerging standards but these initial results show that there is value in these explorations.,2020-07-23T12:01:22Z,2020-07-23T12:01:22Z,http://arxiv.org/abs/2007.11948v1,http://arxiv.org/pdf/2007.11948v1,eess.IV
The Effect of Frame Rate on 3D Video Quality and Bitrate,"Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos","Increasing the frame rate of a 3D video generally results in improved Quality of Experience (QoE). However, higher frame rates involve a higher degree of complexity in capturing, transmission, storage, and display. The question that arises here is what frame rate guarantees high viewing quality of experience given the existing/required 3D devices and technologies (3D cameras, 3D TVs, compression, transmission bandwidth, and storage capacity). This question has already been addressed for the case of 2D video, but not for 3D. The objective of this paper is to study the relationship between 3D quality and bitrate at different frame rates. Our performance evaluations show that increasing the frame rate of 3D videos beyond 60 fps may not be visually distinguishable. In addition, our experiments show that when the available bandwidth is reduced, the highest possible 3D quality of experience can be achieved by adjusting (decreasing) the frame rate instead of increasing the compression ratio. The results of our study are of particular interest to network providers for rate adaptation in variable bitrate channels.",2018-03-13T14:17:14Z,2018-03-13T14:17:14Z,http://arxiv.org/abs/1803.04826v1,http://arxiv.org/pdf/1803.04826v1,eess.IV
A cow structural model for video analytics of cow health,"He Liu, Amy R. Reibman, Jacquelyn P. Boerman","In livestock farming, animal health directly influences productivity. For dairy cows, many health conditions can be evaluated by trained observers based on visual appearance and movement. However, to manually evaluate every cow in a commercial farm is expensive and impractical. This paper introduces a video-analytic system which automatically detects the cow structure from captured video sequences. A side-view cow structural model is designed to describe the spatial positions of the joints (keypoints) of the cow, and we develop a system using deep learning to automatically extract the structural model from videos. The proposed detection system can detect multiple cows in the same frame and provide robust performance under practical challenges like obstacles (fences) and poor illumination. Compared to other object detection methods, this system provides better detection results and successfully isolates the keypoints of each cow even when they are close to each other.",2020-03-12T17:10:17Z,2020-03-12T17:10:17Z,http://arxiv.org/abs/2003.05903v1,http://arxiv.org/pdf/2003.05903v1,eess.IV
Spatio-Temporal Processing for Automatic Vehicle Detection in Wide-Area   Aerial Video,"Xin Gao, Jeno Szep, Pratik Satam, Salim Hariri, Sundaresh Ram, Jeffrey J. Rodriguez","Vehicle detection in aerial videos often requires post-processing to eliminate false detections. This paper presents a spatio-temporal processing scheme to improve automatic vehicle detection performance by replacing the thresholding step of existing detection algorithms with multi-neighborhood hysteresis thresholding for foreground pixel classification. The proposed scheme also performs spatial post-processing, which includes morphological opening and closing to shape and prune the detected objects, and temporal post-processing to further reduce false detections. We evaluate the performance of the proposed spatial processing on two local aerial video datasets and one parking vehicle dataset, and the performance of the proposed spatio-temporal processing scheme on five local aerial video datasets and one public dataset. Experimental evaluation shows that the proposed schemes improve vehicle detection performance for each of the nine algorithms when evaluated on seven datasets. Overall, the use of the proposed spatio-temporal processing scheme improves average F-score to above 0.8 and achieves an average reduction of 83.8% in false positives.",2020-10-27T03:11:12Z,2020-10-27T03:11:12Z,http://arxiv.org/abs/2010.14025v1,http://arxiv.org/pdf/2010.14025v1,eess.IV
A software decoder implementation for H.266/VVC video coding standard,"Bin Zhu, Shan Liu, Yuan Liu, Yi Luo, Jing Ye, Haiyan Xu, Ying Huang, Hualong Jiao, Xiaozhong Xu, Xianguo Zhang, Chenchen Gu","Versatile Video Coding Standard (H.266/VVC) was completed by Joint Video Expert Team (JVET) of ITU-T and ISO/IEC, in July 2020. This new ITU recommendation/international standard is a successor to the well-known H.265/HEVC video coding standard with roughly doubled compression efficiency, but also at the cost of an increased computational complexity. The complexity of H.266/VVC decoder processing modules is studied in this paper. An optimized decoder implementation using SIMD instruction extensions and additional parallel processing including data and task level parallelism is presented, which can achieve real-time decoding of 4K 60fps VVC bitstreams on an x86 based CPU.",2020-12-04T20:20:03Z,2020-12-08T02:31:40Z,http://arxiv.org/abs/2012.02832v2,http://arxiv.org/pdf/2012.02832v2,eess.IV
Gated Fusion Network for SAO Filter and Inter Frame Prediction in   Versatile Video Coding,"Shiba Kuanar, Dwarikanath Mahapatra, Vassilis Athitsos, K. R Rao","To achieve higher coding efficiency, Versatile Video Coding (VVC) includes several novel components, but at the expense of increasing decoder computational complexity. These technologies at a low bit rate often create contouring and ringing effects on the reconstructed frames and introduce various blocking artifacts at block boundaries. To suppress those visual artifacts, the VVC framework supports four post-processing filter operations. The interoperation of these filters introduces extra signaling bits and eventually becomes overhead at higher resolution video processing. In this paper, a novel deep learning-based model is proposed for sample adaptive offset (SAO) nonlinear filtering operation and substantiated the merits of intra-inter frame quality enhancement. We introduced a variable filter size multi-scale CNN (MSCNN) to improve the denoising operation and incorporated strided deconvolution for further computation improvement. We demonstrated that our deconvolution model can effectively be trained by leveraging the high-frequency edge features learned in a parallel fashion using feature fusion and residual learning. The simulation results demonstrate that the proposed method outperforms the baseline VVC method in BD-BR, BD-PSNR measurements and achieves an average of 3.762 % bit rate saving on the standard video test sequences.",2021-05-25T21:39:25Z,2021-05-25T21:39:25Z,http://arxiv.org/abs/2105.12229v1,http://arxiv.org/pdf/2105.12229v1,eess.IV
Prediction-Aware Quality Enhancement of VVC Using CNN,"Fatemeh Nasiri, Wassim Hamidouche, Luce Morin, Nicolas Dhollande, Gildas Cocherel","The upcoming video coding standard, Versatile Video Coding (VVC), has shown great improvement compared to its predecessor, High Efficiency Video Coding (HEVC), in terms of bitrate saving. Despite its substantial performance, compressed videos might still suffer from quality degradation at low bitrates due to coding artifacts such as blockiness, blurriness and ringing. In this work, we exploit Convolutional Neural Networks (CNN) to enhance quality of VVC coded frames after decoding in order to reduce low bitrate artifacts. The main contribution of this work is the use of coding information from the compressed bitstream. More precisely, the prediction information of intra frames is used for training the network in addition to the reconstruction information. The proposed method is applied on both luminance and chrominance components of intra coded frames of VVC. Experiments on VVC Test Model (VTM) show that, both in low and high bitrates, the use of coding information can improve the BD-rate performance by about 1% and 6% for luma and chroma components, respectively.",2021-12-08T10:53:27Z,2021-12-08T10:53:27Z,http://arxiv.org/abs/2112.04225v1,http://arxiv.org/pdf/2112.04225v1,eess.IV
Intra Encoding Complexity Control with a Time-Cost Model for Versatile   Video Coding,"Yan Huang, Jizheng Xu, Li Zhang, Yan Zhao, Li Song","For the latest video coding standard Versatile Video Coding (VVC), the encoding complexity is much higher than previous video coding standards to achieve a better coding efficiency, especially for intra coding. The complexity becomes a major barrier of its deployment and use. Even with many fast encoding algorithms, it is still practically important to control the encoding complexity to a given level. Inspired by rate control algorithms, we propose a scheme to precisely control the intra encoding complexity of VVC. In the proposed scheme, a Time-PlanarCost (viz. Time-Cost, or T-C) model is utilized for CTU encoding time estimation. By combining a set of predefined parameters and the T-C model, CTU-level complexity can be roughly controlled. Then to achieve a precise picture-level complexity control, a framework is constructed including uneven complexity pre-allocation, preset selection and feedback. Experimental results show that, for the challenging intra coding scenario, the complexity error quickly converges to under 3.21%, while keeping a reasonable time saving and rate-distortion (RD) performance. This proves the efficiency of the proposed methods.",2022-06-13T02:54:58Z,2022-06-13T02:54:58Z,http://arxiv.org/abs/2206.05889v1,http://arxiv.org/pdf/2206.05889v1,eess.IV
Rate-Distortion Optimal Transform Coefficient Selection for Unoccupied   Regions in Video-Based Point Cloud Compression,"Christian Herglotz, Nils Genser, André Kaup","This paper presents a novel method to determine rate-distortion optimized transform coefficients for efficient compression of videos generated from point clouds. The method exploits a generalized frequency selective extrapolation approach that iteratively determines rate-distortion-optimized coefficients for all basis functions of two-dimensional discrete cosine and sine transforms. The method is applied to blocks containing both occupied and unoccupied pixels in video based point cloud compression for HEVC encoding. In the proposed algorithm, only the values of the transform coefficients are changed such that resulting bit streams are compliant to the V-PCC standard. For all-intra coded point clouds, bitrate savings of more than 4% for geometry and more than 6% for texture error metrics with respect to standard encoding can be observed. These savings are more than twice as high as savings obtained using competing methods from literature. In the randomaccess case, our proposed method outperforms competing V-PCC methods by more than 0.5%.",2022-06-24T09:53:59Z,2022-06-24T09:53:59Z,http://arxiv.org/abs/2206.12186v1,http://arxiv.org/pdf/2206.12186v1,eess.IV
Key frames assisted hybrid encoding for photorealistic compressive video   sensing,"Honghao Huang, Jiajie Teng, Yu Liang, Chengyang Hu, Minghua Chen, Sigang Yang, Hongwei Chen","Snapshot compressive imaging (SCI) encodes high-speed scene video into a snapshot measurement and then computationally makes reconstructions, allowing for efficient high-dimensional data acquisition. Numerous algorithms, ranging from regularization-based optimization and deep learning, are being investigated to improve reconstruction quality, but they are still limited by the ill-posed and information-deficient nature of the standard SCI paradigm. To overcome these drawbacks, we propose a new key frames assisted hybrid encoding paradigm for compressive video sensing, termed KH-CVS, that alternatively captures short-exposure key frames without coding and long-exposure encoded compressive frames to jointly reconstruct photorealistic video. With the use of optical flow and spatial warping, a deep convolutional neural network framework is constructed to integrate the benefits of these two types of frames. Extensive experiments on both simulations and real data from the prototype we developed verify the superiority of the proposed method.",2022-07-26T03:27:17Z,2022-07-26T03:27:17Z,http://arxiv.org/abs/2207.12627v1,http://arxiv.org/pdf/2207.12627v1,eess.IV
One Transform To Compute Them All: Efficient Fusion-Based Full-Reference   Video Quality Assessment,"Abhinau K. Venkataramanan, Cosmin Stejerean, Ioannis Katsavounidis, Alan C. Bovik","The Visual Multimethod Assessment Fusion (VMAF) algorithm has recently emerged as a state-of-the-art approach to video quality prediction, that now pervades the streaming and social media industry. However, since VMAF requires the evaluation of a heterogeneous set of quality models, it is computationally expensive. Given other advances in hardware-accelerated encoding, quality assessment is emerging as a significant bottleneck in video compression pipelines. Towards alleviating this burden, we propose a novel Fusion of Unified Quality Evaluators (FUNQUE) framework, by enabling computation sharing and by using a transform that is sensitive to visual perception to boost accuracy. Further, we expand the FUNQUE framework to define a collection of improved low-complexity fused-feature models that advance the state-of-the-art of video quality performance with respect to both accuracy, by 4.2\% to 5.3\%, and computational efficiency, by factors of 3.8 to 11 times!",2023-04-06T23:26:54Z,2023-11-18T05:15:39Z,http://arxiv.org/abs/2304.03412v2,http://arxiv.org/pdf/2304.03412v2,eess.IV
Power Reduction Opportunities on End-User Devices in Quality-Steady   Video Streaming,"Christian Herglotz, Werner Robitza, Alexander Raake, Tobias Hossfeld, André Kaup","This paper uses a crowdsourced dataset of online video streaming sessions to investigate opportunities to reduce the power consumption while considering QoE. For this, we base our work on prior studies which model both the end-user's QoE and the end-user device's power consumption with the help of high-level video features such as the bitrate, the frame rate, and the resolution. On top of existing research, which focused on reducing the power consumption at the same QoE optimizing video parameters, we investigate potential power savings by other means such as using a different playback device, a different codec, or a predefined maximum quality level. We find that based on the power consumption of the streaming sessions from the crowdsourcing dataset, devices could save more than 55% of power if all participants adhere to low-power settings.",2023-05-24T13:10:40Z,2023-05-24T13:10:40Z,http://arxiv.org/abs/2305.15117v1,http://arxiv.org/pdf/2305.15117v1,eess.IV
Recyclable Semi-supervised Method Based on Multi-model Ensemble for   Video Scene Parsing,"Biao Wu, Shaoli Liu, Diankai Zhang, Chengjian Zheng, Si Gao, Xiaofeng Zhang, Ning Wang","Pixel-level Scene Understanding is one of the fundamental problems in computer vision, which aims at recognizing object classes, masks and semantics of each pixel in the given image. Since the real-world is actually video-based rather than a static state, learning to perform video semantic segmentation is more reasonable and practical for realistic applications. In this paper, we adopt Mask2Former as architecture and ViT-Adapter as backbone. Then, we propose a recyclable semi-supervised training method based on multi-model ensemble. Our method achieves the mIoU scores of 62.97% and 65.83% on Development test and final test respectively. Finally, we obtain the 2nd place in the Video Scene Parsing in the Wild Challenge at CVPR 2023.",2023-06-05T14:04:38Z,2023-06-05T14:04:38Z,http://arxiv.org/abs/2306.02894v1,http://arxiv.org/pdf/2306.02894v1,eess.IV
Blind Video Quality Assessment at the Edge,"Zhanxuan Mei, Yun-Cheng Wang, C. -C. Jay Kuo","Owing to the proliferation of user-generated videos on the Internet, blind video quality assessment (BVQA) at the edge attracts growing attention. The usage of deep-learning-based methods is restricted to be applied at the edge due to their large model sizes and high computational complexity. In light of this, a novel lightweight BVQA method called GreenBVQA is proposed in this work. GreenBVQA features a small model size, low computational complexity, and high performance. Its processing pipeline includes: video data cropping, unsupervised representation generation, supervised feature selection, and mean-opinion-score (MOS) regression and ensembles. We conduct experimental evaluations on three BVQA datasets and show that GreenBVQA can offer state-of-the-art performance in PLCC and SROCC metrics while demanding significantly smaller model sizes and lower computational complexity. Thus, GreenBVQA is well-suited for edge devices.",2023-06-17T16:11:02Z,2023-10-29T05:34:26Z,http://arxiv.org/abs/2306.10386v2,http://arxiv.org/pdf/2306.10386v2,eess.IV
Spatio-Temporal Perception-Distortion Trade-off in Learned Video SR,"Nasrin Rahimi, A. Murat Tekalp","Perception-distortion trade-off is well-understood for single-image super-resolution. However, its extension to video super-resolution (VSR) is not straightforward, since popular perceptual measures only evaluate naturalness of spatial textures and do not take naturalness of flow (temporal coherence) into account. To this effect, we propose a new measure of spatio-temporal perceptual video quality emphasizing naturalness of optical flow via the perceptual straightness hypothesis (PSH) for meaningful spatio-temporal perception-distortion trade-off. We also propose a new architecture for perceptual VSR (PSVR) to explicitly enforce naturalness of flow to achieve realistic spatio-temporal perception-distortion trade-off according to the proposed measures. Experimental results with PVSR support the hypothesis that a meaningful perception-distortion tradeoff for video should account for the naturalness of motion in addition to naturalness of texture.",2023-07-04T08:14:13Z,2023-07-04T08:14:13Z,http://arxiv.org/abs/2307.01556v1,http://arxiv.org/pdf/2307.01556v1,eess.IV
Compression Ratio Learning and Semantic Communications for Video Imaging,"Bowen Zhang, Zhijin Qin, Geoffrey Ye Li","Camera sensors have been widely used in intelligent robotic systems. Developing camera sensors with high sensing efficiency has always been important to reduce the power, memory, and other related resources. Inspired by recent success on programmable sensors and deep optic methods, we design a novel video compressed sensing system with spatially-variant compression ratios, which achieves higher imaging quality than the existing snapshot compressed imaging methods with the same sensing costs. In this article, we also investigate the data transmission methods for programmable sensors, where the performance of communication systems is evaluated by the reconstructed images or videos rather than the transmission of sensor data itself. Usually, different reconstruction algorithms are designed for applications in high dynamic range imaging, video compressive sensing, or motion debluring. This task-aware property inspires a semantic communication framework for programmable sensors. In this work, a policy-gradient based reinforcement learning method is introduced to achieve the explicit trade-off between the compression (or transmission) rate and the image distortion. Numerical results show the superiority of the proposed methods over existing baselines.",2023-10-10T01:45:13Z,2023-10-10T01:45:13Z,http://arxiv.org/abs/2310.06246v1,http://arxiv.org/pdf/2310.06246v1,eess.IV
Bitrate Ladder Construction using Visual Information Fidelity,"Krishna Srikar Durbha, Hassene Tmar, Cosmin Stejerean, Ioannis Katsavounidis, Alan C. Bovik","Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",2023-12-12T22:48:50Z,2024-02-29T03:44:59Z,http://arxiv.org/abs/2312.07780v2,http://arxiv.org/pdf/2312.07780v2,eess.IV
3DAttGAN: A 3D Attention-based Generative Adversarial Network for Joint   Space-Time Video Super-Resolution,"Congrui Fu, Hui Yuan, Liquan Shen, Raouf Hamzaoui, Hao Zhang","In many applications, including surveillance, entertainment, and restoration, there is a need to increase both the spatial resolution and the frame rate of a video sequence. The aim is to improve visual quality, refine details, and create a more realistic viewing experience. Existing space-time video super-resolution methods do not effectively use spatio-temporal information. To address this limitation, we propose a generative adversarial network for joint space-time video super-resolution. The generative network consists of three operations: shallow feature extraction, deep feature extraction, and reconstruction. It uses three-dimensional (3D) convolutions to process temporal and spatial information simultaneously and includes a novel 3D attention mechanism to extract the most important channel and spatial information. The discriminative network uses a two-branch structure to handle details and motion information, making the generated results more accurate. Experimental results on the Vid4, Vimeo-90K, and REDS datasets demonstrate the effectiveness of the proposed method. The source code is publicly available at https://github.com/FCongRui/3DAttGan.git.",2024-07-24T03:11:47Z,2024-07-24T03:11:47Z,http://arxiv.org/abs/2407.16965v1,http://arxiv.org/pdf/2407.16965v1,eess.IV
On the Rate-Distortion-Complexity Trade-offs of Neural Video Coding,"Yi-Hsin Chen, Kuan-Wei Ho, Martin Benjak, Jörn Ostermann, Wen-Hsiao Peng","This paper aims to delve into the rate-distortion-complexity trade-offs of modern neural video coding. Recent years have witnessed much research effort being focused on exploring the full potential of neural video coding. Conditional autoencoders have emerged as the mainstream approach to efficient neural video coding. The central theme of conditional autoencoders is to leverage both spatial and temporal information for better conditional coding. However, a recent study indicates that conditional coding may suffer from information bottlenecks, potentially performing worse than traditional residual coding. To address this issue, recent conditional coding methods incorporate a large number of high-resolution features as the condition signal, leading to a considerable increase in the number of multiply-accumulate operations, memory footprint, and model size. Taking DCVC as the common code base, we investigate how the newly proposed conditional residual coding, an emerging new school of thought, and its variants may strike a better balance among rate, distortion, and complexity.",2024-10-04T20:02:40Z,2024-10-04T20:02:40Z,http://arxiv.org/abs/2410.03898v1,http://arxiv.org/pdf/2410.03898v1,eess.IV
ECVC: Exploiting Non-Local Correlations in Multiple Frames for   Contextual Video Compression,"Wei Jiang, Junru Li, Kai Zhang, Li Zhang","In Learned Video Compression (LVC), improving inter prediction, such as enhancing temporal context mining and mitigating accumulated errors, is crucial for boosting rate-distortion performance. Existing LVCs mainly focus on mining the temporal movements while neglecting non-local correlations among frames. Additionally, current contextual video compression models use a single reference frame, which is insufficient for handling complex movements. To address these issues, we propose leveraging non-local correlations across multiple frames to enhance temporal priors, significantly boosting rate-distortion performance. To mitigate error accumulation, we introduce a partial cascaded fine-tuning strategy that supports fine-tuning on full-length sequences with constrained computational resources. This method reduces the train-test mismatch in sequence lengths and significantly decreases accumulated errors. Based on the proposed techniques, we present a video compression scheme ECVC. Experiments demonstrate that our ECVC achieves state-of-the-art performance, reducing $10.5\%$ and $11.5\%$ more bit-rates than previous SOTA method DCVC-FM over VTM-13.2 low delay B (LDB) under the intra period (IP) of $32$ and $-1$, respectively. Code will be available at https://github.com/JiangWeibeta/ECVC.",2024-10-13T03:13:27Z,2024-12-04T13:02:24Z,http://arxiv.org/abs/2410.09706v2,http://arxiv.org/pdf/2410.09706v2,eess.IV
RF PIX2PIX Unsupervised Wi-Fi to Video Translation,Michael Drob,"With the proliferation of Wi-Fi devices in the environment, our surroundings are increasingly illuminated with low-level RF scatter. This scatter illuminates objects in the environment much like radar or LIDAR. We show that a novel unsupervised network, based on the PIX2PIX GAN architecture, can recover and visually reconstruct scene information solely from Wi-Fi background energy; in contrast to a significantly less accurate approach by Kefayati (et. all) which requires careful object labeling to recover object location from a scene. This is accomplished by learning a more robust mapping function between the channel state information (CSI) from Wi-Fi packets and Video image sample distributions.",2021-02-14T00:39:45Z,2021-02-14T00:39:45Z,http://arxiv.org/abs/2102.09345v1,http://arxiv.org/pdf/2102.09345v1,"eess.IV, eess.SP"
ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC 29/WG 11 - JCT3V-C0032: A human   visual system based 3D video quality metric,"Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos","This contribution proposes a full-reference Human-Visual-System based 3D video quality metric. In this report, the presented metric is used to evaluate the quality of compressed stereo pair formed from a decoded view and a synthesized view. The performance of the proposed metric is verified through a series of subjective tests and compared with that of PSNR, SSIM, MS-SSIM, VIFp, and VQM metrics. The experimental results show that HV3D has the highest correlation with Mean Opinion Scores (MOS) compared to other tested metrics.",2018-03-13T14:08:35Z,2018-03-13T14:08:35Z,http://arxiv.org/abs/1803.05506v1,http://arxiv.org/pdf/1803.05506v1,eess.IV
Pupil Lovalization And Tracking For Video-Based Iris Biometrics,"Nedra Benletaief, Amel Benazza-Benyahia, Stephane Derrode","In this paper, we are interested in iris biometric applications. More precisely, our contribution consists in designing both a pupil detection and a tracking procedure from video sequences acquired by low-cost webcams. The novelty of our approach relies on the fact that it is operational even with a minimal user cooperation and, under bad illuminations and acquisition conditions. A robust classification algorithm is designed to detect the pupil. Moreover, a pupil tracker based on the extended Kalman filter is applied in order to reduce the processing time. Experimental results are performed in order to evaluate the performances of the proposed detection and tracking system",2020-02-16T19:50:02Z,2020-02-16T19:50:02Z,http://arxiv.org/abs/2002.11674v1,http://arxiv.org/pdf/2002.11674v1,eess.IV
A Technical Overview of AV1,"Jingning Han, Bohan Li, Debargha Mukherjee, Ching-Han Chiang, Adrian Grange, Cheng Chen, Hui Su, Sarah Parker, Sai Deng, Urvang Joshi, Yue Chen, Yunqing Wang, Paul Wilkins, Yaowu Xu, James Bankoski",The AV1 video compression format is developed by the Alliance for Open Media consortium. It achieves more than 30% reduction in bit-rate compared to its predecessor VP9 for the same decoded video quality. This paper provides a technical overview of the AV1 codec design that enables the compression performance gains with considerations for hardware feasibility.,2020-08-13T19:54:39Z,2021-02-08T20:06:55Z,http://arxiv.org/abs/2008.06091v2,http://arxiv.org/pdf/2008.06091v2,eess.IV
Modelling multi-cell edge video analytics,"Jaume Anguera Peris, Viktoria Fodor","Edge intelligence is a scalable solution for analyzing distributed data, but it cannot provide reliable services in large-scale cellular networks unless the inherent aspects of fading and interference are also taken into consideration. In this paper, we present the first mathematical framework for modelling edge video analytics in multi-cell cellular systems. We derive the expressions for the coverage probability, the ergodic capacity, the probability of successfully completing the video analytics within a target delay requirement, and the effective frame rate. We also analyze the effect of the system parameters on the accuracy of the detection algorithm, the supported frame rate at the edge server, and the system fairness.",2022-02-16T17:20:31Z,2022-02-16T17:20:31Z,http://arxiv.org/abs/2202.08200v1,http://arxiv.org/pdf/2202.08200v1,eess.SP
Modeling the Energy Consumption of HEVC Intra Decoding,"Christian Herglotz, Dominic Springer, Andrea Eichenseer, André Kaup","Battery life is one of the major limitations to mobile device use, which makes research on energy efficient soft- and hardware an important task. This paper investigates the energy required by a CPU when decoding compressed bitstream videos on mobile platforms. A model is derived that describes the energy consumption of the new HEVC decoder for intra coded videos. We show that the relative estimation error of the model is smaller than 3.2% and that the model can be used to build encoders aiming at minimizing decoding energy.",2022-03-03T15:05:14Z,2022-03-03T15:05:14Z,http://arxiv.org/abs/2203.01755v1,http://arxiv.org/pdf/2203.01755v1,eess.IV
Estimating the HEVC Decoding Energy Using the Decoder Processing Time,"Christian Herglotz, Elisabeth Walencik, André Kaup",This paper presents a method to accurately estimate the required decoding energy for a given HEVC software decoding solution. We show that the decoder's processing time as returned by common C++ and UNIX functions is a highly suitable parameter to obtain valid estimations for the actual decoding energy. We verify this hypothesis by performing an exhaustive measurement series using different decoder setups and video bit streams. Our findings can be used by developers and researchers in the search for new energy saving video compression algorithms.,2022-03-03T15:25:45Z,2022-03-03T15:25:45Z,http://arxiv.org/abs/2203.01767v1,http://arxiv.org/pdf/2203.01767v1,eess.IV
Enhancing VVC with Deep Learning based Multi-Frame Post-Processing,"Duolikun Danier, Chen Feng, Fan Zhang, David Bull","This paper describes a CNN-based multi-frame post-processing approach based on a perceptually-inspired Generative Adversarial Network architecture, CVEGAN. This method has been integrated with the Versatile Video Coding Test Model (VTM) 15.2 to enhance the visual quality of the final reconstructed content. The evaluation results on the CLIC 2022 validation sequences show consistent coding gains over the original VVC VTM at the same bitrates when assessed by PSNR. The integrated codec has been submitted to the Challenge on Learned Image Compression (CLIC) 2022 (video track), and the team name associated with this submission is BVI_VC.",2022-05-19T10:28:35Z,2022-05-19T10:28:35Z,http://arxiv.org/abs/2205.09458v1,http://arxiv.org/pdf/2205.09458v1,eess.IV
Video Decoding Energy Reduction Using Temporal-Domain Filtering,"Christian Herglotz, Matthias Kränzler, Robert Ludwig, André Kaup","In this paper, we study decoding energy reduction opportunities using temporal-domain filtering and subsampling methods. In particular, we study spatiotemporal filtering using a contrast sensitivity function and temporal downscaling, i.e., frame rate reduction. We apply these concepts as a pre-filtering to the video before compression and evaluate the bitrate, the decoding energy, and the visual quality with a dedicated metric targeting temporally down-scaled sequences. We find that decoding energy savings yield 35% when halving the frame rate and that spatiotemporal filtering can lead to up to 5% of additional savings, depending on the content.",2023-06-12T07:38:09Z,2023-06-12T07:38:09Z,http://arxiv.org/abs/2306.06917v1,http://arxiv.org/pdf/2306.06917v1,eess.IV
Efficient coding of 360° videos exploiting inactive regions in   projection formats,"Christian Herglotz, Mohammadreza Jamali, Stéphane Coulombe, Carlos Vazquez, Ahmad Vakili","This paper presents an efficient method for encoding common projection formats in 360$^\circ$ video coding, in which we exploit inactive regions. These regions are ignored in the reconstruction of the equirectangular format or the viewport in virtual reality applications. As the content of these pixels is irrelevant, we neglect the corresponding pixel values in ratedistortion optimization, residual transformation, as well as inloop filtering and achieve bitrate savings of up to 10%.",2023-07-17T09:33:46Z,2023-07-17T09:33:46Z,http://arxiv.org/abs/2307.08344v1,http://arxiv.org/pdf/2307.08344v1,eess.IV
A Neural Enhancement Post-Processor with a Dynamic AV1 Encoder   Configuration Strategy for CLIC 2024,"Darren Ramsook, Anil Kokaram","At practical streaming bitrates, traditional video compression pipelines frequently lead to visible artifacts that degrade perceptual quality. This submission couples the effectiveness of a neural post-processor with a different dynamic optimsation strategy for achieving an improved bitrate/quality compromise. The neural post-processor is refined via adversarial training and employs perceptual loss functions. By optimising the post-processor and encoder directly our method demonstrates significant improvement in video fidelity. The neural post-processor achieves substantial VMAF score increases of +6.72 and +1.81 at bitrates of 50 kb/s and 500 kb/s respectively.",2024-01-31T17:34:56Z,2024-01-31T17:34:56Z,http://arxiv.org/abs/2401.18021v1,http://arxiv.org/pdf/2401.18021v1,eess.IV
Improved Encoding for Overfitted Video Codecs,"Thomas Leguay, Théo Ladune, Pierrick Philippe, Olivier Deforges","Overfitted neural video codecs offer a decoding complexity orders of magnitude smaller than their autoencoder counterparts. Yet, this low complexity comes at the cost of limited compression efficiency, in part due to their difficulty capturing accurate motion information. This paper proposes to guide motion information learning with an optical flow estimator. A joint rate-distortion optimization is also introduced to improve rate distribution across the different frames. These contributions maintain a low decoding complexity of 1300 multiplications per pixel while offering compression performance close to the conventional codec HEVC and outperforming other overfitted codecs. This work is made open-source at https://orange-opensource. github.io/Cool-Chic/",2025-01-28T14:16:28Z,2025-01-28T14:16:28Z,http://arxiv.org/abs/2501.16976v1,http://arxiv.org/pdf/2501.16976v1,eess.IV
Advanced Geometry Surface Coding for Dynamic Point Cloud Compression,"Jian Xiong, Hao Gao, Miaohui Wang, Hongliang Li, King Ngi Ngan, Weisi Lin","In video-based dynamic point cloud compression (V-PCC), 3D point clouds are projected onto 2D images for compressing with the existing video codecs. However, the existing video codecs are originally designed for natural visual signals, and it fails to account for the characteristics of point clouds. Thus, there are still problems in the compression of geometry information generated from the point clouds. Firstly, the distortion model in the existing rate-distortion optimization (RDO) is not consistent with the geometry quality assessment metrics. Secondly, the prediction methods in video codecs fail to account for the fact that the highest depth values of a far layer is greater than or equal to the corresponding lowest depth values of a near layer. This paper proposes an advanced geometry surface coding (AGSC) method for dynamic point clouds (DPC) compression. The proposed method consists of two modules, including an error projection model-based (EPM-based) RDO and an occupancy map-based (OM-based) merge prediction. Firstly, the EPM model is proposed to describe the relationship between the distortion model in the existing video codec and the geometry quality metric. Secondly, the EPM-based RDO method is presented to project the existing distortion model on the plane normal and is simplified to estimate the average normal vectors of coding units (CUs). Finally, we propose the OM-based merge prediction approach, in which the prediction pixels of merge modes are refined based on the occupancy map. Experiments tested on the standard point clouds show that the proposed method achieves an average 9.84\% bitrate saving for geometry compression.",2021-03-11T09:24:05Z,2021-03-11T09:24:05Z,http://arxiv.org/abs/2103.06549v1,http://arxiv.org/pdf/2103.06549v1,eess.IV
Quality-driven Variable Frame-Rate for Green Video Coding in Broadcast   Applications,"Glenn Herrou, Charles Bonnineau, Wassim Hamidouche, Patrick Dumenil, Jerome Fournier, Luce Morin","The Digital Video Broadcasting (DVB) has proposed to introduce the Ultra-High Definition services in three phases: UHD-1 phase 1, UHD-1 phase 2 and UHD-2. The UHD-1 phase 2 specification includes several new features such as High Dynamic Range (HDR) and High Frame-Rate (HFR). It has been shown in several studies that HFR (+100 fps) enhances the perceptual quality and that this quality enhancement is content-dependent. On the other hand, HFR brings several challenges to the transmission chain including codec complexity increase and bit-rate overhead, which may delay or even prevent its deployment in the broadcast echo-system. In this paper, we propose a Variable Frame Rate (VFR) solution to determine the minimum (critical) frame-rate that preserves the perceived video quality of HFR video. The frame-rate determination is modeled as a 3-class classification problem which consists in dynamically and locally selecting one frame-rate among three: 30, 60 and 120 frames per second. Two random forests classifiers are trained with a ground truth carefully built by experts for this purpose. The subjective results conducted on ten HFR video contents, not included in the training set, clearly show the efficiency of the proposed solution enabling to locally determine the lowest possible frame-rate while preserving the quality of the HFR content. Moreover, our VFR solution enables significant bit-rate savings and complexity reductions at both encoder and decoder sides.",2020-12-29T15:11:31Z,2020-12-29T15:11:31Z,http://arxiv.org/abs/2012.14796v1,http://arxiv.org/pdf/2012.14796v1,eess.SP
Deep-based Film Grain Removal and Synthesis,"Zoubida Ameur, Wassim Hamidouche, Edouard François, Miloš Radosavljević, Daniel Menard, Claire-Hélène Demarty","In this paper, deep learning-based techniques for film grain removal and synthesis that can be applied in video coding are proposed. Film grain is inherent in analog film content because of the physical process of capturing images and video on film. It can also be present in digital content where it is purposely added to reflect the era of analog film and to evoke certain emotions in the viewer or enhance the perceived quality. In the context of video coding, the random nature of film grain makes it both difficult to preserve and very expensive to compress. To better preserve it while compressing the content efficiently, film grain is removed and modeled before video encoding and then restored after video decoding. In this paper, a film grain removal model based on an encoder-decoder architecture and a film grain synthesis model based on a \ac{cgan} are proposed. Both models are trained on a large dataset of pairs of clean (grain-free) and grainy images. Quantitative and qualitative evaluations of the developed solutions were conducted and showed that the proposed film grain removal model is effective in filtering film grain at different intensity levels using two configurations: 1) a non-blind configuration where the film grain level of the grainy input is known and provided as input, 2) a blind configuration where the film grain level is unknown. As for the film grain synthesis task, the experimental results show that the proposed model is able to reproduce realistic film grain with a controllable intensity level specified as input.",2022-06-15T09:25:42Z,2022-06-15T09:25:42Z,http://arxiv.org/abs/2206.07411v1,http://arxiv.org/pdf/2206.07411v1,eess.IV
Enhancing HDR Video Compression through CNN-based Effective Bit Depth   Adaptation,"Chen Feng, Zihao Qi, Duolikun Danier, Fan Zhang, Xiaozhong Xu, Shan Liu, David Bull","It is well known that high dynamic range (HDR) video can provide more immersive visual experiences compared to conventional standard dynamic range content. However, HDR content is typically more challenging to encode due to the increased detail associated with the wider dynamic range. In this paper, we improve HDR compression performance using the effective bit depth adaptation approach (EBDA). This method reduces the effective bit depth of the original video content before encoding and reconstructs the full bit depth using a CNN-based up-sampling method at the decoder. In this work, we modify the MFRNet network architecture to enable multiple frame processing, and the new network, multi-frame MFRNet, has been integrated into the EBDA framework using two Versatile Video Coding (VVC) host codecs: VTM 16.2 and the Fraunhofer Versatile Video Encoder (VVenC 1.4.0). The proposed approach was evaluated under the JVET HDR Common Test Conditions using the Random Access configuration. The results show coding gains over both the original VVC VTM 16.2 and VVenC 1.4.0 (w/o EBDA) on JVET HDR tested sequences, with average bitrate savings of 2.9% (over VTM) and 4.8% (against VVenC) based on the Bjontegaard Delta measurement. The source code of multi-frame MFRNet has been released at https://github.com/fan-aaron-zhang/MF-MFRNet.",2022-07-18T14:27:54Z,2022-07-18T14:27:54Z,http://arxiv.org/abs/2207.08634v1,http://arxiv.org/pdf/2207.08634v1,eess.IV
Modeling of Energy Consumption and Streaming Video QoE using a   Crowdsourcing Dataset,"Christian Herglotz, Werner Robitza, Matthias Kränzler, André Kaup, Alexander Raake","In the past decade, we have witnessed an enormous growth in the demand for online video services. Recent studies estimate that nowadays, more than 1% of the global greenhouse gas emissions can be attributed to the production and use of devices performing online video tasks. As such, research on the true power consumption of devices and their energy efficiency during video streaming is highly important for a sustainable use of this technology. At the same time, over-the-top providers strive to offer high-quality streaming experiences to satisfy user expectations. Here, energy consumption and QoE partly depend on the same system parameters. Hence, a joint view is needed for their evaluation. In this paper, we perform a first analysis of both end-user power efficiency and Quality of Experience of a video streaming service. We take a crowdsourced dataset comprising 447,000 streaming events from YouTube and estimate both the power consumption and perceived quality. The power consumption is modeled based on previous work which we extended towards predicting the power usage of different devices and codecs. The user-perceived QoE is estimated using a standardized model. Our results indicate that an intelligent choice of streaming parameters can optimize both the QoE and the power efficiency of the end user device. Further, the paper discusses limitations of the approach and identifies directions for future research.",2022-10-11T13:37:11Z,2022-10-11T13:37:11Z,http://arxiv.org/abs/2210.05444v1,http://arxiv.org/pdf/2210.05444v1,eess.IV
I$^2$VC: A Unified Framework for Intra- & Inter-frame Video Compression,"Meiqin Liu, Chenming Xu, Yukai Gu, Chao Yao, Yao Zhao","Video compression aims to reconstruct seamless frames by encoding the motion and residual information from existing frames. Previous neural video compression methods necessitate distinct codecs for three types of frames (I-frame, P-frame and B-frame), which hinders a unified approach and generalization across different video contexts. Intra-codec techniques lack the advanced Motion Estimation and Motion Compensation (MEMC) found in inter-codec, leading to fragmented frameworks lacking uniformity. Our proposed Intra- & Inter-frame Video Compression (I$^2$VC) framework employs a single spatio-temporal codec that guides feature compression rates according to content importance. This unified codec transforms the dependence across frames into a conditional coding scheme, thus integrating intra- and inter-frame compression into one cohesive strategy. Given the absence of explicit motion data, achieving competent inter-frame compression with only a conditional codec poses a challenge. To resolve this, our approach includes an implicit inter-frame alignment mechanism. With the pre-trained diffusion denoising process, the utilization of a diffusion-inverted reference feature rather than random noise supports the initial compression state. This process allows for selective denoising of motion-rich regions based on decoded features, facilitating accurate alignment without the need for MEMC. Our experimental findings, across various compression configurations (AI, LD and RA) and frame types, prove that I$^2$VC outperforms the state-of-the-art perceptual learned codecs. Impressively, it exhibits a 58.4% enhancement in perceptual reconstruction performance when benchmarked against the H.266/VVC standard (VTM). Official implementation can be found at https://github.com/GYukai/I2VC.",2024-05-23T09:07:35Z,2024-06-01T05:22:34Z,http://arxiv.org/abs/2405.14336v3,http://arxiv.org/pdf/2405.14336v3,eess.IV
Exploiting Change Blindness for Video Coding: Perspectives from a Less   Promising User Study,"Mitra Amiri, Steven Le Moan, Christian Herglotz","What the human visual system can perceive is strongly limited by the capacity of our working memory and attention. Such limitations result in the human observer's inability to perceive large-scale changes in a stimulus, a phenomenon known as change blindness. In this paper, we started with the premise that this phenomenon can be exploited in video coding, especially HDR-video compression where the bitrate is high. We designed an HDR-video encoding approach that relies on spatially and temporally varying quantization parameters within the framework of HEVC video encoding. In the absence of a reliable change blindness prediction model, to extract compression candidate regions (CCR) we used an existing saliency prediction algorithm. We explored different configurations and carried out a subjective study to test our hypothesis. While our methodology did not lead to significantly superior performance in terms of the ratio between perceived quality and bitrate, we were able to determine potential flaws in our methodology, such as the employed saliency model for CCR prediction (chosen for computational efficiency, but eventually not sufficiently accurate), as well as a very strong subjective bias due to observers priming themselves early on in the experiment about the type of artifacts they should look for, thus creating a scenario with little ecological validity.",2024-07-31T16:36:14Z,2024-07-31T16:36:14Z,http://arxiv.org/abs/2408.00052v1,http://arxiv.org/pdf/2408.00052v1,eess.IV
Adaptive Heart Rate Estimation from Face Videos,"Utkarsh Sharma, Terumi Umematsu, Masanori Tsujikawa, Yoshifumi Onishi","We propose a novel heart rate (HR) estimation method from facial videos that dynamically adapts the HR pulse extraction algorithm to separately deal with noise from 'rigid' head motion and 'non-rigid' facial expression. We first identify the noise type, based on which, we apply specific noise removal steps. Experiments performed on popular database show that the proposed method reduces HR estimation error by over 32%.",2019-04-18T05:28:27Z,2019-04-18T05:28:27Z,http://arxiv.org/abs/1905.12366v1,http://arxiv.org/pdf/1905.12366v1,eess.SP
Bitrate Ladder Prediction Methods for Adaptive Video Streaming: A Review   and Benchmark,"Ahmed Telili, Wassim Hamidouche, Hadi Amirpour, Sid Ahmed Fezza, Luce Morin, Christian Timmerer","HTTP adaptive streaming (HAS) has emerged as a widely adopted approach for over-the-top (OTT) video streaming services, due to its ability to deliver a seamless streaming experience. A key component of HAS is the bitrate ladder, which provides the encoding parameters (e.g., bitrate-resolution pairs) to encode the source video. The representations in the bitrate ladder allow the client's player to dynamically adjust the quality of the video stream based on network conditions by selecting the most appropriate representation from the bitrate ladder. The most straightforward and lowest complexity approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly exhaustive search encoding. This paper provides a comprehensive review of various methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study focusing exclusively on various learning-based approaches for predicting content-optimized bitrate ladders across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art encoders, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides baseline methods and insights, which will be valuable for future research in the field of bitrate ladder prediction. The source code of the proposed benchmark and the dataset will be made publicly available upon acceptance of the paper.",2023-10-23T17:58:24Z,2023-10-30T19:11:04Z,http://arxiv.org/abs/2310.15163v2,http://arxiv.org/pdf/2310.15163v2,eess.IV
Rate Splitting Multiple Access-Enabled Adaptive Panoramic Video Semantic   Transmission,"Haixiao Gao, Mengying Sun, Xiaodong Xu, Shujun Han, Bizhu Wang, Jingxuan Zhang, Ping Zhang","In this paper, we propose an adaptive panoramic video semantic transmission (APVST) framework enabled by rate splitting multiple access (RSMA). The APVST framework consists of a semantic transmitter and receiver, utilizing a deep joint source-channel coding structure to adaptively extract and encode semantic features from panoramic frames. To achieve higher spectral efficiency and conserve bandwidth, APVST employs an entropy model and a dimension-adaptive module to control the transmission rate. Additionally, we take weighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and weighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion evaluation metrics for panoramic videos and design a weighted self-attention module for APVST. This module integrates weights and feature maps to enhance the quality of the immersive experience. Considering the overlap in the field of view when users watch panoramic videos, we further utilize RSMA to split the required panoramic video semantic streams into common and private messages for transmission. We propose an RSMA-enabled semantic stream transmission scheme and formulate a joint problem of latency and immersive experience quality by optimizing the allocation ratios of power, common rate, and channel bandwidth, aiming to maximize the quality of service (QoS) scores for users. To address the above problem, we propose a deep reinforcement learning algorithm based on proximal policy optimization (PPO) with high efficiency to handle dynamically changing environments. Simulation results demonstrate that our proposed APVST framework saves up to 20% and 50% of channel bandwidth compared to other semantic and traditional video transmission schemes, respectively. Moreover, our study confirms the efficiency of RSMA in panoramic video transmission, achieving performance gains of 13% and 20% compared to NOMA and OFDMA.",2024-02-26T14:04:54Z,2024-06-23T09:32:06Z,http://arxiv.org/abs/2402.16581v2,http://arxiv.org/pdf/2402.16581v2,eess.IV
Convolutional neural networks automate detection for tracking of   submicron scale particles in 2D and 3D,"Jay M. Newby, Alison M. Schaefer, Phoebe T. Lee, M. Gregory Forest, Samuel K. Lai","Particle tracking is a powerful biophysical tool that requires conversion of large video files into position time series, i.e. traces of the species of interest for data analysis. Current tracking methods, based on a limited set of input parameters to identify bright objects, are ill-equipped to handle the spectrum of spatiotemporal heterogeneity and poor signal-to-noise ratios typically presented by submicron species in complex biological environments. Extensive user involvement is frequently necessary to optimize and execute tracking methods, which is not only inefficient but introduces user bias. To develop a fully automated tracking method, we developed a convolutional neural network for particle localization from image data, comprised of over 6,000 parameters, and employed machine learning techniques to train the network on a diverse portfolio of video conditions. The neural network tracker provides unprecedented automation and accuracy, with exceptionally low false positive and false negative rates on both 2D and 3D simulated videos and 2D experimental videos of difficult-to-track species.",2017-04-10T18:39:46Z,2018-10-06T18:32:09Z,http://arxiv.org/abs/1704.03009v2,http://arxiv.org/pdf/1704.03009v2,q-bio.QM
LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium   imaging videos,"Elke Kirschbaum, Manuel Haußmann, Steffen Wolf, Hannah Sonntag, Justus Schneider, Shehabeldin Elzoheiry, Oliver Kann, Daniel Durstewitz, Fred A. Hamprecht","Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or ""motifs"", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction. Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used. An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance. In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.",2018-06-26T13:21:48Z,2019-02-22T12:03:29Z,http://arxiv.org/abs/1806.09963v3,http://arxiv.org/pdf/1806.09963v3,q-bio.NC
Respiratory Rate Estimation from Face Videos,"Mingliang Chen, Qiang Zhu, Harrison Zhang, Min Wu, Quanzeng Wang","Vital signs, such as heart rate (HR), heart rate variability (HRV), respiratory rate (RR), are important indicators for a person's health. Vital signs are traditionally measured with contact sensors, and may be inconvenient and cause discomfort during continuous monitoring. Commercial cameras are promising contact-free sensors, and remote photoplethysmography (rPPG) have been studied to remotely monitor heart rate from face videos. For remote RR measurement, most prior art was based on small periodical motions of chest regions caused by breathing cycles, which are vulnerable to subjects' voluntary movements. This paper explores remote RR measurement based on rPPG obtained from face videos. The paper employs motion compensation, two-phase temporal filtering, and signal pruning to capture signals with high quality. The experimental results demonstrate that the proposed framework can obtain accurate RR results and can provide HR, HRV and RR measurement synergistically in one framework.",2019-09-08T16:43:46Z,2019-09-08T16:43:46Z,http://arxiv.org/abs/1909.03503v1,http://arxiv.org/pdf/1909.03503v1,eess.IV
Video-Capable Ultrasonic Wireless Communications through Biological   Tissues,"Gizem Tabak, Sijung Yang, Rita Miller, Michael Oelze, Andrew Singer","Wireless implanted medical devices (IMDs) provide ease and comfort to an increasing number of patients and physicians. Currently, radiofrequency electromagnetic waves are the most commonly used method for communicating wirelessly with IMDs. However, due to the restrictions on the available bandwidth and the employable power, data rates of RF-based IMDs are limited to 267 kbps. Considering standard definition video streaming requiring 1.2 mbps and high definition requiring 3 mbps bitrates, it is not possible to use such devices for high data rate communication applications such as video streaming. In this work, an alternative method that utilizes ultrasonic waves for IMDs to relay information at high data rates is introduced. Advanced signal processing and communication techniques are tailored to realize the full potential of the ultrasonic channel through biological tissues. Consequently, the experiments demonstrate that video-capable data rates can be achieved with mm-sized transducers communicating through water, ex vivo beef liver, ex vivo pork chop, and in situ through the rabbit abdomen.",2019-09-29T00:17:36Z,2020-03-05T16:04:22Z,http://arxiv.org/abs/1909.13172v2,http://arxiv.org/pdf/1909.13172v2,eess.SP
Sensor-aided block matching algorithm for translational motion   estimation through a depth map,"Karim El Khoury, Pascal Pellegrin, Antonin Descampe, Sébastien Lugan, Benoit Macq","A large number of cameras embedded on smart-phones, drones or inside cars have a direct access to external motion sensing from gyroscopes and accelerometers. On these power-limited devices, video compression must be of low-complexity. For this reason, we propose a ""Sensor-Aided Block Matching Algorithm"" which exploits the presence of a motion sensor synchronized with a camera to reduce the complexity of the motion estimation process in an inter-frame video codec. Our solution extends the work previously done on rotational motion estimation to an original estimation of the translational motion through a depth map. The proposed algorithm provides a complexity reduction factor of approximately 2.5 compared to optimized block-matching motion compensated inter-frame video codecs while maintaining high image quality and providing as by-product a depth map of the scene.",2020-01-31T13:45:19Z,2020-01-31T13:45:19Z,http://arxiv.org/abs/2001.11829v1,http://arxiv.org/pdf/2001.11829v1,eess.IV
HSMF-Net: Semantic Viewport Prediction for Immersive Telepresence and   On-Demand 360-degree Video,"Tamay Aykut, Basak Gülezyüz, Bernd Girod, Eckehard Steinbach","The acceptance of immersive telepresence systems is impeded by the latency that is present when mediating the realistic feeling of presence in a remote environment to a local human user. A disagreement between the user's ego-motion and the visual response provokes the emergence of motion sickness. Viewport or head motion (HM) prediction techniques play a key role in compensating the noticeable delay between the user and the remote site. We present a deep learning-based viewport prediction paradigm that fuses past HM trajectories with scene semantics in a late-fusion manner. Real HM profiles are used to evaluate the proposed approach. A mean compensation rate as high as 99.99% is obtained, clearly outperforming the state-of-the-art. An on-demand 360-degree video streaming framework is presented to prove its general validity. The proposed approach increases the perceived video quality while requiring a significantly lower transmission rate.",2020-09-08T22:45:40Z,2020-09-08T22:45:40Z,http://arxiv.org/abs/2009.04015v1,http://arxiv.org/pdf/2009.04015v1,eess.IV
Conditional Coding for Flexible Learned Video Compression,"Théo Ladune, Pierrick Philippe, Wassim Hamidouche, Lu Zhang, Olivier Déforges","This paper introduces a novel framework for end-to-end learned video coding. Image compression is generalized through conditional coding to exploit information from reference frames, allowing to process intra and inter frames with the same coder. The system is trained through the minimization of a rate-distortion cost, with no pre-training or proxy loss. Its flexibility is assessed under three coding configurations (All Intra, Low-delay P and Random Access), where it is shown to achieve performance competitive with the state-of-the-art video codec HEVC.",2021-04-16T07:20:59Z,2021-04-28T06:32:25Z,http://arxiv.org/abs/2104.07930v3,http://arxiv.org/pdf/2104.07930v3,"eess.IV, eess.SP"
Model Selection CNN-based VVC QualityEnhancement,"Fatemeh Nasiri, Wassim Hamidouche, Luce Morin, Nicolas Dhollande, Gildas Cocherel","Artifact removal and filtering methods are inevitable parts of video coding. On one hand, new codecs and compression standards come with advanced in-loop filters and on the other hand, displays are equipped with high capacity processing units for post-treatment of decoded videos. This paper proposes a Convolutional Neural Network (CNN)-based post-processing algorithm for intra and inter frames of Versatile Video Coding (VVC) coded streams. Depending on the frame type, this method benefits from normative prediction signal by feeding it as an additional input along with reconstructed signal and a Quantization Parameter (QP)-map to the CNN. Moreover, an optional Model Selection (MS) strategy is adopted to pick the best trained model among available ones at the encoder side and signal it to the decoder side. This MS strategy is applicable at both frame level and block level. The experiments under the Random Access (RA) configuration of the VVC Test Model (VTM-10.0) show that the proposed prediction-aware algorithm can bring an additional BD-BR gain of -1.3% compared to the method without the prediction information. Furthermore, the proposed MS scheme brings -0.5% more BD-BR gain on top of the prediction-aware method.",2021-05-07T15:43:22Z,2021-05-07T15:43:22Z,http://arxiv.org/abs/2105.03338v1,http://arxiv.org/pdf/2105.03338v1,eess.IV
Multi-source Domain Adaptation Using Gradient Reversal Layer for Mitotic   Cell Detection,Satoshi Kondo,This is a write-up of our method submitted to Mitosis Domain Generalization (MIDOG 2021) Challenge held in MICCAI2021 conference.,2021-09-02T01:19:26Z,2021-09-02T01:19:26Z,http://arxiv.org/abs/2109.01503v1,http://arxiv.org/pdf/2109.01503v1,eess.IV
Beyond Bjøntegaard: Limits of Video Compression Performance   Comparisons,"Christian Herglotz, Matthias Kränzler, Ruben Mons, André Kaup","For 20 years, the gold standard to evaluate the performance of video codecs is to calculate average differences between ratedistortion curves, also called the ""Bj{\o}ntegaard Delta"". With the help of this tool, the compression performance of codecs can be compared. In the past years, we could observe that the calculus was also deployed for other metrics than bitrate and distortion in terms of peak signal-to-noise ratio, for example other quality metrics such as video multi-method assessment fusion or hardware-dependent metrics such as the decoding energy. However, it is unclear whether the Bj{\o}ntegaard Delta is a valid way to evaluate these metrics. To this end, this paper reviews several interpolation methods and evaluates their accuracy using different performancemetrics. As a result, we propose to use a novel approach based on Akima interpolation, which returns the most accurate results for a large variety of performance metrics. The approximation accuracy of this new method is determined to be below a bound of 1.5%.",2022-02-25T08:56:36Z,2022-06-24T08:40:52Z,http://arxiv.org/abs/2202.12565v3,http://arxiv.org/pdf/2202.12565v3,eess.IV
A CNN-based Post-Processor for Perceptually-Optimized Immersive Media   Compression,"Angeliki Katsenou, Fan Zhang, David Bull","In recent years, resolution adaptation based on deep neural networks has enabled significant performance gains for conventional (2D) video codecs. This paper investigates the effectiveness of spatial resolution resampling in the context of immersive content. The proposed approach reduces the spatial resolution of input multi-view videos before encoding, and reconstructs their original resolution after decoding. During the up-sampling process, an advanced CNN model is used to reduce potential re-sampling, compression, and synthesis artifacts. This work has been fully tested with the TMIV coding standard using a Versatile Video Coding (VVC) codec. The results demonstrate that the proposed method achieves a significant rate-quality performance improvement for the majority of the test sequences, with an average BD-VMAF improvement of 3.07 overall sequences.",2022-02-25T17:59:46Z,2022-02-25T17:59:46Z,http://arxiv.org/abs/2202.12852v1,http://arxiv.org/pdf/2202.12852v1,eess.IV
Decoding-Energy-Rate-Distortion Optimization for Video Coding,"Christian Herglotz, Andreas Heindel, André Kaup","This paper presents a method for generating coded video bit streams requiring less decoding energy than conventionally coded bit streams. To this end, we propose extending the standard rate-distortion optimization approach to also consider the decoding energy. In the encoder, the decoding energy is estimated during runtime using a feature-based energy model. These energy estimates are then used to calculate decoding-energy-rate-distortion costs that are minimized by the encoder. This ultimately leads to optimal trade-offs between these three parameters. Therefore, we introduce the mathematical theory for describing decoding-energy-rate-distortion optimization and the proposed encoder algorithm is explained in detail. For rate-energy control, a new encoder parameter is introduced. Finally, measurements of the software decoding process for HEVC-coded bit streams are performed. Results show that this approach can lead to up to 30% of decoding energy reduction at a constant visual objective quality when accepting a bitrate increase at the same order of magnitude.",2022-03-02T13:36:41Z,2022-03-02T13:36:41Z,http://arxiv.org/abs/2203.01099v1,http://arxiv.org/pdf/2203.01099v1,eess.IV
Personalized QoE Enhancement for Adaptive Video Streaming: A Digital   Twin-Assisted Scheme,"Xinyu Huang, Conghao Zhou, Wen Wu, Mushu Li, Huaqing Wu, Xuemin, Shen","In this paper, we present a digital twin (DT)-assisted adaptive video streaming scheme to enhance personalized quality-of-experience (PQoE). Since PQoE models are user-specific and time-varying, existing schemes based on universal and time-invariant PQoE models may suffer from performance degradation. To address this issue, we first propose a DT-assisted PQoE model construction method to obtain accurate user-specific PQoE models. Specifically, user DTs (UDTs) are respectively constructed for individual users, which can acquire and utilize users' data to accurately tune PQoE model parameters in real time. Next, given the obtained PQoE models, we formulate a resource management problem to maximize the overall long-term PQoE by taking the dynamics of user' locations, video content requests, and buffer statuses into account. To solve this problem, a deep reinforcement learning algorithm is developed to jointly determine segment version selection, and communication and computing resource allocation. Simulation results on the real-world dataset demonstrate that the proposed scheme can effectively enhance PQoE compared with benchmark schemes.",2022-05-09T03:00:06Z,2022-05-09T03:00:06Z,http://arxiv.org/abs/2205.04014v1,http://arxiv.org/pdf/2205.04014v1,eess.IV
Modeling the HEVC Encoding Energy Using the Encoder Processing Time,"Geetha Ramasubbu, André Kaup, Christian Herglotz","The global significance of energy consumption of video communication renders research on the energy need of video coding an important task. To do so, usually, a dedicated setup is needed that measures the energy of the encoding and decoding system. However, such measurements are costly and complex. To this end, this paper presents the results of an exhaustive measurement series using the x265 encoder implementation of HEVC and analyzes the relation between encoding time and encoding energy. Finally, we introduce a simple encoding energy estimation model which employs the encoding time of a lightweight encoding process to estimate the encoding energy of complex encoding configurations. The proposed model reaches a mean estimation error of 11.35% when averaged over all presets. The results from this work are useful when the encoding energy estimate is required to develop new energy-efficient video compression algorithms.",2022-07-06T13:41:38Z,2024-10-02T01:35:38Z,http://arxiv.org/abs/2207.02676v4,http://arxiv.org/pdf/2207.02676v4,eess.IV
Spatio-temporal prediction in video coding by spatially refined motion   compensation,"Jürgen Seiler, André Kaup","The purpose of this contribution is to introduce a new method of signal prediction in video coding. Unlike most existent prediction methods that either use temporal or use spatial correlations to generate the prediction signal, the proposed method uses spatial and temporal correlations at the same time. The spatio-temporal prediction is obtained by first performing motion compensation for a macroblock, followed by a refinement step that pays attention to the correlations between the macroblock and its surroundings. At the decoder, the refinement step can be performed in the same manner, thus no additional side information has to be transmitted. Implementation of the spatial refinement step into the H.264/AVC video codec leads to reduction in data rate of up to nearly 15% and increase in PSNR of up to 0.75 dB, compared to pure motion compensated prediction.",2022-07-08T09:13:06Z,2022-07-08T09:13:06Z,http://arxiv.org/abs/2207.03766v1,http://arxiv.org/pdf/2207.03766v1,eess.IV
Decoding Energy Modeling For Versatile Video Coding,"Matthias Kränzler, Christian Herglotz, André Kaup","In previous research, it was shown that the software decoding energy demand of High Efficiency Video Coding (HEVC) can be reduced by 15$\%$ by using a decoding-energy-rate-distortion optimization algorithm. To achieve this, the energy demand of the decoder has to be modeled by a bit stream feature-based model with sufficiently high accuracy. Therefore, we propose two bit stream feature-based models for the upcoming Versatile Video Coding (VVC) standard. The newly introduced models are compared with models from literature, which are used for HEVC. An evaluation of the proposed models reveals that the mean estimation error is similar to the results of the literature and yields an estimation error of 1.85% with 10-fold cross-validation.",2022-09-21T11:16:43Z,2022-09-21T11:16:43Z,http://arxiv.org/abs/2209.10266v1,http://arxiv.org/pdf/2209.10266v1,eess.IV
A Comparative Analysis of the Time and Energy Demand of Versatile Video   Coding and High Efficiency Video Coding Reference Decoders,"Matthias Kränzler, Christian Herglotz, André Kaup","This paper investigates the decoding energy and decoding time demand of VTM-7.0 in relation to HM-16.20. We present the first detailed comparison of two video codecs in terms of software decoder energy consumption. The evaluation shows that the energy demand of the VTM decoder is increased significantly compared to HM and that the increase depends on the coding configuration. For the coding configuration randomaccess, we find that the decoding energy is increased by over 80% at a decoding time increase of over 70%. Furthermore, results indicate that the energy demand increases by up to 207% when Single Instruction Multiple Data (SIMD) instructions are disabled, which corresponds to the HM implementation style. By measurements, it is revealed that the coding tools MIP, AMVR, TPM, LFNST, and MTS increase the energy efficiency of the decoder. Furthermore, we propose a new coding configuration based on our analysis, which reduces the energy demand of the VTM decoder by over 17% on average.",2022-09-21T11:51:32Z,2022-09-21T11:51:32Z,http://arxiv.org/abs/2209.10283v1,http://arxiv.org/pdf/2209.10283v1,eess.IV
Compressive Image Classification using Deterministic Sensing Matrices,"Sheel Shah, Kushal Kejriwal",We look at the use of deterministic sensing matrices for compressed sensing and provide worst-case bounds on the classification accuracy of SVMs on compressively sensed data.,2022-10-15T11:53:40Z,2022-10-15T11:53:40Z,http://arxiv.org/abs/2210.10777v1,http://arxiv.org/pdf/2210.10777v1,eess.IV
Multi-rate adaptive transform coding for video compression,"Lyndon R. Duong, Bohan Li, Cheng Chen, Jingning Han","Contemporary lossy image and video coding standards rely on transform coding, the process through which pixels are mapped to an alternative representation to facilitate efficient data compression. Despite impressive performance of end-to-end optimized compression with deep neural networks, the high computational and space demands of these models has prevented them from superseding the relatively simple transform coding found in conventional video codecs. In this study, we propose learned transforms and entropy coding that may either serve as (non)linear drop-in replacements, or enhancements for linear transforms in existing codecs. These transforms can be multi-rate, allowing a single model to operate along the entire rate-distortion curve. To demonstrate the utility of our framework, we augmented the DCT with learned quantization matrices and adaptive entropy coding to compress intra-frame AV1 block prediction residuals. We report substantial BD-rate and perceptual quality improvements over more complex nonlinear transforms at a fraction of the computational cost.",2022-10-25T20:11:42Z,2023-02-18T00:10:04Z,http://arxiv.org/abs/2210.14308v2,http://arxiv.org/pdf/2210.14308v2,eess.IV
Digital Twin-Assisted Collaborative Transcoding for Better User   Satisfaction in Live Streaming,"Xinyu Huang, Mushu Li, Wen Wu, Conghao Zhou, Xuemin Sherman Shen","In this paper, we propose a digital twin (DT)-assisted cloud-edge collaborative transcoding scheme to enhance user satisfaction in live streaming. We first present a DT-assisted transcoding workload estimation (TWE) model for the cloud-edge collaborative transcoding. Particularly, two DTs are constructed for emulating the cloud-edge collaborative transcoding process by analyzing spatial-temporal information of individual videos and transcoding configurations of transcoding queues, respectively. Two light-weight Bayesian neural networks are adopted to fit the TWE models in DTs, respectively. We then formulate a transcoding-path selection problem to maximize long-term user satisfaction within an average service delay threshold, taking into account the dynamics of video arrivals and video requests. The problem is transformed into a standard Markov decision process by using the Lyapunov optimization and solved by a deep reinforcement learning algorithm. Simulation results based on the real-world dataset demonstrate that the proposed scheme can effectively enhance user satisfaction compared with benchmark schemes.",2022-11-13T13:49:35Z,2022-11-13T13:49:35Z,http://arxiv.org/abs/2211.06906v1,http://arxiv.org/pdf/2211.06906v1,"eess.IV, eess.SP"
Quaternion Tensor Completion with Sparseness for Color Video Recovery,"Liqiao Yang, Kit Ian Kou, Jifei Miao, Yang Liu, Maggie Pui Man Hoi","A novel low-rank completion algorithm based on the quaternion tensor is proposed in this paper. This approach uses the TQt-rank of quaternion tensor to maintain the structure of RGB channels throughout the entire process. In more detail, the pixels in each frame are encoded on three imaginary parts of a quaternion as an element in a quaternion matrix. Each quaternion matrix is then stacked into a quaternion tensor. A logarithmic function and truncated nuclear norm are employed to characterize the rank of the quaternion tensor in order to promote the low rankness of the tensor. Moreover, by introducing a newly defined quaternion tensor discrete cosine transform-based (QTDCT) regularization to the low-rank approximation framework, the optimized recovery results can be obtained in the local details of color videos. In particular, the sparsity of the quaternion tensor is reasonably characterized by l1 norm in the QDCT domain. This strategy is optimized via the two-step alternating direction method of multipliers (ADMM) framework. Numerical experimental results for recovering color videos show the obvious advantage of the proposed method over other potential competing approaches.",2022-12-16T09:20:21Z,2022-12-16T09:20:21Z,http://arxiv.org/abs/2212.08361v1,http://arxiv.org/pdf/2212.08361v1,eess.IV
Analysis of displacement compensation methods for wavelet lifting of   medical 3-D thorax CT volume data,"Wolfgang Schnurrer, Jürgen Seiler, Eugen Wige, André Kaup","A huge advantage of the wavelet transform in image and video compression is its scalability. Wavelet-based coding of medical computed tomography (CT) data becomes more and more popular. While much effort has been spent on encoding of the wavelet coefficients, the extension of the transform by a compensation method as in video coding has not gained much attention so far. We will analyze two compensation methods for medical CT data and compare the characteristics of the displacement compensated wavelet transform with video data. We will show that for thorax CT data the transform coding gain can be improved by a factor of 2 and the quality of the lowpass band can be improved by 8 dB in terms of PSNR compared to the original transform without compensation.",2023-01-11T08:04:07Z,2023-01-11T08:04:07Z,http://arxiv.org/abs/2301.04351v1,http://arxiv.org/pdf/2301.04351v1,eess.IV
An Error-Surface-Based Fractional Motion Estimation Algorithm and   Hardware Implementation for VVC,"Shushi Chen, Leilei Huang, Jiahao Liu, Chao Liu, Yibo Fan","Versatile Video Coding (VVC) introduces more coding tools to improve compression efficiency compared to its predecessor High Efficiency Video Coding (HEVC). For inter-frame coding, Fractional Motion Estimation (FME) still has a high computational effort, which limits the real-time processing capability of the video encoder. In this context, this paper proposes an error-surface-based FME algorithm and the corresponding hardware implementation. The algorithm creates an error surface constructed by the Rate-Distortion (R-D) cost of the integer motion vector (IMV) and its neighbors. This method requires no iteration and interpolation, thus reducing the area and power consumption and increasing the throughput of the hardware. The experimental results show that the corresponding BDBR loss is only 0.47% compared to VTM 16.0 in LD-P configuration. The hardware implementation was synthesized using GF 28nm process. It can support 13 different sizes of CU varying from 128x128 to 8x8. The measured throughput can reach 4K@30fps at 400MHz, with a gate count of 192k and power consumption of 12.64 mW. And the throughput can reach 8K@30fps at 631MHz when only quadtree is searched. To the best of our knowledge, this work is the first hardware architecture for VVC FME with interpolation-free strategies",2023-02-13T08:04:52Z,2023-02-13T08:04:52Z,http://arxiv.org/abs/2302.06167v1,http://arxiv.org/pdf/2302.06167v1,eess.IV
Encoder Complexity Control in SVT-AV1 by Speed-Adaptive Preset Switching,"Lena Eichermüller, Gaurang Chaudhari, Ioannis Katsavounidis, Zhijun Lei, Hassene Tmar, André Kaup, Christian Herglotz","Current developments in video encoding technology lead to continuously improving compression performance but at the expense of increasingly higher computational demands. Regarding the online video traffic increases during the last years and the concomitant need for video encoding, encoder complexity control mechanisms are required to restrict the processing time to a sufficient extent in order to find a reasonable trade-off between performance and complexity. We present a complexity control mechanism in SVT-AV1 by using speed-adaptive preset switching to comply with the remaining time budget. This method enables encoding with a user-defined time constraint within the complete preset range with an average precision of 8.9 \% without introducing any additional latencies.",2023-07-11T12:28:04Z,2023-07-11T12:28:04Z,http://arxiv.org/abs/2307.05208v1,http://arxiv.org/pdf/2307.05208v1,eess.IV
Power Modeling for Virtual Reality Video Playback Applications,"Christian Herglotz, Stéphane Coulombe, Ahmad Vakili, André Kaup","This paper proposes a method to evaluate and model the power consumption of modern virtual reality playback and streaming applications on smartphones. Due to the high computational complexity of the virtual reality processing toolchain, the corresponding power consumption is very high, which reduces operating times of battery-powered devices. To tackle this problem, we analyze the power consumption in detail by performing power measurements. Furthermore, we construct a model to estimate the true power consumption with a mean error of less than 3.5%. The model can be used to save power at critical battery levels by changing the streaming video parameters. Particularly, the results show that the power consumption is significantly reduced by decreasing the input video resolution.",2023-07-17T09:23:05Z,2023-07-17T09:23:05Z,http://arxiv.org/abs/2307.08338v1,http://arxiv.org/pdf/2307.08338v1,eess.IV
Full-reference Video Quality Assessment for User Generated Content   Transcoding,"Zihao Qi, Chen Feng, Duolikun Danier, Fan Zhang, Xiaozhong Xu, Shan Liu, David Bull","Unlike video coding for professional content, the delivery pipeline of User Generated Content (UGC) involves transcoding where unpristine reference content needs to be compressed repeatedly. In this work, we observe that existing full-/no-reference quality metrics fail to accurately predict the perceptual quality difference between transcoded UGC content and the corresponding unpristine references. Therefore, they are unsuited for guiding the rate-distortion optimisation process in the transcoding process. In this context, we propose a bespoke full-reference deep video quality metric for UGC transcoding. The proposed method features a transcoding-specific weakly supervised training strategy employing a quality ranking-based Siamese structure. The proposed method is evaluated on the YouTube-UGC VP9 subset and the LIVE-Wild database, demonstrating state-of-the-art performance compared to existing VQA methods.",2023-12-19T16:42:28Z,2023-12-19T16:42:28Z,http://arxiv.org/abs/2312.12317v1,http://arxiv.org/pdf/2312.12317v1,eess.IV
A Video Coding Method Based on Neural Network for CLIC2024,"Zhengang Li, Jingchi Zhang, Yonghua Wang, Xing Zeng, Zhen Zhang, Yunlin Long, Menghu Jia, Ning Wang","This paper presents a video coding scheme that combines traditional optimization methods with deep learning methods based on the Enhanced Compression Model (ECM). In this paper, the traditional optimization methods adaptively adjust the quantization parameter (QP). The key frame QP offset is set according to the video content characteristics, and the coding tree unit (CTU) level QP of all frames is also adjusted according to the spatial-temporal perception information. Block importance mapping technology (BIM) is also introduced, which adjusts the QP according to the block importance. Meanwhile, the deep learning methods propose a convolutional neural network-based loop filter (CNNLF), which is turned on/off based on the rate-distortion optimization at the CTU and frame level. Besides, intra-prediction using neural networks (NN-intra) is proposed to further improve compression quality, where 8 neural networks are used for predicting blocks of different sizes. The experimental results show that compared with ECM-3.0, the proposed traditional methods and adding deep learning methods improve the PSNR by 0.54 dB and 1 dB at 0.05Mbps, respectively; 0.38 dB and 0.71dB at 0.5 Mbps, respectively, which proves the superiority of our method.",2024-01-08T01:48:35Z,2024-01-08T01:48:35Z,http://arxiv.org/abs/2401.03623v1,http://arxiv.org/pdf/2401.03623v1,eess.IV
XLR (piXel Loss Rate): a Lightweight Indicator to Measure Video QoE in   IP Networks,"César Díaz, Pablo Pérez, Julián Cabrera, Jaime Ruiz, Narciso García","A novel Key Quality Indicator for video delivery applications, XLR (piXel Loss Rate), is defined, characterized, and evaluated. The proposed indicator is an objective measure that captures the effects of transmission errors in the received video, has a good correlation with subjective Mean Opinion Scores, and provides comparable results with state-of-the-art Full-Reference metrics. Moreover, XLR can be estimated using only a lightweight analysis on the compressed bitstream, thus allowing a No-Reference operational method. Therefore, XLR can be used for measuring the quality of experience without latency at any network location. Thus, it is a relevant tool for network planning, specially in new high-demanding scenarios. The experiments carried out show the outstanding performance of its linear-dimension score and the reliability of the bitstream-based estimation.",2024-02-08T16:58:13Z,2024-02-08T16:58:13Z,http://arxiv.org/abs/2402.05820v1,http://arxiv.org/pdf/2402.05820v1,eess.IV
Energy- and Quality-Aware Video Request Policy for Wireless Adaptive   Streaming Clients,"César Díaz, Antonio Fernández, Fernando Sacristán, Narciso García","We present a straightforward, non-intrusive adaptive bit rate streaming segment quality selection policy which aims at extending battery lifetime during playback while limiting the impact on the user's quality of experience, thus benefiting consumers of video streaming services. This policy relies on the relationship between the available channel bandwidth and the bit rate of the representations in the quality ladder. It results from the characterization of the energy consumed by smartphones when running adaptive streaming client applications for different network connections (Wifi, 4G, and 5G) and the modeling of the energy consumed as a function of said relationship. Results show that a significant amount of energy can be saved (10 to 30%) by slightly modifying the default policy at the expense of a controlled reduction of video quality.",2024-02-08T20:47:00Z,2024-02-08T20:47:00Z,http://arxiv.org/abs/2402.06050v1,http://arxiv.org/pdf/2402.06050v1,eess.IV
Energy Reduction Opportunities in HDR Video Encoding,"Christian Herglotz, Steven Le Moan, Alexandre Mercat","This paper investigates the energy consumption of video encoding for high dynamic range videos. Specifically, we compare the energy consumption of the compression process using 10-bit input sequences, a tone-mapped 8-bit input sequence at 10-bit internal bit depth, and encoding an 8-bit input sequence using an encoder with an internal bit depth of 8 bit. We find that linear scaling of the luminance and chrominance values leads to degradations of the visual quality, but that significant encoding complexity and thus encoding energy can be saved. An important reason for this is the availability of vector instructions, which are not available for the 10-bit encoder. Furthermore, we find that at sufficiently low target bitrates, the compression efficiency at an internal bit depth of 8 bit exceeds the compression efficiency of regular 10-bit encoding.",2024-06-17T12:56:47Z,2024-06-17T12:56:47Z,http://arxiv.org/abs/2406.11492v1,http://arxiv.org/pdf/2406.11492v1,eess.IV
SVT-AV1 Encoding Bitrate Estimation Using Motion Search Information,"Lena Eichermüller, Gaurang Chaudhari, Ioannis Katsavounidis, Zhijun Lei, Hassene Tmar, Christian Herglotz, André Kaup","Enabling high compression efficiency while keeping encoding energy consumption at a low level, requires prioritization of which videos need more sophisticated encoding techniques. However, the effects vary highly based on the content, and information on how good a video can be compressed is required. This can be measured by estimating the encoded bitstream size prior to encoding. We identified the errors between estimated motion vectors from Motion Search, an algorithm that predicts temporal changes in videos, correlates well to the encoded bitstream size. Combining Motion Search with Random Forests, the encoding bitrate can be estimated with a Pearson correlation of above 0.96.",2024-07-08T13:06:33Z,2024-07-08T13:06:33Z,http://arxiv.org/abs/2407.05900v1,http://arxiv.org/pdf/2407.05900v1,eess.IV
Variable Rate Learned Wavelet Video Coding with Temporal Layer   Adaptivity,"Anna Meyer, André Kaup","Learned wavelet video coders provide an explainable framework by performing discrete wavelet transforms in temporal, horizontal, and vertical dimensions. With a temporal transform based on motion-compensated temporal filtering (MCTF), spatial and temporal scalability is obtained. In this paper, we introduce variable rate support and a mechanism for quality adaption to different temporal layers for a higher coding efficiency. Moreover, we propose a multi-stage training strategy that allows training with multiple temporal layers. Our experiments demonstrate Bj{\o}ntegaard Delta bitrate savings of at least -17% compared to a learned MCTF model without these extensions. Our method also outperforms other learned video coders like DCVC-DC. Training and inference code is available at: https://github.com/FAU-LMS/Learned-pMCTF.",2024-10-21T10:57:08Z,2024-10-21T10:57:08Z,http://arxiv.org/abs/2410.15873v1,http://arxiv.org/pdf/2410.15873v1,eess.IV
The Compositional Nature of Event Representations in the Human Brain,"Andrei Barbu, N. Siddharth, Caiming Xiong, Jason J. Corso, Christiane D. Fellbaum, Catherine Hanson, Stephen José Hanson, Sébastien Hélie, Evguenia Malaia, Barak A. Pearlmutter, Jeffrey Mark Siskind, Thomas Michael Talavage, Ronnie B. Wilbur","How does the human brain represent simple compositions of constituents: actors, verbs, objects, directions, and locations? Subjects viewed videos during neuroimaging (fMRI) sessions from which sentential descriptions of those videos were identified by decoding the brain representations based only on their fMRI activation patterns. Constituents (e.g., ""fold"" and ""shirt"") were independently decoded from a single presentation. Independent constituent classification was then compared to joint classification of aggregate concepts (e.g., ""fold-shirt""); results were similar as measured by accuracy and correlation. The brain regions used for independent constituent classification are largely disjoint and largely cover those used for joint classification. This allows recovery of sentential descriptions of stimulus videos by composing the results of the independent constituent classifiers. Furthermore, classifiers trained on the words one set of subjects think of when watching a video can recognise sentences a different subject thinks of when watching a different video.",2015-05-25T15:49:03Z,2015-05-25T15:49:03Z,http://arxiv.org/abs/1505.06670v1,http://arxiv.org/pdf/1505.06670v1,q-bio.NC
Modeling Generalized Rate-Distortion Functions,"Zhengfang Duanmu, Wentao Liu, Zhou Wang","Many multimedia applications require precise understanding of the rate-distortion characteristics measured by the function relating visual quality to media attributes, for which we term it the generalized rate-distortion (GRD) function. In this study, we explore the GRD behavior of compressed digital videos in a three-dimensional space of bitrate, resolution, and viewing device/condition. Our analysis on a large-scale video dataset reveals that empirical parametric models are systematically biased while exhaustive search methods require excessive computation time to depict the GRD surfaces. By exploiting the properties that all GRD functions share, we develop an Robust Axial-Monotonic Clough-Tocher (RAMCT) interpolation method to model the GRD function. This model allows us to accurately reconstruct the complete GRD function of a source video content from a moderate number of measurements. To further reduce the computational cost, we present a novel sampling scheme based on a probabilistic model and an information measure. The proposed sampling method constructs a sequence of quality queries by minimizing the overall informativeness in the remaining samples. Experimental results show that the proposed algorithm significantly outperforms state-of-the-art approaches in accuracy and efficiency. Finally, we demonstrate the usage of the proposed model in three applications: rate-distortion curve prediction, per-title encoding profile generation, and video encoder comparison.",2019-06-12T14:43:13Z,2019-06-12T14:43:13Z,http://arxiv.org/abs/1906.05178v1,http://arxiv.org/pdf/1906.05178v1,eess.IV
Efficient Multi-Stage Video Denoising with Recurrent Spatio-Temporal   Fusion,"Matteo Maggioni, Yibin Huang, Cheng Li, Shuai Xiao, Zhongqian Fu, Fenglong Song","In recent years, denoising methods based on deep learning have achieved unparalleled performance at the cost of large computational complexity. In this work, we propose an Efficient Multi-stage Video Denoising algorithm, called EMVD, to drastically reduce the complexity while maintaining or even improving the performance. First, a fusion stage reduces the noise through a recursive combination of all past frames in the video. Then, a denoising stage removes the noise in the fused frame. Finally, a refinement stage restores the missing high frequency in the denoised frame. All stages operate on a transform-domain representation obtained by learnable and invertible linear operators which simultaneously increase accuracy and decrease complexity of the model. A single loss on the final output is sufficient for successful convergence, hence making EMVD easy to train. Experiments on real raw data demonstrate that EMVD outperforms the state of the art when complexity is constrained, and even remains competitive against methods whose complexities are several orders of magnitude higher. Further, the low complexity and memory requirements of EMVD enable real-time video denoising on commercial SoC in mobile devices.",2021-03-09T13:08:19Z,2023-03-30T10:29:13Z,http://arxiv.org/abs/2103.05407v2,http://arxiv.org/pdf/2103.05407v2,eess.IV
4-D Epanechnikov Mixture Regression in Light Field Image Compression,"Boning Liu, Yan Zhao, Xiaomeng Jiang, Shigang Wang, Jian Wei","With the emergence of light field imaging in recent years, the compression of its elementary image array (EIA) has become a significant problem. Our coding framework includes modeling and reconstruction. For the modeling, the covariance-matrix form of the 4-D Epanechnikov kernel (4-D EK) and its correlated statistics were deduced to obtain the 4-D Epanechnikov mixture models (4-D EMMs). A 4-D Epanechnikov mixture regression (4-D EMR) was proposed based on this 4-D EK, and a 4-D adaptive model selection (4-D AMLS) algorithm was designed to realize the optimal modeling for a pseudo video sequence (PVS) of the extracted key-EIA. A linear function based reconstruction (LFBR) was proposed based on the correlation between adjacent elementary images (EIs). The decoded images realized a clear outline reconstruction and superior coding efficiency compared to high-efficiency video coding (HEVC) and JPEG 2000 below approximately 0.05 bpp. This work realized an unprecedented theoretical application by (1) proposing the 4-D Epanechnikov kernel theory, (2) exploiting the 4-D Epanechnikov mixture regression and its application in the modeling of the pseudo video sequence of light field images, (3) using 4-D adaptive model selection for the optimal number of models, and (4) employing a linear function-based reconstruction according to the content similarity.",2021-08-14T04:39:10Z,2021-08-14T04:39:10Z,http://arxiv.org/abs/2108.06464v1,http://arxiv.org/pdf/2108.06464v1,eess.IV
Real-Time Video Content Popularity Detection Based on Mean Change Point   Analysis,"Sotiris Skaperas, Lefteris Mamatas, Arsenia Chorti","Video content is responsible for more than 70% of the global IP traffic. Consequently, it is important for content delivery infrastructures to rapidly detect and respond to changes in content popularity dynamics. In this paper, we propose the employment of on-line change point (CP) analysis to implement real-time, autonomous and low-complexity video content popularity detection. Our proposal, denoted as real-time change point detector (RCPD), estimates the existence, the number and the direction of changes on the average number of video visits by combining: (i) off-line and on-line CP detection algorithms; (ii) an improved time-series segmentation heuristic for the reliable detection of multiple CPs; and (iii) two algorithms for the identification of the direction of changes. The proposed detector is validated against synthetic data, as well as a large database of real YouTube video visits. It is demonstrated that the RCPD can accurately identify changes in the average content popularity and the direction of change. In particular, the success rate of the RCPD over synthetic data is shown to exceed 94% for medium and large changes in content popularity. Additionally,the dynamic time warping distance, between the actual and the estimated changes, has been found to range between20sampleson average, over synthetic data, to52samples, in real data.The rapid responsiveness of the RCPD is instrumental in the deployment of real-time, lightweight load balancing solutions, as shown in a real example.",2020-03-26T17:18:41Z,2020-03-26T17:18:41Z,http://arxiv.org/abs/2003.12044v1,http://arxiv.org/pdf/2003.12044v1,eess.SP
A Framework to Map VMAF with the Probability of Just Noticeable   Difference between Video Encoding Recipes,"Jingwen Zhu, Suiyi Ling, Yoann Baveye, Patrick Le Callet","Just Noticeable Difference (JND) model developed based on Human Vision System (HVS) through subjective studies is valuable for many multimedia use cases. In the streaming industries, it is commonly applied to reach a good balance between compression efficiency and perceptual quality when selecting video encoding recipes. Nevertheless, recent state-of-the-art deep learning based JND prediction model relies on large-scale JND ground truth that is expensive and time consuming to collect. Most of the existing JND datasets contain limited number of contents and are limited to a certain codec (e.g., H264). As a result, JND prediction models that were trained on such datasets are normally not agnostic to the codecs. To this end, in order to decouple encoding recipes and JND estimation, we propose a novel framework to map the difference of objective Video Quality Assessment (VQA) scores, i.e., VMAF, between two given videos encoded with different encoding recipes from the same content to the probability of having just noticeable difference between them. The proposed probability mapping model learns from DCR test data, which is significantly cheaper compared to standard JND subjective test. As we utilize objective VQA metric (e.g., VMAF that trained with contents encoded with different codecs) as proxy to estimate JND, our model is agnostic to codecs and computationally efficient. Throughout extensive experiments, it is demonstrated that the proposed model is able to estimate JND values efficiently.",2022-05-16T10:48:03Z,2022-05-20T12:18:42Z,http://arxiv.org/abs/2205.07565v2,http://arxiv.org/pdf/2205.07565v2,eess.IV
Content Adaptive Wavelet Lifting for Scalable Lossless Video Coding,"Daniela Lanz, Christian Herbert, André Kaup","Scalable lossless video coding is an important aspect for many professional applications. Wavelet-based video coding decomposes an input sequence into a lowpass and a highpass subband by filtering along the temporal axis. The lowpass subband can be used for previewing purposes, while the highpass subband provides the residual content for lossless reconstruction of the original sequence. The recursive application of the wavelet transform to the lowpass subband of the previous stage yields coarser temporal resolutions of the input sequence. This allows for lower bit rates, but also affects the visual quality of the lowpass subband. So far, the number of total decomposition levels is determined for the entire input sequence in advance. However, if the motion in the video sequence is strong or if abrupt scene changes occur, a further decomposition leads to a low-quality lowpass subband. Therefore, we propose a content adaptive wavelet transform, which locally adapts the depth of the decomposition to the content of the input sequence. Thereby, the visual quality of the low-pass subband is increased by up to 10.28 dB compared to a uniform wavelet transform with the same number of total decomposition levels, while the required rate is reduced by 1.06% additionally.",2023-02-02T10:51:23Z,2023-02-02T10:51:23Z,http://arxiv.org/abs/2302.01007v1,http://arxiv.org/pdf/2302.01007v1,eess.IV
"""Seeing'' Electric Network Frequency from Events","Lexuan Xu, Guang Hua, Haijian Zhang, Lei Yu, Ning Qiao","Most of the artificial lights fluctuate in response to the grid's alternating current and exhibit subtle variations in terms of both intensity and spectrum, providing the potential to estimate the Electric Network Frequency (ENF) from conventional frame-based videos. Nevertheless, the performance of Video-based ENF (V-ENF) estimation largely relies on the imaging quality and thus may suffer from significant interference caused by non-ideal sampling, motion, and extreme lighting conditions. In this paper, we show that the ENF can be extracted without the above limitations from a new modality provided by the so-called event camera, a neuromorphic sensor that encodes the light intensity variations and asynchronously emits events with extremely high temporal resolution and high dynamic range. Specifically, we first formulate and validate the physical mechanism for the ENF captured in events, and then propose a simple yet robust Event-based ENF (E-ENF) estimation method through mode filtering and harmonic enhancement. Furthermore, we build an Event-Video ENF Dataset (EV-ENFD) that records both events and videos in diverse scenes. Extensive experiments on EV-ENFD demonstrate that our proposed E-ENF method can extract more accurate ENF traces, outperforming the conventional V-ENF by a large margin, especially in challenging environments with object motions and extreme lighting conditions. The code and dataset are available at https://xlx-creater.github.io/E-ENF.",2023-05-04T07:04:40Z,2023-05-04T07:04:40Z,http://arxiv.org/abs/2305.02597v1,http://arxiv.org/pdf/2305.02597v1,eess.IV
Contrastive Self-Supervised Learning for Spatio-Temporal Analysis of   Lung Ultrasound Videos,"Li Chen, Jonathan Rubin, Jiahong Ouyang, Naveen Balaraju, Shubham Patil, Courosh Mehanian, Sourabh Kulhare, Rachel Millin, Kenton W Gregory, Cynthia R Gregory, Meihua Zhu, David O Kessler, Laurie Malia, Almaz Dessie, Joni Rabiner, Di Coneybeare, Bo Shopsin, Andrew Hersh, Cristian Madar, Jeffrey Shupp, Laura S Johnson, Jacob Avila, Kristin Dwyer, Peter Weimersheimer, Balasundar Raju, Jochen Kruecker, Alvin Chen","Self-supervised learning (SSL) methods have shown promise for medical imaging applications by learning meaningful visual representations, even when the amount of labeled data is limited. Here, we extend state-of-the-art contrastive learning SSL methods to 2D+time medical ultrasound video data by introducing a modified encoder and augmentation method capable of learning meaningful spatio-temporal representations, without requiring constraints on the input data. We evaluate our method on the challenging clinical task of identifying lung consolidations (an important pathological feature) in ultrasound videos. Using a multi-center dataset of over 27k lung ultrasound videos acquired from over 500 patients, we show that our method can significantly improve performance on downstream localization and classification of lung consolidation. Comparisons against baseline models trained without SSL show that the proposed methods are particularly advantageous when the size of labeled training data is limited (e.g., as little as 5% of the training set).",2023-10-14T17:53:44Z,2023-10-14T17:53:44Z,http://arxiv.org/abs/2310.10689v1,http://arxiv.org/pdf/2310.10689v1,eess.IV
"Viewport Prediction, Bitrate Selection, and Beamforming Design for   THz-Enabled 360° Video Streaming","Mehdi Setayesh, Vincent W. S. Wong","360{\deg} videos require significant bandwidth to provide an immersive viewing experience. Wireless systems using terahertz (THz) frequency band can meet this high data rate demand. However, self-blockage is a challenge in such systems. To ensure reliable transmission, this paper explores THz-enabled 360{\deg} video streaming through multiple multi-antenna access points (APs). Guaranteeing users' quality of experience (QoE) requires accurate viewport prediction to determine which video tiles to send, followed by asynchronous bitrate selection for those tiles and beamforming design at the APs. To address users' privacy and data heterogeneity, we propose a content-based viewport prediction framework, wherein users' head movement prediction models are trained using a personalized federated learning (PFL) algorithm. To address asynchronous decision-making for tile bitrates and dynamic THz link connections, we formulate the optimization of bitrate selection and beamforming as a macro-action decentralized partially observable Markov decision process (MacDec-POMDP) problem. To efficiently tackle this problem for multiple users, we develop two deep reinforcement learning (DRL) algorithms based on multi-agent actor-critic methods and propose a hierarchical learning framework to train the actor and critic networks. Experimental results show that our proposed approach provides a higher QoE when compared with three benchmark algorithms.",2024-01-23T21:50:56Z,2024-12-07T20:37:59Z,http://arxiv.org/abs/2401.13114v2,http://arxiv.org/pdf/2401.13114v2,"eess.IV, eess.SP"
Video Coding with Cross-Component Sample Offset,"Han Gao, Xin Zhao, Tianqi Liu, Shan Liu","Beyond the exploration of traditional spatial, temporal and subjective visual signal redundancy in image and video compression, recent research has focused on leveraging cross-color component redundancy to enhance coding efficiency. Cross-component coding approaches are motivated by the statistical correlations among different color components, such as those in the Y'CbCr color space, where luma (Y) color component typically exhibits finer details than chroma (Cb/Cr) color components. Inspired by previous cross-component coding algorithms, this paper introduces a novel in-loop filtering approach named Cross-Component Sample Offset (CCSO). CCSO utilizes co-located and neighboring luma samples to generate correction signals for both luma and chroma reconstructed samples. It is a multiplication-free, non-linear mapping process implemented using a look-up-table. The input to the mapping is a group of reconstructed luma samples, and the output is an offset value applied on the center luma or co-located chroma sample. Experimental results demonstrate that the proposed CCSO can be applied to both image and video coding, resulting in improved coding efficiency and visual quality. The method has been adopted into an experimental next-generation video codec beyond AV1 developed by the Alliance for Open Media (AOMedia), achieving significant objective coding gains up to 3.5\,\% and 1.8\,\% for PSNR and VMAF quality metrics, respectively, under random access configuration. Additionally, CCSO notably improves the subjective visual quality.",2024-06-03T21:27:56Z,2024-06-03T21:27:56Z,http://arxiv.org/abs/2406.01795v1,http://arxiv.org/pdf/2406.01795v1,eess.IV
BVI-AOM: A New Training Dataset for Deep Video Compression Optimization,"Jakub Nawała, Yuxuan Jiang, Fan Zhang, Xiaoqing Zhu, Joel Sole, David Bull","Deep learning is now playing an important role in enhancing the performance of conventional hybrid video codecs. These learning-based methods typically require diverse and representative training material for optimization in order to achieve model generalization and optimal coding performance. However, existing datasets either offer limited content variability or come with restricted licensing terms constraining their use to research purposes only. To address these issues, we propose a new training dataset, named BVI-AOM, which contains 956 uncompressed sequences at various resolutions from 270p to 2160p, covering a wide range of content and texture types. The dataset comes with more flexible licensing terms and offers competitive performance when used as a training set for optimizing deep video coding tools. The experimental results demonstrate that when used as a training set to optimize two popular network architectures for two different coding tools, the proposed dataset leads to additional bitrate savings of up to 0.29 and 2.98 percentage points in terms of PSNR-Y and VMAF, respectively, compared to an existing training dataset, BVI-DVC, which has been widely used for deep video coding. The BVI-AOM dataset is available at https://github.com/fan-aaron-zhang/bvi-aom",2024-08-06T15:54:55Z,2024-10-23T09:34:55Z,http://arxiv.org/abs/2408.03265v3,http://arxiv.org/pdf/2408.03265v3,eess.IV
EEG-based Decoding of Selective Visual Attention in Superimposed Videos,"Yuanyuan Yao, Wout De Swaef, Simon Geirnaert, Alexander Bertrand","Selective attention enables humans to efficiently process visual stimuli by enhancing important locations or objects and filtering out irrelevant information. Locating visual attention is a fundamental problem in neuroscience with potential applications in brain-computer interfaces. Conventional paradigms often use synthetic stimuli or static images, but visual stimuli in real life contain smooth and highly irregular dynamics. In this study, we show that these irregular dynamics in natural videos can be decoded from electroencephalography (EEG) signals to perform selective visual attention decoding. To this end, we propose an experimental paradigm in which participants attend to one of two superimposed videos, each showing a center-aligned person performing a stage act. We then train a stimulus-informed decoder to extract EEG signal components that are correlated with the motion patterns of the attended object, and show that this decoder can be used on unseen data to detect which of both objects is attended. Eye movements are also found to be correlated to the motion patterns in the attended video, despite the spatial overlap between the target and the distractor. We further show that these eye movements do not dominantly drive the EEG-based decoding and that complementary information exists in EEG and gaze data. Moreover, our results indicate that EEG also captures information about unattended objects. To our knowledge, this study is the first to explore EEG-based selective visual attention decoding on natural videos, opening new possibilities for experiment design in related fields.",2024-09-19T08:37:18Z,2024-09-19T08:37:18Z,http://arxiv.org/abs/2409.12562v1,http://arxiv.org/pdf/2409.12562v1,"eess.SP, q-bio.NC"
The Bjøntegaard Bible -- Why your Way of Comparing Video Codecs May   Be Wrong,"Christian Herglotz, Hannah Och, Anna Meyer, Geetha Ramasubbu, Lena Eichermüller, Matthias Kränzler, Fabian Brand, Kristian Fischer, Dat Thanh Nguyen, Andy Regensky, André Kaup","In this paper, we provide an in-depth assessment on the Bj{\o}ntegaard Delta. We construct a large data set of video compression performance comparisons using a diverse set of metrics including PSNR, VMAF, bitrate, and processing energies. These metrics are evaluated for visual data types such as classic perspective video, 360$^\circ$ video, point clouds, and screen content. As compression technology, we consider multiple hybrid video codecs as well as state-of-the-art neural network based compression methods. Using additional supporting points inbetween standard points defined by parameters such as the quantization parameter, we assess the interpolation error of the Bj{\o}ntegaard-Delta (BD) calculus and its impact on the final BD value. From the analysis, we find that the BD calculus is most accurate in the standard application of rate-distortion comparisons with mean errors below 0.5 percentage points. For other applications and special cases, e.g., VMAF quality, energy considerations, or inter-codec comparisons, the errors are higher (up to 5 percentage points), but can be halved by using a higher number of supporting points. We finally come up with recommendations on how to use the BD calculus such that the validity of the resulting BD-values is maximized. Main recommendations are as follows: First, relative curve differences should be plotted and analyzed. Second, the logarithmic domain should be used for saturating metrics such as SSIM and VMAF. Third, BD values below a certain threshold indicated by the subset error should not be used to draw recommendations. Fourth, using two supporting points is sufficient to obtain rough performance estimates.",2023-04-25T14:24:37Z,2023-12-22T12:32:37Z,http://arxiv.org/abs/2304.12852v2,http://arxiv.org/pdf/2304.12852v2,eess.IV
dAJC: A 2.02mW 50Mbps Direct Analog to MJPEG Converter for Video Sensor   Node using Low-Noise Switched Capacitor MAC-Quantizer with Auto-Calibration   and Sparsity-Aware ADC,"Gourab Barik, Gaurav Kumar K, Baibhab Chatterjee, Shovan Maity, Sumon Bose, Shreyas Sen","With the advancement in the field of the Internet of Things(IoT) and Internet of Bodies(IoB), video camera applications using Video Sensor Nodes(VSNs) have gained importance in the field of autonomous driving, health monitoring, robot control, and security camera applications. However, these applications typically involve high data rates due to the transmission of high-resolution video signals, resulting from high data volume generated from the analog-to-digital converters (ADCs). This significant data deluge poses processing and storage overheads, exacerbating the problem. To address this challenge, we propose a low-power solution aimed at reducing the power consumption in Video Sensor Nodes (VSNs) by shifting the computation from the digital domain to the inherently energy-efficient analog domain. Unlike standard architectures where computation and processing are typically performed in digital signal processing (DSP) blocks after the ADCs, our approach eliminates the need for such blocks. Instead, we leverage a switched capacitor-based computation unit in the analog domain, resulting in a reduction in power consumption. We achieve a $\sim4X$ reduction in power consumption compared to digital implementations. Furthermore, we employ a sparsity-aware ADC, which is enabled only for significant compressed samples that contribute to a small fraction ($\le5\%$) of the total captured analog samples, we achieve a $\sim20X$ lower ADC conversion energy without any considerable degradation, contributing to the overall energy savings in the system.",2024-07-02T00:27:32Z,2024-07-02T00:27:32Z,http://arxiv.org/abs/2407.11023v1,http://arxiv.org/pdf/2407.11023v1,"eess.SP, eess.IV"
Dynamic Tomography Reconstruction by Projection-Domain Separable   Modeling,"Berk Iskender, Marc L. Klasky, Yoram Bresler","In dynamic tomography the object undergoes changes while projections are being acquired sequentially in time. The resulting inconsistent set of projections cannot be used directly to reconstruct an object corresponding to a time instant. Instead, the objective is to reconstruct a spatio-temporal representation of the object, which can be displayed as a movie. We analyze conditions for unique and stable solution of this ill-posed inverse problem, and present a recovery algorithm, validating it experimentally. We compare our approach to one based on the recently proposed GMLR variation on deep prior for video, demonstrating the advantages of the proposed approach.",2022-04-21T07:51:06Z,2022-06-04T21:16:14Z,http://arxiv.org/abs/2204.09935v2,http://arxiv.org/pdf/2204.09935v2,"eess.IV, eess.SP"
Frame-type Sensitive RDO Control for Content-Adaptive-encoding,"Vibhoothi, François Pitié, Anil Kokaram","Video transcoding is an increasingly important application in the streaming media industry. It has become important to investigate the optimisation of transcoder parameters for a single clip simply because of the immense number of playbacks for popular clips. In this paper, we explore the use of a canned optimiser to estimate the optimal RD tradeoff achievable for a particular clip. We show that by adjusting the Lagrange multiplier in RD optimisation on keyframes alone we can achieve more than 10$\times$ the previous BD-Rate gains possible without affecting quality for any operating point.",2022-06-23T21:02:39Z,2022-07-11T16:51:31Z,http://arxiv.org/abs/2206.11976v2,http://arxiv.org/pdf/2206.11976v2,"eess.IV, eess.SP"
