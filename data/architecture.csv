title,authors,summary,published,updated,link,pdf_url,categories
On Assessing the Complexity of Software Architectures,Jianjun Zhao,This paper proposes some new architectural metrics which are appropriate for evaluating the architectural attributes of a software system. The main feature of our approach is to assess the complexity of a software architecture by analyzing various types of architectural dependences in the architecture.,2001-05-05T09:18:11Z,2001-05-05T09:18:11Z,http://arxiv.org/abs/cs/0105010v1,http://arxiv.org/pdf/cs/0105010v1,"cs.SE, D.2.8; D.2.11"
Applying Slicing Technique to Software Architectures,Jianjun Zhao,"Software architecture is receiving increasingly attention as a critical design level for software systems. As software architecture design resources (in the form of architectural specifications) are going to be accumulated, the development of techniques and tools to support architectural understanding, testing, reengineering, maintenance, and reuse will become an important issue. This paper introduces a new form of slicing, named architectural slicing, to aid architectural understanding and reuse. In contrast to traditional slicing, architectural slicing is designed to operate on the architectural specification of a software system, rather than the source code of a program. Architectural slicing provides knowledge about the high-level structure of a software system, rather than the low-level implementation details of a program. In order to compute an architectural slice, we present the architecture information flow graph which can be used to represent information flows in a software architecture. Based on the graph, we give a two-phase algorithm to compute an architectural slice.",2001-05-05T08:09:08Z,2001-05-05T08:09:08Z,http://arxiv.org/abs/cs/0105008v1,http://arxiv.org/pdf/cs/0105008v1,"cs.SE, D.2.4; D.2.5; D.2.7; D.2.11"
InstaNAS: Instance-aware Neural Architecture Search,"An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, Min Sun","Conventional Neural Architecture Search (NAS) aims at finding a single architecture that achieves the best performance, which usually optimizes task related learning objectives such as accuracy. However, a single architecture may not be representative enough for the whole dataset with high diversity and variety. Intuitively, electing domain-expert architectures that are proficient in domain-specific features can further benefit architecture related objectives such as latency. In this paper, we propose InstaNAS---an instance-aware NAS framework---that employs a controller trained to search for a ""distribution of architectures"" instead of a single architecture; This allows the model to use sophisticated architectures for the difficult samples, which usually comes with large architecture related cost, and shallow architectures for those easy samples. During the inference phase, the controller assigns each of the unseen input samples with a domain expert architecture that can achieve high accuracy with customized inference costs. Experiments within a search space inspired by MobileNetV2 show InstaNAS can achieve up to 48.8% latency reduction without compromising accuracy on a series of datasets against MobileNetV2.",2018-11-26T06:29:39Z,2019-05-23T09:25:04Z,http://arxiv.org/abs/1811.10201v3,http://arxiv.org/pdf/1811.10201v3,"cs.LG, cs.CV, stat.ML"
Disentangled Neural Architecture Search,"Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi","Neural architecture search has shown its great potential in various areas recently. However, existing methods rely heavily on a black-box controller to search architectures, which suffers from the serious problem of lacking interpretability. In this paper, we propose disentangled neural architecture search (DNAS) which disentangles the hidden representation of the controller into semantically meaningful concepts, making the neural architecture search process interpretable. Based on systematical study, we discover the correlation between network architecture and its performance, and propose a dense-sampling strategy to conduct a targeted search in promising regions that may generate well-performing architectures. We show that: 1) DNAS successfully disentangles the architecture representations, including operation selection, skip connections, and number of layers. 2) Benefiting from interpretability, DNAS can find excellent architectures under different FLOPS restrictions flexibly. 3) Dense-sampling leads to neural architecture search with higher efficiency and better performance. On the NASBench-101 dataset, DNAS achieves state-of-the-art performance of 94.21% using less than 1/13 computational cost of baseline methods. On ImageNet dataset, DNAS discovers the competitive architectures that achieves 22.7% test error. our method provides a new perspective of understanding neural architecture search.",2020-09-24T03:35:41Z,2020-09-24T03:35:41Z,http://arxiv.org/abs/2009.13266v1,http://arxiv.org/pdf/2009.13266v1,"cs.LG, cs.NE, stat.ML"
Domain-Specific Quantum Architecture Optimization,"Wan-Hsuan Lin, Bochen Tan, Murphy Yuezhen Niu, Jason Kimko, Jason Cong","With the steady progress in quantum computing over recent years, roadmaps for upscaling quantum processors have relied heavily on the targeted qubit architectures. So far, similarly to the early age of classical computing, these designs have been crafted by human experts. These general-purpose architectures, however, leave room for customization and optimization, especially when targeting popular near-term QC applications. In classical computing, customized architectures have demonstrated significant performance and energy efficiency gains over general-purpose counterparts. In this paper, we present a framework for optimizing quantum architectures, specifically through customizing qubit connectivity. It is the first work that (1) provides performance guarantees by integrating architecture optimization with an optimal compiler, (2) evaluates the impact of connectivity customization under a realistic crosstalk error model, and (3) benchmarks on realistic circuits of near-term interest, such as the quantum approximate optimization algorithm (QAOA) and quantum convolutional neural network (QCNN). We demonstrate up to 59% fidelity improvement in simulation by optimizing the heavy-hexagon architecture for QAOA circuits, and up to 14% improvement on the grid architecture. For the QCNN circuit, architecture optimization improves fidelity by 11% on the heavy-hexagon architecture and 605% on the grid architecture.",2022-07-29T05:16:02Z,2022-07-29T05:16:02Z,http://arxiv.org/abs/2207.14482v1,http://arxiv.org/pdf/2207.14482v1,"cs.AR, quant-ph"
Using Dependence Analysis to Support Software Architecture Understanding,Jianjun Zhao,"Software architecture is receiving increasingly attention as a critical design level for software systems. As software architecture design resources (in the form of architectural descriptions) are going to be accumulated, the development of techniques and tools to support architectural understanding, testing, reengineering, maintaining, and reusing will become an important issue. In this paper we introduce a new dependence analysis technique, named architectural dependence analysis to support software architecture development. In contrast to traditional dependence analysis, architectural dependence analysis is designed to operate on an architectural description of a software system, rather than the source code of a conventional program. Architectural dependence analysis provides knowledge of dependences for the high-level architecture of a software system, rather than the low-level implementation details of a conventional program.",2001-05-05T08:41:43Z,2001-05-05T08:41:43Z,http://arxiv.org/abs/cs/0105009v1,http://arxiv.org/pdf/cs/0105009v1,"cs.SE, D.2.4; D.2.5; D.2.7; D.2.11"
Verifying Patterns of Dynamic Architectures using Model Checking,"Diego Marmsoler, Silvio Degenhardt","Architecture patterns capture architectural design experience and provide abstract solutions to recurring architectural design problems. They consist of a description of component types and restrict component connection and activation. Therefore, they guarantee some desired properties for architectures employing the pattern. Unfortunately, most documented patterns do not provide a formal guarantee of whether their specification indeed leads to the desired guarantee. Failure in doing so, however, might lead to wrong architectures, i.e., architectures wrongly supposed to show certain desired properties. Since architectures, in general, have a high impact on the quality of the resulting system and architectural flaws are only difficult, if not to say impossible, to repair, this may lead to badly reparable quality issues in the resulting system. To address this problem, we propose an approach based on model checking to verify pattern specifications w.r.t. their guarantees. In the following we apply the approach to three well-known patterns for dynamic architectures: the Singleton, the Model-View-Controller, and the Broker pattern. Thereby, we discovered ambiguities and missing constraints for all three specifications. Thus, we conclude that verifying patterns of dynamic architectures using model checking is feasible and useful to discover ambiguities and flaws in pattern specifications.",2017-03-21T02:55:32Z,2017-03-21T02:55:32Z,http://arxiv.org/abs/1703.07033v1,http://arxiv.org/pdf/1703.07033v1,"cs.SE, D.2.11; D.2.4; D.2.2"
Fast Task-Aware Architecture Inference,"Efi Kokiopoulou, Anja Hauth, Luciano Sbaiz, Andrea Gesmundo, Gabor Bartok, Jesse Berent","Neural architecture search has been shown to hold great promise towards the automation of deep learning. However in spite of its potential, neural architecture search remains quite costly. To this point, we propose a novel gradient-based framework for efficient architecture search by sharing information across several tasks. We start by training many model architectures on several related (training) tasks. When a new unseen task is presented, the framework performs architecture inference in order to quickly identify a good candidate architecture, before any model is trained on the new task. At the core of our framework lies a deep value network that can predict the performance of input architectures on a task by utilizing task meta-features and the previous model training experiments performed on related tasks. We adopt a continuous parametrization of the model architecture which allows for efficient gradient-based optimization. Given a new task, an effective architecture is quickly identified by maximizing the estimated performance with respect to the model architecture parameters with simple gradient ascent. It is key to point out that our goal is to achieve reasonable performance at the lowest cost. We provide experimental results showing the effectiveness of the framework despite its high computational efficiency.",2019-02-15T12:00:24Z,2019-02-15T12:00:24Z,http://arxiv.org/abs/1902.05781v1,http://arxiv.org/pdf/1902.05781v1,"cs.LG, stat.ML"
Evolution Patterns: Designing and Reusing Architectural Evolution   Knowledge to Introduce Architectural Styles,"Dalila Tamzalit, Tom Mens","Software architectures are critical in the successful development and evolution of software-intensive systems. While formal and automated support for architectural descriptions has been widely addressed, their evolution is equally crucial, but significantly less well-understood and supported. In order to face a recurring evolution need, we introduce the concept of evolution pattern. It formalises an architectural evolution through both a set of concepts and a reusable evolution process. We propose it through the recurring need of introducing an architectural style on existing software architectures. We formally describe and analyse the feasibility of architectural evolution patterns, and provide a practical validation by implementing them in COSABuilder, an Eclipse plugin for the COSA architectural description language.",2016-05-20T11:08:50Z,2016-05-20T11:08:50Z,http://arxiv.org/abs/1605.06289v1,http://arxiv.org/pdf/1605.06289v1,"cs.SE, D.2.11; D.2.7"
Differentiable Neural Architecture Transformation for Reproducible   Architecture Improvement,"Do-Guk Kim, Heung-Chang Lee","Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to improve the given neural architecture to have better performance while maintaining computational costs. However, NAT has limitations about a lack of reproducibility. In this paper, we propose differentiable neural architecture transformation that is reproducible and efficient. The proposed method shows stable performance on various architectures. Extensive reproducibility experiments on two datasets, i.e., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to other models and datasets.",2020-06-15T09:03:48Z,2020-06-15T09:03:48Z,http://arxiv.org/abs/2006.08231v1,http://arxiv.org/pdf/2006.08231v1,"cs.LG, cs.CV, stat.ML"
Efficient Search of Multiple Neural Architectures with Different   Complexities via Importance Sampling,"Yuhei Noda, Shota Saito, Shinichi Shirakawa","Neural architecture search (NAS) aims to automate architecture design processes and improve the performance of deep neural networks. Platform-aware NAS methods consider both performance and complexity and can find well-performing architectures with low computational resources. Although ordinary NAS methods result in tremendous computational costs owing to the repetition of model training, one-shot NAS, which trains the weights of a supernetwork containing all candidate architectures only once during the search process, has been reported to result in a lower search cost. This study focuses on the architecture complexity-aware one-shot NAS that optimizes the objective function composed of the weighted sum of two metrics, such as the predictive performance and number of parameters. In existing methods, the architecture search process must be run multiple times with different coefficients of the weighted sum to obtain multiple architectures with different complexities. This study aims at reducing the search cost associated with finding multiple architectures. The proposed method uses multiple distributions to generate architectures with different complexities and updates each distribution using the samples obtained from multiple distributions based on importance sampling. The proposed method allows us to obtain multiple architectures with different complexities in a single architecture search, resulting in reducing the search cost. The proposed method is applied to the architecture search of convolutional neural networks on the CIAFR-10 and ImageNet datasets. Consequently, compared with baseline methods, the proposed method finds multiple architectures with varying complexities while requiring less computational effort.",2022-07-21T07:06:03Z,2022-07-21T07:06:03Z,http://arxiv.org/abs/2207.10334v1,http://arxiv.org/pdf/2207.10334v1,"cs.NE, cs.LG, stat.ML"
Architecture Diagrams: A Graphical Language for Architecture Style   Specification,"Anastasia Mavridou, Eduard Baranov, Simon Bliudze, Joseph Sifakis","Architecture styles characterise families of architectures sharing common characteristics. We have recently proposed configuration logics for architecture style specification. In this paper, we study a graphical notation to enhance readability and easiness of expression. We study simple architecture diagrams and a more expressive extension, interval architecture diagrams. For each type of diagrams, we present its semantics, a set of necessary and sufficient consistency conditions and a method that allows to characterise compositionally the specified architectures. We provide several examples illustrating the application of the results. We also present a polynomial-time algorithm for checking that a given architecture conforms to the architecture style specified by a diagram.",2016-08-11T00:26:15Z,2016-08-11T00:26:15Z,http://arxiv.org/abs/1608.03324v1,http://arxiv.org/pdf/1608.03324v1,"cs.SE, D2.2; D.2.11; D.2.13"
Probabilistic Dual Network Architecture Search on Graphs,"Yiren Zhao, Duo Wang, Xitong Gao, Robert Mullins, Pietro Lio, Mateja Jamnik","We present the first differentiable Network Architecture Search (NAS) for Graph Neural Networks (GNNs). GNNs show promising performance on a wide range of tasks, but require a large amount of architecture engineering. First, graphs are inherently a non-Euclidean and sophisticated data structure, leading to poor adaptivity of GNN architectures across different datasets. Second, a typical graph block contains numerous different components, such as aggregation and attention, generating a large combinatorial search space. To counter these problems, we propose a Probabilistic Dual Network Architecture Search (PDNAS) framework for GNNs. PDNAS not only optimises the operations within a single graph block (micro-architecture), but also considers how these blocks should be connected to each other (macro-architecture). The dual architecture (micro- and marco-architectures) optimisation allows PDNAS to find deeper GNNs on diverse datasets with better performance compared to other graph NAS methods. Moreover, we use a fully gradient-based search approach to update architectural parameters, making it the first differentiable graph NAS method. PDNAS outperforms existing hand-designed GNNs and NAS results, for example, on the PPI dataset, PDNAS beats its best competitors by 1.67 and 0.17 in F1 scores.",2020-03-21T15:06:47Z,2020-03-21T15:06:47Z,http://arxiv.org/abs/2003.09676v1,http://arxiv.org/pdf/2003.09676v1,"cs.LG, stat.ML"
Neural Circuit Architectural Priors for Quadruped Locomotion,"Nikhil X. Bhattasali, Venkatesh Pattabiraman, Lerrel Pinto, Grace W. Lindsay","Learning-based approaches to quadruped locomotion commonly adopt generic policy architectures like fully connected MLPs. As such architectures contain few inductive biases, it is common in practice to incorporate priors in the form of rewards, training curricula, imitation data, or trajectory generators. In nature, animals are born with priors in the form of their nervous system's architecture, which has been shaped by evolution to confer innate ability and efficient learning. For instance, a horse can walk within hours of birth and can quickly improve with practice. Such architectural priors can also be useful in ANN architectures for AI. In this work, we explore the advantages of a biologically inspired ANN architecture for quadruped locomotion based on neural circuits in the limbs and spinal cord of mammals. Our architecture achieves good initial performance and comparable final performance to MLPs, while using less data and orders of magnitude fewer parameters. Our architecture also exhibits better generalization to task variations, even admitting deployment on a physical robot without standard sim-to-real methods. This work shows that neural circuits can provide valuable architectural priors for locomotion and encourages future work in other sensorimotor skills.",2024-10-09T17:59:45Z,2024-10-09T17:59:45Z,http://arxiv.org/abs/2410.07174v1,http://arxiv.org/pdf/2410.07174v1,"q-bio.NC, cs.AI, cs.LG, cs.NE, cs.RO"
NAT: Neural Architecture Transformer for Accurate and Compact   Architectures,"Yong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Jian Chen, Peilin Zhao, Junzhou Huang","Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However, even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g., convolution or pooling), which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately, such a constrained optimization problem is NP-hard. To make the problem feasible, we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g., skip connection or directly removing the connection). Based on MDP, we learn NAT by exploiting reinforcement learning to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies, we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate that the transformed architecture by NAT significantly outperforms both its original form and those architectures optimized by existing methods.",2019-10-31T14:29:09Z,2020-01-13T13:39:25Z,http://arxiv.org/abs/1910.14488v5,http://arxiv.org/pdf/1910.14488v5,"cs.LG, stat.ML"
Building a Hierarchical Architecture and Communication Model for the   Quantum Internet,"Binjie He, Dong Zhang, Seng W. Loke, Shengrui Lin, Luke Lu","The research of architecture has tremendous significance in realizing quantum Internet. Although there is not yet a standard quantum Internet architecture, the distributed architecture is one of the possible solutions, which utilizes quantum repeaters or dedicated entanglement sources in a flat structure for entanglement preparation & distribution. In this paper, we analyze the distributed architecture in detail and demonstrate that it has three limitations: 1) possible high maintenance overhead, 2) possible low-performance entanglement distribution, and 3) unable to support optimal entanglement routing. We design a hierarchical quantum Internet architecture and a communication model to solve the problems above. We also present a W-state Based Centralized Entanglement Preparation & Distribution (W-state Based CEPD) scheme and a Centralized Entanglement Routing (CER) algorithm within our hierarchical architecture and perform an experimental comparison with other entanglement preparation & distribution schemes and entanglement routing algorithms within the distributed architecture. The evaluation results show that the entanglement distribution efficiency of hierarchical architecture is 11.5% higher than that of distributed architecture on average (minimum 3.3%, maximum 37.3%), and the entanglement routing performance of hierarchical architecture is much better than that of a distributed architecture according to the fidelity and throughput.",2024-02-19T03:26:32Z,2024-07-09T06:36:28Z,http://arxiv.org/abs/2402.11806v2,http://arxiv.org/pdf/2402.11806v2,"quant-ph, cs.NI"
Efficient logic architecture in training gradient boosting decision tree   for high-performance and edge computing,"Takuya Tanaka, Ryosuke Kasahara, Daishiro Kobayashi","This study proposes a logic architecture for the high-speed and power efficiently training of a gradient boosting decision tree model of binary classification. We implemented the proposed logic architecture on an FPGA and compared training time and power efficiency with three general GBDT software libraries using CPU and GPU. The training speed of the logic architecture on the FPGA was 26-259 times faster than the software libraries. The power efficiency of the logic architecture was 90-1,104 times higher than the software libraries. The results show that the logic architecture suits for high-performance and edge computing.",2018-12-20T00:28:12Z,2018-12-20T00:28:12Z,http://arxiv.org/abs/1812.08295v1,http://arxiv.org/pdf/1812.08295v1,"cs.LG, stat.ML"
Xapagy: a cognitive architecture for narrative reasoning,Ladislau Bölöni,"We introduce the Xapagy cognitive architecture: a software system designed to perform narrative reasoning. The architecture has been designed from scratch to model and mimic the activities performed by humans when witnessing, reading, recalling, narrating and talking about stories.",2011-05-17T20:28:31Z,2011-05-17T20:28:31Z,http://arxiv.org/abs/1105.3486v1,http://arxiv.org/pdf/1105.3486v1,"cs.AI, 68T01, I.2.0"
HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking,"Shen Yan, Biyi Fang, Faen Zhang, Yu Zheng, Xiao Zeng, Hui Xu, Mi Zhang","The use of automatic methods, often referred to as Neural Architecture Search (NAS), in designing neural network architectures has recently drawn considerable attention. In this work, we present an efficient NAS approach, named HM- NAS, that generalizes existing weight sharing based NAS approaches. Existing weight sharing based NAS approaches still adopt hand-designed heuristics to generate architecture candidates. As a consequence, the space of architecture candidates is constrained in a subset of all possible architectures, making the architecture search results sub-optimal. HM-NAS addresses this limitation via two innovations. First, HM-NAS incorporates a multi-level architecture encoding scheme to enable searching for more flexible network architectures. Second, it discards the hand-designed heuristics and incorporates a hierarchical masking scheme that automatically learns and determines the optimal architecture. Compared to state-of-the-art weight sharing based approaches, HM-NAS is able to achieve better architecture search performance and competitive model evaluation accuracy. Without the constraint imposed by the hand-designed heuristics, our searched networks contain more flexible and meaningful architectures that existing weight sharing based NAS approaches are not able to discover.",2019-08-31T04:02:16Z,2019-09-07T08:33:25Z,http://arxiv.org/abs/1909.00122v2,http://arxiv.org/pdf/1909.00122v2,"cs.LG, cs.CV, stat.ML"
Accelerating Evolutionary Neural Architecture Search via Multi-Fidelity   Evaluation,"Shangshang Yang, Ye Tian, Xiaoshu Xiang, Shichen Peng, Xingyi Zhang","Evolutionary neural architecture search (ENAS) has recently received increasing attention by effectively finding high-quality neural architectures, which however consumes high computational cost by training the architecture encoded by each individual for complete epochs in individual evaluation. Numerous ENAS approaches have been developed to reduce the evaluation cost, but it is often difficult for most of these approaches to achieve high evaluation accuracy. To address this issue, in this paper we propose an accelerated ENAS via multifidelity evaluation termed MFENAS, where the individual evaluation cost is significantly reduced by training the architecture encoded by each individual for only a small number of epochs. The balance between evaluation cost and evaluation accuracy is well maintained by suggesting a multi-fidelity evaluation, which identifies the potentially good individuals that cannot survive from previous generations by integrating multiple evaluations under different numbers of training epochs. For high diversity of neural architectures, a population initialization strategy is devised to produce different neural architectures varying from ResNet-like architectures to Inception-like ones. Experimental results on CIFAR-10 show that the architecture obtained by the proposed MFENAS achieves a 2.39% test error rate at the cost of only 0.6 GPU days on one NVIDIA 2080TI GPU, demonstrating the superiority of the proposed MFENAS over state-of-the-art NAS approaches in terms of both computational cost and architecture quality. The architecture obtained by the proposed MFENAS is then transferred to CIFAR-100 and ImageNet, which also exhibits competitive performance to the architectures obtained by existing NAS approaches. The source code of the proposed MFENAS is available at https://github.com/DevilYangS/MFENAS/.",2021-08-10T09:32:26Z,2021-08-10T09:32:26Z,http://arxiv.org/abs/2108.04541v1,http://arxiv.org/pdf/2108.04541v1,"cs.AI, 68W50, 68T07, I.2.6"
What Should Future Wireless Network Architectures Be?,"Lu Yang, Ping Li, Miaomiao Dong, Bo Bai, Dmitry Zaporozhets, Xiang Chen, Wei Han, Baochun Li","The accelerated convergence of digital and real-world lifestyles has imposed unprecedented demands on today's wireless network architectures, as it is highly desirable for such architectures to support wireless devices everywhere with high capacity and minimal signaling overhead. Conventional architectures, such as cellular architectures, are not able to satisfy these requirements simultaneously, and are thus no longer suitable for the future era. In this paper, we propose a capacity-centric (C$^2$) architecture for future wireless communication networks. It is designed based on the principles of maximizing the number of non-overlapping clusters with the average cluster capacity guaranteed to be higher than a certain threshold, and thus provides a flexible way to balance the capacity requirement against the signaling overhead. Our analytical results reveal that C$^2$ has superior generality, wherein both the cellular and the fully coordinated architectures can be viewed as its extreme cases. Simulation results show that the average capacity of C$^2$ is at least three times higher compared to that of the cellular architecture. More importantly, different from the widely adopted conventional wisdom that base-station distributions dominate architecture designs, we find that the C$^2$ architecture is independent of base-station distributions, and instead user-side information should be the focus in future wireless network architecture designs.",2021-10-07T03:20:57Z,2021-10-12T17:41:43Z,http://arxiv.org/abs/2110.03157v2,http://arxiv.org/pdf/2110.03157v2,"cs.IT, math.IT"
Multicast-based Architecture for IP Mobility: Simulation Analysis and   Comparison with Basic Mobile IP,Ahmed Helmy,"With the introduction of a newer generation of wireless devices and technologies, the need for an efficient architecture for IP mobility is becoming more apparent. Several architectures have been proposed to support IP mobility. Most studies, however, show that current architectures, in general, fall short from satisfying the performance requirements for wireless applications, mainly audio. Other studies have shown performance improvement by using multicast to reduce latency and packet loss during handoff. In this study, we propose a multicast-based architecture to support IP mobility. We evaluate our approach through simulation, and we compare it to mainstream approaches for IP mobility, mainly, the Mobile IP protocol. Comparison is performed according to the required performance criteria, such as smooth handoff and efficient routing.   Our simulation results show significant improvement for the proposed architecture. On average, basic Mobile IP consumes almost twice as much network bandwidth, and experiences more than twice as much end-to-end and handoff delays, as does our proposed architecture. Furthermore, we propose an extension to Mobile IP to support our architecture with minimal modification.",2000-06-10T23:53:12Z,2000-06-10T23:53:12Z,http://arxiv.org/abs/cs/0006022v1,http://arxiv.org/pdf/cs/0006022v1,"cs.NI, cs.PF, C.2.1; C.2.2"
Networking in the Physical World,"Michael Neufeld, Craig Partridge",In this work we propose a network meta-architecture based on fundamental laws of physics and a physical model of computation. This meta-architecture may be used to frame discussions about novel network architectures as well as cross-layer alterations to the canonical network stack.,2008-08-18T00:51:57Z,2008-08-18T00:51:57Z,http://arxiv.org/abs/0808.2325v1,http://arxiv.org/pdf/0808.2325v1,"cs.NI, C.2.0"
Understanding Architectures Learnt by Cell-based Neural Architecture   Search,"Yao Shu, Wei Wang, Shaofeng Cai","Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness have attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. Our empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient information. Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.",2019-09-20T15:49:45Z,2020-01-01T13:57:23Z,http://arxiv.org/abs/1909.09569v3,http://arxiv.org/pdf/1909.09569v3,"cs.LG, cs.CV, stat.ML"
SGAS: Sequential Greedy Architecture Search,"Guohao Li, Guocheng Qian, Itzel C. Delgadillo, Matthias Müller, Ali Thabet, Bernard Ghanem","Architecture design has become a crucial component of successful deep learning. Recent progress in automatic neural architecture search (NAS) shows a lot of promise. However, discovered architectures often fail to generalize in the final evaluation. Architectures with a higher validation accuracy during the search phase may perform worse in the evaluation. Aiming to alleviate this common issue, we introduce sequential greedy architecture search (SGAS), an efficient method for neural architecture search. By dividing the search procedure into sub-problems, SGAS chooses and prunes candidate operations in a greedy fashion. We apply SGAS to search architectures for Convolutional Neural Networks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments show that SGAS is able to find state-of-the-art architectures for tasks such as image classification, point cloud classification and node classification in protein-protein interaction graphs with minimal computational cost. Please visit https://www.deepgcns.org/auto/sgas for more information about SGAS.",2019-11-30T12:39:55Z,2020-04-02T12:55:03Z,http://arxiv.org/abs/1912.00195v2,http://arxiv.org/pdf/1912.00195v2,"cs.LG, cs.CV, stat.ML"
A Flexible Approach to Automated RNN Architecture Generation,"Martin Schrimpf, Stephen Merity, James Bradbury, Richard Socher","The process of designing neural architectures requires expert knowledge and extensive trial and error. While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components. We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width. The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization. Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains. The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.",2017-12-20T04:20:40Z,2017-12-20T04:20:40Z,http://arxiv.org/abs/1712.07316v1,http://arxiv.org/pdf/1712.07316v1,"cs.CL, cs.LG, stat.ML"
Differentially-private Federated Neural Architecture Search,"Ishika Singh, Haoyi Zhou, Kunlin Yang, Meng Ding, Bill Lin, Pengtao Xie","Neural architecture search, which aims to automatically search for architectures (e.g., convolution, max pooling) of neural networks that maximize validation performance, has achieved remarkable progress recently. In many application scenarios, several parties would like to collaboratively search for a shared neural architecture by leveraging data from all parties. However, due to privacy concerns, no party wants its data to be seen by other parties. To address this problem, we propose federated neural architecture search (FNAS), where different parties collectively search for a differentiable architecture by exchanging gradients of architecture variables without exposing their data to other parties. To further preserve privacy, we study differentially-private FNAS (DP-FNAS), which adds random noise to the gradients of architecture variables. We provide theoretical guarantees of DP-FNAS in achieving differential privacy. Experiments show that DP-FNAS can search highly-performant neural architectures while protecting the privacy of individual parties. The code is available at https://github.com/UCSD-AI4H/DP-FNAS",2020-06-16T20:53:52Z,2020-06-22T21:48:41Z,http://arxiv.org/abs/2006.10559v2,http://arxiv.org/pdf/2006.10559v2,"cs.LG, cs.CR, stat.ML"
Machines and Algorithms,Peter A Boyle,"I discuss the evolution of computer architectures with a focus on QCD and with reference to the interplay between architecture, engineering, data motion and algorithms. New architectures are discussed and recent performance results are displayed. I also review recent progress in multilevel solver and integation algorithms.",2017-02-01T11:04:34Z,2017-02-01T11:04:34Z,http://arxiv.org/abs/1702.00208v1,http://arxiv.org/pdf/1702.00208v1,"hep-lat, cs.AR, cs.DC, physics.comp-ph"
From Requirements to Architecture: An AI-Based Journey to   Semi-Automatically Generate Software Architectures,"Tobias Eisenreich, Sandro Speth, Stefan Wagner","Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. To evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies.",2024-01-25T10:56:58Z,2024-01-25T10:56:58Z,http://arxiv.org/abs/2401.14079v1,http://arxiv.org/pdf/2401.14079v1,"cs.SE, cs.AI, D.2.2"
Depthwise Separable Convolutions with Deep Residual Convolutions,"Md Arid Hasan, Krishno Dey","The recent advancement of edge computing enables researchers to optimize various deep learning architectures to employ them in edge devices. In this study, we aim to optimize Xception architecture which is one of the most popular deep learning algorithms for computer vision applications. The Xception architecture is highly effective for object detection tasks. However, it comes with a significant computational cost. The computational complexity of Xception sometimes hinders its deployment on resource-constrained edge devices. To address this, we propose an optimized Xception architecture tailored for edge devices, aiming for lightweight and efficient deployment. We incorporate the depthwise separable convolutions with deep residual convolutions of the Xception architecture to develop a small and efficient model for edge devices. The resultant architecture reduces parameters, memory usage, and computational load. The proposed architecture is evaluated on the CIFAR 10 object detection dataset. The evaluation result of our experiment also shows the proposed architecture is smaller in parameter size and requires less training time while outperforming Xception architecture performance.",2024-11-12T04:47:32Z,2024-11-12T04:47:32Z,http://arxiv.org/abs/2411.07544v1,http://arxiv.org/pdf/2411.07544v1,"cs.CV, I.2.7"
A Study of the Learning Progress in Neural Architecture Search   Techniques,"Prabhant Singh, Tobias Jacobs, Sebastien Nicolas, Mischa Schmidt","In neural architecture search, the structure of the neural network to best model a given dataset is determined by an automated search process. Efficient Neural Architecture Search (ENAS), proposed by Pham et al. (2018), has recently received considerable attention due to its ability to find excellent architectures within a comparably short search time. In this work, which is motivated by the quest to further improve the learning speed of architecture search, we evaluate the learning progress of the controller which generates the architectures in ENAS. We measure the progress by comparing the architectures generated by it at different controller training epochs, where architectures are evaluated after having re-trained them from scratch. As a surprising result, we find that the learning curves are completely flat, i.e., there is no observable progress of the controller in terms of the performance of its generated architectures. This observation is consistent across the CIFAR-10 and CIFAR-100 datasets and two different search spaces. We conclude that the high quality of the models generated by ENAS is a result of the search space design rather than the controller training, and our results indicate that one-shot architecture design is an efficient alternative to architecture search by ENAS.",2019-06-18T14:00:19Z,2019-06-18T14:00:19Z,http://arxiv.org/abs/1906.07590v1,http://arxiv.org/pdf/1906.07590v1,"cs.LG, cs.NE, stat.ML"
Auto-GNN: Neural Architecture Search of Graph Neural Networks,"Kaixiong Zhou, Qingquan Song, Xiao Huang, Xia Hu","Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.   To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.",2019-09-07T04:10:41Z,2019-09-10T01:14:33Z,http://arxiv.org/abs/1909.03184v2,http://arxiv.org/pdf/1909.03184v2,"cs.LG, stat.ML"
Balanced One-shot Neural Architecture Optimization,"Renqian Luo, Tao Qin, Enhong Chen","The ability to rank candidate architectures is the key to the performance of neural architecture search~(NAS). One-shot NAS is proposed to reduce the expense but shows inferior performance against conventional NAS and is not adequately stable. We investigate into this and find that the ranking correlation between architectures under one-shot training and the ones under stand-alone full training is poor, which misleads the algorithm to discover better architectures. Further, we show that the training of architectures of different sizes under the current one-shot method is imbalanced, which causes the evaluated performances of the architectures to be less predictable of their ground-truth performances and affects the ranking correlation heavily. Consequently, we propose Balanced NAO where we introduce balanced training of the supernet during the search procedure to encourage more updates for large architectures than small architectures by sampling architectures in proportion to their model sizes. Comprehensive experiments verify that our proposed method is effective and robust which leads to a more stable search. The final discovered architecture shows significant improvements against baselines with a test error rate of 2.60\% on CIFAR-10 and top-1 accuracy of 74.4% on ImageNet under the mobile setting. Code and model checkpoints will be publicly available. The code is available at github.com/renqianluo/NAO_pytorch.",2019-09-24T11:18:52Z,2020-03-31T04:20:18Z,http://arxiv.org/abs/1909.10815v2,http://arxiv.org/pdf/1909.10815v2,"cs.LG, cs.NE, stat.ML"
The Mathematical Syntax of Architectures,Christoph F. Strnadl,"Despite several (accepted) standards, core notions typically employed in information technology or system engineering architectures lack the precise and exact foundations encountered in logic, algebra, and other branches of mathematics.   In this contribution we define the syntactical aspects of the term architecture in a mathematically rigorous way. We motivate our particular choice by demonstrating (i) how commonly understood and expected properties of an architecture -- as defined by various standards -- can be suitably identified or derived within our formalization, (ii) how our concept is fully compatible with real life (business) architectures, and (iii) how our definition complements recent foundational work in this area (Wilkinson 2018, Dickersen 2020, Efatmaneshnik 2020).   We furthermore develop a rigorous notion of architectural similarity based on the notion of homomorphisms allowing the class of architectures to be regarded as a category, Arch. We demonstrate the applicability of our concepts to theory by deriving theorems on the classification of certain types of architectures. Inter alia, we derive a no go theorem proving that, in contrast to n-tier architectures, one cannot sensibly define generic architectural modularity on the syntactical level alone.",2020-04-07T21:18:31Z,2024-11-30T13:10:33Z,http://arxiv.org/abs/2004.03719v3,http://arxiv.org/pdf/2004.03719v3,"cs.LO, cs.IT, math.IT, D.2.11; F.4.m; C.2.1"
Learned Transferable Architectures Can Surpass Hand-Designed   Architectures for Large Scale Speech Recognition,"Liqiang He, Dan Su, Dong Yu","In this paper, we explore the neural architecture search (NAS) for automatic speech recognition (ASR) systems. With reference to the previous works in the computer vision field, the transferability of the searched architecture is the main focus of our work. The architecture search is conducted on the small proxy dataset, and then the evaluation network, constructed with the searched architecture, is evaluated on the large dataset. Especially, we propose a revised search space for speech recognition tasks which theoretically facilitates the search algorithm to explore the architectures with low complexity. Extensive experiments show that: (i) the architecture searched on the small proxy dataset can be transferred to the large dataset for the speech recognition tasks. (ii) the architecture learned in the revised search space can greatly reduce the computational overhead and GPU memory usage with mild performance degradation. (iii) the searched architecture can achieve more than 20% and 15% (average on the four test sets) relative improvements respectively on the AISHELL-2 dataset and the large (10k hours) dataset, compared with our best hand-designed DFSMN-SAN architecture. To the best of our knowledge, this is the first report of NAS results with large scale dataset (up to 10K hours), indicating the promising application of NAS to industrial ASR systems.",2020-08-25T08:41:36Z,2021-05-08T09:24:57Z,http://arxiv.org/abs/2008.11589v3,http://arxiv.org/pdf/2008.11589v3,"eess.AS, cs.SD"
Retargeting GCC: Do We Reinvent the Wheel Every Time?,"Saravana Perumal P, Amey Karkare","Porting GCC to new architecture requires writing a Machine Description (MD) file that contains mapping from GCC's intermediate form to the target assembly code. Constructing an MD file is a difficult task because it requires the user to understand both (a) the internals of GCC, and (b) the intricacies of the target architecture. Instruction sets of different architectures exhibit significant amount of semantic similarities across a large class (for example, the instruction sets for RISC architectures) and differ only in syntax. Therefore, it is expected that MD files of machines with similar architectures should also have similarities. To confirm our hypothesis, we created ""mdcompare"", a tool to (a) extract RTL patterns (machine independent abstraction of RTL templates) from MD files of well known architectures and (b) compare the similarity of patterns across architectures. The results are encouraging; we found that 28% -- 70% RTL expressions are similar across pairs of MD files, the similarity percentage being on the higher side for pairs of similar architectures.",2013-09-30T00:48:02Z,2013-09-30T00:48:02Z,http://arxiv.org/abs/1309.7685v1,http://arxiv.org/pdf/1309.7685v1,"cs.PL, cs.SE, D.3.4"
Probabilistic Neural Architecture Search,"Francesco Paolo Casale, Jonathan Gordon, Nicolo Fusi","In neural architecture search (NAS), the space of neural network architectures is automatically explored to maximize predictive accuracy for a given task. Despite the success of recent approaches, most existing methods cannot be directly applied to large scale problems because of their prohibitive computational complexity or high memory usage. In this work, we propose a Probabilistic approach to neural ARchitecture SEarCh (PARSEC) that drastically reduces memory requirements while maintaining state-of-the-art computational complexity, making it possible to directly search over more complex architectures and larger datasets. Our approach only requires as much memory as is needed to train a single architecture from our search space. This is due to a memory-efficient sampling procedure wherein we learn a probability distribution over high-performing neural network architectures. Importantly, this framework enables us to transfer the distribution of architectures learnt on smaller problems to larger ones, further reducing the computational cost. We showcase the advantages of our approach in applications to CIFAR-10 and ImageNet, where our approach outperforms methods with double its computational cost and matches the performance of methods with costs that are three orders of magnitude larger.",2019-02-13T20:38:18Z,2019-02-13T20:38:18Z,http://arxiv.org/abs/1902.05116v1,http://arxiv.org/pdf/1902.05116v1,"stat.ML, cs.LG"
Dynamic Scheduling of MPI-based Distributed Deep Learning Training Jobs,"Tim Capes, Vishal Raheja, Mete Kemertas, Iqbal Mohomed","There is a general trend towards solving problems suited to deep learning with more complex deep learning architectures trained on larger training sets. This requires longer compute times and greater data parallelization or model parallelization. Both data and model parallelism have been historically faster in parameter server architectures, but data parallelism is starting to be faster in ring architectures due to algorithmic improvements. In this paper, we analyze the math behind ring architectures and make an informed adaptation of dynamic scheduling to ring architectures. To do so, we formulate a non-convex, non-linear, NP-hard integer programming problem and a new efficient doubling heuristic for its solution. We build upon Horovod: an open source ring architecture framework over TensorFlow. We show that Horovod jobs have a low cost to stop and restart and that stopping and restarting ring architecture jobs leads to faster completion times. These two facts make dynamic scheduling of ring architecture jobs feasible. Lastly, we simulate a scheduler using these runs and show a more than halving of average job time on some workload patterns.",2019-08-21T18:49:26Z,2019-08-21T18:49:26Z,http://arxiv.org/abs/1908.08082v1,http://arxiv.org/pdf/1908.08082v1,"cs.LG, cs.DC, stat.ML"
Neural Architecture Search in Embedding Space,Chun-Ting Liu,"The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The results of the experiment were efficient and indicated that NASES was highly efficient to discover final architecture only in $<$3.5 GPU hours. The beneficial-performance and effectiveness of NASES was impressive when the architecture-embedding searching and weight initialization were applied.",2019-09-09T03:28:55Z,2020-03-26T08:06:33Z,http://arxiv.org/abs/1909.03615v3,http://arxiv.org/pdf/1909.03615v3,"cs.LG, stat.ML"
Neural Predictor for Neural Architecture Search,"Wei Wen, Hanxiao Liu, Hai Li, Yiran Chen, Gabriel Bender, Pieter-Jan Kindermans","Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracy based on the architecture. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than 20 times as sample efficient as Regularized Evolution on the NASBench-101 benchmark and can compete on ImageNet with more complex approaches based on weight sharing, such as ProxylessNAS.",2019-12-02T15:10:59Z,2019-12-02T15:10:59Z,http://arxiv.org/abs/1912.00848v1,http://arxiv.org/pdf/1912.00848v1,"cs.LG, stat.ML"
FCC-GAN: A Fully Connected and Convolutional Net Architecture for GANs,"Sukarna Barua, Sarah Monazam Erfani, James Bailey","Generative Adversarial Networks (GANs) are a powerful class of generative models. Despite their successes, the most appropriate choice of a GAN network architecture is still not well understood. GAN models for image synthesis have adopted a deep convolutional network architecture, which eliminates or minimizes the use of fully connected and pooling layers in favor of convolution layers in the generator and discriminator of GANs. In this paper, we demonstrate that a convolution network architecture utilizing deep fully connected layers and pooling layers can be more effective than the traditional convolution-only architecture, and we propose FCC-GAN, a fully connected and convolutional GAN architecture. Models based on our FCC-GAN architecture learn both faster than the conventional architecture and also generate higher quality of samples. We demonstrate the effectiveness and stability of our approach across four popular image datasets.",2019-05-07T08:58:16Z,2019-05-27T06:06:04Z,http://arxiv.org/abs/1905.02417v2,http://arxiv.org/pdf/1905.02417v2,"cs.LG, stat.ML"
Neural Architecture Optimization with Graph VAE,"Jian Li, Yong Liu, Jiankun Liu, Weiping Wang","Due to their high computational efficiency on a continuous space, gradient optimization methods have shown great potential in the neural architecture search (NAS) domain. The mapping of network representation from the discrete space to a latent space is the key to discovering novel architectures, however, existing gradient-based methods fail to fully characterize the networks. In this paper, we propose an efficient NAS approach to optimize network architectures in a continuous space, where the latent space is built upon variational autoencoder (VAE) and graph neural networks (GNN). The framework jointly learns four components: the encoder, the performance predictor, the complexity predictor and the decoder in an end-to-end manner. The encoder and the decoder belong to a graph VAE, mapping architectures between continuous representations and network architectures. The predictors are two regression models, fitting the performance and computational complexity, respectively. Those predictors ensure the discovered architectures characterize both excellent performance and high computational efficiency. Extensive experiments demonstrate our framework not only generates appropriate continuous representations but also discovers powerful neural architectures.",2020-06-18T07:05:48Z,2020-06-18T07:05:48Z,http://arxiv.org/abs/2006.10310v1,http://arxiv.org/pdf/2006.10310v1,"cs.LG, stat.ML"
Neural Network Architecture Optimization through Submodularity and   Supermodularity,"Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang","Deep learning models' architectures, including depth and width, are key factors influencing models' performance, such as test accuracy and computation time. This paper solves two problems: given computation time budget, choose an architecture to maximize accuracy, and given accuracy requirement, choose an architecture to minimize computation time. We convert this architecture optimization into a subset selection problem. With accuracy's submodularity and computation time's supermodularity, we propose efficient greedy optimization algorithms. The experiments demonstrate our algorithm's ability to find more accurate models or faster models. By analyzing architecture evolution with growing time budget, we discuss relationships among accuracy, time and architecture, and give suggestions on neural network architecture design.",2016-09-01T00:59:30Z,2018-02-21T03:45:19Z,http://arxiv.org/abs/1609.00074v3,http://arxiv.org/pdf/1609.00074v3,"stat.ML, cs.LG"
Optimizing Deep Neural Network Architecture: A Tabu Search Based   Approach,"Tarun Kumar Gupta, Khalid Raza","The performance of Feedforward neural network (FNN) fully de-pends upon the selection of architecture and training algorithm. FNN architecture can be tweaked using several parameters, such as the number of hidden layers, number of hidden neurons at each hidden layer and number of connections between layers. There may be exponential combinations for these architectural attributes which may be unmanageable manually, so it requires an algorithm which can automatically design an optimal architecture with high generalization ability. Numerous optimization algorithms have been utilized for FNN architecture determination. This paper proposes a new methodology which can work on the estimation of hidden layers and their respective neurons for FNN. This work combines the advantages of Tabu search (TS) and Gradient descent with momentum backpropagation (GDM) training algorithm to demonstrate how Tabu search can automatically select the best architecture from the populated architectures based on minimum testing error criteria. The proposed approach has been tested on four classification benchmark dataset of different size.",2018-08-17T20:12:26Z,2018-08-17T20:12:26Z,http://arxiv.org/abs/1808.05979v1,http://arxiv.org/pdf/1808.05979v1,"cs.LG, cs.AI, cs.NE, stat.ML"
Efficient Neural Architecture Search for End-to-end Speech Recognition   via Straight-Through Gradients,"Huahuan Zheng, Keyu An, Zhijian Ou","Neural Architecture Search (NAS), the process of automating architecture engineering, is an appealing next step to advancing end-to-end Automatic Speech Recognition (ASR), replacing expert-designed networks with learned, task-specific architectures. In contrast to early computational-demanding NAS methods, recent gradient-based NAS methods, e.g., DARTS (Differentiable ARchiTecture Search), SNAS (Stochastic NAS) and ProxylessNAS, significantly improve the NAS efficiency. In this paper, we make two contributions. First, we rigorously develop an efficient NAS method via Straight-Through (ST) gradients, called ST-NAS. Basically, ST-NAS uses the loss from SNAS but uses ST to back-propagate gradients through discrete variables to optimize the loss, which is not revealed in ProxylessNAS. Using ST gradients to support sub-graph sampling is a core element to achieve efficient NAS beyond DARTS and SNAS. Second, we successfully apply ST-NAS to end-to-end ASR. Experiments over the widely benchmarked 80-hour WSJ and 300-hour Switchboard datasets show that the ST-NAS induced architectures significantly outperform the human-designed architecture across the two datasets. Strengths of ST-NAS such as architecture transferability and low computation cost in memory and time are also reported.",2020-11-11T09:18:58Z,2020-11-11T09:18:58Z,http://arxiv.org/abs/2011.05649v1,http://arxiv.org/pdf/2011.05649v1,"eess.AS, cs.LG"
Improved Conformer-based End-to-End Speech Recognition Using Neural   Architecture Search,"Yukun Liu, Ta Li, Pengyuan Zhang, Yonghong Yan","Recently neural architecture search(NAS) has been successfully used in image classification, natural language processing, and automatic speech recognition(ASR) tasks for finding the state-of-the-art(SOTA) architectures than those human-designed architectures. NAS can derive a SOTA and data-specific architecture over validation data from a pre-defined search space with a search algorithm. Inspired by the success of NAS in ASR tasks, we propose a NAS-based ASR framework containing one search space and one differentiable search algorithm called Differentiable Architecture Search(DARTS). Our search space follows the convolution-augmented transformer(Conformer) backbone, which is a more expressive ASR architecture than those used in existing NAS-based ASR frameworks. To improve the performance of our method, a regulation method called Dynamic Search Schedule(DSS) is employed. On a widely used Mandarin benchmark AISHELL-1, our best-searched architecture outperforms the baseline Conform model significantly with about 11% CER relative improvement, and our method is proved to be pretty efficient by the search cost comparisons.",2021-04-12T12:17:55Z,2021-04-13T01:36:56Z,http://arxiv.org/abs/2104.05390v2,http://arxiv.org/pdf/2104.05390v2,"eess.AS, cs.SD"
A Framework for the Design and Realization of Alternative   Superconducting Quantum Architectures,"Jagatheesan Kunasaikaran, Kevin Mato, Robert Wille","Superconducting quantum hardware architectures have been designed by considering the physical constraints of the underlying physics. These general-purpose architectures leave room for customization and optimization that can be exploited with alternative architectures specific to the quantum applications that will be executed on the quantum hardware. However, the corresponding design steps are hardly integrated yet and still rely heavily on manual labor. In this work, we provide a software framework that aims at providing a foundation to address this drawback. To this end, we first review the design of superconducting quantum hardware architectures and, afterwards, propose a cohesive framework encapsulating the design flow of an application-specific quantum hardware architecture. The resulting framework integrates high-level architecture generation optimized for a quantum application, the physical layout of the architecture, as well as optimization of the layout in a methodical manner. The framework with a reference implementation is available via https://github.com/cda-tum/dasqa under an open-source license.",2023-05-11T18:00:02Z,2024-05-03T14:28:46Z,http://arxiv.org/abs/2305.07052v2,http://arxiv.org/pdf/2305.07052v2,"quant-ph, cs.ET"
A Low-Latency FFT-IFFT Cascade Architecture,Keshab K. Parhi,"This paper addresses the design of a partly-parallel cascaded FFT-IFFT architecture that does not require any intermediate buffer. Folding can be used to design partly-parallel architectures for FFT and IFFT. While many cascaded FFT-IFFT architectures can be designed using various folding sets for the FFT and the IFFT, for a specified folded FFT architecture, there exists a unique folding set to design the IFFT architecture that does not require an intermediate buffer. Such a folding set is designed by processing the output of the FFT as soon as possible (ASAP) in the folded IFFT. Elimination of the intermediate buffer reduces latency and saves area. The proposed approach is also extended to interleaved processing of multi-channel time-series. The proposed FFT-IFFT cascade architecture saves about N/2 memory elements and N/4 clock cycles of latency compared to a design with identical folding sets. For the 2-interleaved FFT-IFFT cascade, the memory and latency savings are, respectively, N/2 units and N/2 clock cycles, compared to a design with identical folding sets.",2023-09-16T16:12:22Z,2023-09-16T16:12:22Z,http://arxiv.org/abs/2309.09035v1,http://arxiv.org/pdf/2309.09035v1,"cs.AR, eess.SP"
MixerFlow: MLP-Mixer meets Normalising Flows,"Eshant English, Matthias Kirchler, Christoph Lippert","Normalising flows are generative models that transform a complex density into a simpler density through the use of bijective transformations enabling both density estimation and data generation from a single model. %However, the requirement for bijectivity imposes the use of specialised architectures. In the context of image modelling, the predominant choice has been the Glow-based architecture, whereas alternative architectures remain largely unexplored in the research community. In this work, we propose a novel architecture called MixerFlow, based on the MLP-Mixer architecture, further unifying the generative and discriminative modelling architectures. MixerFlow offers an efficient mechanism for weight sharing for flow-based models. Our results demonstrate comparative or superior density estimation on image datasets and good scaling as the image resolution increases, making MixerFlow a simple yet powerful alternative to the Glow-based architectures. We also show that MixerFlow provides more informative embeddings than Glow-based architectures and can integrate many structured transformations such as splines or Kolmogorov-Arnold Networks.",2023-10-25T17:10:37Z,2024-06-27T04:23:30Z,http://arxiv.org/abs/2310.16777v2,http://arxiv.org/pdf/2310.16777v2,"stat.ML, cs.CV, cs.LG"
An Adaptive System Architecture for Multimodal Intelligent   Transportation Systems,"Muhammad Farooq, Nima Afraz, Fatemeh Golpayegani","Multimodal intelligent transportation systems (M-ITS) encompass a range of transportation services that utilise various modes of transport and incorporate intelligent technologies for enhanced efficiency and user experience. There are several challenges in M-ITS including data integration, Interoperability, scalability, user experience, etc. To address these challenges, such a system requires an adaptive system architecture that enables M-ITS to operate as an integrated ecosystem. In this paper, we provide an adaptive, user-centric, and layered architecture for multimodal transportation systems. The proposed architecture ensures scalability for seamless interactions of various subcomponents, that are often managed by different stakeholders. Concurrently, the data architecture is detailed, covering diverse data sources, advanced analytics, and stringent governance, providing a robust basis for intelligent decision-making. We provide two example use cases of the proposed architecture, showing how the data architecture and the system architecture can be fused and serve multimodal intelligent transport services.",2024-02-13T21:53:43Z,2024-02-13T21:53:43Z,http://arxiv.org/abs/2402.08817v1,http://arxiv.org/pdf/2402.08817v1,"eess.SY, cs.SY"
Comparative analysis of neural network architectures for short-term   FOREX forecasting,"Theodoros Zafeiriou, Dimitris Kalles","The present document delineates the analysis, design, implementation, and benchmarking of various neural network architectures within a short-term frequency prediction system for the foreign exchange market (FOREX). Our aim is to simulate the judgment of the human expert (technical analyst) using a system that responds promptly to changes in market conditions, thus enabling the optimization of short-term trading strategies. We designed and implemented a series of LSTM neural network architectures which are taken as input the exchange rate values and generate the short-term market trend forecasting signal and an ANN custom architecture based on technical analysis indicator simulators We performed a comparative analysis of the results and came to useful conclusions regarding the suitability of each architecture and the cost in terms of time and computational power to implement them. The ANN custom architecture produces better prediction quality with higher sensitivity using fewer resources and spending less time than LSTM architectures. The ANN custom architecture appears to be ideal for use in low-power computing systems and for use cases that need fast decisions with the least possible computational cost.",2024-05-13T14:51:02Z,2024-05-13T14:51:02Z,http://arxiv.org/abs/2405.08045v1,http://arxiv.org/pdf/2405.08045v1,"q-fin.MF, cs.AI, cs.LG, cs.NE"
Brain architecture: A design for natural computation,Marcus Kaiser,"Fifty years ago, John von Neumann compared the architecture of the brain with that of computers that he invented and which is still in use today. In those days, the organisation of computers was based on concepts of brain organisation. Here, we give an update on current results on the global organisation of neural systems. For neural systems, we outline how the spatial and topological architecture of neuronal and cortical networks facilitates robustness against failures, fast processing, and balanced network activation. Finally, we discuss mechanisms of self-organization for such architectures. After all, the organization of the brain might again inspire computer architecture.",2008-02-27T13:00:38Z,2008-02-27T13:00:38Z,http://arxiv.org/abs/0802.4010v1,http://arxiv.org/pdf/0802.4010v1,"q-bio.NC, cs.AI, cs.NE, physics.soc-ph"
Transfer NAS: Knowledge Transfer between Search Spaces with Transformer   Agents,"Zalán Borsos, Andrey Khorlin, Andrea Gesmundo","Recent advances in Neural Architecture Search (NAS) have produced state-of-the-art architectures on several tasks. NAS shifts the efforts of human experts from developing novel architectures directly to designing architecture search spaces and methods to explore them efficiently. The search space definition captures prior knowledge about the properties of the architectures and it is crucial for the complexity and the performance of the search algorithm. However, different search space definitions require restarting the learning process from scratch. We propose a novel agent based on the Transformer that supports joint training and efficient transfer of prior knowledge between multiple search spaces and tasks.",2019-06-19T13:57:33Z,2019-06-19T13:57:33Z,http://arxiv.org/abs/1906.08102v1,http://arxiv.org/pdf/1906.08102v1,"cs.LG, stat.ML"
A High Accuracy and High Sensitivity System Architecture for Electrical   Impedance Tomography System,"Hui Li, Boxiao Liu, Yongfu Li, Guoxing Wang, Yong Lian","A high accuracy and high sensitivity system architecture is proposed for the read-out circuit of electrical impedance tomography system-on-chip. The switched ratiometric technique is applied in the proposed architecture. The proposed system architecture minimizes the device noise by processing signals from both read-out electrodes and the stimulus. The quantized signals are post-processed in the digital processing unit for proper signal demodulation and impedance ratio calculation. Our proposed architecture improves the sensitivity of the read-out circuit, cancels out the gain fluctuations in the system, and overcomes the effects of motion artifacts on measurements.",2018-10-02T18:20:53Z,2018-10-02T18:20:53Z,http://arxiv.org/abs/1810.09517v1,http://arxiv.org/pdf/1810.09517v1,"physics.med-ph, cs.ET, eess.SP"
Architectural form as space-time cell,"Luisa Consiglieri, Vitor Consiglieri","The architecture has its basis in a dialectic search of new choices of representation. We deal with the form on the contemporary architecture under two approaches: expression and content. We examine how mathematical principles based on natural growth can be applied in architectural design in order to create a dynamic, rather than static, structure. The dynamic process of a cell and its growth provides the basic structure. We exemplify the impact of the new forms on the new society that already began.",2013-12-27T13:17:08Z,2013-12-27T13:17:08Z,http://arxiv.org/abs/1312.7256v1,http://arxiv.org/pdf/1312.7256v1,"math.HO, 00A67, 97G40, 97I60, 97M80"
An Introduction to Neural Architecture Search for Convolutional Networks,"George Kyriakides, Konstantinos Margaritis","Neural Architecture Search (NAS) is a research field concerned with utilizing optimization algorithms to design optimal neural network architectures. There are many approaches concerning the architectural search spaces, optimization algorithms, as well as candidate architecture evaluation methods. As the field is growing at a continuously increasing pace, it is difficult for a beginner to discern between major, as well as emerging directions the field has followed. In this work, we provide an introduction to the basic concepts of NAS for convolutional networks, along with the major advances in search spaces, algorithms and evaluation techniques.",2020-05-22T09:33:22Z,2020-05-22T09:33:22Z,http://arxiv.org/abs/2005.11074v1,http://arxiv.org/pdf/2005.11074v1,"cs.LG, cs.NE, stat.ML, A.1"
Architectural Blueprints: The 4+1 View Model of Software Architecture,Philippe Kruchten,"This article presents a model for describing the architecture of software-intensive systems, based on the use of multiple, concurrent views. This use of multiple views allows to address separately the concerns of the various stakeholders of the architecture: end-user, developers, systems engineers, project managers, etc., and to handle separately the functional and non functional requirements. Each of the five views is described, together with a notation to capture it. The views are designed using an architecture-centered, scenario-driven, iterative development process.",2020-06-08T22:38:39Z,2020-06-08T22:38:39Z,http://arxiv.org/abs/2006.04975v1,http://arxiv.org/pdf/2006.04975v1,"cs.SE, D.2.11"
Growing an architecture for a neural network,"Sergey Khashin, Ekaterina Shemyakova","We propose a new kind of automatic architecture search algorithm. The algorithm alternates pruning connections and adding neurons, and it is not restricted to layered architectures only. Here architecture is an arbitrary oriented graph with some weights (along with some biases and an activation function), so there may be no layered structure in such a network. The algorithm minimizes the complexity of staying within a given error. We demonstrate our algorithm on the brightness prediction problem of the next point through the previous points on an image. Our second test problem is the approximation of the bivariate function defining the brightness of a black and white image. Our optimized networks significantly outperform the standard solution for neural network architectures in both cases.",2021-08-04T18:17:22Z,2021-08-04T18:17:22Z,http://arxiv.org/abs/2108.02231v1,http://arxiv.org/pdf/2108.02231v1,"cs.LG, cs.NE, 68T07"
Performance Comparison of Deep Learning Architectures for Artifact   Removal in Gastrointestinal Endoscopic Imaging,"Taira Watanabe, Kensuke Tanioka, Satoru Hiwa, Tomoyuki Hiroyasu","Endoscopic images typically contain several artifacts. The artifacts significantly impact image analysis result in computer-aided diagnosis. Convolutional neural networks (CNNs), a type of deep learning, can removes such artifacts. Various architectures have been proposed for the CNNs, and the accuracy of artifact removal varies depending on the choice of architecture. Therefore, it is necessary to determine the artifact removal accuracy, depending on the selected architecture. In this study, we focus on endoscopic surgical instruments as artifacts, and determine and discuss the artifact removal accuracy using seven different CNN architectures.",2022-01-01T01:04:51Z,2022-01-01T01:04:51Z,http://arxiv.org/abs/2201.00084v1,http://arxiv.org/pdf/2201.00084v1,"eess.IV, cs.CV, cs.LG"
A Simplified Sub-Nyquist Receiver Architecture for Joint DOA and   Frequency Estimation,"Liang Liu, Ping Wei","Joint estimation of carrier frequency and direction of arrival (DOA) for multiple signals has been found in many practical applications such as Cognitive Radio (CR). However, Nyquist sampling mechanism is costly or implemented due to wide spectrum range. Taking advantage of sub-Nyquist sampling technology, some array receiver architectures are proposed to realize joint estimation of carrier frequency and DOA. To further decrease equivalent sampling rate and hardware complexity, we propose a simplifying receiver architecture based on our previous work. We come up with joint DOA and frequency estimation algorithms for the novel architecture. The simulations demonstrate that the receiver architecture and the proposed approaches are feasible.",2016-05-28T06:41:41Z,2016-05-28T06:41:41Z,http://arxiv.org/abs/1605.08851v1,http://arxiv.org/pdf/1605.08851v1,"cs.IT, math.IT"
Sensor Networks Architecture for Vehicles and Pedestrians Traffic   Control,"Ovidiu Banias, Daniel-Ioan Curiac, Radu-Emil Precup","In this paper we present a sensor network based architecture for urban traffic management, hierarchically structured on three layers: sensing, processing& aggregation and control. On proposed architecture we define traffic decongestion methods for vehicles and also for pedestrians. Finally, we presented a case study on how traffic control can be implemented in a concrete situation, based on the proposed architecture, pointing future directions of development.",2018-07-11T08:07:41Z,2018-07-11T08:07:41Z,http://arxiv.org/abs/1807.09222v1,http://arxiv.org/pdf/1807.09222v1,"eess.SP, cs.NI, cs.SE"
"A MAC-less Neural Inference Processor Supporting Compressed, Variable   Precision Weights",Vincenzo Liguori,"This paper introduces two architectures for the inference of convolutional neural networks (CNNs). Both architectures exploit weight sparsity and compression to reduce computational complexity and bandwidth. The first architecture uses multiply-accumulators (MACs) but avoids unnecessary multiplications by skipping zero weights. The second architecture exploits weight sparsity at the level of their bit representation by substituting resource-intensive MACs with much smaller Bit Layer Multiply Accumulators (BLMACs). The use of BLMACs also allows variable precision weights as variable size integers and even floating points. Some details of an implementation of the second architecture are given. Weight compression with arithmetic coding is also discussed as well as bandwidth implications. Finally, some implementation results for a pathfinder design and various technologies are presented.",2020-12-10T23:13:17Z,2020-12-10T23:13:17Z,http://arxiv.org/abs/2012.06018v1,http://arxiv.org/pdf/2012.06018v1,"cs.CV, B.0"
Multi-Channel FFT Architectures Designed via Folding and Interleaving,"Nanda K. Unnikrishnan, Keshab K. Parhi","Computing the FFT of a single channel is well understood in the literature. However, computing the FFT of multiple channels in a systematic manner has not been fully addressed. This paper presents a framework to design a family of multi-channel FFT architectures using {\em folding} and {\em interleaving}. Three distinct multi-channel FFT architectures are presented in this paper. These architectures differ in the input and output preprocessing steps and are based on different folding sets, i.e., different orders of execution.",2022-02-19T15:26:56Z,2022-02-19T15:26:56Z,http://arxiv.org/abs/2202.09623v1,http://arxiv.org/pdf/2202.09623v1,"eess.SP, cs.AR"
Hybrid Y-Net Architecture for Singing Voice Separation,"Rashen Fernando, Pamudu Ranasinghe, Udula Ranasinghe, Janaka Wijayakulasooriya, Pantaleon Perera","This research paper presents a novel deep learning-based neural network architecture, named Y-Net, for achieving music source separation. The proposed architecture performs end-to-end hybrid source separation by extracting features from both spectrogram and waveform domains. Inspired by the U-Net architecture, Y-Net predicts a spectrogram mask to separate vocal sources from a mixture signal. Our results demonstrate the effectiveness of the proposed architecture for music source separation with fewer parameters. Overall, our work presents a promising approach for improving the accuracy and efficiency of music source separation.",2023-03-05T07:54:49Z,2023-03-05T07:54:49Z,http://arxiv.org/abs/2303.02599v1,http://arxiv.org/pdf/2303.02599v1,"cs.SD, cs.LG, eess.AS"
Architecture Singularity Distance Computations for Linear Pentapods,"Aditya Kapilavai, Georg Nawratil","The kinematic/robotic community is not only interested in measuring the closeness of a given robot configuration to its next singular one but also in a geometric meaningful index evaluating how far the robot design is away from being architecturally singular. Such an architecture singularity distance, which can be used by engineers as a criterion within the design process, is presented for a certain class of parallel manipulators of Stewart-Gough type; namely so-called linear pentapods. Geometrically the architecture singular designs are well-understood and can be subclassified into several cases, which allows to solve the optimization problem of computing the closest architecture singular design to a given linear pentapod with algorithms from numerical algebraic geometry.",2023-12-14T17:29:26Z,2023-12-14T17:29:26Z,http://arxiv.org/abs/2312.09160v1,http://arxiv.org/pdf/2312.09160v1,"cs.RO, math.AG"
Virtual-Threading: Advanced General Purpose Processors Architecture,Andrei I. Yafimau,"The paper describes the new computers architecture, the main features of which has been claimed in the Russian Federation patent 2312388 and in the US patent application 11/991331. This architecture is intended to effective support of the General Purpose Parallel Computing (GPPC), the essence of which is extremely frequent switching of threads between states of activity and states of viewed in the paper the algorithmic latency. To emphasize the same impact of the architectural latency and the algorithmic latency upon GPPC, is introduced the new notion of the generalized latency and is defined its quantitative measure - the Generalized Latency Tolerance (GLT). It is shown that a well suited for GPPC implementation architecture should have high level of GLT and is described such architecture, which is called the Virtual-Threaded Machine. This architecture originates a processor virtualization in the direction of activities virtualization, which is orthogonal to the well-known direction of memory virtualization. The key elements of the architecture are 1) the distributed fine grain representation of the architectural register file, which elements are hardware swapped through levels of a microarchitectural memory, 2) the prioritized fine grain direct hardware multiprogramming, 3) the access controlled virtual addressing and 4) the hardware driven semaphores. The composition of these features lets to introduce new styles of operating system (OS) programming, which is free of interruptions, and of applied programming with a very rare using the OS services.",2009-10-21T11:31:14Z,2009-10-21T11:31:14Z,http://arxiv.org/abs/0910.4052v1,http://arxiv.org/pdf/0910.4052v1,"cs.AR, cs.OS, C.1.2; D.4.1; D.4.2"
ProxylessNAS: Direct Neural Architecture Search on Target Task and   Hardware,"Han Cai, Ligeng Zhu, Song Han","Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6$\times$ fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2$\times$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.",2018-12-02T05:29:53Z,2019-02-23T01:36:47Z,http://arxiv.org/abs/1812.00332v2,http://arxiv.org/pdf/1812.00332v2,"cs.LG, cs.CV, stat.ML"
Sub-Architecture Ensemble Pruning in Neural Architecture Search,"Yijun Bian, Qingquan Song, Mengnan Du, Jun Yao, Huanhuan Chen, Xia Hu","Neural architecture search (NAS) is gaining more and more attention in recent years due to its flexibility and remarkable capability to reduce the burden of neural network design. To achieve better performance, however, the searching process usually costs massive computations that might not be affordable for researchers and practitioners. While recent attempts have employed ensemble learning methods to mitigate the enormous computational cost, however, they neglect a key property of ensemble methods, namely diversity, which leads to collecting more similar sub-architectures with potential redundancy in the final design. To tackle this problem, we propose a pruning method for NAS ensembles called ""Sub-Architecture Ensemble Pruning in Neural Architecture Search (SAEP)."" It targets to leverage diversity and to achieve sub-ensemble architectures at a smaller size with comparable performance to ensemble architectures that are not pruned. Three possible solutions are proposed to decide which sub-architectures to prune during the searching process. Experimental results exhibit the effectiveness of the proposed method by largely reducing the number of sub-architectures without degrading the performance.",2019-10-01T13:26:54Z,2021-05-28T03:37:48Z,http://arxiv.org/abs/1910.00370v2,http://arxiv.org/pdf/1910.00370v2,"cs.LG, cs.CV, cs.NE, stat.ML"
ArcText: A Unified Text Approach to Describing Convolutional Neural   Network Architectures,"Yanan Sun, Ziyao Ren, Gary G. Yen, Bing Xue, Mengjie Zhang, Jiancheng Lv","The superiority of Convolutional Neural Networks (CNNs) largely relies on their architectures that are often manually crafted with extensive human expertise. Unfortunately, such kind of domain knowledge is not necessarily owned by each of the users interested. Data mining on existing CNN can discover useful patterns and fundamental sub-comments from their architectures, providing researchers with strong prior knowledge to design proper CNN architectures when they have no expertise in CNNs. There have been various state-of-the-art data mining algorithms at hand, while there is only rare work that has been done for the mining. One of the main reasons is the gap between CNN architectures and data mining algorithms. Specifically, the current CNN architecture descriptions cannot be exactly vectorized to the input of data mining algorithms. In this paper, we propose a unified approach, named ArcText, to describing CNN architectures based on text. Particularly, four different units and an ordering method have been elaborately designed in ArcText, to uniquely describe the same architecture with sufficient information. Also, the resulted description can be exactly converted back to the corresponding CNN architecture. ArcText bridges the gap between CNN architectures and data mining researchers, and has the potentiality to be utilized to wider scenarios.",2020-02-16T17:17:16Z,2020-05-29T08:43:12Z,http://arxiv.org/abs/2002.10233v4,http://arxiv.org/pdf/2002.10233v4,"cs.IR, cs.LG, eess.IV"
Automatically Designing CNN Architectures for Medical Image Segmentation,"Aliasghar Mortazi, Ulas Bagci","Deep neural network architectures have traditionally been designed and explored with human expertise in a long-lasting trial-and-error process. This process requires huge amount of time, expertise, and resources. To address this tedious problem, we propose a novel algorithm to optimally find hyperparameters of a deep network architecture automatically. We specifically focus on designing neural architectures for medical image segmentation task. Our proposed method is based on a policy gradient reinforcement learning for which the reward function is assigned a segmentation evaluation utility (i.e., dice index). We show the efficacy of the proposed method with its low computational cost in comparison with the state-of-the-art medical image segmentation networks. We also present a new architecture design, a densely connected encoder-decoder CNN, as a strong baseline architecture to apply the proposed hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As an application, we train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from a baseline segmentation architecture, the resulting network architecture obtains the state-of-the-art results in accuracy without performing any trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.",2018-07-19T23:47:12Z,2018-07-19T23:47:12Z,http://arxiv.org/abs/1807.07663v1,http://arxiv.org/pdf/1807.07663v1,"stat.ML, cs.CV, cs.LG"
Bayesian Neural Architecture Search using A Training-Free Performance   Metric,"Andrés Camero, Hao Wang, Enrique Alba, Thomas Bäck","Recurrent neural networks (RNNs) are a powerful approach for time series prediction. However, their performance is strongly affected by their architecture and hyperparameter settings. The architecture optimization of RNNs is a time-consuming task, where the search space is typically a mixture of real, integer and categorical values. To allow for shrinking and expanding the size of the network, the representation of architectures often has a variable length. In this paper, we propose to tackle the architecture optimization problem with a variant of the Bayesian Optimization (BO) algorithm. To reduce the evaluation time of candidate architectures the Mean Absolute Error Random Sampling (MRS), a training-free method to estimate the network performance, is adopted as the objective function for BO. Also, we propose three fixed-length encoding schemes to cope with the variable-length architecture representation. The result is a new perspective on accurate and efficient design of RNNs, that we validate on three problems. Our findings show that 1) the BO algorithm can explore different network architectures using the proposed encoding schemes and successfully designs well-performing architectures, and 2) the optimization time is significantly reduced by using MRS, without compromising the performance as compared to the architectures obtained from the actual training procedure.",2020-01-29T08:42:58Z,2021-04-23T07:48:42Z,http://arxiv.org/abs/2001.10726v2,http://arxiv.org/pdf/2001.10726v2,"cs.LG, cs.AI, cs.NE, stat.ML"
Systematically reviewing the layered architectural pattern principles   and their use to reconstruct software architectures,"Alvine B. Belle, Ghizlane El Boussaidi, Timothy C. Lethbridge, Segla Kpodjedo, Hafedh Mili, Andres Paz","Architectural reconstruction is a reverse engineering activity aiming at recovering the missing decisions on a system. It can help identify the components, within a legacy software application, according to the application's architectural pattern. It is useful to identify architectural technical debt. We are interested in identifying layers within a layered application since the layered pattern is one of the most used patterns to structure large systems. Earlier component reconstruction work focusing on that pattern relied on generic component identification criteria, such as cohesion and coupling. Recent work has identified architectural-pattern specific criteria to identify components within that pattern. However, the architectural-pattern specific criteria that the layered pattern embodies are loosely defined. In this paper, we present a first systematic literature review (SLR) of the literature aiming at inventorying such criteria for layers within legacy applications and grouping them under four principles that embody the fundamental design principles under-lying the architectural pattern. We identify six such criteria in the form of design rules. We also perform a second systematic literature review to synthesize the literature on software architecture reconstruction in the light of these criteria. We report those principles, the rules they encompass, their representation, and their usage in software architecture reconstruction.",2021-12-02T23:45:04Z,2021-12-02T23:45:04Z,http://arxiv.org/abs/2112.01644v1,http://arxiv.org/pdf/2112.01644v1,"cs.SE, N/A, D.2.2; D.2.10; D.2.11"
Intelligent Data-Driven Architectural Features Orchestration for Network   Slicing,"Rodrigo Moreira, Flavio de Oliveira Silva, Tereza Cristina Melo de Brito Carvalho, Joberto S. B. Martins","Network slicing is a crucial enabler and a trend for the Next Generation Mobile Network (NGMN) and various other new systems like the Internet of Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning are key elements with a crucial role in the network-slicing processes since the NS process needs to orchestrate resources and functionalities, and machine learning can potentially optimize the orchestration process. However, existing network-slicing architectures lack the ability to define intelligent approaches to orchestrate features and resources in the slicing process. This paper discusses machine learning-based orchestration of features and capabilities in network slicing architectures. Initially, the slice resource orchestration and allocation in the slicing planning, configuration, commissioning, and operation phases are analyzed. In sequence, we highlight the need for optimized architectural feature orchestration and recommend using ML-embed agents, federated learning intrinsic mechanisms for knowledge acquisition, and a data-driven approach embedded in the network slicing architecture. We further develop an architectural features orchestration case embedded in the SFI2 network slicing architecture. An attack prevention security mechanism is developed for the SFI2 architecture using distributed embedded and cooperating ML agents. The case presented illustrates the architectural feature's orchestration process and benefits, highlighting its importance for the network slicing process.",2024-01-12T12:32:36Z,2024-01-12T12:32:36Z,http://arxiv.org/abs/2401.06538v1,http://arxiv.org/pdf/2401.06538v1,"cs.NI, cs.AI, cs.LG, I.2.11; C.2.1; I.2.1"
The Evolution of Multimodal Model Architectures,"Shakti N. Wadekar, Abhishek Chaurasia, Aman Chadha, Eugenio Culurciello","This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape. Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain. Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types. The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model. The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage. Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers. On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model's input stage. The identified architecture types aid the monitoring of any-to-any multimodal model development. Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models. Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques. To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability.",2024-05-28T07:48:15Z,2024-05-28T07:48:15Z,http://arxiv.org/abs/2405.17927v1,http://arxiv.org/pdf/2405.17927v1,"cs.AI, cs.CL, cs.CV, cs.LG, eess.AS"
A Novel Reconfigurable Architecture of a DSP Processor for Efficient   Mapping of DSP Functions using Field Programmable DSP Arrays,"Amitabha Sinha, Mitrava Sarkar, Soumojit Acharyya, Suranjan Chakraborty","Development of modern integrated circuit technologies makes it feasible to develop cheaper, faster and smaller special purpose signal processing function circuits. Digital Signal processing functions are generally implemented either on ASICs with inflexibility, or on FPGAs with bottlenecks of relatively smaller utilization factor or lower speed compared to ASIC. Field Programmable DSP Array (FPDA) is the proposed DSP dedicated device, redolent to FPGA, but with basic fixed common modules (CMs) (like adders, subtractors, multipliers, scaling units, shifters) instead of CLBs. This paper introduces the development of reconfigurable system architecture with a focus on FPDA that integrates different DSP functions like DFT, FFT, DCT, FIR, IIR, and DWT etc. The switching between DSP functions is occurred by reconfiguring the interconnection between CMs. Validation of the proposed architecture has been achieved on Virtex5 FPGA. The architecture provides sufficient amount of flexibility, parallelism and scalability.",2013-06-01T09:04:40Z,2013-06-01T09:04:40Z,http://arxiv.org/abs/1306.0089v1,http://arxiv.org/pdf/1306.0089v1,"cs.AR, 68R01"
An Efficient List Decoder Architecture for Polar Codes,"Jun Lin, Zhiyuan Yan","Long polar codes can achieve the symmetric capacity of arbitrary binary-input discrete memoryless channels under a low complexity successive cancelation (SC) decoding algorithm. However, for polar codes with short and moderate code length, the decoding performance of the SC algorithm is inferior. The cyclic redundancy check (CRC) aided successive cancelation list (SCL) decoding algorithm has better error performance than the SC algorithm for short or moderate polar codes. In this paper, we propose an efficient list decoder architecture for the CRC aided SCL algorithm, based on both algorithmic reformulations and architectural techniques. In particular, an area efficient message memory architecture is proposed to reduce the area of the proposed decoder architecture. An efficient path pruning unit suitable for large list size is also proposed. For a polar code of length 1024 and rate $\frac{1}{2}$, when list size $L=2$ and 4, the proposed list decoder architecture is implemented under a TSMC 90nm CMOS technology. Compared with the list decoders in the literature, our decoder achieves 1.33 to 1.96 times hardware efficiency.",2014-09-16T19:39:49Z,2014-11-16T15:59:04Z,http://arxiv.org/abs/1409.4744v2,http://arxiv.org/pdf/1409.4744v2,"cs.AR, cs.IT, math.IT"
Architecture and Function of Mechanosensitive Membrane Protein Lattices,"Osman Kahraman, Peter D. Koch, William S. Klug, Christoph A. Haselwandter","Experiments have revealed that membrane proteins can form two-dimensional clusters with regular translational and orientational protein arrangements, which may allow cells to modulate protein function. However, the physical mechanisms yielding supramolecular organization and collective function of membrane proteins remain largely unknown. Here we show that bilayer-mediated elastic interactions between membrane proteins can yield regular and distinctive lattice architectures of protein clusters, and may provide a link between lattice architecture and lattice function. Using the mechanosensitive channel of large conductance (MscL) as a model system, we obtain relations between the shape of MscL and the supramolecular architecture of MscL lattices. We predict that the tetrameric and pentameric MscL symmetries observed in previous structural studies yield distinct lattice architectures of MscL clusters and that, in turn, these distinct MscL lattice architectures yield distinct lattice activation barriers. Our results suggest general physical mechanisms linking protein symmetry, the lattice architecture of membrane protein clusters, and the collective function of membrane protein lattices.",2016-11-02T22:50:55Z,2016-11-02T22:50:55Z,http://arxiv.org/abs/1611.00833v1,http://arxiv.org/pdf/1611.00833v1,"q-bio.BM, cond-mat.soft, physics.bio-ph, q-bio.SC"
Path-Level Network Transformation for Efficient Architecture Search,"Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, Yong Yu","We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.",2018-06-07T12:25:05Z,2018-06-07T12:25:05Z,http://arxiv.org/abs/1806.02639v1,http://arxiv.org/pdf/1806.02639v1,"cs.LG, cs.AI, stat.ML"
DARTS: Differentiable Architecture Search,"Hanxiao Liu, Karen Simonyan, Yiming Yang","This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",2018-06-24T00:06:13Z,2019-04-23T06:29:32Z,http://arxiv.org/abs/1806.09055v2,http://arxiv.org/pdf/1806.09055v2,"cs.LG, cs.CL, cs.CV, stat.ML"
MONAS: Multi-Objective Neural Architecture Search using Reinforcement   Learning,"Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping Chou, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan","Recent studies on neural architecture search have shown that automatically designed neural networks perform as good as expert-crafted architectures. While most existing works aim at finding architectures that optimize the prediction accuracy, these architectures may have complexity and is therefore not suitable being deployed on certain computing environment (e.g., with limited power budgets). We propose MONAS, a framework for Multi-Objective Neural Architectural Search that employs reward functions considering both prediction accuracy and other important objectives (e.g., power consumption) when searching for neural network architectures. Experimental results showed that, compared to the state-ofthe-arts, models found by MONAS achieve comparable or better classification accuracy on computer vision applications, while satisfying the additional objectives such as peak power.",2018-06-27T08:12:01Z,2018-12-03T06:54:48Z,http://arxiv.org/abs/1806.10332v2,http://arxiv.org/pdf/1806.10332v2,"cs.LG, cs.AI, stat.ML"
AGAN: Towards Automated Design of Generative Adversarial Networks,"Hanchao Wang, Jun Huan","Recent progress in Generative Adversarial Networks (GANs) has shown promising signs of improving GAN training via architectural change. Despite some early success, at present the design of GAN architectures requires human expertise, laborious trial-and-error testings, and often draws inspiration from its image classification counterpart. In the current paper, we present the first neural architecture search algorithm, automated neural architecture search for deep generative models, or AGAN for abbreviation, that is specifically suited for GAN training. For unsupervised image generation tasks on CIFAR-10, our algorithm finds architecture that outperforms state-of-the-art models under same regularization techniques. For supervised tasks, the automatically searched architectures also achieve highly competitive performance, outperforming best human-invented architectures at resolution $32\times32$. Moreover, we empirically demonstrate that the modules learned by AGAN are transferable to other image generation tasks such as STL-10.",2019-06-25T10:12:32Z,2019-06-25T10:12:32Z,http://arxiv.org/abs/1906.11080v1,http://arxiv.org/pdf/1906.11080v1,"cs.LG, cs.AI, stat.ML"
Detecting Architectural Erosion using Runtime Verification,"Diego Marmsoler, Ana Petrovska","The architecture of a system captures important design decisions for the system. Over time, changes in a system's implementation may lead to violations of specific design decisions. This problem is common in industry and known as architectural erosion. Since it may have severe consequences on the quality of a system, research has focused on the development of tools and techniques to address the presented problem. As of today, most of the approaches to detect architectural erosion employ static analysis techniques. While these techniques are well-suited for the analysis of static architectures, they reach their limit when it comes to dynamic architectures. Thus, in this paper, we propose an alternative approach based on runtime verification. To this end, we propose a systematic way to translate a formal specification of architectural constraints to monitors, which can be used to detect violations of these constraints. The approach is implemented in Eclipse/EMF, demonstrated through a running example, and evaluated using two case studies.",2019-09-12T22:25:13Z,2019-09-12T22:25:13Z,http://arxiv.org/abs/1909.05973v1,http://arxiv.org/pdf/1909.05973v1,"cs.SE, cs.DC, cs.LO, Software and its engineering.Dynamic analysis"
Deep learning architectures for automated image segmentation,Debleena Sengupta,"Image segmentation is widely used in a variety of computer vision tasks, such as object localization and recognition, boundary detection, and medical imaging. This thesis proposes deep learning architectures to improve automatic object localization and boundary delineation for salient object segmentation in natural images and for 2D medical image segmentation. First, we propose and evaluate a novel dilated dense encoder-decoder architecture with a custom dilated spatial pyramid pooling block to accurately localize and delineate boundaries for salient object segmentation. The dilation offers better spatial understanding and the dense connectivity preserves features learned at shallower levels of the network for better localization. Tested on three publicly available datasets, our architecture outperforms the state-of-the-art for one and is very competitive on the other two. Second, we propose and evaluate a custom 2D dilated dense UNet architecture for accurate lesion localization and segmentation in medical images. This architecture can be utilized as a stand-alone segmentation framework or used as a rich feature extracting backbone to aid other models in medical image segmentation. Our architecture outperforms all baseline models for accurate lesion localization and segmentation on a new dataset. We furthermore explore the main considerations that should be taken into account for 3D medical image segmentation, among them preprocessing techniques and specialized loss functions.",2019-09-19T19:46:30Z,2019-09-19T19:46:30Z,http://arxiv.org/abs/1909.10333v1,http://arxiv.org/pdf/1909.10333v1,"eess.IV, cs.LG, stat.ML"
Optimized Gated Deep Learning Architectures for Sensor Fusion,"Myung Seok Shim, Peng Li","Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control. Deep neural networks have been adopted for sensor fusion in a body of recent studies. Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolutional neural networks (CNN). In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature level fusion weights. Using driving mode prediction and human activity recognition datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.",2018-10-08T18:24:12Z,2018-10-08T18:24:12Z,http://arxiv.org/abs/1810.04160v1,http://arxiv.org/pdf/1810.04160v1,"cs.LG, stat.ML"
GraphNAS: Graph Neural Architecture Search with Reinforcement Learning,"Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, Yue Hu","Graph Neural Networks (GNNs) have been popularly used for analyzing non-Euclidean data such as social network data and biological data. Despite their success, the design of graph neural networks requires a lot of manual work and domain knowledge. In this paper, we propose a Graph Neural Architecture Search method (GraphNAS for short) that enables automatic search of the best graph neural architecture based on reinforcement learning. Specifically, GraphNAS first uses a recurrent network to generate variable-length strings that describe the architectures of graph neural networks, and then trains the recurrent network with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation data set. Extensive experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that GraphNAS can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network. On node classification tasks, GraphNAS can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy.",2019-04-22T07:13:10Z,2019-08-20T03:00:40Z,http://arxiv.org/abs/1904.09981v2,http://arxiv.org/pdf/1904.09981v2,"cs.LG, cs.AI, stat.ML"
NASIB: Neural Architecture Search withIn Budget,"Abhishek Singh, Anubhav Garg, Jinan Zhou, Shiv Ram Dubey, Debo Dutta","Neural Architecture Search (NAS) represents a class of methods to generate the optimal neural network architecture and typically iterate over candidate architectures till convergence over some particular metric like validation loss. They are constrained by the available computation resources, especially in enterprise environments. In this paper, we propose a new approach for NAS, called NASIB, which adapts and attunes to the computation resources (budget) available by varying the exploration vs. exploitation trade-off. We reduce the expert bias by searching over an augmented search space induced by Superkernels. The proposed method can provide the architecture search useful for different computation resources and different domains beyond image classification of natural images where we lack bespoke architecture motifs and domain expertise. We show, on CIFAR10, that itis possible to search over a space that comprises of 12x more candidate operations than the traditional prior art in just 1.5 GPU days, while reaching close to state of the art accuracy. While our method searches over an exponentially larger search space, it could lead to novel architectures that require lesser domain expertise, compared to the majority of the existing methods.",2019-10-19T00:12:39Z,2019-10-19T00:12:39Z,http://arxiv.org/abs/1910.08665v1,http://arxiv.org/pdf/1910.08665v1,"cs.LG, cs.CV, stat.ML"
BayesNAS: A Bayesian Approach for Neural Architecture Search,"Hongpeng Zhou, Minghao Yang, Jun Wang, Wei Pan","One-Shot Neural Architecture Search (NAS) is a promising method to significantly reduce search time without any separate training. It can be treated as a Network Compression problem on the architecture parameters from an over-parameterized network. However, there are two issues associated with most one-shot NAS methods. First, dependencies between a node and its predecessors and successors are often disregarded which result in improper treatment over zero operations. Second, architecture parameters pruning based on their magnitude is questionable. In this paper, we employ the classic Bayesian learning approach to alleviate these two issues by modeling architecture parameters using hierarchical automatic relevance determination (HARD) priors. Unlike other NAS methods, we train the over-parameterized network for only one epoch then update the architecture. Impressively, this enabled us to find the architecture on CIFAR-10 within only 0.2 GPU days using a single GPU. Competitive performance can be also achieved by transferring to ImageNet. As a byproduct, our approach can be applied directly to compress convolutional neural networks by enforcing structural sparsity which achieves extremely sparse networks without accuracy deterioration.",2019-05-13T09:00:13Z,2019-06-07T21:12:09Z,http://arxiv.org/abs/1905.04919v2,http://arxiv.org/pdf/1905.04919v2,"cs.LG, stat.ML"
EENA: Efficient Evolution of Neural Architecture,"Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Erhu Zhao, Yongjun Xu","Latest algorithms for automatic neural architecture search perform remarkable but are basically directionless in search space and computational expensive in training of every intermediate architecture. In this paper, we propose a method for efficient architecture search called EENA (Efficient Evolution of Neural Architecture). Due to the elaborately designed mutation and crossover operations, the evolution process can be guided by the information have already been learned. Therefore, less computational effort will be required while the searching and training time can be reduced significantly. On CIFAR-10 classification, EENA using minimal computational resources (0.65 GPU-days) can design highly effective neural architecture which achieves 2.56% test error with 8.47M parameters. Furthermore, the best architecture discovered is also transferable for CIFAR-100.",2019-05-10T02:34:23Z,2019-08-27T03:32:59Z,http://arxiv.org/abs/1905.07320v3,http://arxiv.org/pdf/1905.07320v3,"cs.NE, cs.CV, cs.LG, stat.ML"
uRbAn: A Multipath Routing based Architecture with Energy and Mobility   Management for Quality of Service Support in Mobile Ad hoc Networks,Ash Mohammad Abbas,"Designing a wireless node that supports quality of service (QoS) in a mobile ad hoc network is a challenging task. In this paper, we propose an architecture of a wireless node that may be used to form a mobile ad hoc network that supports QoS. We discuss the core functionalities required for such a node and how those functionalities can be incorporated. A feature of our architecture is that the node has the ability to utilize multiple paths, if available, for the provision of QoS. However, in the absence of multiple paths it can utilize the resources provided by a single path between the source and the destination. We follow a modular approach where each module is expanded iteratively. We compare the features of our architecture with the existing architectures proposed in the literature. Our architecture has provisions of energy and mobility management and it can be customized to design a system-on-chip (SoC).",2011-02-24T17:25:50Z,2011-02-24T17:25:50Z,http://arxiv.org/abs/1102.5043v1,http://arxiv.org/pdf/1102.5043v1,"cs.NI, 68M10, 68M12, C.2.1; C.2.2"
Stabilizing Differentiable Architecture Search via Perturbation-based   Regularization,"Xiangning Chen, Cho-Jui Hsieh","Differentiable architecture search (DARTS) is a prevailing NAS solution to identify architectures. Based on the continuous relaxation of the architecture space, DARTS learns a differentiable architecture weight and largely reduces the search cost. However, its stability has been challenged for yielding deteriorating architectures as the search proceeds. We find that the precipitous validation loss landscape, which leads to a dramatic performance drop when distilling the final architecture, is an essential factor that causes instability. Based on this observation, we propose a perturbation-based regularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve the generalizability of DARTS-based methods. In particular, our new formulations stabilize DARTS-based methods by either random smoothing or adversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the effectiveness of our approach and due to the improved stability, we achieve performance gain across various search spaces on 4 datasets. Furthermore, we mathematically show that SDARTS implicitly regularizes the Hessian norm of the validation loss, which accounts for a smoother loss landscape and improved performance.",2020-02-12T23:46:58Z,2021-01-12T19:17:24Z,http://arxiv.org/abs/2002.05283v3,http://arxiv.org/pdf/2002.05283v3,"cs.LG, cs.CV, stat.ML"
Automatically Searching for U-Net Image Translator Architecture,"Han Shu, Yunhe Wang","Image translators have been successfully applied to many important low level image processing tasks. However, classical network architecture of image translator like U-Net, is borrowed from other vision tasks like biomedical image segmentation. This straightforward adaptation may not be optimal and could cause redundancy in the network structure. In this paper, we propose an automatic architecture searching method for image translator. By utilizing evolutionary algorithm, we investigate a more efficient network architecture which costs less computation resources and achieves better performance than the original one. Extensive qualitative and quantitative experiments are conducted to demonstrate the effectiveness of the proposed method. Moreover, we transplant the searched network architecture to other datasets which are not involved in the architecture searching procedure. Efficiency of the searched architecture on these datasets further demonstrates the generalization of the method.",2020-02-26T16:05:23Z,2020-02-26T16:05:23Z,http://arxiv.org/abs/2002.11581v1,http://arxiv.org/pdf/2002.11581v1,"eess.IV, cs.CV"
A Generic Graph-based Neural Architecture Encoding Scheme for   Predictor-based NAS,"Xuefei Ning, Yin Zheng, Tianchen Zhao, Yu Wang, Huazhong Yang","This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme, a.k.a. GATES, to improve the predictor-based neural architecture search. Specifically, different from existing graph-based schemes, GATES models the operations as the transformation of the propagating information, which mimics the actual data processing of neural architecture. GATES is a more reasonable modeling of the neural architectures, and can encode architectures from both the ""operation on node"" and ""operation on edge"" cell search spaces consistently. Experimental results on various search spaces confirm GATES's effectiveness in improving the performance predictor. Furthermore, equipped with the improved performance predictor, the sample efficiency of the predictor-based neural architecture search (NAS) flow is boosted. Codes are available at https://github.com/walkerning/aw_nas.",2020-04-04T09:54:49Z,2020-09-01T01:06:51Z,http://arxiv.org/abs/2004.01899v3,http://arxiv.org/pdf/2004.01899v3,"cs.LG, cs.NE, stat.ML"
Learning Architectures from an Extended Search Space for Language   Modeling,"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li","Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",2020-05-06T05:02:33Z,2020-06-05T06:23:49Z,http://arxiv.org/abs/2005.02593v2,http://arxiv.org/pdf/2005.02593v2,"cs.LG, cs.CL, stat.ML"
DARTS-ASR: Differentiable Architecture Search for Multilingual Speech   Recognition and Adaptation,"Yi-Chen Chen, Jui-Yang Hsu, Cheng-Kuang Lee, Hung-yi Lee","In previous works, only parameter weights of ASR models are optimized under fixed-topology architecture. However, the design of successful model architecture has always relied on human experience and intuition. Besides, many hyperparameters related to model architecture need to be manually tuned. Therefore in this paper, we propose an ASR approach with efficient gradient-based architecture search, DARTS-ASR. In order to examine the generalizability of DARTS-ASR, we apply our approach not only on many languages to perform monolingual ASR, but also on a multilingual ASR setting. Following previous works, we conducted experiments on a multilingual dataset, IARPA BABEL. The experiment results show that our approach outperformed the baseline fixed-topology architecture by 10.2% and 10.0% relative reduction on character error rates under monolingual and multilingual ASR settings respectively. Furthermore, we perform some analysis on the searched architectures by DARTS-ASR.",2020-05-13T11:32:27Z,2020-07-26T02:40:53Z,http://arxiv.org/abs/2005.07029v2,http://arxiv.org/pdf/2005.07029v2,"eess.AS, cs.CL, cs.LG, cs.SD"
DC-NAS: Divide-and-Conquer Neural Architecture Search,"Yunhe Wang, Yixing Xu, Dacheng Tao","Most applications demand high-performance deep neural architectures costing limited resources. Neural architecture searching is a way of automatically exploring optimal deep neural networks in a given huge search space. However, all sub-networks are usually evaluated using the same criterion; that is, early stopping on a small proportion of the training dataset, which is an inaccurate and highly complex approach. In contrast to conventional methods, here we present a divide-and-conquer (DC) approach to effectively and efficiently search deep neural architectures. Given an arbitrary search space, we first extract feature representations of all sub-networks according to changes in parameters or output features of each layer, and then calculate the similarity between two different sampled networks based on the representations. Then, a k-means clustering is conducted to aggregate similar architectures into the same cluster, separately executing sub-network evaluation in each cluster. The best architecture in each cluster is later merged to obtain the optimal neural architecture. Experimental results conducted on several benchmarks illustrate that DC-NAS can overcome the inaccurate evaluation problem, achieving a $75.1\%$ top-1 accuracy on the ImageNet dataset, which is higher than that of state-of-the-art methods using the same search space.",2020-05-29T09:02:16Z,2020-05-29T09:02:16Z,http://arxiv.org/abs/2005.14456v1,http://arxiv.org/pdf/2005.14456v1,"cs.LG, stat.ML"
Interpretable Neural Architecture Search via Bayesian Optimisation with   Weisfeiler-Lehman Kernels,"Binxin Ru, Xingchen Wan, Xiaowen Dong, Michael Osborne","Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method optimises the architecture in a highly data-efficient manner: it is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. More importantly, our method affords interpretability by discovering useful network features and their corresponding impact on the network performance. Indeed, we demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.",2020-06-13T04:10:34Z,2021-02-19T05:36:54Z,http://arxiv.org/abs/2006.07556v2,http://arxiv.org/pdf/2006.07556v2,"cs.LG, stat.ML"
Multi-fidelity Neural Architecture Search with Knowledge Distillation,"Ilya Trofimov, Nikita Klyuchnikov, Mikhail Salnikov, Alexander Filippov, Evgeny Burnaev","Neural architecture search (NAS) targets at finding the optimal architecture of a neural network for a problem or a family of problems. Evaluations of neural architectures are very time-consuming. One of the possible ways to mitigate this issue is to use low-fidelity evaluations, namely training on a part of a dataset, fewer epochs, with fewer channels, etc. In this paper, we propose a bayesian multi-fidelity method for neural architecture search: MF-KD. The method relies on a new approach to low-fidelity evaluations of neural architectures by training for a few epochs using a knowledge distillation. Knowledge distillation adds to a loss function a term forcing a network to mimic some teacher network. We carry out experiments on CIFAR-10, CIFAR-100, and ImageNet-16-120. We show that training for a few epochs with such a modified loss function leads to a better selection of neural architectures than training for a few epochs with a logistic loss. The proposed method outperforms several state-of-the-art baselines.",2020-06-15T12:32:38Z,2021-05-19T09:17:16Z,http://arxiv.org/abs/2006.08341v2,http://arxiv.org/pdf/2006.08341v2,"cs.LG, stat.ML"
Fine-Grained Stochastic Architecture Search,"Shraman Ray Chaudhuri, Elad Eban, Hanhan Li, Max Moroz, Yair Movshovitz-Attias","State-of-the-art deep networks are often too large to deploy on mobile devices and embedded systems. Mobile neural architecture search (NAS) methods automate the design of small models but state-of-the-art NAS methods are expensive to run. Differentiable neural architecture search (DNAS) methods reduce the search cost but explore a limited subspace of candidate architectures. In this paper, we introduce Fine-Grained Stochastic Architecture Search (FiGS), a differentiable search method that searches over a much larger set of candidate architectures. FiGS simultaneously selects and modifies operators in the search space by applying a structured sparse regularization penalty based on the Logistic-Sigmoid distribution. We show results across 3 existing search spaces, matching or outperforming the original search algorithms and producing state-of-the-art parameter-efficient models on ImageNet (e.g., 75.4% top-1 with 2.6M params). Using our architectures as backbones for object detection with SSDLite, we achieve significantly higher mAP on COCO (e.g., 25.8 with 3.0M params) than MobileNetV3 and MnasNet.",2020-06-17T01:04:14Z,2020-06-17T01:04:14Z,http://arxiv.org/abs/2006.09581v1,http://arxiv.org/pdf/2006.09581v1,"cs.LG, stat.ML"
Equivalence in Deep Neural Networks via Conjugate Matrix Ensembles,Mehmet Süzen,"A numerical approach is developed for detecting the equivalence of deep learning architectures. The method is based on generating Mixed Matrix Ensembles (MMEs) out of deep neural network weight matrices and {\it conjugate circular ensemble} matching the neural architecture topology. Following this, the empirical evidence supports the {\it phenomenon} that difference between spectral densities of neural architectures and corresponding {\it conjugate circular ensemble} are vanishing with different decay rates at the long positive tail part of the spectrum i.e., cumulative Circular Spectral Difference (CSD). This finding can be used in establishing equivalences among different neural architectures via analysis of fluctuations in CSD. We investigated this phenomenon for a wide range of deep learning vision architectures and with circular ensembles originating from statistical quantum mechanics. Practical implications of the proposed method for artificial and natural neural architectures discussed such as the possibility of using the approach in Neural Architecture Search (NAS) and classification of biological neural networks.",2020-06-14T12:34:13Z,2020-08-30T20:44:57Z,http://arxiv.org/abs/2006.13687v2,http://arxiv.org/pdf/2006.13687v2,"cs.LG, cond-mat.dis-nn, stat.ML, 68-02, 65-F35, 52-C45, 68-T07, I.2.6; G.3"
Ranking architectures using meta-learning,"Alina Dubatovka, Efi Kokiopoulou, Luciano Sbaiz, Andrea Gesmundo, Gabor Bartok, Jesse Berent","Neural architecture search has recently attracted lots of research efforts as it promises to automate the manual design of neural networks. However, it requires a large amount of computing resources and in order to alleviate this, a performance prediction network has been recently proposed that enables efficient architecture search by forecasting the performance of candidate architectures, instead of relying on actual model training. The performance predictor is task-aware taking as input not only the candidate architecture but also task meta-features and it has been designed to collectively learn from several tasks. In this work, we introduce a pairwise ranking loss for training a network able to rank candidate architectures for a new unseen task conditioning on its task meta-features. We present experimental results, showing that the ranking network is more effective in architecture search than the previously proposed performance predictor.",2019-11-26T12:04:51Z,2019-11-26T12:04:51Z,http://arxiv.org/abs/1911.11481v1,http://arxiv.org/pdf/1911.11481v1,"cs.LG, stat.ML"
Stacked Boosters Network Architecture for Short Term Load Forecasting in   Buildings,"Tuukka Salmi, Jussi Kiljander, Daniel Pakkala","This paper presents a novel deep learning architecture for short term load forecasting of building energy loads. The architecture is based on a simple base learner and multiple boosting systems that are modelled as a single deep neural network. The architecture transforms the original multivariate time series into multiple cascading univariate time series. Together with sparse interactions, parameter sharing and equivariant representations, this approach makes it possible to combat against overfitting while still achieving good presentation power with a deep network architecture. The architecture is evaluated in several short-term load forecasting tasks with energy data from an office building in Finland. The proposed architecture outperforms state-of-the-art load forecasting model in all the tasks.",2020-01-23T08:35:36Z,2020-04-16T05:20:40Z,http://arxiv.org/abs/2001.08406v2,http://arxiv.org/pdf/2001.08406v2,"cs.LG, stat.ML"
Architectural Analysis of FPGA Technology Impact,"Oriol Arcas-Abella, Abhinav Agarwal","The use of high-level languages for designing hardware is gaining popularity since they increase design productivity by providing higher abstractions. However, one drawback of such abstraction level has been the difficulty of relating the low-level implementation problems back to the original high-level design, which is paramount for architectural optimization. In this work (developed between April 2013 and April 2014), we propose a methodology to analyze the effects of technology over the architecture, and to generate architectural-level area, delay and power metrics. Such feedback allows the designer to quickly gauge the impact of architectural decisions on the quality of generated hardware and opens the door to automatic architectural analysis. We demonstrate the use of our technique on three FPGA platforms using two designs: a Reed-Solomon error correction decoder and a 32-bit pipelined processor implementation.",2020-08-31T08:41:51Z,2020-08-31T08:41:51Z,http://arxiv.org/abs/2008.13430v1,http://arxiv.org/pdf/2008.13430v1,"cs.AR, B.6.3"
A Functional Safety Assessment Method for Cooperative Automotive   Architecture,"Sangeeth Kochanthara, Niels Rood, Arash Khabbaz Saberi, Loek Cleophas, Yanja Dajsuren, Mark van den Brand","The scope of automotive functions has grown from a single-vehicle as an entity to multiple vehicles working together as an entity, referred to as cooperative driving. The current automotive safety standard, ISO 26262, is designed for single vehicles. With the increasing number of cooperative driving capable vehicles on the road, it is now imperative to systematically assess the functional safety of architectures of these vehicles. Many methods are proposed to assess architectures with respect to different quality attributes in the software architecture domain, but to the best of our knowledge, functional safety assessment of automotive architectures is not explored in the literature. We present a method, that leverages existing research in software architecture and safety engineering domains, to check whether the functional safety requirements for a cooperative driving scenario are fulfilled in the technical architecture of a vehicle. We apply our method on a real-life academic prototype for a cooperative driving scenario, platooning, and discuss our insights.",2021-04-28T12:30:37Z,2021-05-02T14:40:59Z,http://arxiv.org/abs/2104.13729v2,http://arxiv.org/pdf/2104.13729v2,"cs.SE, cs.SY, eess.SY"
Differentiable Architecture Pruning for Transfer Learning,"Nicolo Colombo, Yang Gao","We propose a new gradient-based approach for extracting sub-architectures from a given large model. Contrarily to existing pruning methods, which are unable to disentangle the network architecture and the corresponding weights, our architecture-pruning scheme produces transferable new structures that can be successfully retrained to solve different tasks. We focus on a transfer-learning setup where architectures can be trained on a large data set but very few data points are available for fine-tuning them on new tasks. We define a new gradient-based algorithm that trains architectures of arbitrarily low complexity independently from the attached weights. Given a search space defined by an existing large neural model, we reformulate the architecture search task as a complexity-penalized subset-selection problem and solve it through a two-temperature relaxation scheme. We provide theoretical convergence guarantees and validate the proposed transfer-learning strategy on real data.",2021-07-07T17:44:59Z,2021-07-07T17:44:59Z,http://arxiv.org/abs/2107.03375v1,http://arxiv.org/pdf/2107.03375v1,"cs.LG, cs.CV, stat.ML"
A Quantum Internet Architecture,"Rodney Van Meter, Ryosuke Satoh, Naphan Benchasattabuse, Takaaki Matsuo, Michal Hajdušek, Takahiko Satoh, Shota Nagayama, Shigeya Suzuki","Entangled quantum communication is advancing rapidly, with laboratory and metropolitan testbeds under development, but to date there is no unifying Quantum Internet architecture. We propose a Quantum Internet architecture centered around the Quantum Recursive Network Architecture (QRNA), using RuleSet-based connections established using a two-pass connection setup. Scalability and internetworking (for both technological and administrative boundaries) are achieved using recursion in naming and connection control. In the near term, this architecture will support end-to-end, two-party entanglement on minimal hardware, and it will extend smoothly to multi-party entanglement and the use of quantum error correction on advanced hardware in the future. For a network internal gateway protocol, we recommend (but do not require) qDijkstra with seconds per Bell pair as link cost for routing; the external gateway protocol is designed to build recursively. The strength of our architecture is shown by assessing extensibility and demonstrating how robust protocol operation can be confirmed using the RuleSet paradigm.",2021-12-14T01:18:49Z,2021-12-14T01:18:49Z,http://arxiv.org/abs/2112.07092v1,http://arxiv.org/pdf/2112.07092v1,"quant-ph, cs.NI"
MLP-ASR: Sequence-length agnostic all-MLP architectures for speech   recognition,"Jin Sakuma, Tatsuya Komatsu, Robin Scheibler","We propose multi-layer perceptron (MLP)-based architectures suitable for variable length input. MLP-based architectures, recently proposed for image classification, can only be used for inputs of a fixed, pre-defined size. However, many types of data are naturally variable in length, for example, acoustic signals. We propose three approaches to extend MLP-based architectures for use with sequences of arbitrary length. The first one uses a circular convolution applied in the Fourier domain, the second applies a depthwise convolution, and the final relies on a shift operation. We evaluate the proposed architectures on an automatic speech recognition task with the Librispeech and Tedlium2 corpora. The best proposed MLP-based architectures improves WER by 1.0 / 0.9%, 0.9 / 0.5% on Librispeech dev-clean/dev-other, test-clean/test-other set, and 0.8 / 1.1% on Tedlium2 dev/test set using 86.4% the size of self-attention-based architecture.",2022-02-17T06:06:09Z,2022-02-17T06:06:09Z,http://arxiv.org/abs/2202.08456v1,http://arxiv.org/pdf/2202.08456v1,"eess.AS, cs.LG, cs.SD"
A Uniform Approach to Compare Architectures in Decentralized   Discrete-Event Systems,"K. Ritsuka, Karen Rudie","Solutions to decentralized discrete-event systems problems are characterized by the way local decisions are fused to yield a global decision. A fusion rule is colloquially called an architecture. Current approaches do not provide a direct way to compare existing architectures. Determining whether an architecture is more permissive than another architecture had relied on producing examples ad hoc and on individual inspiration that puts the conditions for solvability in each architecture into some form that admits comparison. In response to these research efforts, a method based on morphisms between graphs has been extracted to yield a uniform approach to compare the permissiveness of the architectures.",2022-10-29T06:40:52Z,2023-12-26T04:10:09Z,http://arxiv.org/abs/2210.16511v4,http://arxiv.org/pdf/2210.16511v4,"eess.SY, cs.SY"
Architecting Safer Autonomous Aviation Systems,"Jane Fenn, Mark Nicholson, Ganesh Pai, Michael Wilkinson","The aviation literature gives relatively little guidance to practitioners about the specifics of architecting systems for safety, particularly the impact of architecture on allocating safety requirements, or the relative ease of system assurance resulting from system or subsystem level architectural choices. As an exemplar, this paper considers common architectural patterns used within traditional aviation systems and explores their safety and safety assurance implications when applied in the context of integrating artificial intelligence (AI) and machine learning (ML) based functionality. Considering safety as an architectural property, we discuss both the allocation of safety requirements and the architectural trade-offs involved early in the design lifecycle. This approach could be extended to other assured properties, similar to safety, such as security. We conclude with a discussion of the safety considerations that emerge in the context of candidate architectural patterns that have been proposed in the recent literature for enabling autonomy capabilities by integrating AI and ML. A recommendation is made for the generation of a property-driven architectural pattern catalogue.",2023-01-09T21:02:18Z,2023-01-09T21:02:18Z,http://arxiv.org/abs/2301.08138v1,http://arxiv.org/pdf/2301.08138v1,"cs.SE, cs.AI, cs.SY, eess.SY"
DNArch: Learning Convolutional Neural Architectures by Backpropagation,"David W. Romero, Neil Zeghidour","We present Differentiable Neural Architectures (DNArch), a method that jointly learns the weights and the architecture of Convolutional Neural Networks (CNNs) by backpropagation. In particular, DNArch allows learning (i) the size of convolutional kernels at each layer, (ii) the number of channels at each layer, (iii) the position and values of downsampling layers, and (iv) the depth of the network. To this end, DNArch views neural architectures as continuous multidimensional entities, and uses learnable differentiable masks along each dimension to control their size. Unlike existing methods, DNArch is not limited to a predefined set of possible neural components, but instead it is able to discover entire CNN architectures across all feasible combinations of kernel sizes, widths, depths and downsampling. Empirically, DNArch finds performant CNN architectures for several classification and dense prediction tasks on sequential and image data. When combined with a loss term that controls the network complexity, DNArch constrains its search to architectures that respect a predefined computational budget during training.",2023-02-10T17:56:49Z,2023-07-22T19:45:46Z,http://arxiv.org/abs/2302.05400v2,http://arxiv.org/pdf/2302.05400v2,"cs.LG, stat.ML"
The Devil is in the Upsampling: Architectural Decisions Made Simpler for   Denoising with Deep Image Prior,"Yilin Liu, Jiang Li, Yunkui Pang, Dong Nie, Pew-thian Yap","Deep Image Prior (DIP) shows that some network architectures naturally bias towards smooth images and resist noises, a phenomenon known as spectral bias. Image denoising is an immediate application of this property. Although DIP has removed the requirement of large training sets, it still presents two practical challenges for denoising: architectural design and noise-fitting, which are often intertwined. Existing methods mostly handcraft or search for the architecture from a large design space, due to the lack of understanding on how the architectural choice corresponds to the image. In this study, we analyze from a frequency perspective to demonstrate that the unlearnt upsampling is the main driving force behind the denoising phenomenon in DIP. This finding then leads to strategies for estimating a suitable architecture for every image without a laborious search. Extensive experiments show that the estimated architectures denoise and preserve the textural details better than current methods with up to 95% fewer parameters. The under-parameterized nature also makes them especially robust to a higher level of noise.",2023-04-22T13:50:27Z,2023-08-27T00:50:26Z,http://arxiv.org/abs/2304.11409v2,http://arxiv.org/pdf/2304.11409v2,"eess.IV, cs.CV"
Enhanced Measurement of Neutral Atom Qubits with Machine Learning,"L. Phuttitarn, B. M. Becker, R. Chinnarasu, T. M. Graham, M. Saffman","We demonstrate qubit state measurements assisted by a supervised convolutional neural network (CNN) in a neutral atom quantum processor. We present two CNN architectures for analyzing neutral atom qubit readout data: a compact 5-layer single-qubit CNN architecture and a 6-layer multi-qubit CNN architecture. We benchmark both architectures against a conventional Gaussian threshold analysis method. In a sparse array (9 {\mu}m atom separation) which experiences negligible crosstalk, we observed up to 32% and 56% error reduction for the multi-qubit and single-qubit architectures respectively, as compared to the benchmark. In a tightly spaced array (5 {\mu}m atom separation), which suffers from readout crosstalk, we observed up to 43% and 32% error reduction in the multi-qubit and single-qubit CNN architectures respectively, as compared to the benchmark. By examining the correlation between the predicted states of neighboring qubits, we found that the multi-qubit CNN architecture reduces the crosstalk correlation up to 78.5%. This work demonstrates a proof of concept for a CNN network to be implemented as a real-time readout processing method on a neutral atom quantum computer, enabling faster readout time and improved fidelity.",2023-11-20T22:16:46Z,2024-05-01T20:05:52Z,http://arxiv.org/abs/2311.12217v2,http://arxiv.org/pdf/2311.12217v2,"quant-ph, physics.atom-ph"
HoneyDOC: An Efficient Honeypot Architecture Enabling All-Round Design,"Wenjun Fan, Zhihui Du, Max Smith-Creasey, David Fernández","Honeypots are designed to trap the attacker with the purpose of investigating its malicious behavior. Owing to the increasing variety and sophistication of cyber attacks, how to capture high-quality attack data has become a challenge in the context of honeypot area. All-round honeypots, which mean significant improvement in sensibility, countermeasure and stealth, are necessary to tackle the problem. In this paper, we propose a novel honeypot architecture termed HoneyDOC to support all-round honeypot design and implementation. Our HoneyDOC architecture clearly identifies three essential independent and collaborative modules, Decoy, Captor and Orchestrator. Based on the efficient architecture, a Software-Defined Networking (SDN) enabled honeypot system is designed, which supplies high programmability for technically sustaining the features for capturing high-quality data. A proof-of-concept system is implemented to validate its feasibility and effectiveness. The experimental results show the benefits by using the proposed architecture comparing to the previous honeypot solutions.",2024-02-09T16:27:45Z,2024-02-09T16:27:45Z,http://arxiv.org/abs/2402.06516v1,http://arxiv.org/pdf/2402.06516v1,"cs.CR, cs.NI, C.2.0; C.2.1"
Smart HPA: A Resource-Efficient Horizontal Pod Auto-scaler for   Microservice Architectures,"Hussain Ahmad, Christoph Treude, Markus Wagner, Claudia Szabo","Microservice architectures have gained prominence in both academia and industry, offering enhanced agility, reusability, and scalability. To simplify scaling operations in microservice architectures, container orchestration platforms such as Kubernetes feature Horizontal Pod Auto-scalers (HPAs) designed to adjust the resources of microservices to accommodate fluctuating workloads. However, existing HPAs are not suitable for resource-constrained environments, as they make scaling decisions based on the individual resource capacities of microservices, leading to service unavailability and performance degradation. Furthermore, HPA architectures exhibit several issues, including inefficient data processing and a lack of coordinated scaling operations. To address these concerns, we propose Smart HPA, a flexible resource-efficient horizontal pod auto-scaler. It features a hierarchical architecture that integrates both centralized and decentralized architectural styles to leverage their respective strengths while addressing their limitations. We introduce resource-efficient heuristics that empower Smart HPA to exchange resources among microservices, facilitating effective auto-scaling of microservices in resource-constrained environments. Our experimental results show that Smart HPA outperforms the Kubernetes baseline HPA by reducing resource overutilization, overprovisioning, and underprovisioning while increasing resource allocation to microservice applications.",2024-02-27T01:22:46Z,2024-02-27T01:22:46Z,http://arxiv.org/abs/2403.07909v1,http://arxiv.org/pdf/2403.07909v1,"cs.DC, cs.SY, eess.SY"
Architecture-Aware Learning Curve Extrapolation via Graph Ordinary   Differential Equation,"Yanna Ding, Zijie Huang, Xiao Shou, Yihang Guo, Yizhou Sun, Jianxi Gao","Learning curve extrapolation predicts neural network performance from early training epochs and has been applied to accelerate AutoML, facilitating hyperparameter tuning and neural architecture search. However, existing methods typically model the evolution of learning curves in isolation, neglecting the impact of neural network (NN) architectures, which influence the loss landscape and learning trajectories. In this work, we explore whether incorporating neural network architecture improves learning curve modeling and how to effectively integrate this architectural information. Motivated by the dynamical system view of optimization, we propose a novel architecture-aware neural differential equation model to forecast learning curves continuously. We empirically demonstrate its ability to capture the general trend of fluctuating learning curves while quantifying uncertainty through variational parameters. Our model outperforms current state-of-the-art learning curve extrapolation methods and pure time-series modeling approaches for both MLP and CNN-based learning curves. Additionally, we explore the applicability of our method in Neural Architecture Search scenarios, such as training configuration ranking.",2024-12-20T04:28:02Z,2025-01-19T02:54:49Z,http://arxiv.org/abs/2412.15554v3,http://arxiv.org/pdf/2412.15554v3,"cs.LG, cs.AI, stat.ML"
Software is a directed multigraph (and so is software process),"Robert Dabrowski, Krzysztof Stencel, Grzegorz Timoszuk","For a software system, its architecture is typically defined as the fundamental organization of the system incorporated by its components, their relationships to one another and their environment, and the principles governing their design. If contributed to by the artifacts coresponding to engineering processes that govern the system's evolution, the definition gets natually extended into the architecture of software and software process. Obviously, as long as there were no software systems, managing their architecture was no problem at all; when there were only small systems, managing their architecture became a mild problem; and now we have gigantic software systems, and managing their architecture has become an equally gigantic problem (to paraphrase Edsger Dijkstra). In this paper we propose a simple, yet we believe effective, model for organizing architecture of software systems. First of all we postulate that only a hollistic approach that supports continuous integration and verification for all software and software process architectural artifacts is the one worth taking. Next we indicate a graph-based model that not only allows collecting and maintaining the architectural knowledge in respect to both software and software process, but allows to conveniently create various quantitive metric to asses their respective quality or maturity. Such model is actually independent of the development methodologies that are currently in-use, that is it could well be applied for projects managed in an adaptive, as well as in a formal approach. Eventually we argue that the model could actually be implemented by already existing tools, in particular graph databases are a convenient implementation of architectural repository.",2011-03-21T15:37:38Z,2011-03-21T15:37:38Z,http://arxiv.org/abs/1103.4056v1,http://arxiv.org/pdf/1103.4056v1,"cs.SE, D.2.8; D.2.9; D.2.11; D.2.13"
Communication Delay Co-Design in $\mathcal{H}_2$ Distributed Control   Using Atomic Norm Minimization,Nikolai Matni,"When designing distributed controllers for large-scale systems, the actuation, sensing and communication architectures of the controller can no longer be taken as given. In particular, controllers implemented using dense architectures typically outperform controllers implemented using simpler ones -- however, it is also desirable to minimize the cost of building the architecture used to implement a controller. The recently introduced Regularization for Design (RFD) framework poses the controller architecture/control law co-design problem as one of jointly optimizing the competing metrics of controller architecture cost and closed loop performance, and shows that this task can be accomplished by augmenting the variational solution to an optimal control problem with a suitable atomic norm penalty. Although explicit constructions for atomic norms useful for the design of actuation, sensing and joint actuation/sensing architectures are introduced, no such construction is given for atomic norms used to design communication architectures. This paper describes an atomic norm that can be used to design communication architectures for which the resulting distributed optimal controller is specified by the solution to a convex program. Using this atomic norm we then show that in the context of $\mathcal{H}_2$ distributed optimal control, the communication architecture/control law co-design task can be performed through the use of finite dimensional second order cone programming.",2014-04-19T01:50:55Z,2015-09-20T05:17:11Z,http://arxiv.org/abs/1404.4911v3,http://arxiv.org/pdf/1404.4911v3,"math.OC, cs.SY"
Comprehensive Evaluation of Deep Learning Architectures for Prediction   of DNA/RNA Sequence Binding Specificities,"Ameni Trabelsi, Mohamed Chaabane, Asa Ben Hur","Motivation: Deep learning architectures have recently demonstrated their power in predicting DNA- and RNA-binding specificities. Existing methods fall into three classes: Some are based on Convolutional Neural Networks (CNNs), others use Recurrent Neural Networks (RNNs), and others rely on hybrid architectures combining CNNs and RNNs. However, based on existing studies it is still unclear which deep learning architecture is achieving the best performance. Thus an in-depth analysis and evaluation of the different methods is needed to fully evaluate their relative. Results: In this study, We present a systematic exploration of various deep learning architectures for predicting DNA- and RNA-binding specificities. For this purpose, we present deepRAM, an end-to-end deep learning tool that provides an implementation of novel and previously proposed architectures; its fully automatic model selection procedure allows us to perform a fair and unbiased comparison of deep learning architectures. We find that an architecture that uses k-mer embedding to represent the sequence, a convolutional layer and a recurrent layer, outperforms all other methods in terms of model accuracy. Our work provides guidelines that will assist the practitioner in choosing the best architecture for the task at hand, and provides some insights on the differences between the models learned by convolutional and recurrent networks. In particular, we find that although recurrent networks improve model accuracy, this comes at the expense of a loss in the interpretability of the features learned by the model. Availability and implementation: The source code for deepRAM is available at https://github.com/MedChaabane/deepRAM",2019-01-29T20:33:52Z,2019-01-29T20:33:52Z,http://arxiv.org/abs/1901.10526v1,http://arxiv.org/pdf/1901.10526v1,"cs.LG, q-bio.QM, stat.ML"
Neural Architecture Search for Class-incremental Learning,"Shenyang Huang, Vincent François-Lavet, Guillaume Rabusseau","In class-incremental learning, a model learns continuously from a sequential data stream in which new classes occur. Existing methods often rely on static architectures that are manually crafted. These methods can be prone to capacity saturation because a neural network's ability to generalize to new concepts is limited by its fixed capacity. To understand how to expand a continual learner, we focus on the neural architecture design problem in the context of class-incremental learning: at each time step, the learner must optimize its performance on all classes observed so far by selecting the most competitive neural architecture. To tackle this problem, we propose Continual Neural Architecture Search (CNAS): an autoML approach that takes advantage of the sequential nature of class-incremental learning to efficiently and adaptively identify strong architectures in a continual learning setting. We employ a task network to perform the classification task and a reinforcement learning agent as the meta-controller for architecture search. In addition, we apply network transformations to transfer weights from previous learning step and to reduce the size of the architecture search space, thus saving a large amount of computational resources. We evaluate CNAS on the CIFAR-100 dataset under varied incremental learning scenarios with limited computational power (1 GPU). Experimental results demonstrate that CNAS outperforms architectures that are optimized for the entire dataset. In addition, CNAS is at least an order of magnitude more efficient than naively using existing autoML methods.",2019-09-14T22:16:02Z,2019-09-14T22:16:02Z,http://arxiv.org/abs/1909.06686v1,http://arxiv.org/pdf/1909.06686v1,"cs.LG, stat.ML"
BNAS:An Efficient Neural Architecture Search Approach Using Broad   Scalable Architecture,"Zixiang Ding, Yaran Chen, Nannan Li, Dongbin Zhao, Zhiquan Sun, C. L. Philip Chen","In this paper, we propose Broad Neural Architecture Search (BNAS) where we elaborately design broad scalable architecture dubbed Broad Convolutional Neural Network (BCNN) to solve the above issue. On one hand, the proposed broad scalable architecture has fast training speed due to its shallow topology. Moreover, we also adopt reinforcement learning and parameter sharing used in ENAS as the optimization strategy of BNAS. Hence, the proposed approach can achieve higher search efficiency. On the other hand, the broad scalable architecture extracts multi-scale features and enhancement representations, and feeds them into global average pooling layer to yield more reasonable and comprehensive representations. Therefore, the performance of broad scalable architecture can be promised. In particular, we also develop two variants for BNAS who modify the topology of BCNN. In order to verify the effectiveness of BNAS, several experiments are performed and experimental results show that 1) BNAS delivers 0.19 days which is 2.37x less expensive than ENAS who ranks the best in reinforcement learning-based NAS approaches, 2) compared with small-size (0.5 millions parameters) and medium-size (1.1 millions parameters) models, the architecture learned by BNAS obtains state-of-the-art performance (3.58% and 3.24% test error) on CIFAR-10, 3) the learned architecture achieves 25.3% top-1 error on ImageNet just using 3.9 millions parameters.",2020-01-18T15:07:55Z,2021-01-20T07:09:11Z,http://arxiv.org/abs/2001.06679v5,http://arxiv.org/pdf/2001.06679v5,"stat.ML, cs.LG"
Disentangling Neural Architectures and Weights: A Case Study in   Supervised Classification,"Nicolo Colombo, Yang Gao","The history of deep learning has shown that human-designed problem-specific networks can greatly improve the classification performance of general neural models. In most practical cases, however, choosing the optimal architecture for a given task remains a challenging problem. Recent architecture-search methods are able to automatically build neural models with strong performance but fail to fully appreciate the interaction between neural architecture and weights. This work investigates the problem of disentangling the role of the neural structure and its edge weights, by showing that well-trained architectures may not need any link-specific fine-tuning of the weights. We compare the performance of such weight-free networks (in our case these are binary networks with {0, 1}-valued weights) with random, weight-agnostic, pruned and standard fully connected networks. To find the optimal weight-agnostic network, we use a novel and computationally efficient method that translates the hard architecture-search problem into a feasible optimization problem.More specifically, we look at the optimal task-specific architectures as the optimal configuration of binary networks with {0, 1}-valued weights, which can be found through an approximate gradient descent strategy. Theoretical convergence guarantees of the proposed algorithm are obtained by bounding the error in the gradient approximation and its practical performance is evaluated on two real-world data sets. For measuring the structural similarities between different architectures, we use a novel spectral approach that allows us to underline the intrinsic differences between real-valued networks and weight-free architectures.",2020-09-11T11:22:22Z,2020-09-11T11:22:22Z,http://arxiv.org/abs/2009.05346v1,http://arxiv.org/pdf/2009.05346v1,"cs.LG, cs.CV, stat.ML"
Fail-Safe Controller Architectures for Quadcopter with Motor Failures,"Gene Patrick S. Rible, Nicolette Ann A. Arriola, Manuel C. Ramos, Jr","A fail-safe algorithm in case of motor failure was developed, simulated, and tested. For practical fail-safe flight, the quadcopter may fly with only three or two opposing propellers. Altitude for two-propeller architecture was maintained by a PID controller that is independent from the inner and outer controllers. A PID controller on propeller force deviations from equilibrium was augmented to the inner controller of the three-propeller architecture. Both architectures used LQR for the inner attitude controller and a damped second order outer controller that zeroes the error along the horizontal coordinates. The restrictiveness, stability, robustness, and symmetry of these architectures were investigated with respect to their output limits, initial conditions, and controller frequencies. Although the three-propeller architecture allows for distribution of propeller forces, the two-propeller architecture is more efficient, robust, and stable. The two-propeller architecture is also robust to model uncertainties. It was shown that higher yaw rate leads to greater stability when operating in fail-safe mode.",2020-09-22T01:12:00Z,2020-09-22T01:12:00Z,http://arxiv.org/abs/2009.10260v1,http://arxiv.org/pdf/2009.10260v1,"cs.RO, cs.SY, eess.SY, I.2.9"
HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D   Medical Image Segmentation using HyperNet,"Cheng Peng, Andriy Myronenko, Ali Hatamizadeh, Vish Nath, Md Mahfuzur Rahman Siddiquee, Yufan He, Daguang Xu, Rama Chellappa, Dong Yang","Semantic segmentation of 3D medical images is a challenging task due to the high variability of the shape and pattern of objects (such as organs or tumors). Given the recent success of deep learning in medical image segmentation, Neural Architecture Search (NAS) has been introduced to find high-performance 3D segmentation network architectures. However, because of the massive computational requirements of 3D data and the discrete optimization nature of architecture search, previous NAS methods require a long search time or necessary continuous relaxation, and commonly lead to sub-optimal network architectures. While one-shot NAS can potentially address these disadvantages, its application in the segmentation domain has not been well studied in the expansive multi-scale multi-path search space. To enable one-shot NAS for medical image segmentation, our method, named HyperSegNAS, introduces a HyperNet to assist super-net training by incorporating architecture topology information. Such a HyperNet can be removed once the super-net is trained and introduces no overhead during architecture search. We show that HyperSegNAS yields better performing and more intuitive architectures compared to the previous state-of-the-art (SOTA) segmentation networks; furthermore, it can quickly and accurately find good architecture candidates under different computing constraints. Our method is evaluated on public datasets from the Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.",2021-12-20T16:21:09Z,2022-03-24T13:56:32Z,http://arxiv.org/abs/2112.10652v2,http://arxiv.org/pdf/2112.10652v2,"eess.IV, cs.CV"
Neural Network Architecture Beyond Width and Depth,"Zuowei Shen, Haizhao Yang, Shijun Zhang","This paper proposes a new neural network architecture by introducing an additional dimension called height beyond width and depth. Neural network architectures with height, width, and depth as hyper-parameters are called three-dimensional architectures. It is shown that neural networks with three-dimensional architectures are significantly more expressive than the ones with two-dimensional architectures (those with only width and depth as hyper-parameters), e.g., standard fully connected networks. The new network architecture is constructed recursively via a nested structure, and hence we call a network with the new architecture nested network (NestNet). A NestNet of height $s$ is built with each hidden neuron activated by a NestNet of height $\le s-1$. When $s=1$, a NestNet degenerates to a standard network with a two-dimensional architecture. It is proved by construction that height-$s$ ReLU NestNets with $\mathcal{O}(n)$ parameters can approximate $1$-Lipschitz continuous functions on $[0,1]^d$ with an error $\mathcal{O}(n^{-(s+1)/d})$, while the optimal approximation error of standard ReLU networks with $\mathcal{O}(n)$ parameters is $\mathcal{O}(n^{-2/d})$. Furthermore, such a result is extended to generic continuous functions on $[0,1]^d$ with the approximation error characterized by the modulus of continuity. Finally, we use numerical experimentation to show the advantages of the super-approximation power of ReLU NestNets.",2022-05-19T10:29:11Z,2023-01-14T17:51:33Z,http://arxiv.org/abs/2205.09459v4,http://arxiv.org/pdf/2205.09459v4,"cs.LG, stat.ML"
High-Performance and Scalable Fault-Tolerant Quantum Computation with   Lattice Surgery on a 2.5D Architecture,"Yosuke Ueno, Taku Saito, Teruo Tanimoto, Yasunari Suzuki, Yutaka Tabuchi, Shuhei Tamate, Hiroshi Nakamura","Due to the high error rate of a qubit, detecting and correcting errors on it is essential for fault-tolerant quantum computing (FTQC). Among several FTQC techniques, lattice surgery (LS) using surface code (SC) is currently promising. To demonstrate practical quantum advantage as early as possible, it is indispensable to propose a high-performance and low-overhead FTQC architecture specialized for a given FTQC scheme based on detailed analysis.   In this study, we first categorize the factors, or hazards, that degrade LS-based FTQC performance and propose a performance evaluation methodology to decompose the impact of each hazard, inspired by the CPI stack. We propose the Bypass architecture based on the bottleneck analysis using the proposed evaluation methodology. The proposed Bypass architecture is a 2.5-dimensional architecture consisting of dense and sparse qubit layers and successfully eliminates the bottleneck to achieve high-performance and scalable LS-based FTQC. We evaluate the proposed architecture with a circuit-level stabilizer simulator and a cycle-accurate LS simulator with practical quantum phase estimation problems. The results show that the Bypass architecture improves the fidelity of FTQC and achieves both a 1.73x speedup and a 17% reduction in classical/quantum hardware resources over a conventional 2D architecture.",2024-11-26T15:27:59Z,2024-11-26T15:27:59Z,http://arxiv.org/abs/2411.17519v1,http://arxiv.org/pdf/2411.17519v1,"quant-ph, cs.AR"
"The ""Whiteboard"" Architecture: a way to integrate heterogeneous   components of NLP systems","Christian Boitet, Mark Seligman","We present a new software architecture for NLP systems made of heterogeneous components, and demonstrate an architectural prototype we have built at ATR in the context of Speech Translation.",1994-11-04T13:16:25Z,1994-11-04T13:16:25Z,http://arxiv.org/abs/cmp-lg/9411010v1,http://arxiv.org/pdf/cmp-lg/9411010v1,"cmp-lg, cs.CL"
Data-stationary Architecture to Execute Quantum Algorithms Classically,J. R. Burger,"This paper presents a data stationary architecture in which each word has an attached address field. Address fields massively update in parallel to record data interchanges. Words do not move until memory is read for post processing. A sea of such cells can test large-scale quantum algorithms, although other programming is possible.",2004-12-09T22:10:48Z,2004-12-09T22:10:48Z,http://arxiv.org/abs/cs/0412040v1,http://arxiv.org/pdf/cs/0412040v1,"cs.AR, C.1.2"
Semi-Supervised Neural Architecture Search,"Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu","Neural architecture search (NAS) relies on a good controller to generate better architectures or predict the accuracy of given architectures. However, training the controller requires both abundant and high-quality pairs of architectures and their accuracy, while it is costly to evaluate an architecture and obtain its accuracy. In this paper, we propose SemiNAS, a semi-supervised NAS approach that leverages numerous unlabeled architectures (without evaluation and thus nearly no cost). Specifically, SemiNAS 1) trains an initial accuracy predictor with a small set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of large amount of architectures (without evaluation); and 3) adds the generated data pairs to the original data to further improve the predictor. The trained accuracy predictor can be applied to various NAS algorithms by predicting the accuracy of candidate architectures for them. SemiNAS has two advantages: 1) It reduces the computational cost under the same accuracy guarantee. On NASBench-101 benchmark dataset, it achieves comparable accuracy with gradient-based method while using only 1/7 architecture-accuracy pairs. 2) It achieves higher accuracy under the same computational cost. It achieves 94.02% test accuracy on NASBench-101, outperforming all the baselines when using the same number of architectures. On ImageNet, it achieves 23.5% top-1 error rate (under 600M FLOPS constraint) using 4 GPU-days for search. We further apply it to LJSpeech text to speech task and it achieves 97% intelligibility rate in the low-resource setting and 15% test error rate in the robustness setting, with 9%, 7% improvements over the baseline respectively.",2020-02-24T17:23:00Z,2020-11-03T09:44:09Z,http://arxiv.org/abs/2002.10389v4,http://arxiv.org/pdf/2002.10389v4,"cs.LG, stat.ML"
Neural Architecture Optimization,"Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, Tie-Yan Liu","Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.",2018-08-22T06:07:03Z,2019-09-04T12:53:35Z,http://arxiv.org/abs/1808.07233v5,http://arxiv.org/pdf/1808.07233v5,"cs.LG, stat.ML"
AReN: Assured ReLU NN Architecture for Model Predictive Control of LTI   Systems,"James Ferlez, Yasser Shoukry","In this paper, we consider the problem of automatically designing a Rectified Linear Unit (ReLU) Neural Network (NN) architecture that is sufficient to implement the optimal Model Predictive Control (MPC) strategy for an LTI system with quadratic cost. Specifically, we propose AReN, an algorithm to generate Assured ReLU Architectures. AReN takes as input an LTI system with quadratic cost specification, and outputs a ReLU NN architecture with the assurance that there exist network weights that exactly implement the associated MPC controller. AReN thus offers new insight into the design of ReLU NN architectures for the control of LTI systems: instead of training a heuristically chosen NN architecture on data -- or iterating over many architectures until a suitable one is found -- AReN can suggest an adequate NN architecture before training begins. While several previous works were inspired by the fact that both ReLU NN controllers and optimal MPC controller are both Continuous, Piecewise-Linear (CPWL) functions, exploiting this similarity to design NN architectures with correctness guarantees has remained elusive. AReN achieves this using two novel features. First, we reinterpret a recent result about the implementation of CPWL functions via ReLU NNs to show that a CPWL function may be implemented by a ReLU architecture that is determined by the number of distinct affine regions in the function. Second, we show that we can efficiently over-approximate the number of affine regions in the optimal MPC controller without solving the MPC problem exactly. Together, these results connect the MPC problem to a ReLU NN implementation without explicitly solving the MPC and directly translates this feature to a ReLU NN architecture that comes with the assurance that it can implement the MPC controller. We show through numerical results the effectiveness of AReN in designing an NN architecture.",2019-11-05T04:08:59Z,2019-11-05T04:08:59Z,http://arxiv.org/abs/1911.01608v1,http://arxiv.org/pdf/1911.01608v1,"cs.LG, cs.SY, eess.SY, math.OC"
Implementing CNN Layers on the Manticore Cluster-Based Many-Core   Architecture,"Andreas Kurth, Fabian Schuiki, Luca Benini",This document presents implementations of fundamental convolutional neural network (CNN) layers on the Manticore cluster-based many-core architecture and discusses their characteristics and trade-offs.,2021-04-16T10:07:28Z,2021-04-16T10:07:28Z,http://arxiv.org/abs/2104.08009v1,http://arxiv.org/pdf/2104.08009v1,"cs.DC, cs.AR, cs.CV, cs.LG, C.4; C.1.4; F.2.1; I.2"
Lecture Notes: Neural Network Architectures,Evelyn Herberg,"These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.",2023-04-11T10:54:36Z,2023-04-18T15:57:29Z,http://arxiv.org/abs/2304.05133v2,http://arxiv.org/pdf/2304.05133v2,"cs.LG, math.OC, 68T07"
CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory   Accelerators,"Songyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, Ying Wang","In recent years, various computing-in-memory (CIM) processors have been presented, showing superior performance over traditional architectures. To unleash the potential of various CIM architectures, such as device precision, crossbar size, and crossbar number, it is necessary to develop compilation tools that are fully aware of the CIM architectural details and implementation diversity. However, due to the lack of architectural support in current popular open-source compiling stacks, existing CIM designs either manually deploy networks or build their own compilers, which is time-consuming and labor-intensive. Although some works expose the specific CIM device programming interfaces to compilers, they are often bound to a fixed CIM architecture, lacking the flexibility to support the CIM architectures with different computing granularity. On the other hand, existing compilation works usually consider the scheduling of limited operation types (such as crossbar-bound matrix-vector multiplication). Unlike conventional processors, CIM accelerators are featured by their diverse architecture, circuit, and device, which cannot be simply abstracted by a single level if we seek to fully explore the advantages brought by CIM. Therefore, we propose CIM-MLC, a universal multi-level compilation framework for general CIM architectures. We first establish a general hardware abstraction for CIM architectures and computing modes to represent various CIM accelerators. Based on the proposed abstraction, CIM-MLC can compile tasks onto a wide range of CIM accelerators having different devices, architectures, and programming interfaces. More importantly, compared with existing compilation work, CIM-MLC can explore the mapping and scheduling strategies across multiple architectural tiers, which form a tractable yet effective design space, to achieve better scheduling and instruction generation results.",2024-01-23T01:33:09Z,2024-05-08T06:44:41Z,http://arxiv.org/abs/2401.12428v2,http://arxiv.org/pdf/2401.12428v2,"cs.AR, cs.CL, D.3.4"
An Intelligent Native Network Slicing Security Architecture Empowered by   Federated Learning,"Rodrigo Moreira, Rodolfo S. Villaca, Moises R. N. Ribeiro, Joberto S. B. Martins, Joao Henrique Correa, Tereza C. Carvalho, Flavio de Oliveira Silva","Network Slicing (NS) has transformed the landscape of resource sharing in networks, offering flexibility to support services and applications with highly variable requirements in areas such as the next-generation 5G/6G mobile networks (NGMN), vehicular networks, industrial Internet of Things (IoT), and verticals. Although significant research and experimentation have driven the development of network slicing, existing architectures often fall short in intrinsic architectural intelligent security capabilities. This paper proposes an architecture-intelligent security mechanism to improve the NS solutions. We idealized a security-native architecture that deploys intelligent microservices as federated agents based on machine learning, providing intra-slice and architectural operation security for the Slicing Future Internet Infrastructures (SFI2) reference architecture. It is noteworthy that federated learning approaches match the highly distributed modern microservice-based architectures, thus providing a unifying and scalable design choice for NS platforms addressing both service and security. Using ML-Agents and Security Agents, our approach identified Distributed Denial-of-Service (DDoS) and intrusion attacks within the slice using generic and non-intrusive telemetry records, achieving an average accuracy of approximately $95.60\%$ in the network slicing architecture and $99.99\%$ for the deployed slice -- intra-slice. This result demonstrates the potential for leveraging architectural operational security and introduces a promising new research direction for network slicing architectures.",2024-10-04T21:12:23Z,2024-10-04T21:12:23Z,http://arxiv.org/abs/2410.05312v1,http://arxiv.org/pdf/2410.05312v1,"cs.CR, cs.AI, cs.ET, cs.LG, cs.NI, I.2; I.6; F.2.2"
Adaptive Interaction Using the Adaptive Agent Oriented Software   Architecture (AAOSA),"Babak Hodjat, Makoto Amamiya","User interfaces that adapt their characteristics to those of the user are referred to as adaptive interfaces. We propose Adaptive Agent Oriented Software Architecture (AAOSA) as a new way of designing adaptive interfaces. AAOSA is a new approach to software design based on an agent-oriented architecture. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is responsible for the independent specialized processes of the agent. A distributed learning policy that makes use of this architecture is used for purposes of system adaptability.",1998-12-11T23:05:51Z,1998-12-11T23:05:51Z,http://arxiv.org/abs/cs/9812015v1,http://arxiv.org/pdf/cs/9812015v1,"cs.HC, cs.DC, D.2.2; I.2.7; J.7; I.2.11"
Perpetual Adaptation of Software to Hardware: An Extensible Architecture   for Providing Code Optimization as a Central System Service,"Thomas Kistler, Michael Franz","We present an open architecture for just-in-time code generation and dynamic code optimization that is flexible, customizable, and extensible. While previous research has primarily investigated functional aspects of such a system, architectural aspects have so far remained unexplored. In this paper, we argue that these properties are important to generate optimal code for a variety of hardware architectures and different processor generations within processor families. These properties are also important to make system-level code generation useful in practice.",1999-03-22T21:24:35Z,1999-03-22T21:24:35Z,http://arxiv.org/abs/cs/9903014v1,http://arxiv.org/pdf/cs/9903014v1,"cs.OS, cs.PL, D.3.4"
An Overview of a Grid Architecture for Scientific Computing,"A. Waananen, M. Ellert, A. Konstantinov, B. Konya, O. Smirnova",This document gives an overview of a Grid testbed architecture proposal for the NorduGrid project. The aim of the project is to establish an inter-Nordic testbed facility for implementation of wide area computing and data handling. The architecture is supposed to define a Grid system suitable for solving data intensive problems at the Large Hadron Collider at CERN. We present the various architecture components needed for such a system. After that we go on to give a description of the dynamics by showing the task flow.,2002-05-14T19:22:00Z,2002-05-14T19:22:00Z,http://arxiv.org/abs/cs/0205021v1,http://arxiv.org/pdf/cs/0205021v1,"cs.DC, C.2.4;C.5;J.2"
A Grid Based Architecture for High-Performance NLP,"Baden Hughes, Steven Bird","We describe the design and early implementation of an extensible, component-based software architecture for natural language engineering applications which interfaces with high performance distributed computing services. The architecture leverages existing linguistic resource description and discovery mechanisms based on metadata descriptions, combining these in a compatible fashion with other software definition abstractions. Within this architecture, application design is highly flexible, allowing disparate components to be combined to suit the overall application functionality, and formally described independently of processing concerns. An application specification language provides abstraction from the programming environment and allows ease of interface with high performance computational grids via a broker.",2003-08-05T00:46:08Z,2003-08-05T00:46:08Z,http://arxiv.org/abs/cs/0308008v1,http://arxiv.org/pdf/cs/0308008v1,"cs.DC, cs.CL, J.5; D.1; C.2"
An Information Network Overlay Architecture for the NSDL,"Carl Lagoze, Dean B. Krafft, Susan Jesuroga, Tim Cornwell, Ellen J. Cramer, Eddie Shin","We describe the underlying data model and implementation of a new architecture for the National Science Digital Library (NSDL) by the Core Integration Team (CI). The architecture is based on the notion of an information network overlay. This network, implemented as a graph of digital objects in a Fedora repository, allows the representation of multiple information entities and their relationships. The architecture provides the framework for contextualization and reuse of resources, which we argue is essential for the utility of the NSDL as a tool for teaching and learning.",2005-01-27T19:49:11Z,2005-02-02T21:04:50Z,http://arxiv.org/abs/cs/0501080v2,http://arxiv.org/pdf/cs/0501080v2,"cs.DL, H.3.7"
Software Architecture Overview,Andre Adrian,"What is Software Architecture? The rules, paradigmen, pattern that help to construct, build and test a serious piece of software. It is the practical experience boiled down to abstract level. Software Architecture builds on System Engineering and the scientific method as established by Galileo Galilei: Measure what you can and make measureable what you can not. The experiment (test) is more important then the deduction. Pieces of information about software architecture are all over the internet. This paper uses citation as much as possible. The aim is to bring together an overview, not to rephrase the wording.",2005-07-24T09:43:27Z,2005-07-24T09:43:27Z,http://arxiv.org/abs/cs/0507061v1,http://arxiv.org/pdf/cs/0507061v1,"cs.SE, C.0; C.5"
Towards physical laws for software architecture,A. D. Chepelianskii,"Starting from the pioneering works on software architecture precious guidelines have emerged to indicate how computer programs should be organized. For example the ""separation of concerns"" suggests to split a program into modules that overlap in functionality as little as possible. However these recommendations are mainly conceptual and are thus hard to express in a quantitative form. Hence software architecture relies on the individual experience and skill of the designers rather than on quantitative laws. In this article I apply the methods developed for the classification of information on the World-Wide-Web to study the organization of Open Source programs in an attempt to establish the statistical laws governing software architecture.",2010-03-29T08:33:46Z,2010-03-29T08:33:46Z,http://arxiv.org/abs/1003.5455v1,http://arxiv.org/pdf/1003.5455v1,"cs.SE, cs.IR, physics.data-an, physics.soc-ph"
Femtocaching and Device-to-Device Collaboration: A New Architecture for   Wireless Video Distribution,"Negin Golrezaei, Andreas F. Molisch, Alexandros G. Dimakis, Giuseppe Caire","We present a new architecture to handle the ongoing explosive increase in the demand for video content in wireless networks. It is based on distributed caching of the content in femto-basestations with small or non-existing backhaul capacity but with considerable storage space, called helper nodes. We also consider using the mobile terminals themselves as caching helpers, which can distribute video through device-to-device communications. This approach allows an improvement in the video throughput without deployment of any additional infrastructure. The new architecture can improve video throughput by one to two orders-of-magnitude.",2012-04-07T05:30:26Z,2012-04-07T05:30:26Z,http://arxiv.org/abs/1204.1595v1,http://arxiv.org/pdf/1204.1595v1,"cs.NI, cs.IT, math.IT"
Hardware Architecture for List SC Decoding of Polar Codes,"A. Balatsoukas-Stimming, A. J. Raymond, W. J. Gross, A. Burg","We present a hardware architecture and algorithmic improvements for list SC decoding of polar codes. More specifically, we show how to completely avoid copying of the likelihoods, which is algorithmically the most cumbersome part of list SC decoding. The hardware architecture was synthesized for a blocklength of N = 1024 bits and list sizes L = 2, 4 using a UMC 90nm VLSI technology. The resulting decoder can achieve a coded throughput of 181 Mbps at a frequency of 459 MHz.",2013-03-28T14:04:54Z,2014-02-27T15:53:10Z,http://arxiv.org/abs/1303.7127v3,http://arxiv.org/pdf/1303.7127v3,"cs.IT, cs.AR, math.IT"
Hybrid Coding: An Interface for Joint Source-Channel Coding and Network   Communication,"Paolo Minero, Sung Hoon Lim, Young-Han Kim","A new approach to joint source-channel coding is presented in the context of communicating correlated sources over multiple access channels. Similar to the separation architecture, the joint source-channel coding system architecture in this approach is modular, whereby the source encoding and channel decoding operations are decoupled. However, unlike the separation architecture, the same codeword is used for both source coding and channel coding, which allows the resulting hybrid coding scheme to achieve the performance of the best known joint source-channel coding schemes. Applications of the proposed architecture to relay communication are also discussed.",2013-06-03T18:31:40Z,2013-06-03T18:31:40Z,http://arxiv.org/abs/1306.0530v1,http://arxiv.org/pdf/1306.0530v1,"cs.IT, math.IT"
Attending to Characters in Neural Sequence Labeling Models,"Marek Rei, Gamal K. O. Crichton, Sampo Pyysalo","Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.",2016-11-14T12:36:07Z,2016-11-14T12:36:07Z,http://arxiv.org/abs/1611.04361v1,http://arxiv.org/pdf/1611.04361v1,"cs.CL, cs.LG, cs.NE, I.5.1; I.2.6; I.2.7"
Deep Radial Kernel Networks: Approximating Radially Symmetric Functions   with Deep Networks,"Brendan McCane, Lech Szymanski","We prove that a particular deep network architecture is more efficient at approximating radially symmetric functions than the best known 2 or 3 layer networks. We use this architecture to approximate Gaussian kernel SVMs, and subsequently improve upon them with further training. The architecture and initial weights of the Deep Radial Kernel Network are completely specified by the SVM and therefore sidesteps the problem of empirically choosing an appropriate deep network architecture.",2017-03-09T21:26:51Z,2017-03-09T21:26:51Z,http://arxiv.org/abs/1703.03470v1,http://arxiv.org/pdf/1703.03470v1,"cs.LG, 68T05, 68Q32,, I.2.6; I.5.1; I.5.2"
Proceedings of the 3rd International Workshop on Overlay Architectures   for FPGAs (OLAF 2017),"Hayden Kwok-Hay So, John Wawrzynek","The 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017) was held on 22 Feb, 2017 as a co-located workshop at the 25th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA 2017). This year, the program committee selected 3 papers and 3 extended abstracts to be presented at the workshop, which are subsequently collected in this online volume.",2017-04-28T03:49:08Z,2019-03-05T15:06:01Z,http://arxiv.org/abs/1704.08802v2,http://arxiv.org/pdf/1704.08802v2,"cs.AR, C.0; C.1; B.5.2; B.6.3; B.7.2"
Future Mobile Network Architecture: Challenges and Issues,"Muhammad Bilal, Moonsoo Kang","The future mobile networks facing many challenges and to cope these challenges, different standards and project has been proposed so far. Most recently Cognitive Networks has opened a new ground to present suitable architecture and mechanism for these challenges. The objective of this paper is to identify and discuss the challenges to the future mobile networks and to discuss some workable solutions to these challenges. Finally, on the basis of discussion a simple flexible network architecture is proposed.",2018-01-06T19:49:37Z,2018-01-06T19:49:37Z,http://arxiv.org/abs/1801.02076v1,http://arxiv.org/pdf/1801.02076v1,"cs.NI, 68M10, 68M12, C.2.1; C.2.3; C.2.5"
A neuro-inspired architecture for unsupervised continual learning based   on online clustering and hierarchical predictive coding,Constantine Dovrolis,"We propose that the Continual Learning desiderata can be achieved through a neuro-inspired architecture, grounded on Mountcastle's cortical column hypothesis. The proposed architecture involves a single module, called Self-Taught Associative Memory (STAM), which models the function of a cortical column. STAMs are repeated in multi-level hierarchies involving feedforward, lateral and feedback connections. STAM networks learn in an unsupervised manner, based on a combination of online clustering and hierarchical predictive coding. This short paper only presents the architecture and its connections with neuroscience. A mathematical formulation and experimental results will be presented in an extended version of this paper.",2018-10-22T16:27:21Z,2018-10-22T16:27:21Z,http://arxiv.org/abs/1810.09391v1,http://arxiv.org/pdf/1810.09391v1,"cs.LG, cs.AI, q-bio.NC, stat.ML"
A Low-Resolution ADC Module Assisted Hybrid Beamforming Architecture for   mmWave Communications,"Jie Yang, Xi Yang, Shi Jin, Chao-Kai Wen, Michalis Matthaiou","We propose a low-resolution analog-to-digital converter (ADC) module assisted hybrid beamforming architecture for millimeter-wave (mmWave) communications. We prove that the proposed low-cost and flexible architecture can reduce the beam training time and complexity dramatically without degradation in the data transmission performance. In addition, we design a fast beam training method which is suitable for the proposed system architecture. The proposed beam training method requires only L + 1 (where L is the number of paths) time slots which is smaller compared to the state-of-the-art.",2018-03-26T11:23:38Z,2018-03-26T11:23:38Z,http://arxiv.org/abs/1803.09515v1,http://arxiv.org/pdf/1803.09515v1,"cs.IT, math.IT"
Problems and Solutions of Service Architecture in Small and Medium   Enterprise Communities,"Agustinus Andriyanto, Robin Doss","Lack of resources is a challenge for small and medium enterprises (SMEs) in implementing an IT-based system to facilitate more efficient business decisions and expanding the market. A community system based on service-oriented architecture (SOA) can help SMEs alleviate this problem. This paper explores and analyses the frameworks proposed by previous studies in the context of inter-enterprise SOA for SMEs. Several problems being the background of the system implementation are identified. Afterward, the offered solutions are presented, including the system architecture, technology adoption, specific elements, and collaboration model. The study also discusses the system architecture patterns of the reviewed studies as well as the collaboration organizational structures.",2020-04-22T15:53:57Z,2020-04-22T15:53:57Z,http://arxiv.org/abs/2004.10660v1,http://arxiv.org/pdf/2004.10660v1,"cs.SE, D.2.11; H.0"
A Unified Hardware Architecture for Convolutions and Deconvolutions in   CNN,"Lin Bai, Yecheng Lyu, Xinming Huang","In this paper, a scalable neural network hardware architecture for image segmentation is proposed. By sharing the same computing resources, both convolution and deconvolution operations are handled by the same process element array. In addition, access to on-chip and off-chip memories is optimized to alleviate the burden introduced by partial sum. As an example, SegNet-Basic has been implemented using the proposed unified architecture by targeting on Xilinx ZC706 FPGA, which achieves the performance of 151.5 GOPS and 94.3 GOPS for convolution and deconvolution respectively. This unified convolution/deconvolution design is applicable to other CNNs with deconvolution.",2020-05-29T19:54:29Z,2020-05-29T19:54:29Z,http://arxiv.org/abs/2006.00053v1,http://arxiv.org/pdf/2006.00053v1,"eess.SP, cs.AR, eess.IV"
On the Maximum Mutual Information Capacity of Neural Architectures,"Brandon Foggo, Nanpeng Yu","We derive the closed-form expression of the maximum mutual information - the maximum value of $I(X;Z)$ obtainable via training - for a broad family of neural network architectures. The quantity is essential to several branches of machine learning theory and practice. Quantitatively, we show that the maximum mutual information for these families all stem from generalizations of a single catch-all formula. Qualitatively, we show that the maximum mutual information of an architecture is most strongly influenced by the width of the smallest layer of the network - the ""information bottleneck"" in a different sense of the phrase, and by any statistical invariances captured by the architecture.",2020-06-10T19:20:12Z,2020-06-10T19:20:12Z,http://arxiv.org/abs/2006.06037v1,http://arxiv.org/pdf/2006.06037v1,"cs.LG, stat.ML"
Proceedings of the 2nd International Workshop on Overlay Architectures   for FPGAs (OLAF 2016),"Hayden Kwok-Hay So, John Wawrzynek","The 2nd International Workshop on Overlay Architectures for FPGAs (OLAF 2016) was held on 21 Mar, 2016 as a co-located workshop at the 24th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA 2016). This year, the program committee selected 6 papers and 3 extended abstracts to be presented at the workshop, which are subsequently collected in this online volume.",2016-05-26T05:46:03Z,2016-05-26T05:46:03Z,http://arxiv.org/abs/1605.08149v1,http://arxiv.org/pdf/1605.08149v1,"cs.AR, C.0; C.1; B.5.2; B.6.3; B.7.2"
Deep Active Learning with a Neural Architecture Search,"Yonatan Geifman, Ran El-Yaniv","We consider active learning of deep neural networks. Most active learning works in this context have focused on studying effective querying mechanisms and assumed that an appropriate network architecture is a priori known for the problem at hand. We challenge this assumption and propose a novel active strategy whereby the learning algorithm searches for effective architectures on the fly, while actively learning. We apply our strategy using three known querying techniques (softmax response, MC-dropout, and coresets) and show that the proposed approach overwhelmingly outperforms active learning using fixed architectures.",2018-11-19T09:45:20Z,2019-09-05T11:05:25Z,http://arxiv.org/abs/1811.07579v2,http://arxiv.org/pdf/1811.07579v2,"cs.LG, stat.ML"
Social Attention for Autonomous Decision-Making in Dense Traffic,"Edouard Leurent, Jean Mercat","We study the design of learning architectures for behavioural planning in a dense traffic setting. Such architectures should deal with a varying number of nearby vehicles, be invariant to the ordering chosen to describe them, while staying accurate and compact. We observe that the two most popular representations in the literature do not fit these criteria, and perform badly on an complex negotiation task. We propose an attention-based architecture that satisfies all these properties and explicitly accounts for the existing interactions between the traffic participants. We show that this architecture leads to significant performance gains, and is able to capture interactions patterns that can be visualised and qualitatively interpreted. Videos and code are available at https://eleurent.github.io/social-attention/.",2019-11-27T16:14:15Z,2019-11-27T16:14:15Z,http://arxiv.org/abs/1911.12250v1,http://arxiv.org/pdf/1911.12250v1,"cs.LG, stat.ML"
Intelligent Matrix Exponentiation,"Thomas Fischbacher, Iulia M. Comsa, Krzysztof Potempa, Moritz Firsching, Luca Versari, Jyrki Alakuijala","We present a novel machine learning architecture that uses the exponential of a single input-dependent matrix as its only nonlinearity. The mathematical simplicity of this architecture allows a detailed analysis of its behaviour, providing robustness guarantees via Lipschitz bounds. Despite its simplicity, a single matrix exponential layer already provides universal approximation properties and can learn fundamental functions of the input, such as periodic functions or multivariate polynomials. This architecture outperforms other general-purpose architectures on benchmark problems, including CIFAR-10, using substantially fewer parameters.",2020-08-10T07:49:01Z,2020-08-10T07:49:01Z,http://arxiv.org/abs/2008.03936v1,http://arxiv.org/pdf/2008.03936v1,"cs.LG, cs.NE, math.RT, stat.ML"
Evolutionary Algorithm Enhanced Neural Architecture Search for   Text-Independent Speaker Verification,"Xiaoyang Qu, Jianzong Wang, Jing Xiao","State-of-the-art speaker verification models are based on deep learning techniques, which heavily depend on the handdesigned neural architectures from experts or engineers. We borrow the idea of neural architecture search(NAS) for the textindependent speaker verification task. As NAS can learn deep network structures automatically, we introduce the NAS conception into the well-known x-vector network. Furthermore, this paper proposes an evolutionary algorithm enhanced neural architecture search method called Auto-Vector to automatically discover promising networks for the speaker verification task. The experimental results demonstrate our NAS-based model outperforms state-of-the-art speaker verification models.",2020-08-13T05:34:52Z,2020-08-13T05:34:52Z,http://arxiv.org/abs/2008.05695v1,http://arxiv.org/pdf/2008.05695v1,"eess.AS, cs.NE, cs.SD"
Vit-GAN: Image-to-image Translation with Vision Transformes and   Conditional GANS,Yiğit Gündüç,"In this paper, we have developed a general-purpose architecture, Vit-Gan, capable of performing most of the image-to-image translation tasks from semantic image segmentation to single image depth perception. This paper is a follow-up paper, an extension of generator-based model [1] in which the obtained results were very promising. This opened the possibility of further improvements with adversarial architecture. We used a unique vision transformers-based generator architecture and Conditional GANs(cGANs) with a Markovian Discriminator (PatchGAN) (https://github.com/YigitGunduc/vit-gan). In the present work, we use images as conditioning arguments. It is observed that the obtained results are more realistic than the commonly used architectures.",2021-10-11T18:09:16Z,2021-10-11T18:09:16Z,http://arxiv.org/abs/2110.09305v1,http://arxiv.org/pdf/2110.09305v1,"eess.IV, cs.CV"
Persformer: A Transformer Architecture for Topological Machine Learning,"Raphael Reinauer, Matteo Caorsi, Nicolas Berkouk","One of the main challenges of Topological Data Analysis (TDA) is to extract features from persistent diagrams directly usable by machine learning algorithms. Indeed, persistence diagrams are intrinsically (multi-)sets of points in $\mathbb{R}^2$ and cannot be seen in a straightforward manner as vectors. In this article, we introduce $\texttt{Persformer}$, the first Transformer neural network architecture that accepts persistence diagrams as input. The $\texttt{Persformer}$ architecture significantly outperforms previous topological neural network architectures on classical synthetic and graph benchmark datasets. Moreover, it satisfies a universal approximation theorem. This allows us to introduce the first interpretability method for topological machine learning, which we explore in two examples.",2021-12-30T21:10:17Z,2022-09-26T09:15:13Z,http://arxiv.org/abs/2112.15210v2,http://arxiv.org/pdf/2112.15210v2,"cs.LG, math.AT"
Power and Skew Reduction Using Resonant Energy Recycling in 14-nm FinFET   Clocks,"Dhandeep Challagundla, Mehedi Galib, Ignatius Bezzam, Riadul Islam","As the demand for high-performance microprocessors increases, the circuit complexity and the rate of data transfer increases resulting in higher power consumption. We propose a clocking architecture that uses a series LC resonance and inductor matching technique to address this bottleneck. By employing pulsed resonance, the switching power dissipated is recycled back. The inductor matching technique aids in reducing the skew, increasing the robustness of the clock network. This new resonant architecture saves over 43% power and 91% skew clocking a range of 1--5 GHz, compared to a conventional primary-secondary flip-flop-based CMOS architecture.",2022-05-16T19:17:52Z,2022-05-16T19:17:52Z,http://arxiv.org/abs/2205.07949v1,http://arxiv.org/pdf/2205.07949v1,"eess.SY, cs.AR, cs.SY"
Kernel Based Cognitive Architecture for Autonomous Agents,Alexander Serov,One of the main problems of modern cognitive architectures is an excessively schematic approach to modeling the processes of cognitive activity. It does not allow the creation of a universal architecture that would be capable of reproducing mental functions without using a predetermined set of perceptual patterns. This paper considers an evolutionary approach to creating a cognitive functionality. The basis of our approach is the use of the functional kernel which consistently generates the intellectual functions of an autonomous agent. We consider a cognitive architecture which ensures the evolution of the agent on the basis of Symbol Emergence Problem solution. Evolution of cognitive abilities of the agent is described on the basis of the theory of constructivism.,2022-07-02T12:41:32Z,2022-07-02T12:41:32Z,http://arxiv.org/abs/2207.00822v1,http://arxiv.org/pdf/2207.00822v1,"cs.AI, 68T42"
Perspectives on a 6G Architecture,"Rainer Liebhart, Mansoor Shafi, Gajan Shivanandan, Devaki Chandramouli, Laurent Thiebaut","Mobile communications have been undergoing a generational change every ten years. Whilst we are just beginning to roll out 5G networks, significant efforts are planned to standardize 6G that is expected to be commercially introduced by 2030. This paper looks at the use cases for 6G and their impact on the network architecture to meet the anticipated performance requirements. The new architecture is based on integrating various network functions in virtual cloud environments, leveraging the advancement of artificial intelligence in all domains, integrating different sub-networks constituting the 6G system, and on enhanced means of exposing data and services to third parties.",2022-10-07T02:23:37Z,2022-10-07T02:23:37Z,http://arxiv.org/abs/2210.03286v1,http://arxiv.org/pdf/2210.03286v1,"cs.NI, cs.IT, math.IT"
FMM-Net: neural network architecture based on the Fast Multipole Method,"Daria Sushnikova, Pavel Kharyuk, Ivan Oseledets","In this paper, we propose a new neural network architecture based on the H2 matrix. Even though networks with H2-inspired architecture already exist, and our approach is designed to reduce memory costs and improve performance by taking into account the sparsity template of the H2 matrix. In numerical comparison with alternative neural networks, including the known H2-based ones, our architecture showed itself as beneficial in terms of performance, memory, and scalability.",2022-12-25T13:08:09Z,2022-12-25T13:08:09Z,http://arxiv.org/abs/2212.12899v1,http://arxiv.org/pdf/2212.12899v1,"math.NA, cs.AI, cs.LG, cs.NA"
GP-NAS-ensemble: a model for NAS Performance Prediction,"Kunlong Chen, Liu Yang, Yitian Chen, Kunjin Chen, Yidan Xu, Lujun Li","It is of great significance to estimate the performance of a given model architecture without training in the application of Neural Architecture Search (NAS) as it may take a lot of time to evaluate the performance of an architecture. In this paper, a novel NAS framework called GP-NAS-ensemble is proposed to predict the performance of a neural network architecture with a small training dataset. We make several improvements on the GP-NAS model to make it share the advantage of ensemble learning methods. Our method ranks second in the CVPR2022 second lightweight NAS challenge performance prediction track.",2023-01-23T00:17:52Z,2023-01-23T00:17:52Z,http://arxiv.org/abs/2301.09231v1,http://arxiv.org/pdf/2301.09231v1,"cs.LG, stat.AP, stat.ML"
"Analog, In-memory Compute Architectures for Artificial Intelligence","Patrick Bowen, Guy Regev, Nir Regev, Bruno Pedroni, Edward Hanson, Yiran Chen","This paper presents an analysis of the fundamental limits on energy efficiency in both digital and analog in-memory computing architectures, and compares their performance to single instruction, single data (scalar) machines specifically in the context of machine inference. The focus of the analysis is on how efficiency scales with the size, arithmetic intensity, and bit precision of the computation to be performed. It is shown that analog, in-memory computing architectures can approach arbitrarily high energy efficiency as both the problem size and processor size scales.",2023-01-13T21:04:16Z,2023-01-13T21:04:16Z,http://arxiv.org/abs/2302.06417v1,http://arxiv.org/pdf/2302.06417v1,"cs.AR, cs.NE, physics.optics"
Learning high-dimensional causal effect,"Aayush Agarwal, Saksham Bassi","The scarcity of high-dimensional causal inference datasets restricts the exploration of complex deep models. In this work, we propose a method to generate a synthetic causal dataset that is high-dimensional. The synthetic data simulates a causal effect using the MNIST dataset with Bernoulli treatment values. This provides an opportunity to study varieties of models for causal effect estimation. We experiment on this dataset using Dragonnet architecture (Shi et al. (2019)) and modified architectures. We use the modified architectures to explore different types of initial Neural Network layers and observe that the modified architectures perform better in estimations. We observe that residual and transformer models estimate treatment effect very closely without the need for targeted regularization, introduced by Shi et al. (2019).",2023-03-01T20:57:48Z,2023-03-01T20:57:48Z,http://arxiv.org/abs/2303.00821v1,http://arxiv.org/pdf/2303.00821v1,"cs.LG, stat.ME"
Silicon Photonic 2.5D Interposer Networks for Overcoming Communication   Bottlenecks in Scale-out Machine Learning Hardware Accelerators,"Febin Sunny, Ebadollah Taheri, Mahdi Nikdast, Sudeep Pasricha","Modern machine learning (ML) applications are becoming increasingly complex and monolithic (single chip) accelerator architectures cannot keep up with their energy efficiency and throughput demands. Even though modern digital electronic accelerators are gradually adopting 2.5D architectures with multiple smaller chiplets to improve scalability, they face fundamental limitations due to a reliance on slow metallic interconnects. This paper outlines how optical communication and computation can be leveraged in 2.5D platforms to realize energy-efficient and high throughput 2.5D ML accelerator architectures.",2024-03-07T03:38:35Z,2024-03-07T03:38:35Z,http://arxiv.org/abs/2403.04189v1,http://arxiv.org/pdf/2403.04189v1,"cs.AR, cs.LG, eess.SP"
Machine Apophenia: The Kaleidoscopic Generation of Architectural Images,"Alexey Tikhonov, Dmitry Sinyavin","This study investigates the application of generative artificial intelligence in architectural design. We present a novel methodology that combines multiple neural networks to create an unsupervised and unmoderated stream of unique architectural images. Our approach is grounded in the conceptual framework called machine apophenia. We hypothesize that neural networks, trained on diverse human-generated data, internalize aesthetic preferences and tend to produce coherent designs even from random inputs. The methodology involves an iterative process of image generation, description, and refinement, resulting in captioned architectural postcards automatically shared on several social media platforms. Evaluation and ablation studies show the improvement both in technical and aesthetic metrics of resulting images on each step.",2024-07-12T11:11:19Z,2024-07-12T11:11:19Z,http://arxiv.org/abs/2407.09172v1,http://arxiv.org/pdf/2407.09172v1,"cs.AI, cs.CV, 68T01, 68U05, 00A66, 00A67, I.2.1; I.3.3; J.5; H.5.1"
Neural Conjugate Flows: Physics-informed architectures with flow   structure,"Arthur Bizzi, Lucas Nissenbaum, João M. Pereira","We introduce Neural Conjugate Flows (NCF), a class of neural network architectures equipped with exact flow structure. By leveraging topological conjugation, we prove that these networks are not only naturally isomorphic to a continuous group, but are also universal approximators for flows of ordinary differential equation (ODEs). Furthermore, topological properties of these flows can be enforced by the architecture in an interpretable manner. We demonstrate in numerical experiments how this topological group structure leads to concrete computational gains over other physics informed neural networks in estimating and extrapolating latent dynamics of ODEs, while training up to five times faster than other flow-based architectures.",2024-11-13T04:20:29Z,2024-11-13T04:20:29Z,http://arxiv.org/abs/2411.08326v1,http://arxiv.org/pdf/2411.08326v1,"cs.LG, cs.NA, math.NA"
Toward Bundler-Independent Module Federations: Enabling Typed   Micro-Frontend Architectures,"Billy Lando, Wilhelm Hasselbring","Modern web applications demand scalable and modular architectures, driving the adoption of micro-frontends. This paper introduces Bundler-Independent Module Federation (BIMF) as a New Idea, enabling runtime module loading without relying on traditional bundlers, thereby enhancing flexibility and team collaboration. This paper presents the initial implementation of BIMF, emphasizing benefits such as shared dependency management and modular performance optimization. We address key challenges, including debugging, observability, and performance bottlenecks, and propose solutions such as distributed tracing, server-side rendering, and intelligent prefetching. Future work will focus on evaluating observability tools, improving developer experience, and implementing performance optimizations to fully realize BIMF's potential in micro-frontend architectures.",2025-01-30T09:28:04Z,2025-01-30T09:28:04Z,http://arxiv.org/abs/2501.18225v1,http://arxiv.org/pdf/2501.18225v1,"cs.SE, D.2.11"
A Compositional Approach to Creating Architecture Frameworks with an   Application to Distributed AI Systems,"Hans-Martin Heyn, Eric Knauss, Patrizio Pelliccione","Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks. Then, we derive from the identified challenges four rules and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project ``Very efficient deep learning in the IoT"" (VEDLIoT) in the form of a case study.",2022-12-27T18:05:02Z,2022-12-27T18:05:02Z,http://arxiv.org/abs/2212.13570v1,http://arxiv.org/pdf/2212.13570v1,"cs.SE, cs.AI, D.2.1; D.2.11"
"Enhancing Network Slicing Architectures with Machine Learning, Security,   Sustainability and Experimental Networks Integration","Joberto S. B. Martins, Tereza C. Carvalho, Rodrigo Moreira, Cristiano Both, Adnei Donatti, João H. Corrêa, José A. Suruagy, Sand L. Corrêa, Antonio J. G. Abelem, Moisés R. N. Ribeiro, Jose-Marcos Nogueira, Luiz C. S. Magalhães, Juliano Wickboldt, Tiago Ferreto, Ricardo Mello, Rafael Pasquini, Marcos Schwarz, Leobino N. Sampaio, Daniel F. Macedo, José F. de Rezende, Kleber V. Cardoso, Flávio O. Silva","Network Slicing (NS) is an essential technique extensively used in 5G networks computing strategies, mobile edge computing, mobile cloud computing, and verticals like the Internet of Vehicles and industrial IoT, among others. NS is foreseen as one of the leading enablers for 6G futuristic and highly demanding applications since it allows the optimization and customization of scarce and disputed resources among dynamic, demanding clients with highly distinct application requirements. Various standardization organizations, like 3GPP's proposal for new generation networks and state-of-the-art 5G/6G research projects, are proposing new NS architectures. However, new NS architectures have to deal with an extensive range of requirements that inherently result in having NS architecture proposals typically fulfilling the needs of specific sets of domains with commonalities. The Slicing Future Internet Infrastructures (SFI2) architecture proposal explores the gap resulting from the diversity of NS architectures target domains by proposing a new NS reference architecture with a defined focus on integrating experimental networks and enhancing the NS architecture with Machine Learning (ML) native optimizations, energy-efficient slicing, and slicing-tailored security functionalities. The SFI2 architectural main contribution includes the utilization of the slice-as-a-service paradigm for end-to-end orchestration of resources across multi-domains and multi-technology experimental networks. In addition, the SFI2 reference architecture instantiations will enhance the multi-domain and multi-technology integrated experimental network deployment with native ML optimization, energy-efficient aware slicing, and slicing-tailored security functionalities for the practical domain.",2023-07-18T11:22:31Z,2023-07-18T11:22:31Z,http://arxiv.org/abs/2307.09151v1,http://arxiv.org/pdf/2307.09151v1,"cs.NI, cs.AI, I.2.1; C.2.1; C.2.3"
Design and Analysis of a Novel $\mathcal{L}_1$ Adaptive Control   Architecture with Guaranteed Transient Performance,"Chengyu Cao, Naira Hovakimyan","Novel adaptive control architecture is presented that has guaranteed transient performance for system's both signals, input and output, simultaneously.",2006-08-13T22:12:34Z,2006-08-13T22:12:34Z,http://arxiv.org/abs/math/0608320v1,http://arxiv.org/pdf/math/0608320v1,"math.OC, 93C40"
Assessing Random Dynamical Network Architectures for Nanoelectronics,"Christof Teuscher, Natali Gulbahce, Thimo Rohlf","Independent of the technology, it is generally expected that future nanoscale devices will be built from vast numbers of densely arranged devices that exhibit high failure rates. Other than that, there is little consensus on what type of technology and computing architecture holds most promises to go far beyond today's top-down engineered silicon devices. Cellular automata (CA) have been proposed in the past as a possible class of architectures to the von Neumann computing architecture, which is not generally well suited for future parallel and fine-grained nanoscale electronics. While the top-down engineered semi-conducting technology favors regular and locally interconnected structures, future bottom-up self-assembled devices tend to have irregular structures because of the current lack precise control over these processes. In this paper, we will assess random dynamical networks, namely Random Boolean Networks (RBNs) and Random Threshold Networks (RTNs), as alternative computing architectures and models for future information processing devices. We will illustrate that--from a theoretical perspective--they offer superior properties over classical CA-based architectures, such as inherent robustness as the system scales up, more efficient information processing capabilities, and manufacturing benefits for bottom-up designed devices, which motivates this investigation. We will present recent results on the dynamic behavior and robustness of such random dynamical networks while also including manufacturing issues in the assessment.",2008-05-17T16:22:27Z,2008-05-17T16:22:27Z,http://arxiv.org/abs/0805.2684v1,http://arxiv.org/pdf/0805.2684v1,"cs.AR, cond-mat.dis-nn, nlin.CG"
A Model of Layered Architectures,"Diego Marmsoler, Alexander Malkis, Jonas Eckhardt","Architectural styles and patterns play an important role in software engineering. One of the most known ones is the layered architecture style. However, this style is usually only stated informally, which may cause problems such as ambiguity, wrong conclusions, and difficulty when checking the conformance of a system to the style. We address these problems by providing a formal, denotational semantics of the layered architecture style. Mainly, we present a sufficiently abstract and rigorous description of layered architectures. Loosely speaking, a layered architecture consists of a hierarchy of layers, in which services communicate via ports. A layer is modeled as a relation between used and provided services, and layer composition is defined by means of relational composition. Furthermore, we provide a formal definition for the notions of syntactic and semantic dependency between the layers. We show that these dependencies are not comparable in general. Moreover, we identify sufficient conditions under which, in an intuitive sense which we make precise in our treatment, the semantic dependency implies, is implied by, or even coincides with the reflexive-transitive closure of the syntactic dependency. Our results provide a technology-independent characterization of the layered architecture style, which may be used by software architects to ensure that a system is indeed built according to that style.",2015-03-17T04:00:21Z,2015-03-17T04:00:21Z,http://arxiv.org/abs/1503.04916v1,http://arxiv.org/pdf/1503.04916v1,"cs.SE, D.2.11 [Software Engineering]: Software Architectures | Patterns"
SegNAS3D: Network Architecture Search with Derivative-Free Global   Optimization for 3D Image Segmentation,"Ken C. L. Wong, Mehdi Moradi","Deep learning has largely reduced the need for manual feature selection in image segmentation. Nevertheless, network architecture optimization and hyperparameter tuning are mostly manual and time consuming. Although there are increasing research efforts on network architecture search in computer vision, most works concentrate on image classification but not segmentation, and there are very limited efforts on medical image segmentation especially in 3D. To remedy this, here we propose a framework, SegNAS3D, for network architecture search of 3D image segmentation. In this framework, a network architecture comprises interconnected building blocks that consist of operations such as convolution and skip connection. By representing the block structure as a learnable directed acyclic graph, hyperparameters such as the number of feature channels and the option of using deep supervision can be learned together through derivative-free global optimization. Experiments on 43 3D brain magnetic resonance images with 19 structures achieved an average Dice coefficient of 82%. Each architecture search required less than three days on three GPUs and produced architectures that were much smaller than the state-of-the-art manually created architectures.",2019-09-12T21:51:28Z,2019-09-12T21:51:28Z,http://arxiv.org/abs/1909.05962v1,http://arxiv.org/pdf/1909.05962v1,"eess.IV, cs.CV, cs.NE"
Meta Architecture Search,"Albert Shaw, Wei Wei, Weiyang Liu, Le Song, Bo Dai","Neural Architecture Search (NAS) has been quite successful in constructing state-of-the-art models on a variety of tasks. Unfortunately, the computational cost can make it difficult to scale. In this paper, we make the first attempt to study Meta Architecture Search which aims at learning a task-agnostic representation that can be used to speed up the process of architecture search on a large number of tasks. We propose the Bayesian Meta Architecture SEarch (BASE) framework which takes advantage of a Bayesian formulation of the architecture search problem to learn over an entire set of tasks simultaneously. We show that on Imagenet classification, we can find a model that achieves 25.7% top-1 error and 8.1% top-5 error by adapting the architecture in less than an hour from an 8 GPU days pretrained meta-network. By learning a good prior for NAS, our method dramatically decreases the required computation cost while achieving comparable performance to current state-of-the-art methods - even finding competitive models for unseen datasets with very quick adaptation. We believe our framework will open up new possibilities for efficient and massively scalable architecture search research across multiple tasks.",2018-12-22T19:25:08Z,2019-11-15T16:06:14Z,http://arxiv.org/abs/1812.09584v2,http://arxiv.org/pdf/1812.09584v2,"cs.LG, stat.ML"
Fully-/Partially-Connected Hybrid Beamforming Architectures for mmWave   MU-MIMO,"Xiaoshen Song, Thomas Kühne, Giuseppe Caire","Hybrid digital analog (HDA) beamforming has attracted considerable attention in practical implementation of millimeter wave (mmWave) multiuser multiple-input multiple-output (MU-MIMO) systems due to the low power consumption with respect to its fully digital baseband counterpart. The implementation cost, performance, and power efficiency of HDA beamforming depends on the level of connectivity and reconfigurability of the analog beamforming network. In this paper, we investigate the performance of two typical architectures that can be regarded as extreme cases, namely, the fully-connected (FC) and the one-stream-per-subarray (OSPS) architectures. In the FC architecture each RF antenna port is connected to all antenna elements of the array, while in the OSPS architecture the RF antenna ports are connected to disjoint subarrays. We jointly consider the initial beam acquisition and data communication phases, such that the latter takes place by using the beam direction information obtained by the former. We use the state-of-the-art beam alignment (BA) scheme previously proposed by the authors and consider a family of MU-MIMO precoding schemes well adapted to the beam information extracted from the BA phase. We also evaluate the power efficiency of the two HDA architectures taking into account the power dissipation at different hardware components as well as the power backoff under typical power amplifier constraints. Numerical results show that the two architectures achieve similar sum spectral efficiency, while the OSPS architecture is advantageous with respect to the FC case in terms of hardware complexity and power efficiency, at the sole cost of a slightly longer BA time-to-acquisition due to its reduced beam angle resolution.",2019-04-23T12:31:50Z,2019-10-07T09:54:24Z,http://arxiv.org/abs/1904.10276v2,http://arxiv.org/pdf/1904.10276v2,"cs.IT, math.IT"
UNAS: Differentiable Architecture Search Meets Reinforcement Learning,"Arash Vahdat, Arun Mallya, Ming-Yu Liu, Jan Kautz","Neural architecture search (NAS) aims to discover network architectures with desired properties such as high accuracy or low latency. Recently, differentiable NAS (DNAS) has demonstrated promising results while maintaining a search cost orders of magnitude lower than reinforcement learning (RL) based NAS. However, DNAS models can only optimize differentiable loss functions in search, and they require an accurate differentiable approximation of non-differentiable criteria. In this work, we present UNAS, a unified framework for NAS, that encapsulates recent DNAS and RL-based approaches under one framework. Our framework brings the best of both worlds, and it enables us to search for architectures with both differentiable and non-differentiable criteria in one unified framework while maintaining a low search cost. Further, we introduce a new objective function for search based on the generalization gap that prevents the selection of architectures prone to overfitting. We present extensive experiments on the CIFAR-10, CIFAR-100, and ImageNet datasets and we perform search in two fundamentally different search spaces. We show that UNAS obtains the state-of-the-art average accuracy on all three datasets when compared to the architectures searched in the DARTS space. Moreover, we show that UNAS can find an efficient and accurate architecture in the ProxylessNAS search space, that outperforms existing MobileNetV2 based architectures. The source code is available at https://github.com/NVlabs/unas .",2019-12-16T19:31:39Z,2020-08-27T21:48:42Z,http://arxiv.org/abs/1912.07651v2,http://arxiv.org/pdf/1912.07651v2,"cs.LG, cs.CV, stat.ML"
Hybrid MIMO Architectures for Millimeter Wave Communications: Phase   Shifters or Switches?,"Roi Méndez-Rial, Cristian Rusu, Nuria González-Prelcic, Ahmed Alkhateeb, Robert W. Heath Jr","Hybrid analog/digital MIMO architectures were recently proposed as an alternative for fully-digitalprecoding in millimeter wave (mmWave) wireless communication systems. This is motivated by the possible reduction in the number of RF chains and analog-to-digital converters. In these architectures, the analog processing network is usually based on variable phase shifters. In this paper, we propose hybrid architectures based on switching networks to reduce the complexity and the power consumption of the structures based on phase shifters. We define a power consumption model and use it to evaluate the energy efficiency of both structures. To estimate the complete MIMO channel, we propose an open loop compressive channel estimation technique which is independent of the hardware used in the analog processing stage. We analyze the performance of the new estimation algorithm for hybrid architectures based on phase shifters and switches. Using the estimated, we develop two algorithms for the design of the hybrid combiner based on switches and analyze the achieved spectral efficiency. Finally, we study the trade-offs between power consumption, hardware complexity, and spectral efficiency for hybrid architectures based on phase shifting networks and switching networks. Numerical results show that architectures based on switches obtain equal or better channel estimation performance to that obtained using phase shifters, while reducing hardware complexity and power consumption. For equal power consumption, all the hybrid architectures provide similar spectral efficiencies.",2015-12-09T20:36:25Z,2015-12-09T20:36:25Z,http://arxiv.org/abs/1512.03032v1,http://arxiv.org/pdf/1512.03032v1,"cs.IT, math.IT"
Towards Non-I.I.D. and Invisible Data with FedNAS: Federated Deep   Learning via Neural Architecture Search,"Chaoyang He, Murali Annavaram, Salman Avestimehr","Federated Learning (FL) has been proved to be an effective learning framework when data cannot be centralized due to privacy, communication costs, and regulatory restrictions. When training deep learning models under an FL setting, people employ the predefined model architecture discovered in the centralized environment. However, this predefined architecture may not be the optimal choice because it may not fit data with non-identical and independent distribution (non-IID). Thus, we advocate automating federated learning (AutoFL) to improve model accuracy and reduce the manual design effort. We specifically study AutoFL via Neural Architecture Search (NAS), which can automate the design process. We propose a Federated NAS (FedNAS) algorithm to help scattered workers collaboratively searching for a better architecture with higher accuracy. We also build a system based on FedNAS. Our experiments on non-IID dataset show that the architecture searched by FedNAS can outperform the manually predefined architecture.",2020-04-18T08:04:44Z,2021-01-04T02:18:08Z,http://arxiv.org/abs/2004.08546v4,http://arxiv.org/pdf/2004.08546v4,"cs.LG, cs.CV, cs.DC, cs.MA, stat.ML"
Two-Level Lattice Neural Network Architectures for Control of Nonlinear   Systems,"James Ferlez, Xiaowu Sun, Yasser Shoukry","In this paper, we consider the problem of automatically designing a Rectified Linear Unit (ReLU) Neural Network (NN) architecture (number of layers and number of neurons per layer) with the guarantee that it is sufficiently parametrized to control a nonlinear system. Whereas current state-of-the-art techniques are based on hand-picked architectures or heuristic based search to find such NN architectures, our approach exploits the given model of the system to design an architecture; as a result, we provide a guarantee that the resulting NN architecture is sufficient to implement a controller that satisfies an achievable specification. Our approach exploits two basic ideas. First, assuming that the system can be controlled by an unknown Lipschitz-continuous state-feedback controller with some Lipschitz constant upper-bounded by $K_\text{cont}$, we bound the number of affine functions needed to construct a Continuous Piecewise Affine (CPWA) function that can approximate the unknown Lipschitz-continuous controller. Second, we utilize the authors' recent results on a novel NN architecture named as the Two-Level Lattice (TLL) NN architecture, which was shown to be capable of implementing any CPWA function just from the knowledge of the number of affine functions that compromises this CPWA function.",2020-04-20T20:45:08Z,2020-12-18T18:24:51Z,http://arxiv.org/abs/2004.09628v2,http://arxiv.org/pdf/2004.09628v2,"cs.LG, cs.SY, eess.SY, math.OC, stat.ML"
In-memory Implementation of On-chip Trainable and Scalable ANN for AI/ML   Applications,"Abhash Kumar, Jawar Singh, Sai Manohar Beeraka, Bharat Gupta","Traditional von Neumann architecture based processors become inefficient in terms of energy and throughput as they involve separate processing and memory units, also known as~\textit{memory wall}. The memory wall problem is further exacerbated when massive parallelism and frequent data movement are required between processing and memory units for real-time implementation of artificial neural network (ANN) that enables many intelligent applications. One of the most promising approach to address the memory wall problem is to carry out computations inside the memory core itself that enhances the memory bandwidth and energy efficiency for extensive computations. This paper presents an in-memory computing architecture for ANN enabling artificial intelligence (AI) and machine learning (ML) applications. The proposed architecture utilizes deep in-memory architecture based on standard six transistor (6T) static random access memory (SRAM) core for the implementation of a multi-layered perceptron. Our novel on-chip training and inference in-memory architecture reduces energy cost and enhances throughput by simultaneously accessing the multiple rows of SRAM array per precharge cycle and eliminating the frequent access of data. The proposed architecture realizes backpropagation which is the keystone during the network training using newly proposed different building blocks such as weight updation, analog multiplication, error calculation, signed analog to digital conversion, and other necessary signal control units. The proposed architecture was trained and tested on the IRIS dataset which exhibits $\approx46\times$ more energy efficient per MAC (multiply and accumulate) operation compared to earlier classifiers.",2020-05-19T15:36:39Z,2020-05-19T15:36:39Z,http://arxiv.org/abs/2005.09526v1,http://arxiv.org/pdf/2005.09526v1,"eess.SP, cs.AR, cs.LG, cs.NE"
AlphaGAN: Fully Differentiable Architecture Search for Generative   Adversarial Networks,"Yuesong Tian, Li Shen, Li Shen, Guinan Su, Zhifeng Li, Wei Liu","Generative Adversarial Networks (GANs) are formulated as minimax game problems, whereby generators attempt to approach real data distributions by virtue of adversarial learning against discriminators. The intrinsic problem complexity poses the challenge to enhance the performance of generative networks. In this work, we aim to boost model learning from the perspective of network architectures, by incorporating recent progress on automated architecture search into GANs. To this end, we propose a fully differentiable search framework for generative adversarial networks, dubbed alphaGAN. The searching process is formalized as solving a bi-level minimax optimization problem, in which the outer-level objective aims for seeking a suitable network architecture towards pure Nash Equilibrium conditioned on the generator and the discriminator network parameters optimized with a traditional GAN loss in the inner level. The entire optimization performs a first-order method by alternately minimizing the two-level objective in a fully differentiable manner, enabling architecture search to be completed in an enormous search space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our algorithm can obtain high-performing architectures only with 3-GPU hours on a single GPU in the search space comprised of approximate 2 ? 1011 possible configurations. We also provide a comprehensive analysis on the behavior of the searching process and the properties of searched architectures, which would benefit further research on architectures for generative models. Pretrained models and codes are available at https://github.com/yuesongtian/AlphaGAN.",2020-06-16T13:27:30Z,2021-08-07T07:53:29Z,http://arxiv.org/abs/2006.09134v3,http://arxiv.org/pdf/2006.09134v3,"cs.CV, cs.LG, eess.IV"
Neural Circuit Architectural Priors for Embodied Control,"Nikhil X. Bhattasali, Anthony M. Zador, Tatiana A. Engel","Artificial neural networks for motor control usually adopt generic architectures like fully connected MLPs. While general, these tabula rasa architectures rely on large amounts of experience to learn, are not easily transferable to new bodies, and have internal dynamics that are difficult to interpret. In nature, animals are born with highly structured connectivity in their nervous systems shaped by evolution; this innate circuitry acts synergistically with learning mechanisms to provide inductive biases that enable most animals to function well soon after birth and learn efficiently. Convolutional networks inspired by visual circuitry have encoded useful biases for vision. However, it is unknown the extent to which ANN architectures inspired by neural circuitry can yield useful biases for other AI domains. In this work, we ask what advantages biologically inspired ANN architecture can provide in the domain of motor control. Specifically, we translate C. elegans locomotion circuits into an ANN model controlling a simulated Swimmer agent. On a locomotion task, our architecture achieves good initial performance and asymptotic performance comparable with MLPs, while dramatically improving data efficiency and requiring orders of magnitude fewer parameters. Our architecture is interpretable and transfers to new body designs. An ablation analysis shows that constrained excitation/inhibition is crucial for learning, while weight initialization contributes to good initial performance. Our work demonstrates several advantages of biologically inspired ANN architecture and encourages future work in more complex embodied control.",2022-01-13T23:22:16Z,2022-11-27T15:41:09Z,http://arxiv.org/abs/2201.05242v2,http://arxiv.org/pdf/2201.05242v2,"cs.LG, cs.NE, cs.RO, q-bio.NC"
Metropolitan Optical Networks: A Survey on New Architectures and Future   Trends,"Léia Sousa de Sousa, André Costa Drummond","Metropolitan optical networks are undergoing major transformations to continue being able to provide services that meet the requirements of the applications of the future. The arrival of the $5G$ will expand the possibilities for offering IoT applications, autonomous vehicles, and smart cities services while imposing strong pressure on the physical infrastructure currently implemented, as well as on static traffic engineering techniques that do not respond in an agile way to the dynamic and heterogeneous nature of the upcoming traffic patterns. In order to guarantee the strictest quality of service and quality of experience requirements for users, as well as meeting the providers' objectives of maintaining an acceptable trade-off between cost and performance, new architectures for metropolitan optical networks have been proposed in the literature, with a growing interest starting from $2017$. However, due to the proliferation of a dozen of new architectures in recent years, many questions need to be investigated regarding the planning, implementation, and management of these architectures, before they could be considered for practical application. This work presents a comprehensive survey of the new proposed architectures for metropolitan optical networks. Firstly, the main data transmission systems, equipment involved, and the structural organization of the new metro ecosystems are discussed. The already established and the novel architectures are presented, highlighting its characteristics and application, and comparative analysis among these architectures is carried out identifying the future technological trends. Finally, outstanding research questions are drawn to help direct future research on the field.",2022-01-26T02:17:42Z,2022-04-06T10:29:34Z,http://arxiv.org/abs/2201.10709v2,http://arxiv.org/pdf/2201.10709v2,"cs.NI, C.2.1"
Efficient Multi-objective Neural Architecture Search via Lamarckian   Evolution,"Thomas Elsken, Jan Hendrik Metzen, Frank Hutter","Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1)the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption, (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks.",2018-04-24T15:01:07Z,2019-02-26T16:06:36Z,http://arxiv.org/abs/1804.09081v4,http://arxiv.org/pdf/1804.09081v4,"stat.ML, cs.LG"
Network-Coding Approach for Information-Centric Networking,Muhammad Bilal,"The current internet architecture is inefficient in fulfilling the demands of newly emerging internet applications. To address this issue, several over-the-top (OTT) application-level solutions have been employed, making the overall architecture very complex. Information-centric-networking (ICN) architecture has emerged as a promising alternative solution. The ICN architecture decouples the content from the host at the network level and supports the temporary storage of content in an in-network cache. Fundamentally, the ICN can be considered a multisource, multicast content-delivery solution. Because of the benefits of network coding in multicasting scenarios and proven benefits in distributed storage networks, the network coding is apt for the ICN architecture. In this study, we propose a solvable linear network-coding scheme for the ICN architecture. We also propose a practical implementation of the network-coding scheme for the ICN, particularly for the content-centric network (CCN) architecture, which is termed the coded CCN (CCCN). The performance results show that the network-coding scheme improves the performance of the CCN and significantly reduces the network traffic and average download delay.",2018-08-01T14:56:59Z,2018-08-09T07:42:25Z,http://arxiv.org/abs/1808.00348v2,http://arxiv.org/pdf/1808.00348v2,"cs.NI, cs.DC, cs.IT, math.IT, 68M10, 68M11, 68M12, 68M15, 68M14, 94C15, 68P30, 05Cxx, C.2.1; C.2.2; C.2.6; E.1; G.2.2; E.4; G.1.3; I.1"
Efficient Novelty-Driven Neural Architecture Search,"Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su","One-Shot Neural architecture search (NAS) attracts broad attention recently due to its capacity to reduce the computational hours through weight sharing. However, extensive experiments on several recent works show that there is no positive correlation between the validation accuracy with inherited weights from the supernet and the test accuracy after re-training for One-Shot NAS. Different from devising a controller to find the best performing architecture with inherited weights, this paper focuses on how to sample architectures to train the supernet to make it more predictive. A single-path supernet is adopted, where only a small part of weights are optimized in each step, to reduce the memory demand greatly. Furthermore, we abandon devising complicated reward based architecture sampling controller, and sample architectures to train supernet based on novelty search. An efficient novelty search method for NAS is devised in this paper, and extensive experiments demonstrate the effectiveness and efficiency of our novelty search based architecture sampling method. The best architecture obtained by our algorithm with the same search space achieves the state-of-the-art test error rate of 2.51\% on CIFAR-10 with only 7.5 hours search time in a single GPU, and a validation perplexity of 60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell structures to larger datasets ImageNet and WikiText-2, respectively.",2019-07-22T03:17:13Z,2019-07-22T03:17:13Z,http://arxiv.org/abs/1907.09109v1,http://arxiv.org/pdf/1907.09109v1,"cs.LG, cs.NE, stat.ML"
When NAS Meets Robustness: In Search of Robust Architectures against   Adversarial Attacks,"Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin","Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then finetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our ""robust architecture Odyssey"" reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) flow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (~5% absolute gains) under both white-box and black-box attacks, even with fewer parameter numbers. Code is available at https://github.com/gmh14/RobNets.",2019-11-25T04:14:02Z,2020-03-26T01:37:40Z,http://arxiv.org/abs/1911.10695v3,http://arxiv.org/pdf/1911.10695v3,"cs.LG, cs.CR, cs.CV, stat.ML"
Identifying the elastic isotropy of architectured materials based on   deep learning method,"Anran Wei, Jie Xiong, Weidong Yang, Fenglin Guo","With the achievement on the additive manufacturing, the mechanical properties of architectured materials can be precisely designed by tailoring microstructures. As one of the primary design objectives, the elastic isotropy is of great significance for many engineering applications. However, the prevailing experimental and numerical methods are normally too costly and time-consuming to determine the elastic isotropy of architectured materials with tens of thousands of possible microstructures in design space. The quick mechanical characterization is thus desired for the advanced design of architectured materials. Here, a deep learning-based approach is developed as a portable and efficient tool to identify the elastic isotropy of architectured materials directly from the images of their representative microstructures with arbitrary component distributions. The measure of elastic isotropy for architectured materials is derived firstly in this paper to construct a database with associated images of microstructures. Then a convolutional neural network is trained with the database. It is found that the convolutional neural network shows good performance on the isotropy identification. Meanwhile, it exhibits enough robustness to maintain the performance under fluctuated material properties in practical fabrications. Moreover, the well-trained convolutional neural network can be successfully transferred among different types of architectured materials, including two-phase composites and porous materials, which greatly enhance the efficiency of the deep learning-based approach. This study can give new inspirations on the fast mechanical characterization for the big-data driven design of architectured materials.",2020-08-02T10:16:28Z,2020-08-09T03:15:22Z,http://arxiv.org/abs/2008.00442v2,http://arxiv.org/pdf/2008.00442v2,"physics.app-ph, cond-mat.dis-nn, cond-mat.mtrl-sci"
RARTS: An Efficient First-Order Relaxed Architecture Search Method,"Fanghui Xue, Yingyong Qi, Jack Xin","Differentiable architecture search (DARTS) is an effective method for data-driven neural network design based on solving a bilevel optimization problem. Despite its success in many architecture search tasks, there are still some concerns about the accuracy of first-order DARTS and the efficiency of the second-order DARTS. In this paper, we formulate a single level alternative and a relaxed architecture search (RARTS) method that utilizes the whole dataset in architecture learning via both data and network splitting, without involving mixed second derivatives of the corresponding loss functions like DARTS. In our formulation of network splitting, two networks with different but related weights cooperate in search of a shared architecture. The advantage of RARTS over DARTS is justified by a convergence theorem and an analytically solvable model. Moreover, RARTS outperforms DARTS and its variants in accuracy and search efficiency, as shown in adequate experimental results. For the task of searching topological architecture, i.e., the edges and the operations, RARTS obtains a higher accuracy and 60\% reduction of computational cost than second-order DARTS on CIFAR-10. RARTS continues to out-perform DARTS upon transfer to ImageNet and is on par with recent variants of DARTS even though our innovation is purely on the training algorithm without modifying search space. For the task of searching width, i.e., the number of channels in convolutional layers, RARTS also outperforms the traditional network pruning benchmarks. Further experiments on the public architecture search benchmark like NATS-Bench also support the preeminence of RARTS.",2020-08-10T04:55:51Z,2022-06-24T06:36:21Z,http://arxiv.org/abs/2008.03901v2,http://arxiv.org/pdf/2008.03901v2,"cs.LG, cs.CV, stat.ML"
Are Deep Neural Architectures Losing Information? Invertibility Is   Indispensable,"Yang Liu, Zhenyue Qin, Saeed Anwar, Sabrina Caldwell, Tom Gedeon","Ever since the advent of AlexNet, designing novel deep neural architectures for different tasks has consistently been a productive research direction. Despite the exceptional performance of various architectures in practice, we study a theoretical question: what is the condition for deep neural architectures to preserve all the information of the input data? Identifying the information lossless condition for deep neural architectures is important, because tasks such as image restoration require keep the detailed information of the input data as much as possible. Using the definition of mutual information, we show that: a deep neural architecture can preserve maximum details about the given data if and only if the architecture is invertible. We verify the advantages of our Invertible Restoring Autoencoder (IRAE) network by comparing it with competitive models on three perturbed image restoration tasks: image denoising, jpeg image decompression and image inpainting. Experimental results show that IRAE consistently outperforms non-invertible ones. Our model even contains far fewer parameters. Thus, it may be worthwhile to try replacing standard components of deep neural architectures, such as residual blocks and ReLU, with their invertible counterparts. We believe our work provides a unique perspective and direction for future deep learning research.",2020-09-07T15:39:24Z,2020-09-29T00:15:58Z,http://arxiv.org/abs/2009.03173v2,http://arxiv.org/pdf/2009.03173v2,"cs.CV, cs.IT, cs.LG, math.IT"
Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces,"Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mariani, Dushyant Mehta, Chris Lott, Tijmen Blankevoort","Current state-of-the-art Neural Architecture Search (NAS) methods neither efficiently scale to multiple hardware platforms, nor handle diverse architectural search-spaces. To remedy this, we present DONNA (Distilling Optimal Neural Network Architectures), a novel pipeline for rapid, scalable and diverse NAS, that scales to many user scenarios. DONNA consists of three phases. First, an accuracy predictor is built using blockwise knowledge distillation from a reference model. This predictor enables searching across diverse networks with varying macro-architectural parameters such as layer types and attention mechanisms, as well as across micro-architectural parameters such as block repeats and expansion rates. Second, a rapid evolutionary search finds a set of pareto-optimal architectures for any scenario using the accuracy predictor and on-device measurements. Third, optimal models are quickly finetuned to training-from-scratch accuracy. DONNA is up to 100x faster than MNasNet in finding state-of-the-art architectures on-device. Classifying ImageNet, DONNA architectures are 20% faster than EfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5% higher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition to NAS, DONNA is used for search-space extension and exploration, as well as hardware-aware model compression.",2020-12-16T11:00:19Z,2021-08-27T13:02:16Z,http://arxiv.org/abs/2012.08859v3,http://arxiv.org/pdf/2012.08859v3,"cs.LG, cs.AI, cs.CV, cs.NE, stat.ML"
On Redundancy and Diversity in Cell-based Neural Architecture Search,"Xingchen Wan, Binxin Ru, Pedro M. Esperança, Zhenguo Li","Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. In this work, we conduct an empirical post-hoc analysis of architectures from the popular cell-based search spaces and find that the existing search spaces contain a high degree of redundancy: the architecture performance is minimally sensitive to changes at large parts of the cells, and universally adopted designs, like the explicit search for a reduction cell, significantly increase the complexities but have very limited impact on the performance. Across architectures found by a diverse set of search strategies, we consistently find that the parts of the cells that do matter for architecture performance often follow similar and simple patterns. By explicitly constraining cells to include these patterns, randomly sampled architectures can match or even outperform the state of the art. These findings cast doubts into our ability to discover truly novel architectures in the existing cell-based search spaces, and inspire our suggestions for improvement to guide future NAS research. Code is available at https://github.com/xingchenwan/cell-based-NAS-analysis.",2022-03-16T18:59:29Z,2022-03-16T18:59:29Z,http://arxiv.org/abs/2203.08887v1,http://arxiv.org/pdf/2203.08887v1,"stat.ML, cs.AI, cs.LG"
Hierarchical quantum circuit representations for neural architecture   search,"Matt Lourens, Ilya Sinayskiy, Daniel K. Park, Carsten Blank, Francesco Petruccione","Machine learning with hierarchical quantum circuits, usually referred to as Quantum Convolutional Neural Networks (QCNNs), is a promising prospect for near-term quantum computing. The QCNN is a circuit model inspired by the architecture of Convolutional Neural Networks (CNNs). CNNs are successful because they do not need manual feature design and can learn high-level features from raw data. Neural Architecture Search (NAS) builds on this success by learning network architecture and achieves state-of-the-art performance. However, applying NAS to QCNNs presents unique challenges due to the lack of a well-defined search space. In this work, we propose a novel framework for representing QCNN architectures using techniques from NAS, which enables search space design and architecture search. Using this framework, we generate a family of popular QCNNs, those resembling reverse binary trees. We then evaluate this family of models on a music genre classification dataset, GTZAN, to justify the importance of circuit architecture. Furthermore, we employ a genetic algorithm to perform Quantum Phase Recognition (QPR) as an example of architecture search with our representation. This work provides a way to improve model performance without increasing complexity and to jump around the cost landscape to avoid barren plateaus. Finally, we implement the framework as an open-source Python package to enable dynamic QCNN creation and facilitate QCNN search space design for NAS.",2022-10-26T22:58:29Z,2023-05-07T06:54:05Z,http://arxiv.org/abs/2210.15073v3,http://arxiv.org/pdf/2210.15073v3,"quant-ph, cs.AI"
Reducing Runtime Overhead via Use-Based Migration in Neutral Atom   Quantum Architectures,"Andrew Litteken, Jonathan M. Baker, Frederic T. Chong","Neutral atoms are a promising choice for scalable quantum computing architectures. Features such as long distance interactions and native multiqubit gates offer reductions in communication costs and operation count. However, the trapped atoms used as qubits can be lost over the course of computation and due to adverse environmental factors. The value of a lost computation qubit cannot be recovered and requires the reloading of the array and rerunning of the computation, greatly increasing the number of runs of a circuit. Software mitigation strategies exist but exhaust the original mapped locations of the circuit slowly and create more spread out clusters of qubits across the architecture decreasing the probability of success. We increase flexibility by developing strategies that find all reachable qubits, rather only adjacent hardware qubits. Second, we divide the architecture into separate sections, and run the circuit in each section, free of lost atoms. Provided the architecture is large enough, this resets the circuit without having to reload the entire architecture. This increases the number of effective shots before reloading by a factor of two for a circuit that utilizes 30% of the architecture. We also explore using these sections to parallelize execution of circuits, reducing the overall runtime by a total 50% for 30 qubit circuit. These techniques contribute to a dynamic new set of strategies to combat the detrimental effects of lost computational space.",2022-11-28T20:24:17Z,2022-11-28T20:24:17Z,http://arxiv.org/abs/2211.15757v1,http://arxiv.org/pdf/2211.15757v1,"quant-ph, cs.AR, cs.ET"
The autoregressive neural network architecture of the Boltzmann   distribution of pairwise interacting spins systems,Indaco Biazzo,"Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated exceptional results in image and language generation tasks, contributing to the growing popularity of generative models in both scientific and commercial applications. This work presents an exact mapping of the Boltzmann distribution of binary pairwise interacting systems into autoregressive form. The resulting ARNN architecture has weights and biases of its first layer corresponding to the Hamiltonian's couplings and external fields, featuring widely used structures such as the residual connections and a recurrent architecture with clear physical meanings. Moreover, its architecture's explicit formulation enables the use of statistical physics techniques to derive new ARNNs for specific systems. As examples, new effective ARNN architectures are derived from two well-known mean-field systems, the Curie-Weiss and Sherrington-Kirkpatrick models, showing superior performance in approximating the Boltzmann distributions of the corresponding physics model compared to other commonly used architectures. The connection established between the physics of the system and the neural network architecture provides a means to derive new architectures for different interacting systems and interpret existing ones from a physical perspective.",2023-02-16T15:05:37Z,2024-03-25T13:18:39Z,http://arxiv.org/abs/2302.08347v3,http://arxiv.org/pdf/2302.08347v3,"cond-mat.dis-nn, cond-mat.stat-mech, cs.LG, stat.ML"
Towards AI-Architecture Liberty: A Comprehensive Survey on Design and   Generation of Virtual Architecture by Deep Learning,"Anqi Wang, Jiahua Dong, Lik-Hang Lee, Jiachuan Shen, Pan Hui","3D shape generation techniques leveraging deep learning have garnered significant interest from both the computer vision and architectural design communities, promising to enrich the content in the virtual environment. However, research on virtual architectural design remains limited, particularly regarding designer-AI collaboration and deep learning-assisted design. In our survey, we reviewed 149 related articles (81.2% of articles published between 2019 and 2023) covering architectural design, 3D shape techniques, and virtual environments. Through scrutinizing the literature, we first identify the principles of virtual architecture and illuminate its current production challenges, including datasets, multimodality, design intuition, and generative frameworks. We then introduce the latest approaches to designing and generating virtual buildings leveraging 3D shape generation and summarize four characteristics of various approaches to virtual architecture. Based on our analysis, we expound on four research agendas, including agency, communication, user consideration, and integrating tools. Additionally, we highlight four important enablers of ubiquitous interaction with immersive systems in deep learning-assisted architectural generation. Our work contributes to fostering understanding between designers and deep learning techniques, broadening access to designer-AI collaboration. We advocate for interdisciplinary efforts to address this timely research topic, facilitating content designing and generation in the virtual environment.",2023-04-30T15:38:36Z,2024-07-18T07:49:58Z,http://arxiv.org/abs/2305.00510v4,http://arxiv.org/pdf/2305.00510v4,"cs.HC, cs.CV, cs.LG, I.2.1; J.5; J.6; I.3.7"
Probabilistic Results on the Architecture of Mathematical Reasoning   Aligned by Cognitive Alternation,"Minzheng Li, Xiangzhong Fang, Haixin Yang","We envision a machine capable of solving mathematical problems. Dividing the quantitative reasoning system into two parts: thought processes and cognitive processes, we provide probabilistic descriptions of the architecture.",2023-08-17T00:35:11Z,2023-08-17T00:35:11Z,http://arxiv.org/abs/2308.08714v1,http://arxiv.org/pdf/2308.08714v1,"cs.AI, math.PR"
GroupMixer: Patch-based Group Convolutional Neural Network for Breast   Cancer Detection from Histopathological Images,"Ardavan Modarres, Erfan Ebrahim Esfahani, Mahsa Bahrami","Diagnosis of breast cancer malignancy at the early stages is a crucial step for controlling its side effects. Histopathological analysis provides a unique opportunity for malignant breast cancer detection. However, such a task would be tedious and time-consuming for the histopathologists. Deep Neural Networks enable us to learn informative features directly from raw histopathological images without manual feature extraction. Although Convolutional Neural Networks (CNNs) have been the dominant architectures in the computer vision realm, Transformer-based architectures have shown promising results in different computer vision tasks. Although harnessing the capability of Transformer-based architectures for medical image analysis seems interesting, these architectures are large, have a significant number of trainable parameters, and require large datasets to be trained on, which are usually rare in the medical domain. It has been claimed and empirically proved that at least part of the superior performance of Transformer-based architectures in Computer Vision domain originates from patch embedding operation. In this paper, we borrowed the previously introduced idea of integrating a fully Convolutional Neural Network architecture with Patch Embedding operation and presented an efficient CNN architecture for breast cancer malignancy detection from histopathological images. Despite the number of parameters that is significantly smaller than other methods, the accuracy performance metrics achieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x magnifications respectively. We took a step forward and modified the architecture using Group Convolution and Channel Shuffling ideas and reduced the number of trainable parameters even more with a negligible decline in performance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the mentioned magnifications respectively.",2023-11-16T12:19:48Z,2023-11-16T12:19:48Z,http://arxiv.org/abs/2311.09846v1,http://arxiv.org/pdf/2311.09846v1,"eess.IV, cs.CV, cs.LG"
Towards Architecture-Agnostic Untrained Network Priors for Image   Reconstruction with Frequency Regularization,"Yilin Liu, Yunkui Pang, Jiang Li, Yong Chen, Pew-Thian Yap","Untrained networks inspired by deep image priors have shown promising capabilities in recovering high-quality images from noisy or partial measurements without requiring training sets. Their success is widely attributed to implicit regularization due to the spectral bias of suitable network architectures. However, the application of such network-based priors often entails superfluous architectural decisions, risks of overfitting, and lengthy optimization processes, all of which hinder their practicality. To address these challenges, we propose efficient architecture-agnostic techniques to directly modulate the spectral bias of network priors: 1) bandwidth-constrained input, 2) bandwidth-controllable upsamplers, and 3) Lipschitz-regularized convolutional layers. We show that, with just a few lines of code, we can reduce overfitting in underperforming architectures and close performance gaps with high-performing counterparts, minimizing the need for extensive architecture tuning. This makes it possible to employ a more compact model to achieve performance similar or superior to larger models while reducing runtime. Demonstrated on inpainting-like MRI reconstruction task, our results signify for the first time that architectural biases, overfitting, and runtime issues of untrained network priors can be simultaneously addressed without architectural modifications. Our code is publicly available.",2023-12-15T18:01:47Z,2024-07-19T03:54:44Z,http://arxiv.org/abs/2312.09988v3,http://arxiv.org/pdf/2312.09988v3,"eess.IV, cs.CV"
Exploring the Design Space for Message-Driven Systems for Dynamic Graph   Processing using CCA,"Bibrak Qamar Chandio, Maciej Brodowicz, Thomas Sterling","Computer systems that have been successfully deployed for dense regular workloads fall short of achieving scalability and efficiency when applied to irregular and dynamic graph applications. Conventional computing systems rely heavily on static, regular, numeric intensive computations while High Performance Computing systems executing parallel graph applications exhibit little locality, spatial or temporal, and are fine-grained and memory intensive. With the strong interest in AI which depend on these very different use cases combined with the end of Moore's Law at nanoscale, dramatic alternatives in architecture and underlying execution models are required. This paper identifies an innovative non-von Neumann architecture, Continuum Computer Architecture (CCA), that redefines the nature of computing structures to yield powerful innovations in computational methods to deliver a new generation of highly parallel hardware architecture. CCA reflects a genus of highly parallel architectures that while varying in specific quantities (e.g., memory blocks), share a multiple of attributes not found in typical von Neumann machines. Among these are memory-centric components, message-driven asynchronous flow control, and lightweight out-of-order execution across a global name space. Together these innovative non-von Neumann architectural properties guided by a new original execution model will deliver the new future path for extending beyond the von Neumann model. This paper documents a series of interrelated experiments that together establish future directions for next generation non-von Neumann architectures, especially for graph processing.",2024-02-04T18:05:02Z,2024-05-21T20:16:41Z,http://arxiv.org/abs/2402.02576v2,http://arxiv.org/pdf/2402.02576v2,"cs.DC, cs.AR, C.1.5; C.4; C.5; D.1.3"
Self-Tuning Network Control Architectures with Joint Sensor and Actuator   Selection,"Karthik Ganapathy, Iman Shames, Mathias Hudoba de Badyn, Tyler Summers","We formulate a mathematical framework for designing a self-tuning network control architecture, and propose a computationally-feasible greedy algorithm for online architecture optimization. In this setting, the locations of active sensors and actuators in the network, as well as the feedback control policy are jointly adapted using all available information about the network states and dynamics to optimize a performance criterion. We show that the case with full-state feedback can be solved with dynamic programming, and in the linear-quadratic setting, the optimal cost functions and policies are piecewise quadratic and piecewise linear, respectively. Our framework is extended for joint sensor and actuator selection for dynamic output feedback control with both control performance and architecture costs. For large networks where exhaustive architecture search is prohibitive, we describe a greedy heuristic for actuator selection and propose a greedy swapping algorithm for joint sensor and actuator selection. Via numerical experiments, we demonstrate a dramatic performance improvement of greedy self-tuning architectures over fixed architectures. Our general formulation provides an extremely rich and challenging problem space with opportunities to apply a wide variety of approximation methods from stochastic control, system identification, reinforcement learning, and static architecture design for practical model-based control.",2024-01-19T20:18:11Z,2024-01-19T20:18:11Z,http://arxiv.org/abs/2402.16861v1,http://arxiv.org/pdf/2402.16861v1,"eess.SY, cs.SY"
A Centralized Discovery-Based Method for Integrating Data Distribution   Service and Time-Sensitive Networking in In-Vehicle Networks,"Feng Luo, Yi Ren, Yanhua Yu, Yunpeng Li, Zitong Wang","As the electronic and electrical architecture (E/EA) of intelligent and connected vehicles (ICVs) evolves, traditional distributed and signal-oriented architectures are being replaced by centralized, service-oriented architectures (SOA). This new generation of E/EA demands in-vehicle networks (IVNs) that offer high bandwidth, real-time, reliability, and service-oriented. data distribution service (DDS) and time-sensitive networking (TSN) are increasingly adopted to address these requirements. However, research on the integrated deployment of DDS and TSN in automotive applications is still in its infancy. This paper presents a DDS over TSN (DoT) communication architecture based on the centralized discovery architecture (CDA). First, a lightweight DDS implementation (FastDDS-lw) is developed for resource-constrained in-vehicle devices. Next, a DDS flow identification algorithm (DFIA) based on the CDA is introduced to identify potential DDS flows during the discovery phase automatically. Finally, the DoT communication architecture is designed, incorporating FastDDS-lw and DFIA. Experimental results show that the DoT architecture significantly reduces end-to-end latency and jitter for critical DDS flows compared to traditional Ethernet. Additionally, DoT provides an automated network configuration method that completes within a few tens of milliseconds.",2024-09-06T07:56:49Z,2024-09-06T07:56:49Z,http://arxiv.org/abs/2409.05904v1,http://arxiv.org/pdf/2409.05904v1,"cs.NI, cs.SY, eess.SY"
"Deconstructing Recurrence, Attention, and Gating: Investigating the   transferability of Transformers and Gated Recurrent Neural Networks in   forecasting of dynamical systems","Hunter S. Heidenreich, Pantelis R. Vlachas, Petros Koumoutsakos","Machine learning architectures, including transformers and recurrent neural networks (RNNs) have revolutionized forecasting in applications ranging from text processing to extreme weather. Notably, advanced network architectures, tuned for applications such as natural language processing, are transferable to other tasks such as spatiotemporal forecasting tasks. However, there is a scarcity of ablation studies to illustrate the key components that enable this forecasting accuracy. The absence of such studies, although explainable due to the associated computational cost, intensifies the belief that these models ought to be considered as black boxes. In this work, we decompose the key architectural components of the most powerful neural architectures, namely gating and recurrence in RNNs, and attention mechanisms in transformers. Then, we synthesize and build novel hybrid architectures from the standard blocks, performing ablation studies to identify which mechanisms are effective for each task. The importance of considering these components as hyper-parameters that can augment the standard architectures is exhibited on various forecasting datasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96 system, the Kuramoto-Sivashinsky equation, as well as standard real world time-series benchmarks. A key finding is that neural gating and attention improves the performance of all standard RNNs in most tasks, while the addition of a notion of recurrence in transformers is detrimental. Furthermore, our study reveals that a novel, sparsely used, architecture which integrates Recurrent Highway Networks with neural gating and attention mechanisms, emerges as the best performing architecture in high-dimensional spatiotemporal forecasting of dynamical systems.",2024-10-03T16:41:51Z,2024-10-03T16:41:51Z,http://arxiv.org/abs/2410.02654v1,http://arxiv.org/pdf/2410.02654v1,"cs.LG, nlin.CD, physics.comp-ph"
Q3DE: A fault-tolerant quantum computer architecture for multi-bit burst   errors by cosmic rays,"Yasunari Suzuki, Takanori Sugiyama, Tomochika Arai, Wang Liao, Koji Inoue, Teruo Tanimoto","Demonstrating small error rates by integrating quantum error correction (QEC) into an architecture of quantum computing is the next milestone towards scalable fault-tolerant quantum computing (FTQC). Encoding logical qubits with superconducting qubits and surface codes is considered a promising candidate for FTQC architectures. In this paper, we propose an FTQC architecture, which we call Q3DE, that enhances the tolerance to multi-bit burst errors (MBBEs) by cosmic rays with moderate changes and overhead. There are three core components in Q3DE: in-situ anomaly DEtection, dynamic code DEformation, and optimized error DEcoding. In this architecture, MBBEs are detected only from syndrome values for error correction. The effect of MBBEs is immediately mitigated by dynamically increasing the encoding level of logical qubits and re-estimating probable recovery operation with the rollback of the decoding process. We investigate the performance and overhead of the Q3DE architecture with quantum-error simulators and demonstrate that Q3DE effectively reduces the period of MBBEs by 1000 times and halves the size of their region. Therefore, Q3DE significantly relaxes the requirement of qubit density and qubit chip size to realize FTQC. Our scheme is versatile for mitigating MBBEs, i.e., temporal variations of error properties, on a wide range of physical devices and FTQC architectures since it relies only on the standard features of topological stabilizer codes.",2024-12-31T08:04:58Z,2024-12-31T08:04:58Z,http://arxiv.org/abs/2501.00331v1,http://arxiv.org/pdf/2501.00331v1,"quant-ph, cs.AR"
Fast Data Aware Neural Architecture Search via Supernet Accelerated   Evaluation,"Emil Njor, Colby Banbury, Xenofon Fafoutis","Tiny machine learning (TinyML) promises to revolutionize fields such as healthcare, environmental monitoring, and industrial maintenance by running machine learning models on low-power embedded systems. However, the complex optimizations required for successful TinyML deployment continue to impede its widespread adoption. A promising route to simplifying TinyML is through automatic machine learning (AutoML), which can distill elaborate optimization workflows into accessible key decisions. Notably, Hardware Aware Neural Architecture Searches - where a computer searches for an optimal TinyML model based on predictive performance and hardware metrics - have gained significant traction, producing some of today's most widely used TinyML models. Nevertheless, limiting optimization solely to neural network architectures can prove insufficient. Because TinyML systems must operate under extremely tight resource constraints, the choice of input data configuration, such as resolution or sampling rate, also profoundly impacts overall system efficiency. Achieving truly optimal TinyML systems thus requires jointly tuning both input data and model architecture. Despite its importance, this ""Data Aware Neural Architecture Search"" remains underexplored. To address this gap, we propose a new state-of-the-art Data Aware Neural Architecture Search technique and demonstrate its effectiveness on the novel TinyML ``Wake Vision'' dataset. Our experiments show that across varying time and hardware constraints, Data Aware Neural Architecture Search consistently discovers superior TinyML systems compared to purely architecture-focused methods, underscoring the critical role of data-aware optimization in advancing TinyML.",2025-02-18T09:51:03Z,2025-02-18T09:51:03Z,http://arxiv.org/abs/2502.12690v1,http://arxiv.org/pdf/2502.12690v1,"cs.NE, cs.AI, cs.CV, cs.LG, 68T10, 68T20, 68T45"
A Mobile Transient Internet Architecture,"Henry N Jerez, Joud Khoury, Chaouki Abdallah","This paper describes a new architecture for transient mobile networks destined to merge existing and future network architectures, communication implementations and protocol operations by introducing a new paradigm to data delivery and identification. The main goal of our research is to enable seamless end-to-end communication between mobile and stationary devices across multiple networks and through multiple communication environments. The architecture establishes a set of infrastructure components and protocols that set the ground for a Persistent Identification Network (PIN). The basis for the operation of PIN is an identification space consisting of unique location independent identifiers similar to the ones implemented in the Handle system. Persistent Identifiers are used to identify and locate Digital Entities which can include devices, services, users and even traffic. The architecture establishes a primary connection independent logical structure that can operate over conventional networks or more advanced peer-to-peer aggregation networks. Communication is based on routing pools and novel protocols for routing data across several abstraction levels of the network, regardless of the end-points' current association and state...",2006-10-17T04:47:06Z,2006-10-17T04:47:06Z,http://arxiv.org/abs/cs/0610100v1,http://arxiv.org/pdf/cs/0610100v1,"cs.NI, cs.IT, math.IT, C.2.1"
Interface groups and financial transfer architectures,"Jan A. Bergstra, Alban Ponse","Analytic execution architectures have been proposed by the same authors as a means to conceptualize the cooperation between heterogeneous collectives of components such as programs, threads, states and services. Interface groups have been proposed as a means to formalize interface information concerning analytic execution architectures. These concepts are adapted to organization architectures with a focus on financial transfers. Interface groups (and monoids) now provide a technique to combine interface elements into interfaces with the flexibility to distinguish between directions of flow dependent on entity naming.   The main principle exploiting interface groups is that when composing a closed system of a collection of interacting components, the sum of their interfaces must vanish in the interface group modulo reflection. This certainly matters for financial transfer interfaces.   As an example of this, we specify an interface group and within it some specific interfaces concerning the financial transfer architecture for a part of our local academic organization.   Financial transfer interface groups arise as a special case of more general service architecture interfaces.",2007-07-11T14:55:26Z,2007-07-11T14:55:26Z,http://arxiv.org/abs/0707.1639v1,http://arxiv.org/pdf/0707.1639v1,"cs.SE, D.2.2"
An $Θ(\sqrt{n})$-depth Quantum Adder on a 2D NTC Quantum Computer   Architecture,"Byung-Soo Choi, Rodney Van Meter","In this work, we propose an adder for the 2D NTC architecture, designed to match the architectural constraints of many quantum computing technologies. The chosen architecture allows the layout of logical qubits in two dimensions and the concurrent execution of one- and two-qubit gates with nearest-neighbor interaction only. The proposed adder works in three phases. In the first phase, the first column generates the summation output and the other columns do the carry-lookahead operations. In the second phase, these intermediate values are propagated from column to column, preparing for computation of the final carry for each register position. In the last phase, each column, except the first one, generates the summation output using this column-level carry. The depth and the number of qubits of the proposed adder are $\Theta(\sqrt{n})$ and O(n), respectively. The proposed adder executes faster than the adders designed for the 1D NTC architecture when the length of the input registers $n$ is larger than 58.",2010-08-30T15:05:05Z,2010-08-30T15:05:05Z,http://arxiv.org/abs/1008.5093v1,http://arxiv.org/pdf/1008.5093v1,"quant-ph, cs.AR"
The Public Safety Broadband Network: A Novel Architecture with Mobile   Base Stations,"Xu Chen, Dongning Guo, John Grosspietsch","A nationwide interoperable public safety broadband network is being planned by the United States government. The network will be based on long term evolution (LTE) standards and use recently designated spectrum in the 700 MHz band. The public safety network has different objectives and traffic patterns than commercial wireless networks. In particular, the public safety network puts more emphasis on coverage, reliability and latency in the worst case scenario. Moreover, the routine public safety traffic is relatively light, whereas when a major incident occurs, the traffic demand at the incident scene can be significantly heavier than that in a commercial network. Hence it is prohibitively costly to build the public safety network using conventional cellular network architecture consisting of an infrastructure of stationary base transceiver stations. A novel architecture is proposed in this paper for the public safety broadband network. The architecture deploys stationary base stations sparsely to serve light routine traffic and dispatches mobile base stations to incident scenes along with public safety personnel to support heavy traffic. The analysis shows that the proposed architecture can potentially offer more than 75% reduction in terms of the total number of base stations needed.",2013-03-18T21:52:45Z,2013-03-18T21:52:45Z,http://arxiv.org/abs/1303.4439v1,http://arxiv.org/pdf/1303.4439v1,"cs.NI, cs.IT, math.IT"
A Parallel Compressive Imaging Architecture for One-Shot Acquisition,"Tomas Björklund, Enrico Magli","A limitation of many compressive imaging architectures lies in the sequential nature of the sensing process, which leads to long sensing times. In this paper we present a novel architecture that uses fewer detectors than the number of reconstructed pixels and is able to acquire the image in a single acquisition. This paves the way for the development of video architectures that acquire several frames per second. We specifically address the diffraction problem, showing that deconvolution normally used to recover diffraction blur can be replaced by convolution of the sensing matrix, and how measurements of a 0/1 physical sensing matrix can be converted to -1/1 compressive sensing matrix without any extra acquisitions. Simulations of our architecture show that the image quality is comparable to that of a classic Compressive Imaging camera, whereas the proposed architecture avoids long acquisition times due to sequential sensing. This one-shot procedure also allows to employ a fixed sensing matrix instead of a complex device such as a Digital Micro Mirror array or Spatial Light Modulator. It also enables imaging at bandwidths where these are not efficient.",2013-11-04T11:07:47Z,2013-11-04T11:07:47Z,http://arxiv.org/abs/1311.0646v1,http://arxiv.org/pdf/1311.0646v1,"cs.CV, astro-ph.IM"
Quantum perceptron over a field and neural network architecture   selection in a quantum computer,"Adenilton J. da Silva, Teresa B. Ludermir, Wilson R. de Oliveira","In this work, we propose a quantum neural network named quantum perceptron over a field (QPF). Quantum computers are not yet a reality and the models and algorithms proposed in this work cannot be simulated in actual (or classical) computers. QPF is a direct generalization of a classical perceptron and solves some drawbacks found in previous models of quantum perceptrons. We also present a learning algorithm named Superposition based Architecture Learning algorithm (SAL) that optimizes the neural network weights and architectures. SAL searches for the best architecture in a finite set of neural network architectures with linear time over the number of patterns in the training set. SAL is the first learning algorithm to determine neural network architectures in polynomial time. This speedup is obtained by the use of quantum parallelism and a non-linear quantum operator.",2016-01-29T13:53:42Z,2016-01-29T13:53:42Z,http://arxiv.org/abs/1602.00709v1,http://arxiv.org/pdf/1602.00709v1,"quant-ph, cs.NE"
Feed-forward approximations to dynamic recurrent network architectures,Dylan Richard Muir,"Recurrent neural network architectures can have useful computational properties, with complex temporal dynamics and input-sensitive attractor states. However, evaluation of recurrent dynamic architectures requires solution of systems of differential equations, and the number of evaluations required to determine their response to a given input can vary with the input, or can be indeterminate altogether in the case of oscillations or instability. In feed-forward networks, by contrast, only a single pass through the network is needed to determine the response to a given input. Modern machine-learning systems are designed to operate efficiently on feed-forward architectures. We hypothesised that two-layer feedforward architectures with simple, deterministic dynamics could approximate the responses of single-layer recurrent network architectures. By identifying the fixed-point responses of a given recurrent network, we trained two-layer networks to directly approximate the fixed-point response to a given input. These feed-forward networks then embodied useful computations, including competitive interactions, information transformations and noise rejection. Our approach was able to find useful approximations to recurrent networks, which can then be evaluated in linear and deterministic time complexity.",2017-04-21T17:28:47Z,2017-09-15T09:53:08Z,http://arxiv.org/abs/1704.06645v2,http://arxiv.org/pdf/1704.06645v2,"cs.NE, q-bio.NC"
Pipelined Parallel FFT Architecture,"Tanaji U. Kamble, B. G. Patil, Rakhee S. Bhojakar","In this paper, an optimized efficient VLSI architecture of a pipeline Fast Fourier transform (FFT) processor capable of producing the reverse output order sequence is presented. Paper presents Radix-2 multipath delay architecture for FFT calculation. The implementation of FFT in hardware is very critical because for calculation of FFT number of butterfly operations i.e. number of multipliers requires due to which hardware gets increased means indirectly cost of hardware is automatically gets increased. Also multiplier operations are slow that's why it limits the speed of operation of architecture. The optimized VLSI implementation of FFT algorithm is presented in this paper. Here architecture is pipelined to optimize it and to increase the speed of operation. Also to increase the speed of operation 2 levels parallel processing is used.",2017-07-06T09:11:24Z,2017-07-06T09:11:24Z,http://arxiv.org/abs/1707.01697v1,http://arxiv.org/pdf/1707.01697v1,"cs.AR, math.OC"
Diversity Analysis of Millimeter-Wave Massive MIMO Systems,"Dian-Wu Yue, Shuai Xu, Ha H. Nguyen","This paper is concerned with asymptotic diversity analysis for millimeter-wave (mmWave) massive MIMO systems. First, for a single-user mmWave system employing distributed antenna subarray architecture in which the transmitter and receiver consist of Kt and Kr subarrays, respectively, a diversity gain theorem is established when the numbers of antennas at subarrays go to infinity. Specifically, assuming that all subchannels have the same number of propagation paths L, the theorem states that by employing such a distributed antenna-subarray architecture, a diversity gain of KrKtL-Ns+1 can be achieved, where Ns is the number of data streams. This result means that compared to the co-located antenna architecture, using the distributed antenna-subarray architecture can scale up the diversity gain or multiplexing gain proportionally to KrKt. The diversity gain analysis is then extended to the multiuser scenario as well as the scenario with conventional partially-connected RF structure in the literature. Simulation results obtained with the hybrid analog/digital processing corroborate the analysis results and show that the distributed subarray architecture indeed yields significantly better diversity performance than the co-located antenna architectures.",2018-01-01T03:40:57Z,2018-01-01T03:40:57Z,http://arxiv.org/abs/1801.00387v1,http://arxiv.org/pdf/1801.00387v1,"cs.IT, math.IT"
Generative Adversarial Network Architectures For Image Synthesis Using   Capsule Networks,"Yash Upadhyay, Paul Schrater","In this paper, we propose Generative Adversarial Network (GAN) architectures that use Capsule Networks for image-synthesis. Based on the principal of positional-equivariance of features, Capsule Network's ability to encode spatial relationships between the features of the image helps it become a more powerful critic in comparison to Convolutional Neural Networks (CNNs) used in current architectures for image synthesis. Our proposed GAN architectures learn the data manifold much faster and therefore, synthesize visually accurate images in significantly lesser number of training samples and training epochs in comparison to GANs and its variants that use CNNs. Apart from analyzing the quantitative results corresponding the images generated by different architectures, we also explore the reasons for the lower coverage and diversity explored by the GAN architectures that use CNN critics.",2018-06-11T03:54:24Z,2018-11-20T18:33:11Z,http://arxiv.org/abs/1806.03796v4,http://arxiv.org/pdf/1806.03796v4,"cs.CV, cs.LG, cs.NE, stat.ML"
NAS-Bench-101: Towards Reproducible Neural Architecture Search,"Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, Frank Hutter","Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.",2019-02-25T21:56:54Z,2019-05-14T05:33:47Z,http://arxiv.org/abs/1902.09635v2,http://arxiv.org/pdf/1902.09635v2,"cs.LG, stat.ML"
Inductive Transfer for Neural Architecture Optimization,"Martin Wistuba, Tejaswini Pedapati","The recent advent of automated neural network architecture search led to several methods that outperform state-of-the-art human-designed architectures. However, these approaches are computationally expensive, in extreme cases consuming GPU years. We propose two novel methods which aim to expedite this optimization problem by transferring knowledge acquired from previous tasks to new ones. First, we propose a novel neural architecture selection method which employs this knowledge to identify strong and weak characteristics of neural architectures across datasets. Thus, these characteristics do not need to be rediscovered in every search, a strong weakness of current state-of-the-art searches. Second, we propose a method for learning curve extrapolation to determine if a training process can be terminated early. In contrast to existing work, we propose to learn from learning curves of architectures trained on other datasets to improve the prediction accuracy for novel datasets. On five different image classification benchmarks, we empirically demonstrate that both of our orthogonal contributions independently lead to an acceleration, without any significant loss in accuracy.",2019-03-08T16:27:32Z,2019-03-08T16:27:32Z,http://arxiv.org/abs/1903.03536v1,http://arxiv.org/pdf/1903.03536v1,"cs.LG, cs.AI, cs.NE, stat.ML"
Weight Agnostic Neural Networks,"Adam Gaier, David Ha","Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/",2019-06-11T02:40:11Z,2019-09-05T07:54:07Z,http://arxiv.org/abs/1906.04358v2,http://arxiv.org/pdf/1906.04358v2,"cs.LG, cs.NE, stat.ML"
Scalable Neural Architecture Search for 3D Medical Image Segmentation,"Sungwoong Kim, Ildoo Kim, Sungbin Lim, Woonhyuk Baek, Chiheon Kim, Hyungjoo Cho, Boogeon Yoon, Taesup Kim","In this paper, a neural architecture search (NAS) framework is proposed for 3D medical image segmentation, to automatically optimize a neural architecture from a large design space. Our NAS framework searches the structure of each layer including neural connectivities and operation types in both of the encoder and decoder. Since optimizing over a large discrete architecture space is difficult due to high-resolution 3D medical images, a novel stochastic sampling algorithm based on a continuous relaxation is also proposed for scalable gradient based optimization. On the 3D medical image segmentation tasks with a benchmark dataset, an automatically designed architecture by the proposed NAS framework outperforms the human-designed 3D U-Net, and moreover this optimized architecture is well suited to be transferred for different tasks.",2019-06-13T22:39:42Z,2019-06-13T22:39:42Z,http://arxiv.org/abs/1906.05956v1,http://arxiv.org/pdf/1906.05956v1,"cs.LG, cs.CV, eess.IV, stat.ML"
Automatic Configuration of Deep Neural Networks with EGO,"Bas van Stein, Hao Wang, Thomas Bäck","Designing the architecture for an artificial neural network is a cumbersome task because of the numerous parameters to configure, including activation functions, layer types, and hyper-parameters. With the large number of parameters for most networks nowadays, it is intractable to find a good configuration for a given task by hand. In this paper an Efficient Global Optimization (EGO) algorithm is adapted to automatically optimize and configure convolutional neural network architectures. A configurable neural network architecture based solely on convolutional layers is proposed for the optimization. Without using any knowledge on the target problem and not using any data augmentation techniques, it is shown that on several image classification tasks this approach is able to find competitive network architectures in terms of prediction accuracy, compared to the best hand-crafted ones in literature. In addition, a very small training budget (200 evaluations and 10 epochs in training) is spent on each optimized architectures in contrast to the usual long training time of hand-crafted networks. Moreover, instead of the standard sequential evaluation in EGO, several candidate architectures are proposed and evaluated in parallel, which saves the execution overheads significantly and leads to an efficient automation for deep neural network design.",2018-10-10T09:06:15Z,2018-10-10T09:06:15Z,http://arxiv.org/abs/1810.05526v1,http://arxiv.org/pdf/1810.05526v1,"cs.LG, cs.NE, stat.ML"
Graph HyperNetworks for Neural Architecture Search,"Chris Zhang, Mengye Ren, Raquel Urtasun","Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast -- they can search nearly 10 times faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.",2018-10-12T22:21:05Z,2020-12-18T18:01:04Z,http://arxiv.org/abs/1810.05749v3,http://arxiv.org/pdf/1810.05749v3,"cs.LG, cs.CV, stat.ML"
Hybrid spatiotemporal architectures for universal linear optics,"Daiqin Su, Ish Dhand, Lukas G. Helt, Zachary Vernon, Kamil Bradler","We present two hybrid linear-optical architectures that simultaneously exploit spatial and temporal degrees of freedom of light to effect arbitrary discrete unitary transformations. Our architectures combine the benefits of spatial implementations of linear optics, namely low loss and parallel operation, with those of temporal implementations, namely modest resource requirements and access to transformations of potentially unbounded size. We arrive at our architectures by devising and employing decompositions of large discrete unitary transformations into smaller ones, decompositions we expect to have broad utility beyond spatio-temporal linear optics. We show that hybrid architectures promise important advantages over both spatial-only and temporal-only architectures.",2018-12-19T13:42:19Z,2019-06-03T19:16:01Z,http://arxiv.org/abs/1812.07939v2,http://arxiv.org/pdf/1812.07939v2,"quant-ph, math-ph, math.MP, physics.optics"
Understanding Neural Architecture Search Techniques,"George Adam, Jonathan Lorraine","Automatic methods for generating state-of-the-art neural network architectures without human experts have generated significant attention recently. This is because of the potential to remove human experts from the design loop which can reduce costs and decrease time to model deployment. Neural architecture search (NAS) techniques have improved significantly in their computational efficiency since the original NAS was proposed. This reduction in computation is enabled via weight sharing such as in Efficient Neural Architecture Search (ENAS). However, recently a body of work confirms our discovery that ENAS does not do significantly better than random search with weight sharing, contradicting the initial claims of the authors. We provide an explanation for this phenomenon by investigating the interpretability of the ENAS controller's hidden state. We find models sampled from identical controller hidden states have no correlation with various graph similarity metrics, so no notion of structural similarity is learned. This failure mode implies the RNN controller does not condition on past architecture choices. Lastly, we propose a solution to this failure mode by forcing the controller's hidden state to encode pasts decisions by training it with a memory buffer of previously sampled architectures. Doing this improves hidden state interpretability by increasing the correlation between controller hidden states and graph similarity metrics.",2019-03-31T15:48:49Z,2019-11-21T17:49:27Z,http://arxiv.org/abs/1904.00438v2,http://arxiv.org/pdf/1904.00438v2,"cs.LG, stat.ML"
Decentralized On-line Task Reallocation on Parallel Computing   Architectures with Safety-Critical Applications,"Thanakorn Khamvilai, Louis Sutter, Eric Feron, Philippe Baufreton, Francois Neumann","This work presents a decentralized allocation algorithm of safety-critical application on parallel computing architectures, where individual Computational Units can be affected by faults.   The described method consists in representing the architecture by an abstract graph where each node represents a Computational Unit. Applications are also represented by the graph of Computational Units they require for execution. The problem is then to decide how to allocate Computational Units to applications to guarantee execution of the safety-critical application. The problem is formulated as an optimization problem, with the form of an Integer Linear Program. A state-of-the-art solver is then used to solve the problem.   Decentralizing the allocation process is achieved through redundancy of the allocator executed on the architecture. No centralized element decides on the allocation of the entire architecture, thus improving the reliability of the system.   Experimental reproduction of a multi-core architecture is also presented. It is used to demonstrate the capabilities of the proposed allocation process to maintain the operation of a physical system in a decentralized way while individual component fails.",2019-10-03T18:55:50Z,2019-11-19T06:43:46Z,http://arxiv.org/abs/1910.06313v2,http://arxiv.org/pdf/1910.06313v2,"cs.DC, cs.SY, eess.SY, math.OC"
ResNetX: a more disordered and deeper network architecture,"Wenfeng Feng, Xin Zhang, Guangpeng Zhao","Designing efficient network structures has always been the core content of neural network research.   ResNet and its variants have proved to be efficient in architecture.   However, how to theoretically character the influence of network structure on performance is still vague.   With the help of techniques in complex networks, We here provide a natural yet efficient extension to ResNet by folding its backbone chain.   Our architecture has two structural features when being mapped to directed acyclic graphs:   First is a higher degree of the disorder compared with ResNet, which let ResNetX explore a larger number of feature maps with different sizes of receptive fields.   Second is a larger proportion of shorter paths compared to ResNet, which improves the direct flow of information through the entire network.   Our architecture exposes a new dimension, namely ""fold depth"", in addition to existing dimensions of depth, width, and cardinality.   Our architecture is a natural extension to ResNet, and can be integrated with existing state-of-the-art methods with little effort. Image classification results on CIFAR-10 and CIFAR-100 benchmarks suggested that our new network architecture performs better than ResNet.",2019-12-18T16:05:45Z,2019-12-18T16:05:45Z,http://arxiv.org/abs/1912.12165v1,http://arxiv.org/pdf/1912.12165v1,"cs.CV, eess.IV"
A Survey on Neural Architecture Search,"Martin Wistuba, Ambrish Rawat, Tejaswini Pedapati","The growing interest in both the automation of machine learning and deep learning has inevitably led to the development of a wide variety of automated methods for neural architecture search. The choice of the network architecture has proven to be critical, and many advances in deep learning spring from its immediate improvements. However, deep learning techniques are computationally intensive and their application requires a high level of domain knowledge. Therefore, even partial automation of this process helps to make deep learning more accessible to both researchers and practitioners. With this survey, we provide a formalism which unifies and categorizes the landscape of existing methods along with a detailed analysis that compares and contrasts the different approaches. We achieve this via a comprehensive discussion of the commonly adopted architecture search spaces and architecture optimization algorithms based on principles of reinforcement learning and evolutionary algorithms along with approaches that incorporate surrogate and one-shot models. Additionally, we address the new research directions which include constrained and multi-objective architecture search as well as automated data augmentation, optimizer and activation function search.",2019-05-04T00:08:49Z,2019-06-18T09:32:21Z,http://arxiv.org/abs/1905.01392v2,http://arxiv.org/pdf/1905.01392v2,"cs.LG, cs.CV, cs.NE, stat.ML"
Efficient Neural Architecture Search on Low-Dimensional Data for OCT   Image Segmentation,"Nils Gessert, Alexander Schlaefer","Typically, deep learning architectures are handcrafted for their respective learning problem. As an alternative, neural architecture search (NAS) has been proposed where the architecture's structure is learned in an additional optimization step. For the medical imaging domain, this approach is very promising as there are diverse problems and imaging modalities that require architecture design. However, NAS is very time-consuming and medical learning problems often involve high-dimensional data with high computational requirements. We propose an efficient approach for NAS in the context of medical, image-based deep learning problems by searching for architectures on low-dimensional data which are subsequently transferred to high-dimensional data. For OCT-based layer segmentation, we demonstrate that a search on 1D data reduces search time by 87.5% compared to a search on 2D data while the final 2D models achieve similar performance.",2019-05-07T14:01:56Z,2019-05-07T14:01:56Z,http://arxiv.org/abs/1905.02590v1,http://arxiv.org/pdf/1905.02590v1,"eess.IV, cs.CV"
An Explicitly Relational Neural Network Architecture,"Murray Shanahan, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David Barrett, Marta Garnelo","With a view to bridging the gap between deep learning and symbolic AI, we present a novel end-to-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions.",2019-05-24T16:03:06Z,2020-06-23T13:36:06Z,http://arxiv.org/abs/1905.10307v4,http://arxiv.org/pdf/1905.10307v4,"cs.LG, stat.ML"
Weightless neural network parameters and architecture selection in a   quantum computer,"Adenilton J. da Silva, Wilson R. de Oliveira, Teresa B. Ludermir","Training artificial neural networks requires a tedious empirical evaluation to determine a suitable neural network architecture. To avoid this empirical process several techniques have been proposed to automatise the architecture selection process. In this paper, we propose a method to perform parameter and architecture selection for a quantum weightless neural network (qWNN). The architecture selection is performed through the learning procedure of a qWNN with a learning algorithm that uses the principle of quantum superposition and a non-linear quantum operator. The main advantage of the proposed method is that it performs a global search in the space of qWNN architecture and parameters rather than a local search.",2016-01-12T12:53:04Z,2016-01-12T12:53:04Z,http://arxiv.org/abs/1601.03277v1,http://arxiv.org/pdf/1601.03277v1,"quant-ph, cs.NE"
Generalization Tower Network: A Novel Deep Neural Network Architecture   for Multi-Task Learning,"Yuhang Song, Main Xu, Songyang Zhang, Liangyu Huo","Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by incorporating deep neural networks in learning representations from the input to RL. However, the conventional deep neural network architecture is limited in learning representations for multi-task RL (MT-RL), as multiple tasks can refer to different kinds of representations. In this paper, we thus propose a novel deep neural network architecture, namely generalization tower network (GTN), which can achieve MT-RL within a single learned model. Specifically, the architecture of GTN is composed of both horizontal and vertical streams. In our GTN architecture, horizontal streams are used to learn representation shared in similar tasks. In contrast, the vertical streams are introduced to be more suitable for handling diverse tasks, which encodes hierarchical shared knowledge of these tasks. The effectiveness of the introduced vertical stream is validated by experimental results. Experimental results further verify that our GTN architecture is able to advance the state-of-the-art MT-RL, via being tested on 51 Atari games.",2017-10-27T09:11:26Z,2018-01-02T01:06:10Z,http://arxiv.org/abs/1710.10036v3,http://arxiv.org/pdf/1710.10036v3,"cs.LG, stat.ML"
Finding Competitive Network Architectures Within a Day Using UCT,Martin Wistuba,"The design of neural network architectures for a new data set is a laborious task which requires human deep learning expertise. In order to make deep learning available for a broader audience, automated methods for finding a neural network architecture are vital. Recently proposed methods can already achieve human expert level performances. However, these methods have run times of months or even years of GPU computing time, ignoring hardware constraints as faced by many researchers and companies. We propose the use of Monte Carlo planning in combination with two different UCT (upper confidence bound applied to trees) derivations to search for network architectures. We adapt the UCT algorithm to the needs of network architecture search by proposing two ways of sharing information between different branches of the search tree. In an empirical study we are able to demonstrate that this method is able to find competitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day. Extending the search time to five GPU days, we are able to outperform human architectures and our competitors which consider the same types of layers.",2017-12-20T11:24:50Z,2018-07-23T13:57:50Z,http://arxiv.org/abs/1712.07420v2,http://arxiv.org/pdf/1712.07420v2,"cs.LG, cs.CV, stat.ML"
IoT Architectural Framework: Connection and Integration Framework for   IoT Systems,"Onoriode Uviase, Gerald Kotonya","The proliferation of the Internet of Things (IoT) has since seen a growing interest in architectural design and adaptive frameworks to promote the connection between heterogeneous IoT devices and IoT systems. The most widely favoured software architecture in IoT is the Service Oriented Architecture (SOA), which aims to provide a loosely coupled systems to leverage the use and reuse of IoT services at the middle-ware layer, to minimise system integration problems. However, despite the flexibility offered by SOA, the challenges of integrating, scaling and ensuring resilience in IoT systems persist. One of the key causes of poor integration in IoT systems is the lack of an intelligent, connection-aware framework to support interaction in IoT systems. This paper reviews existing architectural frameworks for integrating IoT devices and identifies the key areas that require further research improvements. The paper concludes by proposing a possible solution based on microservice. The proposed IoT integration framework benefits from an intelligent API layer that employs an external service assembler, service auditor, service monitor and service router component to coordinate service publishing, subscription, decoupling and service combination within the architecture.",2018-02-06T04:09:36Z,2018-02-06T04:09:36Z,http://arxiv.org/abs/1803.04780v1,http://arxiv.org/pdf/1803.04780v1,"cs.DC, cs.CY, cs.SE, Internet of Things"
Quantum Entanglement in Deep Learning Architectures,"Yoav Levine, Or Sharir, Nadav Cohen, Amnon Shashua","Modern deep learning has enabled unprecedented achievements in various domains. Nonetheless, employment of machine learning for wave function representations is focused on more traditional architectures such as restricted Boltzmann machines (RBMs) and fully-connected neural networks. In this letter, we establish that contemporary deep learning architectures, in the form of deep convolutional and recurrent networks, can efficiently represent highly entangled quantum systems. By constructing Tensor Network equivalents of these architectures, we identify an inherent reuse of information in the network operation as a key trait which distinguishes them from standard Tensor Network based representations, and which enhances their entanglement capacity. Our results show that such architectures can support volume-law entanglement scaling, polynomially more efficiently than presently employed RBMs. Thus, beyond a quantification of the entanglement capacity of leading deep learning architectures, our analysis formally motivates a shift of trending neural-network based wave function representations closer to the state-of-the-art in machine learning.",2018-03-26T18:30:29Z,2019-02-13T18:25:01Z,http://arxiv.org/abs/1803.09780v3,http://arxiv.org/pdf/1803.09780v3,"quant-ph, cs.LG"
Neural Architecture Generator Optimization,"Binxin Ru, Pedro Esperanca, Fabio Carlucci","Neural Architecture Search (NAS) was first proposed to achieve state-of-the-art performance through the discovery of new architecture patterns, without human intervention. An over-reliance on expert knowledge in the search space design has however led to increased performance (local optima) without significant architectural breakthroughs, thus preventing truly novel solutions from being reached. In this work we 1) are the first to investigate casting NAS as a problem of finding the optimal network generator and 2) we propose a new, hierarchical and graph-based search space capable of representing an extremely large variety of network types, yet only requiring few continuous hyper-parameters. This greatly reduces the dimensionality of the problem, enabling the effective use of Bayesian Optimisation as a search strategy. At the same time, we expand the range of valid architectures, motivating a multi-objective learning approach. We demonstrate the effectiveness of this strategy on six benchmark datasets and show that our search space generates extremely lightweight yet highly competitive models.",2020-04-03T06:38:07Z,2021-01-02T16:28:40Z,http://arxiv.org/abs/2004.01395v3,http://arxiv.org/pdf/2004.01395v3,"cs.LG, cs.NE, stat.ML"
A chatbot architecture for promoting youth resilience,"Chester Holt-Quick, Jim Warren, Karolina Stasiak, Ruth Williams, Grant Christie, Sarah Hetrick, Sarah Hopkins, Tania Cargo, Sally Merry","E-health technologies have the potential to provide scalable and accessible interventions for youth mental health. As part of a developing an ecosystem of e-screening and e-therapy tools for New Zealand young people, a dialog agent, Headstrong, has been designed to promote resilience with methods grounded in cognitive behavioral therapy and positive psychology. This paper describes the architecture underlying the chatbot. The architecture supports a range of over 20 activities delivered in a 4-week program by relatable personas. The architecture provides a visual authoring interface to its content management system. In addition to supporting the original adolescent resilience chatbot, the architecture has been reused to create a 3-week 'stress-detox' intervention for undergraduates, and subsequently for a chatbot to support young people with the impacts of the COVID-19 pandemic, with all three systems having been used in field trials. The Headstrong architecture illustrates the feasibility of creating a domain-focused authoring environment in the context of e-therapy that supports non-technical expert input and rapid deployment.",2020-05-15T04:36:06Z,2020-05-15T04:36:06Z,http://arxiv.org/abs/2005.07355v1,http://arxiv.org/pdf/2005.07355v1,"cs.CL, I.2.7; J.3; H.5.2"
A Tree Architecture of LSTM Networks for Sequential Regression with   Missing Data,"S. Onur Sahin, Suleyman S. Kozat","We investigate regression for variable length sequential data containing missing samples and introduce a novel tree architecture based on the Long Short-Term Memory (LSTM) networks. In our architecture, we employ a variable number of LSTM networks, which use only the existing inputs in the sequence, in a tree-like architecture without any statistical assumptions or imputations on the missing data, unlike all the previous approaches. In particular, we incorporate the missingness information by selecting a subset of these LSTM networks based on ""presence-pattern"" of a certain number of previous inputs. From the mixture of experts perspective, we train different LSTM networks as our experts for various missingness patterns and then combine their outputs to generate the final prediction. We also provide the computational complexity analysis of the proposed architecture, which is in the same order of the complexity of the conventional LSTM architectures for the sequence length. Our method can be readily extended to similar structures such as GRUs, RNNs as remarked in the paper. In the experiments, we achieve significant performance improvements with respect to the state-of-the-art methods for the well-known financial and real life datasets.",2020-05-22T18:57:47Z,2020-05-22T18:57:47Z,http://arxiv.org/abs/2005.11353v1,http://arxiv.org/pdf/2005.11353v1,"cs.LG, stat.ML"
DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient   CNN for Medical Images Segmentation,"Ange Lou, Shuyue Guan, Murray Loew","Recently, deep learning has become much more popular in computer vision area. The Convolution Neural Network (CNN) has brought a breakthrough in images segmentation areas, especially, for medical images. In this regard, U-Net is the predominant approach to medical image segmentation task. The U-Net not only performs well in segmenting multimodal medical images generally, but also in some tough cases of them. However, we found that the classical U-Net architecture has limitation in several aspects. Therefore, we applied modifications: 1) designed efficient CNN architecture to replace encoder and decoder, 2) applied residual module to replace skip connection between encoder and decoder to improve based on the-state-of-the-art U-Net model. Following these modifications, we designed a novel architecture--DC-UNet, as a potential successor to the U-Net architecture. We created a new effective CNN architecture and build the DC-UNet based on this CNN. We have evaluated our model on three datasets with tough cases and have obtained a relative improvement in performance of 2.90%, 1.49% and 11.42% respectively compared with classical U-Net. In addition, we used the Tanimoto similarity to replace the Jaccard similarity for gray-to-gray image comparisons.",2020-05-31T02:23:55Z,2020-05-31T02:23:55Z,http://arxiv.org/abs/2006.00414v1,http://arxiv.org/pdf/2006.00414v1,"eess.IV, cs.CV"
NADS: Neural Architecture Distribution Search for Uncertainty Awareness,"Randy Ardywibowo, Shahin Boluki, Xinyu Gong, Zhangyang Wang, Xiaoning Qian","Machine learning (ML) systems often encounter Out-of-Distribution (OoD) errors when dealing with testing data coming from a distribution different from training data. It becomes important for ML systems in critical applications to accurately quantify its predictive uncertainty and screen out these anomalous inputs. However, existing OoD detection approaches are prone to errors and even sometimes assign higher likelihoods to OoD samples. Unlike standard learning tasks, there is currently no well established guiding principle for designing OoD detection architectures that can accurately quantify uncertainty. To address these problems, we first seek to identify guiding principles for designing uncertainty-aware architectures, by proposing Neural Architecture Distribution Search (NADS). NADS searches for a distribution of architectures that perform well on a given task, allowing us to identify common building blocks among all uncertainty-aware architectures. With this formulation, we are able to optimize a stochastic OoD detection objective and construct an ensemble of models to perform OoD detection. We perform multiple OoD detection experiments and observe that our NADS performs favorably, with up to 57% improvement in accuracy compared to state-of-the-art methods among 15 different testing configurations.",2020-06-11T17:39:07Z,2020-06-11T17:39:07Z,http://arxiv.org/abs/2006.06646v1,http://arxiv.org/pdf/2006.06646v1,"cs.LG, stat.ML"
Neural Ensemble Search for Uncertainty Estimation and Dataset Shift,"Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris Holmes, Frank Hutter, Yee Whye Teh","Ensembles of neural networks achieve superior performance compared to stand-alone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. \emph{Deep ensembles}, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a \emph{fixed} architecture. Instead, we propose two methods for automatically constructing ensembles with \emph{varying} architectures, which implicitly trade-off individual architectures' strengths against the ensemble's diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: \url{https://github.com/automl/nes}",2020-06-15T17:38:15Z,2022-02-21T19:31:23Z,http://arxiv.org/abs/2006.08573v3,http://arxiv.org/pdf/2006.08573v3,"cs.LG, stat.ML"
Understanding Deep Architectures with Reasoning Layer,"Xinshi Chen, Yufei Zhang, Christoph Reisinger, Le Song","Recently, there has been a surge of interest in combining deep learning models with reasoning in order to handle more sophisticated learning tasks. In many cases, a reasoning task can be solved by an iterative algorithm. This algorithm is often unrolled, and used as a specialized layer in the deep architecture, which can be trained end-to-end with other neural components. Although such hybrid deep architectures have led to many empirical successes, the theoretical foundation of such architectures, especially the interplay between algorithm layers and other neural layers, remains largely unexplored. In this paper, we take an initial step towards an understanding of such hybrid deep architectures by showing that properties of the algorithm layers, such as convergence, stability, and sensitivity, are intimately related to the approximation and generalization abilities of the end-to-end model. Furthermore, our analysis matches closely our experimental observations under various conditions, suggesting that our theory can provide useful guidelines for designing deep architectures with reasoning layers.",2020-06-24T00:26:35Z,2020-10-29T22:00:00Z,http://arxiv.org/abs/2006.13401v2,http://arxiv.org/pdf/2006.13401v2,"cs.LG, stat.ML"
NAS-VAD: Neural Architecture Search for Voice Activity Detection,"Daniel Rho, Jinhyeok Park, Jong Hwan Ko","Various neural network-based approaches have been proposed for more robust and accurate voice activity detection (VAD). Manual design of such neural architectures is an error-prone and time-consuming process, which prompted the development of neural architecture search (NAS) that automatically design and optimize network architectures. While NAS has been successfully applied to improve performance in a variety of tasks, it has not yet been exploited in the VAD domain. In this paper, we present the first work that utilizes NAS approaches on the VAD task. To effectively search architectures for the VAD task, we propose a modified macro structure and a new search space with a much broader range of operations that includes attention operations. The results show that the network structures found by the propose NAS framework outperform previous manually designed state-of-the-art VAD models in various noise-added and real-world-recorded datasets. We also show that the architectures searched on a particular dataset achieve improved generalization performance on unseen audio datasets. Our code and models are available at https://github.com/daniel03c1/NAS_VAD.",2022-01-22T12:06:41Z,2022-03-29T08:16:03Z,http://arxiv.org/abs/2201.09032v2,http://arxiv.org/pdf/2201.09032v2,"cs.SD, cs.LG, eess.AS"
SO-MRS: a multi-robot system architecture based on the SOA paradigm and   ontology,"Kamil Skarzynski, Marcin Stepniak, Waldemar Bartyna, Stanislaw Ambroszkiewicz","A generic architecture for a class of distributed robotic systems is presented. The architecture supports openness and heterogeneity, i.e. heterogeneous components may be joined and removed from the systems without affecting its basic functionality. The architecture is based on the paradigm of Service Oriented Architecture (SOA), and a generic representation (ontology) of the environment. A device (e.g. robot) is seen as a collection of its capabilities exposed as services. Generic protocols for publishing, discovering, arranging services are proposed for creating composite services that can accomplish complex tasks in an automatic way. Also generic protocols for execution of composite services are proposed along with simple protocols for monitoring the executions, and for recovery from failures. A software platform built on a multi-robot system (according to the proposed architecture) is a multi-agent system.",2017-09-11T09:07:25Z,2017-09-14T10:04:03Z,http://arxiv.org/abs/1709.03300v2,http://arxiv.org/pdf/1709.03300v2,"cs.RO, cs.MA, cs.SE, 68T40, I.2.9"
Reversible Architectures for Arbitrarily Deep Residual Neural Networks,"Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, Elliot Holtham","Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.",2017-09-12T05:41:13Z,2017-11-18T22:10:57Z,http://arxiv.org/abs/1709.03698v2,http://arxiv.org/pdf/1709.03698v2,"cs.CV, stat.ML"
Efficient Neural Architecture Search via Parameter Sharing,"Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean","We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.",2018-02-09T14:14:37Z,2018-02-12T03:34:00Z,http://arxiv.org/abs/1802.03268v2,http://arxiv.org/pdf/1802.03268v2,"cs.LG, cs.CL, cs.CV, cs.NE, stat.ML"
Cross-layer Optimization for High Speed Adders: A Pareto Driven Machine   Learning Approach,"Yuzhe Ma, Subhendu Roy, Jin Miao, Jiamin Chen, Bei Yu","In spite of maturity to the modern electronic design automation (EDA) tools, optimized designs at architectural stage may become sub-optimal after going through physical design flow. Adder design has been such a long studied fundamental problem in VLSI industry yet designers cannot achieve optimal solutions by running EDA tools on the set of available prefix adder architectures. In this paper, we enhance a state-of-the-art prefix adder synthesis algorithm to obtain a much wider solution space in architectural domain. On top of that, a machine learning-based design space exploration methodology is applied to predict the Pareto frontier of the adders in physical domain, which is infeasible by exhaustively running EDA tools for innumerable architectural solutions. Considering the high cost of obtaining the true values for learning, an active learning algorithm is utilized to select the representative data during learning process, which uses less labeled data while achieving better quality of Pareto frontier. Experimental results demonstrate that our framework can achieve Pareto frontier of high quality over a wide design space, bridging the gap between architectural and physical designs.",2018-07-18T16:23:26Z,2018-10-16T04:37:28Z,http://arxiv.org/abs/1807.07023v2,http://arxiv.org/pdf/1807.07023v2,"cs.AR, cs.LG, B.8.2; I.2.1"
City-GAN: Learning architectural styles using a custom Conditional GAN   architecture,"Maximilian Bachl, Daniel C. Ferreira","Generative Adversarial Networks (GANs) are a well-known technique that is trained on samples (e.g. pictures of fruits) and which after training is able to generate realistic new samples. Conditional GANs (CGANs) additionally provide label information for subclasses (e.g. apple, orange, pear) which enables the GAN to learn more easily and increase the quality of its output samples. We use GANs to learn architectural features of major cities and to generate images of buildings which do not exist. We show that currently available GAN and CGAN architectures are unsuited for this task and propose a custom architecture and demonstrate that our architecture has superior performance for this task and verify its capabilities with extensive experiments.",2019-07-03T11:43:36Z,2020-05-26T20:19:30Z,http://arxiv.org/abs/1907.05280v2,http://arxiv.org/pdf/1907.05280v2,"cs.CV, cs.GR, cs.LG, eess.IV, stat.ML"
Experimental Exploration of Compact Convolutional Neural Network   Architectures for Non-temporal Real-time Fire Detection,"Ganesh Samarth C. A., Neelanjan Bhowmik, Toby P. Breckon","In this work we explore different Convolutional Neural Network (CNN) architectures and their variants for non-temporal binary fire detection and localization in video or still imagery. We consider the performance of experimentally defined, reduced complexity deep CNN architectures for this task and evaluate the effects of different optimization and normalization techniques applied to different CNN architectures (spanning the Inception, ResNet and EfficientNet architectural concepts). Contrary to contemporary trends in the field, our work illustrates a maximum overall accuracy of 0.96 for full frame binary fire detection and 0.94 for superpixel localization using an experimentally defined reduced CNN architecture based on the concept of InceptionV4. We notably achieve a lower false positive rate of 0.06 compared to prior work in the field presenting an efficient, robust and real-time solution for fire region detection.",2019-11-20T16:27:10Z,2019-11-20T16:27:10Z,http://arxiv.org/abs/1911.09010v1,http://arxiv.org/pdf/1911.09010v1,"cs.CV, cs.LG, eess.IV"
Optimal modular architectures for universal linear optics,"Shreya P. Kumar, Ish Dhand","We present modular and optimal architectures for implementing arbitrary discrete unitary transformations on light. These architectures are based on systematically combining smaller M-mode linear optical interferometers together to implement a larger N-mode transformation. Thus this work enables the implementation of large linear optical transformations using smaller modules that act on the spatial or the internal degrees of freedom of light such as polarization, time or orbital angular momentum. The architectures lead to a rectangular gate structure, which is optimal in the sense that realizing arbitrary transformations on these architectures needs a minimal number of optical elements and minimal circuit depth. Moreover, the rectangular structure ensures that each the different optical modes incur balanced optical losses, so the architectures promise substantially enhanced process fidelities as compared to existing schemes.",2020-01-07T13:12:53Z,2020-01-07T13:12:53Z,http://arxiv.org/abs/2001.02012v1,http://arxiv.org/pdf/2001.02012v1,"quant-ph, physics.optics"
Transfer Learning by Cascaded Network to identify and classify lung   nodules for cancer detection,"Shah B. Shrey, Lukman Hakim, Muthusubash Kavitha, Hae Won Kim, Takio Kurita",Lung cancer is one of the most deadly diseases in the world. Detecting such tumors at an early stage can be a tedious task. Existing deep learning architecture for lung nodule identification used complex architecture with large number of parameters. This study developed a cascaded architecture which can accurately segment and classify the benign or malignant lung nodules on computed tomography (CT) images. The main contribution of this study is to introduce a segmentation network where the first stage trained on a public data set can help to recognize the images which included a nodule from any data set by means of transfer learning. And the segmentation of a nodule improves the second stage to classify the nodules into benign and malignant. The proposed architecture outperformed the conventional methods with an area under curve value of 95.67\%. The experimental results showed that the classification accuracy of 97.96\% of our proposed architecture outperformed other simple and complex architectures in classifying lung nodules for lung cancer detection.,2020-09-24T10:35:46Z,2020-09-24T10:35:46Z,http://arxiv.org/abs/2009.11587v1,http://arxiv.org/pdf/2009.11587v1,"eess.IV, cs.LG"
Latency-Controlled Neural Architecture Search for Streaming Speech   Recognition,"Liqiang He, Shulin Feng, Dan Su, Dong Yu","Neural architecture search (NAS) has attracted much attention and has been explored for automatic speech recognition (ASR). In this work, we focus on streaming ASR scenarios and propose the latency-controlled NAS for acoustic modeling. First, based on the vanilla neural architecture, normal cells are altered to causal cells to control the total latency of the architecture. Second, a revised operation space with a smaller receptive field is proposed to generate the final architecture with low latency. Extensive experiments show that: 1) Based on the proposed neural architecture, the neural networks with a medium latency of 550ms (millisecond) and a low latency of 190ms can be learned in the vanilla and revised operation space respectively. 2) For the low latency setting, the evaluation network can achieve more than 19\% (average on the four test sets) relative improvements compared with the hybrid CLDNN baseline, on a 10k-hour large-scale dataset.",2021-05-08T09:01:15Z,2021-09-14T03:49:27Z,http://arxiv.org/abs/2105.03643v3,http://arxiv.org/pdf/2105.03643v3,"eess.AS, cs.SD"
Nested and Balanced Entity Recognition using Multi-Task Learning,"Andreas Waldis, Luca Mazzola","Entity Recognition (ER) within a text is a fundamental exercise in Natural Language Processing, enabling further depending tasks such as Knowledge Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of single words or of a consecutive sequence of terms, constituting the basic building blocks for communication. Mainstream ER approaches are mainly limited to flat structures, concentrating on the outermost entities while ignoring the inner ones. This paper introduces a partly-layered network architecture that deals with the complexity of overlapping and nested cases. The proposed architecture consists of two parts: (1) a shared Sequence Layer and (2) a stacked component with multiple Tagging Layers. The adoption of such an architecture has the advantage of preventing overfit to a specific word-length, thus maintaining performance for longer entities despite their lower frequency. To verify the proposed architecture's effectiveness, we train and evaluate this architecture to recognise two kinds of entities - Concepts (CR) and Named Entities (NER). Our approach achieves state-of-the-art NER performances, while it outperforms previous CR approaches. Considering these promising results, we see the possibility to evolve the architecture for other cases such as the extraction of events or the detection of argumentative components.",2021-06-11T07:52:32Z,2021-06-11T07:52:32Z,http://arxiv.org/abs/2106.06216v1,http://arxiv.org/pdf/2106.06216v1,"cs.CL, cs.IR, cs.LG, cs.NE, I.2.7; I.1.3; F.4.1"
Bilateral Control of Teleoperators with Closed Architecture and   Time-Varying Delay,"Hanlei Wang, Yipeng Li, Tiantian Jiang","This paper investigates bilateral control of teleoperators with closed architecture and subjected to arbitrary bounded time-varying delay. A prominent challenge for bilateral control of such teleoperators lies in the closed architecture, especially in the context not involving interaction force/torque measurement. This yields the long-standing situation that most bilateral control rigorously developed in the literature is hard to be justified as applied to teleoperators with closed architecture. With a new class of dynamic feedback, we propose kinematic and adaptive dynamic controllers for teleoperators with closed architecture, and we show that the proposed kinematic and dynamic controllers are robust with respect to arbitrary bounded time-varying delay. In addition, by exploiting the input-output properties of an inverted form of the dynamics of robot manipulators with closed architecture, we remove the assumption of uniform exponential stability of a linear time-varying system due to the adaptation to the gains of the inner controller in demonstrating stability of the presented adaptive dynamic control. The application of the proposed approach is illustrated by the experimental results using a Phantom Omni and a UR10 robot.",2021-06-23T15:35:23Z,2021-06-23T15:35:23Z,http://arxiv.org/abs/2106.12470v1,http://arxiv.org/pdf/2106.12470v1,"eess.SY, cs.SY"
Connections between Numerical Algorithms for PDEs and Neural Networks,"Tobias Alt, Karl Schrader, Matthias Augustin, Pascal Peter, Joachim Weickert","We investigate numerous structural connections between numerical algorithms for partial differential equations (PDEs) and neural architectures. Our goal is to transfer the rich set of mathematical foundations from the world of PDEs to neural networks. Besides structural insights we provide concrete examples and experimental evaluations of the resulting architectures. Using the example of generalised nonlinear diffusion in 1D, we consider explicit schemes, acceleration strategies thereof, implicit schemes, and multigrid approaches. We connect these concepts to residual networks, recurrent neural networks, and U-net architectures. Our findings inspire a symmetric residual network design with provable stability guarantees and justify the effectiveness of skip connections in neural networks from a numerical perspective. Moreover, we present U-net architectures that implement multigrid techniques for learning efficient solutions of partial differential equation models, and motivate uncommon design choices such as trainable nonmonotone activation functions. Experimental evaluations show that the proposed architectures save half of the trainable parameters and can thus outperform standard ones with the same model complexity. Our considerations serve as a basis for explaining the success of popular neural architectures and provide a blueprint for developing new mathematically well-founded neural building blocks.",2021-07-30T16:42:45Z,2022-03-21T09:31:03Z,http://arxiv.org/abs/2107.14742v2,http://arxiv.org/pdf/2107.14742v2,"math.NA, cs.LG, cs.NA"
Assured Neural Network Architectures for Control and Identification of   Nonlinear Systems,"James Ferlez, Yasser Shoukry","In this paper, we consider the problem of automatically designing a Rectified Linear Unit (ReLU) Neural Network (NN) architecture (number of layers and number of neurons per layer) with the assurance that it is sufficiently parametrized to control a nonlinear system; i.e. control the system to satisfy a given formal specification. This is unlike current techniques, which provide no assurances on the resultant architecture. Moreover, our approach requires only limited knowledge of the underlying nonlinear system and specification. We assume only that the specification can be satisfied by a Lipschitz-continuous controller with a known bound on its Lipschitz constant; the specific controller need not be known. From this assumption, we bound the number of affine functions needed to construct a Continuous Piecewise Affine (CPWA) function that can approximate any Lipschitz-continuous controller that satisfies the specification. Then we connect this CPWA to a NN architecture using the authors' recent results on the Two-Level Lattice (TLL) NN architecture; the TLL architecture was shown to be parameterized by the number of affine functions present in the CPWA function it realizes.",2021-09-21T16:19:59Z,2021-09-21T16:19:59Z,http://arxiv.org/abs/2109.10298v1,http://arxiv.org/pdf/2109.10298v1,"cs.LG, cs.SY, eess.SY, math.OC, stat.ML"
Polynomial-Spline Neural Networks with Exact Integrals,"Jonas A. Actor, Andy Huang, Nathaniel Trask","Using neural networks to solve variational problems, and other scientific machine learning tasks, has been limited by a lack of consistency and an inability to exactly integrate expressions involving neural network architectures. We address these limitations by formulating a novel neural network architecture that combines a polynomial mixture-of-experts model with free knot B1-spline basis functions. Effectively, our architecture performs piecewise polynomial approximation on each cell of a trainable partition of unity. Our architecture exhibits both $h$- and $p$- refinement for regression problems at the convergence rates expected from approximation theory, allowing for consistency in solving variational problems. Moreover, this architecture, its moments, and its partial derivatives can all be integrated exactly, obviating a reliance on sampling or quadrature and enabling error-free computation of variational forms. We demonstrate the success of our network on a range of regression and variational problems that illustrate the consistency and exact integrability of our network architecture.",2021-10-26T22:12:37Z,2021-10-26T22:12:37Z,http://arxiv.org/abs/2110.14055v1,http://arxiv.org/pdf/2110.14055v1,"cs.LG, cs.AI, cs.NA, math.NA"
Trading with the Momentum Transformer: An Intelligent and Interpretable   Architecture,"Kieran Wood, Sven Giegerich, Stephen Roberts, Stefan Zohren","We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.",2021-12-16T00:04:12Z,2022-11-22T22:53:47Z,http://arxiv.org/abs/2112.08534v3,http://arxiv.org/pdf/2112.08534v3,"cs.LG, q-fin.TR, stat.ML"
CGN: A Capacity-Guaranteed Network Architecture for Future Ultra-Dense   Wireless Systems,"Chaowen Deng, Lu Yang, Hao Wu, Dmitry Zaporozhets, Miaomiao Dong, Bo Bai","The sixth generation (6G) era is envisioned to be a fully intelligent and autonomous era, with physical and digital lifestyles merged together. Future wireless network architectures should provide a solid support for such new lifestyles. A key problem thus arises that what kind of network architectures are suitable for 6G. In this paper, we propose a capacity-guaranteed network (CGN) architecture, which provides high capacity for wireless devices densely distributed everywhere, and ensures a superior scalability with low signaling overhead and computation complexity simultaneously. Our theorem proves that the essence of a CGN architecture is to decompose the whole network into non-overlapping clusters with equal cluster sum capacity. Simulation results reveal that in terms of the minimum cluster sum capacity, the proposed CGN can achieve at least 30% performance gain compared with existing base station clustering (BS-clustering) architectures. In addition, our theorem is sufficiently general and can be applied for networks with different distributions of BSs and users.",2022-03-04T00:49:44Z,2022-03-04T00:49:44Z,http://arxiv.org/abs/2203.02078v1,http://arxiv.org/pdf/2203.02078v1,"cs.IT, math.IT"
Neural Architecture Search: Insights from 1000 Papers,"Colin White, Mahmoud Safari, Rhea Sukthanker, Binxin Ru, Thomas Elsken, Arber Zela, Debadeepta Dey, Frank Hutter","In the past decade, advances in deep learning have resulted in breakthroughs in a variety of areas, including computer vision, natural language understanding, speech recognition, and reinforcement learning. Specialized, high-performing neural architectures are crucial to the success of deep learning in these areas. Neural architecture search (NAS), the process of automating the design of neural architectures for a given task, is an inevitable next step in automating machine learning and has already outpaced the best human-designed architectures on many tasks. In the past few years, research in NAS has been progressing rapidly, with over 1000 papers released since 2020 (Deng and Lindauer, 2021). In this survey, we provide an organized and comprehensive guide to neural architecture search. We give a taxonomy of search spaces, algorithms, and speedup techniques, and we discuss resources such as benchmarks, best practices, other surveys, and open-source libraries.",2023-01-20T18:47:24Z,2023-01-25T08:01:55Z,http://arxiv.org/abs/2301.08727v2,http://arxiv.org/pdf/2301.08727v2,"cs.LG, cs.AI, stat.ML"
Cognitive Architecture for Decision-Making Based on Brain Principles   Programming (in Russian),"Anton Kolonin, Andrey Kurpatov, Artem Molchanov, Gennadiy Averyanov","We describe a cognitive architecture intended to solve a wide range of problems based on the five identified principles of brain activity, with their implementation in three subsystems: logical-probabilistic inference, probabilistic formal concepts, and functional systems theory. Building an architecture involves the implementation of a task-driven approach that allows defining the target functions of applied applications as tasks formulated in terms of the operating environment corresponding to the task, expressed in the applied ontology. We provide a basic ontology for a number of practical applications as well as for the subject domain ontologies based upon it, describe the proposed architecture, and give possible examples of the execution of these applications in this architecture.",2023-02-18T16:34:05Z,2023-02-18T16:34:05Z,http://arxiv.org/abs/2302.09377v1,http://arxiv.org/pdf/2302.09377v1,"cs.AI, cs.HC, cs.SY, eess.SY"
QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster   Inference on Mobile Platforms,"Guillaume Berger, Manik Dhingra, Antoine Mercier, Yashesh Savani, Sunny Panchal, Fatih Porikli","In this work, we present QuickSRNet, an efficient super-resolution architecture for real-time applications on mobile platforms. Super-resolution clarifies, sharpens, and upscales an image to higher resolution. Applications such as gaming and video playback along with the ever-improving display capabilities of TVs, smartphones, and VR headsets are driving the need for efficient upscaling solutions. While existing deep learning-based super-resolution approaches achieve impressive results in terms of visual quality, enabling real-time DL-based super-resolution on mobile devices with compute, thermal, and power constraints is challenging. To address these challenges, we propose QuickSRNet, a simple yet effective architecture that provides better accuracy-to-latency trade-offs than existing neural architectures for single-image super resolution. We present training tricks to speed up existing residual-based super-resolution architectures while maintaining robustness to quantization. Our proposed architecture produces 1080p outputs via 2x upscaling in 2.2 ms on a modern smartphone, making it ideal for high-fps real-time applications.",2023-03-08T02:19:54Z,2023-05-14T19:03:51Z,http://arxiv.org/abs/2303.04336v2,http://arxiv.org/pdf/2303.04336v2,"eess.IV, cs.CV, cs.LG"
Mapping quantum algorithms to multi-core quantum computing architectures,"Anabel Ovide, Santiago Rodrigo, Medina Bandic, Hans Van Someren, Sebastian Feld, Sergi Abadal, Eduard Alarcon, Carmen G. Almudever","Current monolithic quantum computer architectures have limited scalability. One promising approach for scaling them up is to use a modular or multi-core architecture, in which different quantum processors (cores) are connected via quantum and classical links. This new architectural design poses new challenges such as the expensive inter-core communication. To reduce these movements when executing a quantum algorithm, an efficient mapping technique is required. In this paper, a detailed critical discussion of the quantum circuit mapping problem for multi-core quantum computing architectures is provided. In addition, we further explore the performance of a mapping method, which is formulated as a partitioning over time graph problem, by performing an architectural scalability analysis.",2023-03-28T16:46:59Z,2023-03-28T16:46:59Z,http://arxiv.org/abs/2303.16125v1,http://arxiv.org/pdf/2303.16125v1,"quant-ph, cs.ET"
Automatic Gradient Descent: Deep Learning without Hyperparameters,"Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, Yisong Yue","The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at ImageNet scale. A PyTorch implementation is available at https://github.com/jxbz/agd and also in Appendix B. Overall, the paper supplies a rigorous theoretical foundation for a next-generation of architecture-dependent optimisers that work automatically and without hyperparameters.",2023-04-11T12:45:52Z,2023-04-11T12:45:52Z,http://arxiv.org/abs/2304.05187v1,http://arxiv.org/pdf/2304.05187v1,"cs.LG, cs.AI, cs.NA, cs.NE, math.NA, stat.ML"
OneCAD: One Classifier for All image Datasets using multimodal learning,"Shakti N. Wadekar, Eugenio Culurciello","Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are widely used Deep Neural Networks (DNNs) for classification task. These model architectures are dependent on the number of classes in the dataset it was trained on. Any change in number of classes leads to change (partial or full) in the model's architecture. This work addresses the question: Is it possible to create a number-of-class-agnostic model architecture?. This allows model's architecture to be independent of the dataset it is trained on. This work highlights the issues with the current architectures (ViTs and CNNs). Also, proposes a training and inference framework OneCAD (One Classifier for All image Datasets) to achieve close-to number-of-class-agnostic transformer model. To best of our knowledge this is the first work to use Mask-Image-Modeling (MIM) with multimodal learning for classification task to create a DNN model architecture agnostic to the number of classes. Preliminary results are shown on natural and medical image datasets. Datasets: MNIST, CIFAR10, CIFAR100 and COVIDx. Code will soon be publicly available on github.",2023-05-11T22:40:47Z,2023-05-11T22:40:47Z,http://arxiv.org/abs/2305.07167v1,http://arxiv.org/pdf/2305.07167v1,"cs.CV, cs.CL, cs.LG, eess.IV"
Implementation of a new authorization system from monolithic solution to   microservice architecture,"David Ahmad Abboud, Damien Jacob","Monolithic applications used to be considered the standard for software development. However, due to the rapid evolution of technology and the increasing demand for scalability and flexibility, these applications have become increasingly inadequate for contemporary environment. In response to these challenges, developers have begun to adopt a microservice (MS) architecture, which offers a modular approach to software creation. However, this transition requires to rethink the enablement system to meet the new requirements. Two MS architectures can be deployed: a centralized or decentralized architecture. Based on the requirements of the application's users, a centralized authorization management architecture was chosen. The purpose of this study is to explain the migration from a Role- Based Access Control (RBAC) authorization system to a centralized microservice authorization architecture. The migration is carried out in two stages: 1) Creation of an authorization microservice 2) Abandonment of RBAC",2023-07-12T08:18:23Z,2023-07-12T08:18:23Z,http://arxiv.org/abs/2307.05994v1,http://arxiv.org/pdf/2307.05994v1,"eess.SY, cs.SY"
S4Sleep: Elucidating the design space of deep-learning-based sleep stage   classification models,"Tiezhi Wang, Nils Strodthoff","Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components and achieve statistically significant performance improvements compared to state-of-the-art approaches on the extensive Sleep Heart Health Study dataset. We anticipate that the architectural insights gained from this study along with the refined methodology for architecture search demonstrated herein will not only prove valuable for future research in sleep staging but also hold relevance for other time series annotation tasks.",2023-10-10T15:42:14Z,2025-01-23T08:00:34Z,http://arxiv.org/abs/2310.06715v3,http://arxiv.org/pdf/2310.06715v3,"cs.LG, eess.SP, stat.ML"
When Representations Align: Universality in Representation Learning   Dynamics,"Loek van Rossem, Andrew M. Saxe","Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits phenomena similar to the ""rich"" and ""lazy"" regime. While many network behaviors depend quantitatively on architecture, our findings point to certain behaviors that are widely conserved once models are sufficiently flexible.",2024-02-14T12:48:17Z,2024-07-05T09:42:01Z,http://arxiv.org/abs/2402.09142v2,http://arxiv.org/pdf/2402.09142v2,"cs.LG, q-bio.NC"
Linear Depth QFT over IBM Heavy-hex Architecture,"Xiangyu Gao, Yuwei Jin, Minghao Guo, Henry Chen, Eddy Z. Zhang","Compiling a given quantum algorithm into a target hardware architecture is a challenging optimization problem. The compiler must take into consideration the coupling graph of physical qubits and the gate operation dependencies. The existing noise in hardware architectures requires the compilation to use as few running cycles as possible. Existing approaches include using SAT solver or heuristics to complete the mapping but these may cause the issue of either long compilation time (e.g., timeout after hours) or suboptimal compilation results in terms of running cycles (e.g., exponentially increasing number of total cycles).   In this paper, we propose an efficient mapping approach for Quantum Fourier Transformation (QFT) circuits over the existing IBM heavy-hex architecture. Such proposal first of all turns the architecture into a structure consisting of a straight line with dangling qubits, and then do the mapping over this generated structure recursively. The calculation shows that there is a linear depth upper bound for the time complexity of these structures and for a special case where there is 1 dangling qubit in every 5 qubits, the time complexity is 5N+O(1). All these results are better than state of the art methods.",2024-02-15T04:41:31Z,2024-02-15T04:41:31Z,http://arxiv.org/abs/2402.09705v1,http://arxiv.org/pdf/2402.09705v1,"quant-ph, cs.AR"
Integrating Physics Inspired Features with Graph Convolution,Rameswar Sahu,"With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress. In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of Graph Neural Network (GNN) architectures. Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard GNNs. These capsules are a group of neurons with vector activations. The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule. Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis. We have studied the usefulness of our architecture with the LorentzNet architecture for quark-gluon tagging. Here, we have replaced the decoding block of LorentzNet with a capsulated decoding block and have called the resulting architecture CapsLorentzNet. Our new architecture can enhance the performance of LorentzNet by 20 \% for the quark-gluon tagging task.",2024-03-18T14:31:09Z,2025-01-24T19:33:06Z,http://arxiv.org/abs/2403.11826v2,http://arxiv.org/pdf/2403.11826v2,"hep-ph, cs.LG, hep-ex"
Systematic construction of continuous-time neural networks for linear   dynamical systems,"Chinmay Datar, Adwait Datar, Felix Dietrich, Wil Schilders","Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space. In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems. We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties. We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing conventional neural architectures with vertical hidden layers may not be favorable. We also provide an upper bound on the numerical errors of our neural networks. Finally, we demonstrate the high accuracy of our constructed networks on three numerical examples.",2024-03-24T16:16:41Z,2024-03-24T16:16:41Z,http://arxiv.org/abs/2403.16215v1,http://arxiv.org/pdf/2403.16215v1,"cs.LG, cs.NA, math.DS, math.NA, 93B17, 65L70, 68T07, I.2.m; G.1.3; G.1.7"
Off-the-Shelf Neural Network Architectures for Forex Time Series   Prediction come at a Cost,"Theodoros Zafeiriou, Dimitris Kalles","Our study focuses on comparing the performance and resource requirements between different Long Short-Term Memory (LSTM) neural network architectures and an ANN specialized architecture for forex market prediction. We analyze the execution time of the models as well as the resources consumed, such as memory and computational power. Our aim is to demonstrate that the specialized architecture not only achieves better results in forex market prediction but also executes using fewer resources and in a shorter time frame compared to LSTM architectures. This comparative analysis will provide significant insights into the suitability of these two types of architectures for time series prediction in the forex market environment.",2024-05-17T10:20:14Z,2024-05-17T10:20:14Z,http://arxiv.org/abs/2405.10679v1,http://arxiv.org/pdf/2405.10679v1,"cs.LG, cs.AI, cs.CE, q-fin.MF"
Approximately Equivariant Neural Processes,"Matthew Ashman, Cristiana Diaconu, Adrian Weller, Wessel Bruinsma, Richard E. Turner","Equivariant deep learning architectures exploit symmetries in learning problems to improve the sample efficiency of neural-network-based models and their ability to generalise. However, when modelling real-world data, learning problems are often not exactly equivariant, but only approximately. For example, when estimating the global temperature field from weather station observations, local topographical features like mountains break translation equivariance. In these scenarios, it is desirable to construct architectures that can flexibly depart from exact equivariance in a data-driven way. Current approaches to achieving this cannot usually be applied out-of-the-box to any architecture and symmetry group. In this paper, we develop a general approach to achieving this using existing equivariant architectures. Our approach is agnostic to both the choice of symmetry group and model architecture, making it widely applicable. We consider the use of approximately equivariant architectures in neural processes (NPs), a popular family of meta-learning models. We demonstrate the effectiveness of our approach on a number of synthetic and real-world regression experiments, showing that approximately equivariant NP models can outperform both their non-equivariant and strictly equivariant counterparts.",2024-06-19T12:17:14Z,2024-11-09T12:08:21Z,http://arxiv.org/abs/2406.13488v2,http://arxiv.org/pdf/2406.13488v2,"stat.ML, cs.LG"
Roles of LLMs in the Overall Mental Architecture,Ron Sun,"To better understand existing LLMs, we may examine the human mental (cognitive/psychological) architecture, and its components and structures. Based on psychological, philosophical, and cognitive science literatures, it is argued that, within the human mental architecture, existing LLMs correspond well with implicit mental processes (intuition, instinct, and so on). However, beyond such implicit processes, explicit processes (with better symbolic capabilities) are also present within the human mental architecture, judging from psychological, philosophical, and cognitive science literatures. Various theoretical and empirical issues and questions in this regard are explored. Furthermore, it is argued that existing dual-process computational cognitive architectures (models of the human cognitive/psychological architecture) provide usable frameworks for fundamentally enhancing LLMs by introducing dual processes (both implicit and explicit) and, in the meantime, can also be enhanced by LLMs. The results are synergistic combinations (in several different senses simultaneously).",2024-10-26T01:13:44Z,2024-10-26T01:13:44Z,http://arxiv.org/abs/2410.20037v1,http://arxiv.org/pdf/2410.20037v1,"q-bio.NC, cs.AI, cs.CY"
Developing Convolutional Neural Networks using a Novel Lamarckian   Co-Evolutionary Algorithm,"Zaniar Sharifi, Khabat Soltanian, Ali Amiri","Neural Architecture Search (NAS) methods autonomously discover high-accuracy neural network architectures, outperforming manually crafted ones. However, The NAS methods require high computational costs due to the high dimension search space and the need to train multiple candidate solutions. This paper introduces LCoDeepNEAT, an instantiation of Lamarckian genetic algorithms, which extends the foundational principles of the CoDeepNEAT framework. LCoDeepNEAT co-evolves CNN architectures and their respective final layer weights. The evaluation process of LCoDeepNEAT entails a single epoch of SGD, followed by the transference of the acquired final layer weights to the genetic representation of the network. In addition, it expedites the process of evolving by imposing restrictions on the architecture search space, specifically targeting architectures comprising just two fully connected layers for classification. Our method yields a notable improvement in the classification accuracy of candidate solutions throughout the evolutionary process, ranging from 2% to 5.6%. This outcome underscores the efficacy and effectiveness of integrating gradient information and evolving the last layer of candidate solutions within LCoDeepNEAT. LCoDeepNEAT is assessed across six standard image classification datasets and benchmarked against eight leading NAS methods. Results demonstrate LCoDeepNEAT's ability to swiftly discover competitive CNN architectures with fewer parameters, conserving computational resources, and achieving superior classification accuracy compared to other approaches.",2024-10-29T19:26:23Z,2024-10-29T19:26:23Z,http://arxiv.org/abs/2410.22487v1,http://arxiv.org/pdf/2410.22487v1,"cs.NE, I.5.1"
Graph-Based Orchestration of Service-Oriented Model-Based Control   Systems,"Julius Beerwerth, Hazem Ibrahim, Bianca Atodiresei, Lorenz Dörschel, Bassam Alrifaee","This paper presents a novel graph-based method for adapting control system architectures at runtime. We use a service-oriented architecture as a basis for its formulation. In our method, adaptation is achieved by selecting the most suitable elements, such as filters and controllers, for a control system architecture to improve control systems objective based on a predefined cost function. Traditional configuration methods, such as state machines, lack flexibility and depend on a predefined control system architecture during runtime. Our graph-based method allows for dynamic changes in the control system architecture, as well as a change in its objective depending on the given system state. Our approach uses a weighted, directed graph to model the control system elements and their interaction. In a case-study with a three-tank system, we show that by using our graph-based method for architecture adaptation, the control system is more flexible, has lower computation time, and higher accuracy than traditional configuration methods.",2024-11-27T16:44:13Z,2024-11-27T16:44:13Z,http://arxiv.org/abs/2411.18503v1,http://arxiv.org/pdf/2411.18503v1,"eess.SY, cs.SY"
Empirical evaluation of normalizing flows in Markov Chain Monte Carlo,"David Nabergoj, Erik Štrumbelj","Recent advances in MCMC use normalizing flows to precondition target distributions and enable jumps to distant regions. However, there is currently no systematic comparison of different normalizing flow architectures for MCMC. As such, many works choose simple flow architectures that are readily available and do not consider other models. Guidelines for choosing an appropriate architecture would reduce analysis time for practitioners and motivate researchers to take the recommended models as foundations to be improved. We provide the first such guideline by extensively evaluating many normalizing flow architectures on various flow-based MCMC methods and target distributions. When the target density gradient is available, we show that flow-based MCMC outperforms classic MCMC for suitable NF architecture choices with minor hyperparameter tuning. When the gradient is unavailable, flow-based MCMC wins with off-the-shelf architectures. We find contractive residual flows to be the best general-purpose models with relatively low sensitivity to hyperparameter choice. We also provide various insights into normalizing flow behavior within MCMC when varying their hyperparameters, properties of target distributions, and the overall computational budget.",2024-12-22T18:52:59Z,2024-12-22T18:52:59Z,http://arxiv.org/abs/2412.17136v1,http://arxiv.org/pdf/2412.17136v1,"cs.LG, stat.CO, stat.ML, 62-08, G.3; I.5.1; I.6.4"
Scaling Up ESM2 Architectures for Long Protein Sequences Analysis: Long   and Quantized Approaches,"Gabriel Bianchin de Oliveira, Helio Pedrini, Zanoni Dias","Various approaches utilizing Transformer architectures have achieved state-of-the-art results in Natural Language Processing (NLP). Based on this success, numerous architectures have been proposed for other types of data, such as in biology, particularly for protein sequences. Notably among these are the ESM2 architectures, pre-trained on billions of proteins, which form the basis of various state-of-the-art approaches in the field. However, the ESM2 architectures have a limitation regarding input size, restricting it to 1,022 amino acids, which necessitates the use of preprocessing techniques to handle sequences longer than this limit. In this paper, we present the long and quantized versions of the ESM2 architectures, doubling the input size limit to 2,048 amino acids.",2025-01-13T23:26:29Z,2025-01-13T23:26:29Z,http://arxiv.org/abs/2501.07747v1,http://arxiv.org/pdf/2501.07747v1,"cs.LG, q-bio.QM"
AHAR: Adaptive CNN for Energy-efficient Human Activity Recognition in   Low-power Edge Devices,"Nafiul Rashid, Berken Utku Demirel, Mohammad Abdullah Al Faruque","Human Activity Recognition (HAR) is one of the key applications of health monitoring that requires continuous use of wearable devices to track daily activities. This paper proposes an Adaptive CNN for energy-efficient HAR (AHAR) suitable for low-power edge devices. Unlike traditional early exit architecture that makes the exit decision based on classification confidence, AHAR proposes a novel adaptive architecture that uses an output block predictor to select a portion of the baseline architecture to use during the inference phase. Experimental results show that traditional early exit architectures suffer from performance loss whereas our adaptive architecture provides similar or better performance as the baseline one while being energy-efficient. We validate our methodology in classifying locomotion activities from two datasets- Opportunity and w-HAR. Compared to the fog/cloud computing approaches for the Opportunity dataset, our baseline and adaptive architecture shows a comparable weighted F1 score of 91.79%, and 91.57%, respectively. For the w-HAR dataset, our baseline and adaptive architecture outperforms the state-of-the-art works with a weighted F1 score of 97.55%, and 97.64%, respectively. Evaluation on real hardware shows that our baseline architecture is significantly energy-efficient (422.38x less) and memory-efficient (14.29x less) compared to the works on the Opportunity dataset. For the w-HAR dataset, our baseline architecture requires 2.04x less energy and 2.18x less memory compared to the state-of-the-art work. Moreover, experimental results show that our adaptive architecture is 12.32% (Opportunity) and 11.14% (w-HAR) energy-efficient than our baseline while providing similar (Opportunity) or better (w-HAR) performance with no significant memory overhead.",2021-02-03T04:49:16Z,2022-01-03T06:10:09Z,http://arxiv.org/abs/2102.01875v3,http://arxiv.org/pdf/2102.01875v3,"cs.LG, eess.SP"
On the Quantum Performance Evaluation of Two Distributed Quantum   Architectures,"Gayane Vardoyan, Matthew Skrzypczyk, Stephanie Wehner","Distributed quantum applications impose requirements on the quality of the quantum states that they consume. When analyzing architecture implementations of quantum hardware, characterizing this quality forms an important factor in understanding their performance. Fundamental characteristics of quantum hardware lead to inherent tradeoffs between the quality of states and traditional performance metrics such as throughput. Furthermore, any real-world implementation of quantum hardware exhibits time-dependent noise that degrades the quality of quantum states over time. Here, we study the performance of two possible architectures for interfacing a quantum processor with a quantum network. The first corresponds to the current experimental state of the art in which the same device functions both as a processor and a network device. The second corresponds to a future architecture that separates these two functions over two distinct devices. We model these architectures as Markov chains and compare their quality of executing quantum operations and producing entangled quantum states as functions of their memory lifetimes, as well as the time that it takes to perform various operations within each architecture. As an illustrative example, we apply our analysis to architectures based on Nitrogen-Vacancy centers in diamond, where we find that for present-day device parameters one architecture is more suited to computation-heavy applications, and the other for network-heavy ones. Besides the detailed study of these architectures, a novel contribution of our work are several formulas that connect an understanding of waiting time distributions to the decay of quantum quality over time for the most common noise models employed in quantum technologies. This provides a valuable new tool for performance evaluation experts, and its applications extend beyond the two architectures studied in this work.",2021-07-26T14:49:47Z,2021-12-23T11:12:18Z,http://arxiv.org/abs/2107.12246v2,http://arxiv.org/pdf/2107.12246v2,"cs.PF, quant-ph"
Split Architecture for Large Scale Wide Area Networks,"Wolfgang John, Alisa Devlic, Zhemin Ding, David Jocha, Andras Kern, Mario Kind, Andreas Köpsel, Viktor Nordell, Sachin Sharma, Pontus Sköldström, Dimitri Staessens, Attila Takacs, Steffen Topp, F. -Joachim Westphal, Hagen Woesner, Andreas Gladisch","This report defines a carrier-grade split architecture based on requirements identified during the SPARC project. It presents the SplitArchitecture proposal, the SPARC concept for Software Defined Networking (SDN) introduced for large-scale wide area networks such as access/aggregation networks, and evaluates technical issues against architectural trade-offs. First we present the control and management architecture of the proposed SplitArchitecture. Here, we discuss a recursive control architecture consisting of hierarchically stacked control planes and provide initial considerations regarding network management integration to SDN in general and SplitArchitecture in particular. Next, OpenFlow extensions to support the carrier-grade SplitArchitecture are discussed. These are: a) Openness & Extensibility; b) Virtualization; c) OAM; d) Resiliency approaches; e) Bootstrapping and topology discovery; f) Service creation; g) Energy-efficient networking; h) QoS aspects; and i) Multilayer aspects. In addition, we discuss selected deployment and adoption scenarios faced by modern operator networks, such as service creation scenarios and peering aspects, i.e., how to interconnect with legacy networks. Finally, we indicate how our SplitArchitecture approach meets carrier grade scalability requirements in access/aggregation network scenarios",2014-02-06T15:26:51Z,2014-02-06T15:26:51Z,http://arxiv.org/abs/1402.2228v1,http://arxiv.org/pdf/1402.2228v1,"cs.NI, C.2.1; C.2.2; C.2.3; C.2.4"
ASIC-based Implementation of Synchronous Section-Carry Based Carry   Lookahead Adders,"P Balasubramanian, N E Mastorakis","The section-carry based carry lookahead adder (SCBCLA) topology was proposed as an improved high-speed alternative to the conventional carry lookahead adder (CCLA) topology in previous works. Self-timed and FPGA-based implementations of SCBCLAs and CCLAs were considered earlier, and it was found that SCBCLAs could help in delay reduction i.e. pave the way for improved speed compared to CCLAs at the expense of some increase in area and/or power parameters. In this work, we consider semi-custom ASIC-based implementations of different variants of SCBCLAs and CCLAs to perform 32-bit dual-operand addition. Based on the simulation results for 32-bit dual-operand addition obtained by targeting a high-end 32/28nm CMOS process, it is found that an optimized SCBCLA architecture reports a 9.8% improvement in figure-of-merit (FOM) compared to an optimized CCLA architecture, where the FOM is defined as the inverse of the product of power, delay, and area. It is generally inferred from the simulations that the SCBCLA architecture could be more beneficial compared to the CCLA architecture in terms of the design metrics whilst benefitting a variety of computer arithmetic operations involving dual-operand and/or multi-operand additions. Also, it is observed that heterogeneous CLA architectures tend to fare well compared to homogeneous CLA architectures, as substantiated by the simulation results.",2016-03-25T17:02:58Z,2016-03-25T17:02:58Z,http://arxiv.org/abs/1603.07961v1,http://arxiv.org/pdf/1603.07961v1,"cs.AR, B.2.4"
Searching for Efficient Multi-Scale Architectures for Dense Image   Prediction,"Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jonathon Shlens","The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.",2018-09-11T22:36:01Z,2018-09-11T22:36:01Z,http://arxiv.org/abs/1809.04184v1,http://arxiv.org/pdf/1809.04184v1,"cs.CV, cs.LG, stat.ML"
Circuit Transformations for Quantum Architectures,"Andrew M. Childs, Eddie Schoute, Cem M. Unsal","Quantum computer architectures impose restrictions on qubit interactions. We propose efficient circuit transformations that modify a given quantum circuit to fit an architecture, allowing for any initial and final mapping of circuit qubits to architecture qubits. To achieve this, we first consider the qubit movement subproblem and use the routing via matchings framework to prove tighter bounds on parallel routing. In practice, we only need to perform partial permutations, so we generalize routing via matchings to that setting. We give new routing procedures for common architecture graphs and for the generalized hierarchical product of graphs, which produces subgraphs of the Cartesian product. Secondly, for serial routing, we consider the token swapping framework and extend a 4-approximation algorithm for general graphs to support partial permutations. We apply these routing procedures to give several circuit transformations, using various heuristic qubit placement subroutines. We implement these transformations in software and compare their performance for large quantum circuits on grid and modular architectures, identifying strategies that work well in practice.",2019-02-25T06:15:31Z,2019-09-06T21:54:51Z,http://arxiv.org/abs/1902.09102v2,http://arxiv.org/pdf/1902.09102v2,"quant-ph, cs.DS"
Posterior-Guided Neural Architecture Search,"Yizhou Zhou, Xiaoyan Sun, Chong Luo, Zheng-Jun Zha, Wenjun Zeng","The emergence of neural architecture search (NAS) has greatly advanced the research on network design. Recent proposals such as gradient-based methods or one-shot approaches significantly boost the efficiency of NAS. In this paper, we formulate the NAS problem from a Bayesian perspective. We propose explicitly estimating the joint posterior distribution over pairs of network architecture and weights. Accordingly, a hybrid network representation is presented which enables us to leverage the Variational Dropout so that the approximation of the posterior distribution becomes fully gradient-based and highly efficient. A posterior-guided sampling method is then presented to sample architecture candidates and directly make evaluations. As a Bayesian approach, our posterior-guided NAS (PGNAS) avoids tuning a number of hyper-parameters and enables a very effective architecture sampling in posterior probability space. Interestingly, it also leads to a deeper insight into the weight sharing used in the one-shot NAS and naturally alleviates the mismatch between the sampled architecture and weights caused by the weight sharing. We validate our PGNAS method on the fundamental image classification task. Results on Cifar-10, Cifar-100 and ImageNet show that PGNAS achieves a good trade-off between precision and speed of search among NAS methods. For example, it takes 11 GPU days to search a very competitive architecture with 1.98% and 14.28% test errors on Cifar10 and Cifar100, respectively.",2019-06-23T05:51:19Z,2019-12-06T05:42:59Z,http://arxiv.org/abs/1906.09557v2,http://arxiv.org/pdf/1906.09557v2,"cs.LG, stat.ML"
Fully-Connected vs. Sub-Connected Hybrid Precoding Architectures for   mmWave MU-MIMO,"Xiaoshen Song, Thomas Kühne, Giuseppe Caire","Hybrid digital analog (HDA) beamforming has attracted considerable attention in practical implementation of millimeter wave (mmWave) multiuser multiple-input multiple-output (MU-MIMO) systems due to its low power consumption with respect to its digital baseband counterpart. The implementation cost, performance, and power efficiency of HDA beamforming depends on the level of connectivity and reconfigurability of the analog beamforming network. In this paper, we investigate the performance of two typical architectures for HDA MU-MIMO, i.e., the fully-connected (FC) architecture where each RF antenna port is connected to all antenna elements of the array, and the one-stream-per-subarray (OSPS) architecture where the RF antenna ports are connected to disjoint subarrays. We jointly consider the initial beam acquisition phase and data communication phase, such that the latter takes place by using the beam direction information obtained in the former phase. For each phase, we propose our own BA and precoding schemes that outperform the counterparts in the literature. We also evaluate the power efficiency of the two HDA architectures taking into account the practical hardware impairments, e.g., the power dissipation at different hardware components as well as the potential power backoff under typical power amplifier (PA) constraints. Numerical results show that the two architectures achieve similar sum spectral efficiency, but the OSPS architecture outperforms the FC case in terms of hardware complexity and power efficiency, only at the cost of a slightly longer time of initial beam acquisition.",2018-10-31T08:47:20Z,2018-10-31T08:47:20Z,http://arxiv.org/abs/1810.13161v1,http://arxiv.org/pdf/1810.13161v1,"cs.IT, math.IT"
Independent language modeling architecture for end-to-end ASR,"Van Tung Pham, Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li","The attention-based end-to-end (E2E) automatic speech recognition (ASR) architecture allows for joint optimization of acoustic and language models within a single network. However, in a vanilla E2E ASR architecture, the decoder sub-network (subnet), which incorporates the role of the language model (LM), is conditioned on the encoder output. This means that the acoustic encoder and the language model are entangled that doesn't allow language model to be trained separately from external text data. To address this problem, in this work, we propose a new architecture that separates the decoder subnet from the encoder output. In this way, the decoupled subnet becomes an independently trainable LM subnet, which can easily be updated using the external text data. We study two strategies for updating the new architecture. Experimental results show that, 1) the independent LM architecture benefits from external text data, achieving 9.3% and 22.8% relative character and word error rate reduction on Mandarin HKUST and English NSC datasets respectively; 2)the proposed architecture works well with external LM and can be generalized to different amount of labelled data.",2019-11-25T07:35:16Z,2019-11-25T07:35:16Z,http://arxiv.org/abs/1912.00863v1,http://arxiv.org/pdf/1912.00863v1,"cs.CL, eess.AS"
Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks,"Shoukang Hu, Xurong Xie, Shansong Liu, Mingyu Cui, Mengzhe Geng, Xunying Liu, Helen Meng","Deep neural networks (DNNs) based automatic speech recognition (ASR) systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search (NAS) techniques are used to automatically learn two types of hyper-parameters of state-of-the-art factored time delay neural networks (TDNNs): i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the DARTS method integrating architecture selection with lattice-free MMI (LF-MMI) TDNN training; Gumbel-Softmax and pipelined DARTS reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures allows efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on the 300-hour Switchboard corpus suggest the auto-configured systems consistently outperform the baseline LF-MMI TDNN systems using manual network design or random architecture search after LHUC speaker adaptation and RNNLM rescoring. Absolute word error rate (WER) reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained on a UASpeech disordered speech recognition task using the proposed NAS approaches.",2020-07-17T08:32:11Z,2021-02-07T14:54:13Z,http://arxiv.org/abs/2007.08818v4,http://arxiv.org/pdf/2007.08818v4,"eess.AS, cs.CL, cs.LG, cs.SD"
Efficient Neural Architecture Search via Proximal Iterations,"Quanming Yao, Ju Xu, Wei-Wei Tu, Zhanxing Zhu","Neural architecture search (NAS) recently attracts much research attention because of its ability to identify better architectures than handcrafted ones. However, many NAS methods, which optimize the search process in a discrete search space, need many GPU days for convergence. Recently, DARTS, which constructs a differentiable search space and then optimizes it by gradient descent, can obtain high-performance architecture and reduces the search time to several days. However, DARTS is still slow as it updates an ensemble of all operations and keeps only one after convergence. Besides, DARTS can converge to inferior architectures due to the strong correlation among operations. In this paper, we propose a new differentiable Neural Architecture Search method based on Proximal gradient descent (denoted as NASP). Different from DARTS, NASP reformulates the search process as an optimization problem with a constraint that only one operation is allowed to be updated during forward and backward propagation. Since the constraint is hard to deal with, we propose a new algorithm inspired by proximal iterations to solve it. Experiments on various tasks demonstrate that NASP can obtain high-performance architectures with 10 times of speedup on the computational time than DARTS.",2019-05-30T16:24:09Z,2019-11-20T23:29:25Z,http://arxiv.org/abs/1905.13577v3,http://arxiv.org/pdf/1905.13577v3,"cs.LG, stat.ML"
STONNE: A Detailed Architectural Simulator for Flexible Neural Network   Accelerators,"Francisco Muñoz-Martínez, José L. Abellán, Manuel E. Acacio, Tushar Krishna","The design of specialized architectures for accelerating the inference procedure of Deep Neural Networks (DNNs) is a booming area of research nowadays. First-generation rigid proposals have been rapidly replaced by more advanced flexible accelerator architectures able to efficiently support a variety of layer types and dimensions. As the complexity of the designs grows, it is more and more appealing for researchers to have cycle-accurate simulation tools at their disposal to allow for fast and accurate design-space exploration, and rapid quantification of the efficacy of architectural enhancements during the early stages of a design. To this end, we present STONNE (Simulation TOol of Neural Network Engines), a cycle-accurate, highly-modular and highly-extensible simulation framework that enables end-to-end evaluation of flexible accelerator architectures running complete contemporary DNN models. We use STONNE to model the recently proposed MAERI architecture and show how it can closely approach the performance results of the publicly available BSV-coded MAERI implementation. Then, we conduct a comprehensive evaluation and demonstrate that the folding strategy implemented for MAERI results in very low compute unit utilization (25% on average across 5 DNN models) which in the end translates into poor performance.",2020-06-10T19:20:52Z,2020-06-10T19:20:52Z,http://arxiv.org/abs/2006.07137v1,http://arxiv.org/pdf/2006.07137v1,"eess.SP, cs.AR, cs.LG"
MetaPerturb: Transferable Regularizer for Heterogeneous Tasks and   Architectures,"Jeongun Ryu, Jaewoong Shin, Hae Beom Lee, Sung Ju Hwang","Regularization and transfer learning are two popular techniques to enhance generalization on unseen data, which is a fundamental problem of machine learning. Regularization techniques are versatile, as they are task- and architecture-agnostic, but they do not exploit a large amount of data available. Transfer learning methods learn to transfer knowledge from one domain to another, but may not generalize across tasks and architectures, and may introduce new training cost for adapting to the target task. To bridge the gap between the two, we propose a transferable perturbation, MetaPerturb, which is meta-learned to improve generalization performance on unseen data. MetaPerturb is implemented as a set-based lightweight network that is agnostic to the size and the order of the input, which is shared across the layers. Then, we propose a meta-learning framework, to jointly train the perturbation function over heterogeneous tasks in parallel. As MetaPerturb is a set-function trained over diverse distributions across layers and tasks, it can generalize to heterogeneous tasks and architectures. We validate the efficacy and generality of MetaPerturb trained on a specific source domain and architecture, by applying it to the training of diverse neural architectures on heterogeneous target datasets against various regularizers and fine-tuning. The results show that the networks trained with MetaPerturb significantly outperform the baselines on most of the tasks and architectures, with a negligible increase in the parameter size and no hyperparameters to tune.",2020-06-13T02:54:59Z,2022-02-15T13:56:01Z,http://arxiv.org/abs/2006.07540v3,http://arxiv.org/pdf/2006.07540v3,"cs.LG, stat.ML"
Triple-level Model Inferred Collaborative Network Architecture for Video   Deraining,"Pan Mu, Zhu Liu, Yaohua Liu, Risheng Liu, Xin Fan","Video deraining is an important issue for outdoor vision systems and has been investigated extensively. However, designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video deraining. In this paper, we develop a model-guided triple-level optimization framework to deduce network architecture with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching (TMICS), for dealing with various video rain circumstances. In particular, to mitigate the problem that existing methods cannot cover various rain streaks distribution, we first design a hyper-parameter optimization model about task variable and hyper-parameter. Based on the proposed optimization model, we design a collaborative structure for video deraining. This structure includes Dominant Network Architecture (DNA) and Companionate Network Architecture (CNA) that is cooperated by introducing an Attention-based Averaging Scheme (AAS). To better explore inter-frame information from videos, we introduce a macroscopic structure searching scheme that searches from Optical Flow Module (OFM) and Temporal Grouping Module (TGM) to help restore latent frame. In addition, we apply the differentiable neural architecture searching from a compact candidate set of task-specific operations to discover desirable rain streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows significant improvements in fidelity and temporal consistency over the state-of-the-art works. Source code is available at https://github.com/vis-opt-group/TMICS.",2021-11-08T13:09:00Z,2021-11-08T13:09:00Z,http://arxiv.org/abs/2111.04459v1,http://arxiv.org/pdf/2111.04459v1,"eess.IV, cs.CV"
Neural Architecture Search for Spiking Neural Networks,"Youngeun Kim, Yuhang Li, Hyoungseob Park, Yeshwanth Venkatesha, Priyadarshini Panda","Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. However, most prior SNN methods use ANN-like architectures (e.g., VGG-Net or ResNet), which could provide sub-optimal performance for temporal sequence processing of binary information in SNNs. To address this, in this paper, we introduce a novel Neural Architecture Search (NAS) approach for finding better SNN architectures. Inspired by recent NAS approaches that find the optimal architecture from activation patterns at initialization, we select the architecture that can represent diverse spike activation patterns across different data samples without training. Moreover, to further leverage the temporal information among the spikes, we search for feed forward connections as well as backward connections (i.e., temporal feedback connections) between layers. Interestingly, SNASNet found by our search algorithm achieves higher performance with backward connections, demonstrating the importance of designing SNN architecture for suitably using temporal information. We conduct extensive experiments on three image recognition benchmarks where we show that SNASNet achieves state-of-the-art performance with significantly lower timesteps (5 timesteps). Code is available at Github.",2022-01-23T16:34:27Z,2022-07-20T18:28:22Z,http://arxiv.org/abs/2201.10355v3,http://arxiv.org/pdf/2201.10355v3,"cs.NE, cs.AI, cs.LG, eess.SP"
Distributed Binary Detection over Fading Channels: Cooperative and   Parallel Architectures,"Nahal Maleki, Azadeh Vosoughi, Nazanin Rahnavard","This paper considers the problem of binary distributed detection of a known signal in correlated Gaussian sensing noise in a wireless sensor network, where the sensors are restricted to use likelihood ratio test (LRT), and communicate with the fusion center (FC) over bandwidth-constrained channels that are subject to fading and noise. To mitigate the deteriorating effect of fading encountered in the conventional parallel fusion architecture, in which the sensors directly communicate with the FC, we propose new fusion architectures that enhance the detection performance, via harvesting cooperative gain (so-called decision diversity gain). In particular, we propose: (i) cooperative fusion architecture with Alamouti's space-time coding (STC) scheme at sensors, (ii) cooperative fusion architecture with signal fusion at sensors, and (iii) parallel fusion architecture with local threshold changing at sensors. For these schemes, we derive the LRT and majority fusion rules at the FC, and provide upper bounds on the average error probabilities for homogeneous sensors, subject to uncorrelated Gaussian sensing noise, in terms of signal-to-noise ratio (SNR) of communication and sensing channels. Our simulation results indicate that, when the FC employs the LRT rule, unless for low communication SNR and moderate/high sensing SNR, performance improvement is feasible with the new fusion architectures. When the FC utilizes the majority rule, such improvement is possible, unless for high sensing SNR.",2015-08-31T16:47:15Z,2016-01-19T19:36:50Z,http://arxiv.org/abs/1508.07913v3,http://arxiv.org/pdf/1508.07913v3,"cs.IT, math.IT"
A Domain Guided CNN Architecture for Predicting Age from Structural   Brain Images,"Pascal Sturmfels, Saige Rutherford, Mike Angstadt, Mark Peterson, Chandra Sripada, Jenna Wiens","Given the wide success of convolutional neural networks (CNNs) applied to natural images, researchers have begun to apply them to neuroimaging data. To date, however, exploration of novel CNN architectures tailored to neuroimaging data has been limited. Several recent works fail to leverage the 3D structure of the brain, instead treating the brain as a set of independent 2D slices. Approaches that do utilize 3D convolutions rely on architectures developed for object recognition tasks in natural 2D images. Such architectures make assumptions about the input that may not hold for neuroimaging. For example, existing architectures assume that patterns in the brain exhibit translation invariance. However, a pattern in the brain may have different meaning depending on where in the brain it is located. There is a need to explore novel architectures that are tailored to brain images. We present two simple modifications to existing CNN architectures based on brain image structure. Applied to the task of brain age prediction, our network achieves a mean absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline that achieves a MAE of 1.6 years. Our results suggest that lessons learned from developing models on natural images may not directly transfer to neuroimaging tasks. Instead, there remains a large space of unexplored questions regarding model development in this area, whose answers may differ from conventional wisdom.",2018-08-11T19:43:22Z,2018-08-11T19:43:22Z,http://arxiv.org/abs/1808.04362v1,http://arxiv.org/pdf/1808.04362v1,"cs.CV, cs.LG, stat.ML"
Scalable and Energy-Efficient Millimeter Massive MIMO Architectures:   Reflect-Array and Transmit-Array Antennas,"Vahid Jamali, Antonia M. Tulino, Georg Fischer, Ralf Müller, Robert Schober","Hybrid analog-digital architectures are considered as promising candidates for implementing millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) systems since they enable a considerable reduction of the required number of costly radio frequency (RF) chains by moving some of the signal processing operations into the analog domain. However, the analog feed network, comprising RF dividers, combiners, phase shifters, and line connections, of hybrid MIMO architectures is not scalable due to its prohibitively high power consumption for large numbers of transmit antennas. Motivated by this limitation, in this paper, we study novel massive MIMO architectures, namely reflect-array (RA) and transmit-array (TA) antennas. We show that the precoders for RA and TA antennas have to meet different constraints compared to those for conventional MIMO architectures. Taking these constraints into account and exploiting the sparsity of mmWave channels, we design an efficient precoder for RA and TA antennas based on the orthogonal matching pursuit algorithm. Furthermore, in order to fairly compare the performance of RA and TA antennas with conventional fully-digital and hybrid MIMO architectures, we develop a unified power consumption model. Our simulation results show that unlike conventional MIMO architectures, RA and TA antennas are highly energy efficient and fully scalable in terms of the number of transmit antennas.",2018-11-07T15:52:26Z,2018-11-07T15:52:26Z,http://arxiv.org/abs/1811.02948v1,http://arxiv.org/pdf/1811.02948v1,"cs.IT, math.IT"
S3NAS: Fast NPU-aware Neural Architecture Search Methodology,"Jaeseong Lee, Duseok Kang, Soonhoi Ha","As the application area of convolutional neural networks (CNN) is growing in embedded devices, it becomes popular to use a hardware CNN accelerator, called neural processing unit (NPU), to achieve higher performance per watt than CPUs or GPUs. Recently, automated neural architecture search (NAS) emerges as the default technique to find a state-of-the-art CNN architecture with higher accuracy than manually-designed architectures for image classification. In this paper, we present a fast NPU-aware NAS methodology, called S3NAS, to find a CNN architecture with higher accuracy than the existing ones under a given latency constraint. It consists of three steps: supernet design, Single-Path NAS for fast architecture exploration, and scaling. To widen the search space of the supernet structure that consists of stages, we allow stages to have a different number of blocks and blocks to have parallel layers of different kernel sizes. For a fast neural architecture search, we apply a modified Single-Path NAS technique to the proposed supernet structure. In this step, we assume a shorter latency constraint than the required to reduce the search space and the search time. The last step is to scale up the network maximally within the latency constraint. For accurate latency estimation, an analytical latency estimator is devised, based on a cycle-level NPU simulator that runs an entire CNN considering the memory access overhead accurately. With the proposed methodology, we are able to find a network in 3 hours using TPUv3, which shows 82.72% top-1 accuracy on ImageNet with 11.66 ms latency. Code are released at https://github.com/cap-lab/S3NAS",2020-09-04T04:45:50Z,2020-09-04T04:45:50Z,http://arxiv.org/abs/2009.02009v1,http://arxiv.org/pdf/2009.02009v1,"cs.LG, cs.CV, stat.ML"
Once Quantization-Aware Training: High Performance Extremely Low-bit   Architecture Search,"Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming Li, Chen Lin, Fengwei Yu, Junjie Yan, Wanli Ouyang","Quantization Neural Networks (QNN) have attracted a lot of attention due to their high efficiency. To enhance the quantization accuracy, prior works mainly focus on designing advanced quantization algorithms but still fail to achieve satisfactory results under the extremely low-bit case. In this work, we take an architecture perspective to investigate the potential of high-performance QNN. Therefore, we propose to combine Network Architecture Search methods with quantization to enjoy the merits of the two sides. However, a naive combination inevitably faces unacceptable time consumption or unstable training problem. To alleviate these problems, we first propose the joint training of architecture and quantization with a shared step size to acquire a large number of quantized models. Then a bit-inheritance scheme is introduced to transfer the quantized models to the lower bit, which further reduces the time cost and meanwhile improves the quantization accuracy. Equipped with this overall framework, dubbed as Once Quantization-Aware Training~(OQAT), our searched model family, OQATNets, achieves a new state-of-the-art compared with various architectures under different bit-widths. In particular, OQAT-2bit-M achieves 61.6% ImageNet Top-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin of 9% with 10% less computation cost. A series of quantization-friendly architectures are identified easily and extensive analysis can be made to summarize the interaction between quantization and neural architectures. Codes and models are released at https://github.com/LaVieEnRoseSMZ/OQA",2020-10-09T03:52:16Z,2021-09-28T06:53:15Z,http://arxiv.org/abs/2010.04354v3,http://arxiv.org/pdf/2010.04354v3,"cs.CV, eess.IV"
Smooth Variational Graph Embeddings for Efficient Neural Architecture   Search,"Jovita Lukasik, David Friede, Arber Zela, Frank Hutter, Margret Keuper","Neural architecture search (NAS) has recently been addressed from various directions, including discrete, sampling-based methods and efficient differentiable approaches. While the former are notoriously expensive, the latter suffer from imposing strong constraints on the search space. Architecture optimization from a learned embedding space for example through graph neural network based variational autoencoders builds a middle ground and leverages advantages from both sides. Such approaches have recently shown good performance on several benchmarks. Yet, their stability and predictive power heavily depends on their capacity to reconstruct networks from the embedding space. In this paper, we propose a two-sided variational graph autoencoder, which allows to smoothly encode and accurately reconstruct neural architectures from various search spaces. We evaluate the proposed approach on neural architectures defined by the ENAS approach, the NAS-Bench-101 and the NAS-Bench-201 search space and show that our smooth embedding space allows to directly extrapolate the performance prediction to architectures outside the seen domain (e.g. with more operations). Thus, it facilitates to predict good network architectures even without expensive Bayesian optimization or reinforcement learning.",2020-10-09T17:05:41Z,2021-05-12T12:44:54Z,http://arxiv.org/abs/2010.04683v3,http://arxiv.org/pdf/2010.04683v3,"cs.LG, cs.AI, cs.CV, stat.ML"
Darts-Conformer: Towards Efficient Gradient-Based Neural Architecture   Search For End-to-End ASR,"Xian Shi, Pan Zhou, Wei Chen, Lei Xie","Neural architecture search (NAS) has been successfully applied to tasks like image classification and language modeling for finding efficient high-performance network architectures. In ASR field especially end-to-end ASR, the related research is still in its infancy. In this work, we focus on applying NAS on the most popular manually designed model: Conformer, and then propose an efficient ASR model searching method that benefits from the natural advantage of differentiable architecture search (Darts) in reducing computational overheads. We fuse Darts mutator and Conformer blocks to form a complete search space, within which a modified architecture called Darts-Conformer cell is found automatically. The entire searching process on AISHELL-1 dataset costs only 0.7 GPU days. Replacing the Conformer encoder by stacking searched cell, we get an end-to-end ASR model (named as Darts-Conformner) that outperforms the Conformer baseline by 4.7\% on the open-source AISHELL-1 dataset. Besides, we verify the transferability of the architecture searched on a small dataset to a larger 2k-hour dataset. To the best of our knowledge, this is the first successful attempt to apply gradient-based architecture search in the attention-based encoder-decoder ASR model.",2021-04-07T02:37:40Z,2021-08-10T04:02:24Z,http://arxiv.org/abs/2104.02868v2,http://arxiv.org/pdf/2104.02868v2,"cs.SD, eess.AS"
Exploration of Quantum Neural Architecture by Mixing Quantum Neuron   Designs,"Zhepeng Wang, Zhiding Liang, Shanglin Zhou, Caiwen Ding, Yiyu Shi, Weiwen Jiang","With the constant increase of the number of quantum bits (qubits) in the actual quantum computers, implementing and accelerating the prevalent deep learning on quantum computers are becoming possible. Along with this trend, there emerge quantum neural architectures based on different designs of quantum neurons. A fundamental question in quantum deep learning arises: what is the best quantum neural architecture? Inspired by the design of neural architectures for classical computing which typically employs multiple types of neurons, this paper makes the very first attempt to mix quantum neuron designs to build quantum neural architectures. We observe that the existing quantum neuron designs may be quite different but complementary, such as neurons from variational quantum circuits (VQC) and Quantumflow. More specifically, VQC can apply real-valued weights but suffer from being extended to multiple layers, while QuantumFlow can build a multi-layer network efficiently, but is limited to use binary weights. To take their respective advantages, we propose to mix them together and figure out a way to connect them seamlessly without additional costly measurement. We further investigate the design principles to mix quantum neurons, which can provide guidance for quantum neural architecture exploration in the future. Experimental results demonstrate that the identified quantum neural architectures with mixed quantum neurons can achieve 90.62% of accuracy on the MNIST dataset, compared with 52.77% and 69.92% on the VQC and QuantumFlow, respectively.",2021-09-08T17:47:54Z,2021-11-07T01:35:42Z,http://arxiv.org/abs/2109.03806v2,http://arxiv.org/pdf/2109.03806v2,"quant-ph, cs.AI"
Equivariant Subgraph Aggregation Networks,"Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, Haggai Maron","Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures.",2021-10-06T16:45:07Z,2022-03-16T22:16:25Z,http://arxiv.org/abs/2110.02910v3,http://arxiv.org/pdf/2110.02910v3,"cs.LG, stat.ML"
Heed the Noise in Performance Evaluations in Neural Architecture Search,"Arkadiy Dushatskiy, Tanja Alderliesten, Peter A. N. Bosman","Neural Architecture Search (NAS) has recently become a topic of great interest. However, there is a potentially impactful issue within NAS that remains largely unrecognized: noise. Due to stochastic factors in neural network initialization, training, and the chosen train/validation dataset split, the performance evaluation of a neural network architecture, which is often based on a single learning run, is also stochastic. This may have a particularly large impact if a dataset is small. We therefore propose to reduce this noise by evaluating architectures based on average performance over multiple network training runs using different random seeds and cross-validation. We perform experiments for a combinatorial optimization formulation of NAS in which we vary noise reduction levels. We use the same computational budget for each noise level in terms of network training runs, i.e., we allow less architecture evaluations when averaging over more training runs. Multiple search algorithms are considered, including evolutionary algorithms which generally perform well for NAS. We use two publicly available datasets from the medical image segmentation domain where datasets are often limited and variability among samples is often high. Our results show that reducing noise in architecture evaluations enables finding better architectures by all considered search algorithms.",2022-02-04T11:20:46Z,2022-05-02T12:49:19Z,http://arxiv.org/abs/2202.02078v2,http://arxiv.org/pdf/2202.02078v2,"cs.NE, cs.CV, eess.IV"
Energy-efficient Dynamic-subarray with Fixed True-time-delay Design for   Terahertz Wideband Hybrid Beamforming,"Longfei Yan, Chong Han, Jinhong Yuan","Hybrid beamforming for Terahertz (THz) ultra-massive multiple-input multiple-output (UM-MIMO) systems is a promising technology for 6G space-air-ground integrated networks, which can overcome huge propagation loss and offer unprecedented data rates. With ultra-wide bandwidth and ultra-large-scale antennas array in THz band, the beam squint becomes one of the critical problems which could reduce the array gain and degrade the data rate substantially. However, the traditional phase-shifters-based hybrid beamforming architectures cannot tackle this issue due to the frequency-flat property of the phase shifters. In this paper, to combat the beam squint while keeping high energy efficiency, a novel dynamic-subarray with fixed true-time-delay (DS-FTTD) architecture is proposed. Compared to the existing studies which use the complicated adjustable TTDs, the DS-FTTD architecture has lower power consumption and hardware complexity, thanks to the low-cost FTTDs. Furthermore, a low-complexity row-decomposition (RD) algorithm is proposed to design hybrid beamforming matrices for the DS-FTTD architecture. Extensive simulation results show that, by using the RD algorithm, the DS-FTTD architecture achieves near-optimal array gain and significantly higher energy efficiency than the existing architectures. Moreover, the spectral efficiency of DS-FTTD architecture with the RD algorithm is robust to the imperfect channel state information.",2022-02-07T06:26:46Z,2022-02-07T06:26:46Z,http://arxiv.org/abs/2202.02965v1,http://arxiv.org/pdf/2202.02965v1,"cs.IT, eess.SP, math.IT"
Deep-Learning Architectures for Multi-Pitch Estimation: Towards Reliable   Evaluation,"Christof Weiß, Geoffroy Peeters","Extracting pitch information from music recordings is a challenging but important problem in music signal processing. Frame-wise transcription or multi-pitch estimation aims for detecting the simultaneous activity of pitches in polyphonic music recordings and has recently seen major improvements thanks to deep-learning techniques, with a variety of proposed network architectures. In this paper, we realize different architectures based on CNNs, the U-net structure, and self-attention components. We propose several modifications to these architectures including self-attention modules for skip connections, recurrent layers to replace the self-attention, and a multi-task strategy with simultaneous prediction of the degree of polyphony. We compare variants of these architectures in different sizes for multi-pitch estimation, focusing on Western classical music beyond the piano-solo scenario using the MusicNet and Schubert Winterreise datasets. Our experiments indicate that most architectures yield competitive results and that larger model variants seem to be beneficial. However, we find that these results substantially depend on randomization effects and the particular choice of the training-test split, which questions the claim of superiority for particular architectures given only small improvements. We therefore investigate the influence of dataset splits in the presence of several movements of a work cycle (cross-version evaluation) and propose a best-practice splitting strategy for MusicNet, which weakens the influence of individual test tracks and suppresses overfitting to specific works and recording conditions. A final evaluation on a mixed dataset suggests that improvements on one specific dataset do not necessarily generalize to other scenarios, thus emphasizing the need for further high-quality multi-pitch datasets in order to reliably measure progress in music transcription tasks.",2022-02-18T13:52:21Z,2022-02-18T13:52:21Z,http://arxiv.org/abs/2202.09198v1,http://arxiv.org/pdf/2202.09198v1,"cs.SD, cs.LG, eess.AS"
TraHGR: Transformer for Hand Gesture Recognition via ElectroMyography,"Soheil Zabihi, Elahe Rahimian, Amir Asif, Arash Mohammadi","Deep learning-based Hand Gesture Recognition (HGR) via surface Electromyogram (sEMG) signals has recently shown significant potential for development of advanced myoelectric-controlled prosthesis. Existing deep learning approaches, typically, include only one model as such can hardly maintain acceptable generalization performance in changing scenarios. In this paper, we aim to address this challenge by capitalizing on the recent advances of hybrid models and transformers. In other words, we propose a hybrid framework based on the transformer architecture, which is a relatively new and revolutionizing deep learning model. The proposed hybrid architecture, referred to as the Transformer for Hand Gesture Recognition (TraHGR), consists of two parallel paths followed by a linear layer that acts as a fusion center to integrate the advantage of each module and provide robustness over different scenarios. We evaluated the proposed architecture TraHGR based on the commonly used second Ninapro dataset, referred to as the DB2. The sEMG signals in the DB2 dataset are measured in the real-life conditions from 40 healthy users, each performing 49 gestures. We have conducted extensive set of experiments to test and validate the proposed TraHGR architecture, and have compared its achievable accuracy with more than five recently proposed HGR classification algorithms over the same dataset. We have also compared the results of the proposed TraHGR architecture with each individual path and demonstrated the distinguishing power of the proposed hybrid architecture. The recognition accuracies of the proposed TraHGR architecture are 86.18%, 88.91%, 81.44%, and 93.84%, which are 2.48%, 5.12%, 8.82%, and 4.30% higher than the state-ofthe-art performance for DB2 (49 gestures), DB2-B (17 gestures), DB2-C (23 gestures), and DB2-D (9 gestures), respectively.",2022-03-28T15:43:56Z,2022-03-31T01:50:55Z,http://arxiv.org/abs/2203.16336v2,http://arxiv.org/pdf/2203.16336v2,"eess.SP, cs.LG"
Efficient Neural Net Approaches in Metal Casting Defect Detection,"Rohit Lal, Bharath Kumar Bolla, Sabeesh Ethiraj","One of the most pressing challenges prevalent in the steel manufacturing industry is the identification of surface defects. Early identification of casting defects can help boost performance, including streamlining production processes. Though, deep learning models have helped bridge this gap and automate most of these processes, there is a dire need to come up with lightweight models that can be deployed easily with faster inference times. This research proposes a lightweight architecture that is efficient in terms of accuracy and inference time compared with sophisticated pre-trained CNN architectures like MobileNet, Inception, and ResNet, including vision transformers. Methodologies to minimize computational requirements such as depth-wise separable convolution and global average pooling (GAP) layer, including techniques that improve architectural efficiencies and augmentations, have been experimented. Our results indicate that a custom model of 590K parameters with depth-wise separable convolutions outperformed pretrained architectures such as Resnet and Vision transformers in terms of accuracy (81.87%) and comfortably outdid architectures such as Resnet, Inception, and Vision transformers in terms of faster inference times (12 ms). Blurpool fared outperformed other techniques, with an accuracy of 83.98%. Augmentations had a paradoxical effect on the model performance. No direct correlation between depth-wise and 3x3 convolutions on inference time, they, however, they played a direct role in improving model efficiency by enabling the networks to go deeper and by decreasing the number of trainable parameters. Our work sheds light on the fact that custom networks with efficient architectures and faster inference times can be built without the need of relying on pre-trained architectures.",2022-08-08T13:54:36Z,2022-08-08T13:54:36Z,http://arxiv.org/abs/2208.04150v1,http://arxiv.org/pdf/2208.04150v1,"cs.CV, cs.LG, eess.IV"
Physics-Constrained Neural Network for Design and Feature-Based   Optimization of Weave Architectures,"Haotian Feng, Sabarinathan P Subramaniyan, Hridyesh Tewani, Pavana Prabhakar","Woven fabrics play an essential role in everyday textiles for clothing/sportswear, water filtration, and retaining walls, to reinforcements in stiff composites for lightweight structures like aerospace, sporting, automotive, and marine industries. Several possible combinations of weave patterns and material choices, which comprise weave architecture, present a challenging question about how they could influence the physical and mechanical properties of woven fabrics and reinforced structures. In this paper, we present a novel Physics-Constrained Neural Network (PCNN) to predict the mechanical properties like the modulus of weave architectures and the inverse problem of predicting pattern/material sequence for a design/target modulus value. The inverse problem is particularly challenging as it usually requires many iterations to find the appropriate architecture using traditional optimization approaches. We show that the proposed PCNN can effectively predict weave architecture for the desired modulus with higher accuracy than several baseline models considered. We present a feature-based optimization strategy to improve the predictions using features in the Grey Level Co-occurrence Matrix (GLCM) space. We combine PCNN with this feature-based optimization to discover near-optimal weave architectures to facilitate the initial design of weave architecture. The proposed frameworks will primarily enable the woven composite analysis and optimization process, and be a starting point to introduce Knowledge-guided Neural Networks into the complex structural analysis.",2022-09-19T16:16:45Z,2023-11-24T12:56:55Z,http://arxiv.org/abs/2209.09154v2,http://arxiv.org/pdf/2209.09154v2,"physics.app-ph, cs.LG"
Performance Enhancement Strategies for Sparse Matrix-Vector   Multiplication (SpMV) and Iterative Linear Solvers,"Thaha Mohammed, Rashid Mehmood","Iterative solutions of sparse linear systems and sparse eigenvalue problems have a fundamental role in vital fields of scientific research and engineering. The crucial computing kernel for such iterative solutions is the multiplication of a sparse matrix by a dense vector. Efficient implementation of sparse matrix-vector multiplication (SpMV) and linear solvers are therefore essential and has been subjected to extensive research across a variety of computing architectures and accelerators such as central processing units (CPUs), graphical processing units (GPUs), many integrated cores (MICs), and field programmable gate arrays (FPGAs). Unleashing the full potential of an architecture/accelerator requires determining the factors that affect an efficient implementation of SpMV. This article presents the first of its kind, in-depth survey covering over two hundred state-of-the-art optimization schemes for solving sparse iterative linear systems with a focus on computing SpMV. A new taxonomy for iterative solutions and SpMV techniques common to all architectures is proposed. This article includes reviews of SpMV techniques for all architectures to consolidate a single taxonomy to encourage cross-architectural and heterogeneous-architecture developments. However, the primary focus is on GPUs. The major contributions as well as the primary, secondary, and tertiary contributions of the SpMV techniques are first highlighted utilizing the taxonomy and then qualitatively compared. A summary of the current state of the research for each architecture is discussed separately. Finally, several open problems and key challenges for future research directions are outlined.",2022-12-14T20:17:26Z,2022-12-14T20:17:26Z,http://arxiv.org/abs/2212.07490v1,http://arxiv.org/pdf/2212.07490v1,"cs.DS, cs.NA, math.NA"
Modulation and Switching Architecture Performances for Frequency   Up-conversion of Complex-Modulated Data Signals based on a SOA-MZI Photonic   Sampling Mixer,"Dimitrios Kastritsis, Thierry Rampone, Kyriakos E. Zoiros, Ammar Sharaiha","A theoretical and experimental performance analysis of a Semiconductor Optical Amplifier - Mach-Zehnder Interferometer (SOA-MZI) photonic sampling mixer used as a frequency up-converter is presented employing Switching and Modulation architectures. An active mode-locked laser, generating 2 ps-width pulses at a repetition rate equal to 10 GHz, is used as a sampling source. An optical carrier intensity modulated by a sinusoidal signal at 1 GHz is up-converted to 9 GHz and 39 GHz. High Conversion Gains (CGs) of about 15 dB are demonstrated for the frequency conversion to 9 GHz using both architectures, whereas up to 4 dB and 9 dB for the conversion to 39 GHz employing Switching and Modulation architectures, respectively. Small-signal equations for the up-converted signal in both architectures are formulated and developed, which permit to quantify the CG from closed-form expressions. The numerically calculated CG values are in very good agreement with those obtained experimentally. The validated equations are subsequently employed to explain the performance differences between the two architectures in terms of the CG. Furthermore, signals modulated by QPSK and 16-QAM complex modulation formats at different baud rates are up-converted from 750 MHz to 9.25 GHz and 39.75 GHz and their Error Vector Magnitude is evaluated and compared. The maximum bit rate that meets the Forward Error Correction (FEC) limit is achieved using the Modulation architecture. It is 1 Gbps and 512 Mbps for QPSK and 16-QAM modulations, respectively.",2022-12-15T10:17:36Z,2022-12-15T10:17:36Z,http://arxiv.org/abs/2302.01290v1,http://arxiv.org/pdf/2302.01290v1,"eess.SP, physics.app-ph, physics.optics"
Densely Connected $G$-invariant Deep Neural Networks with Signed   Permutation Representations,"Devanshu Agrawal, James Ostrowski","We introduce and investigate, for finite groups $G$, $G$-invariant deep neural network ($G$-DNN) architectures with ReLU activation that are densely connected-- i.e., include all possible skip connections. In contrast to other $G$-invariant architectures in the literature, the preactivations of the$G$-DNNs presented here are able to transform by \emph{signed} permutation representations (signed perm-reps) of $G$. Moreover, the individual layers of the $G$-DNNs are not required to be $G$-equivariant; instead, the preactivations are constrained to be $G$-equivariant functions of the network input in a way that couples weights across all layers. The result is a richer family of $G$-invariant architectures never seen previously. We derive an efficient implementation of $G$-DNNs after a reparameterization of weights, as well as necessary and sufficient conditions for an architecture to be ``admissible''-- i.e., nondegenerate and inequivalent to smaller architectures. We include code that allows a user to build a $G$-DNN interactively layer-by-layer, with the final architecture guaranteed to be admissible. We show that there are far more admissible $G$-DNN architectures than those accessible with the ``concatenated ReLU'' activation function from the literature. Finally, we apply $G$-DNNs to two example problems -- (1) multiplication in $\{-1, 1\}$ (with theoretical guarantees) and (2) 3D object classification -- % finding that the inclusion of signed perm-reps significantly boosts predictive performance compared to baselines with only ordinary (i.e., unsigned) perm-reps.",2023-03-08T14:35:03Z,2023-10-17T17:06:04Z,http://arxiv.org/abs/2303.04614v2,http://arxiv.org/pdf/2303.04614v2,"cs.LG, stat.ML"
Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look   Into Operation Importance,"Vasco Lopes, Bruno Degardin, Luís A. Alexandre","Neural Architecture Search (NAS) benchmarks significantly improved the capability of developing and comparing NAS methods while at the same time drastically reduced the computational overhead by providing meta-information about thousands of trained neural networks. However, tabular benchmarks have several drawbacks that can hinder fair comparisons and provide unreliable results. These usually focus on providing a small pool of operations in heavily constrained search spaces -- usually cell-based neural networks with pre-defined outer-skeletons. In this work, we conducted an empirical analysis of the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks in terms of their generability and how different operations influence the performance of the generated architectures. We found that only a subset of the operation pool is required to generate architectures close to the upper-bound of the performance range. Also, the performance distribution is negatively skewed, having a higher density of architectures in the upper-bound range. We consistently found convolution layers to have the highest impact on the architecture's performance, and that specific combination of operations favors top-scoring architectures. These findings shed insights on the correct evaluation and comparison of NAS methods using NAS benchmarks, showing that directly searching on NAS-Bench-201, ImageNet16-120 and TransNAS-Bench-101 produces more reliable results than searching only on CIFAR-10. Furthermore, with this work we provide suggestions for future benchmark evaluations and design. The code used to conduct the evaluations is available at https://github.com/VascoLopes/NAS-Benchmark-Evaluation.",2023-03-29T18:03:28Z,2023-03-29T18:03:28Z,http://arxiv.org/abs/2303.16938v1,http://arxiv.org/pdf/2303.16938v1,"cs.LG, cs.AI, cs.CV, stat.ML"
Towards Efficient Control Flow Handling in Spatial Architecture via   Architecting the Control Flow Plane,"Jinyi Deng, Xinru Tang, Jiahao Zhang, Yuxuan Li, Linyun Zhang, Boxiao Han, Hongjun He, Fengbin Tu, Leibo Liu, Shaojun Wei, Yang Hu, Shouyi Yin","Spatial architecture is a high-performance architecture that uses control flow graphs and data flow graphs as the computational model and producer/consumer models as the execution models. However, existing spatial architectures suffer from control flow handling challenges. Upon categorizing their PE execution models, we find that they lack autonomous, peer-to-peer, and temporally loosely-coupled control flow handling capability. This leads to limited performance in intensive control programs.   A spatial architecture, Marionette, is proposed, with an explicit-designed control flow plane. The Control Flow Plane enables autonomous, peer-to-peer and temporally loosely-coupled control flow handling. The Proactive PE Configuration ensures timely and computation-overlapped configuration to improve handling Branch Divergence. The Agile PE Assignment enhance the pipeline performance of Imperfect Loops. We develop full stack of Marionette (ISA, compiler, simulator, RTL) and demonstrate that in a variety of challenging intensive control programs, compared to state-of-the-art spatial architectures, Marionette outperforms Softbrain, TIA, REVEL, and RipTide by geomean 2.88x, 3.38x, 1.55x, and 2.66x.",2023-07-06T08:26:31Z,2023-09-19T08:36:28Z,http://arxiv.org/abs/2307.02847v2,http://arxiv.org/pdf/2307.02847v2,"cs.AR, C.1.3; F.1.2"
Auto-CsiNet: Scenario-customized Automatic Neural Network Architecture   Generation for Massive MIMO CSI Feedback,"Xiangyi Li, Jiajia Guo, Chao-Kai Wen, Shi Jin","Deep learning has revolutionized the design of the channel state information (CSI) feedback module in wireless communications. However, designing the optimal neural network (NN) architecture for CSI feedback can be a laborious and time-consuming process. Manual design can be prohibitively expensive for customizing NNs to different scenarios. This paper proposes using neural architecture search (NAS) to automate the generation of scenario-customized CSI feedback NN architectures, thereby maximizing the potential of deep learning in exclusive environments. By employing automated machine learning and gradient-descent-based NAS, an efficient and cost-effective architecture design process is achieved. The proposed approach leverages implicit scene knowledge, integrating it into the scenario customization process in a data-driven manner, and fully exploits the potential of deep learning for each specific scenario. To address the issue of excessive search, early stopping and elastic selection mechanisms are employed, enhancing the efficiency of the proposed scheme. The experimental results demonstrate that the automatically generated architecture, known as Auto-CsiNet, outperforms manually-designed models in both reconstruction performance (achieving approximately a 14% improvement) and complexity (reducing it by approximately 50%). Furthermore, the paper analyzes the impact of the scenario on the NN architecture and its capacity.",2023-11-27T15:56:58Z,2023-11-27T15:56:58Z,http://arxiv.org/abs/2311.15950v1,http://arxiv.org/pdf/2311.15950v1,"cs.IT, cs.AI, math.IT"
Curriculum reinforcement learning for quantum architecture search under   hardware errors,"Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier Bonet-Monroig, Vedran Dunjko, Onur Danaci","The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.",2024-02-05T20:33:00Z,2024-02-05T20:33:00Z,http://arxiv.org/abs/2402.03500v1,http://arxiv.org/pdf/2402.03500v1,"quant-ph, cs.AI, cs.LG"
Benchmarking Retinal Blood Vessel Segmentation Models for Cross-Dataset   and Cross-Disease Generalization,"Jeremiah Fadugba, Patrick Köhler, Lisa Koch, Petru Manescu, Philipp Berens","Retinal blood vessel segmentation can extract clinically relevant information from fundus images. As manual tracing is cumbersome, algorithms based on Convolution Neural Networks have been developed. Such studies have used small publicly available datasets for training and measuring performance, running the risk of overfitting. Here, we provide a rigorous benchmark for various architectural and training choices commonly used in the literature on the largest dataset published to date. We train and evaluate five published models on the publicly available FIVES fundus image dataset, which exceeds previous ones in size and quality and which contains also images from common ophthalmological conditions (diabetic retinopathy, age-related macular degeneration, glaucoma). We compare the performance of different model architectures across different loss functions, levels of image qualitiy and ophthalmological conditions and assess their ability to perform well in the face of disease-induced domain shifts. Given sufficient training data, basic architectures such as U-Net perform just as well as more advanced ones, and transfer across disease-induced domain shifts typically works well for most architectures. However, we find that image quality is a key factor determining segmentation outcomes. When optimizing for segmentation performance, investing into a well curated dataset to train a standard architecture yields better results than tuning a sophisticated architecture on a smaller dataset or one with lower image quality. We distilled the utility of architectural advances in terms of their clinical relevance therefore providing practical guidance for model choices depending on the circumstances of the clinical setting",2024-06-21T09:12:34Z,2024-06-21T09:12:34Z,http://arxiv.org/abs/2406.14994v1,http://arxiv.org/pdf/2406.14994v1,"eess.IV, cs.CV"
Flemme: A Flexible and Modular Learning Platform for Medical Images,"Guoqing Zhang, Jingyun Yang, Yang Li","As the rapid development of computer vision and the emergence of powerful network backbones and architectures, the application of deep learning in medical imaging has become increasingly significant. Unlike natural images, medical images lack huge volumes of data but feature more modalities, making it difficult to train a general model that has satisfactory performance across various datasets. In practice, practitioners often suffer from manually creating and testing models combining independent backbones and architectures, which is a laborious and time-consuming process. We propose Flemme, a FLExible and Modular learning platform for MEdical images. Our platform separates encoders from the model architectures so that different models can be constructed via various combinations of supported encoders and architectures. We construct encoders using building blocks based on convolution, transformer, and state-space model (SSM) to process both 2D and 3D image patches. A base architecture is implemented following an encoder-decoder style, with several derived architectures for image segmentation, reconstruction, and generation tasks. In addition, we propose a general hierarchical architecture incorporating a pyramid loss to optimize and fuse vertical features. Experiments demonstrate that this simple design leads to an average improvement of 5.60% in Dice score and 7.81% in mean interaction of units (mIoU) for segmentation models, as well as an enhancement of 5.57% in peak signal-to-noise ratio (PSNR) and 8.22% in structural similarity (SSIM) for reconstruction models. We further utilize Flemme as an analytical tool to assess the effectiveness and efficiency of various encoders across different tasks. Code is available at https://github.com/wlsdzyzl/flemme.",2024-08-18T05:47:33Z,2025-01-07T01:23:54Z,http://arxiv.org/abs/2408.09369v2,http://arxiv.org/pdf/2408.09369v2,"eess.IV, cs.CV"
Optimization of Beyond Diagonal RIS: A Universal Framework Applicable to   Arbitrary Architectures,"Zheyu Wu, Bruno Clerckx","Reconfigurable intelligent surfaces (RISs) are envisioned as a promising technology for future wireless communication systems due to their ability to control the propagation environment in a hardware- and energy-efficient way. Recently, the concept of RISs has been extended to beyond diagonal RISs (BD-RISs), which unlock the full potential of RISs thanks to the presence of tunable interconnections between RIS elements. While various algorithms have been proposed for specific BD-RIS architectures, a universal optimization framework applicable to arbitrary architectures is still lacking. In this paper, we bridge this research gap by proposing an architecture-independent framework for BD-RIS optimization, with the main focus on sum-rate maximization and transmit power minimization in multiuser multi-input single-output (MU-MISO) systems. Specifically, we first incorporate BD-RIS architectures into the models by connecting the scattering matrix with the admittance matrix and introducing appropriate constraints to the admittance matrix. The formulated problems are then solved by our custom-designed partially proximal alternating direction method of multipliers (pp-ADMM) algorithms. The pp-ADMM algorithms are computationally efficient, with each subproblem either admitting a closed-form solution or being easily solvable. We further explore the extension of the proposed framework to general utility functions and multiuser multi-input multi-output (MU-MIMO) systems. Simulation results demonstrate that the proposed approaches achieve a better trade-off between performance and computational efficiency compared to existing methods. We also compare the performance of various BD-RIS architectures in MU-MISO systems using the proposed approach, which has not been explored before due to the lack of an architecture-independent framework.",2024-12-20T15:04:57Z,2024-12-20T15:04:57Z,http://arxiv.org/abs/2412.15965v1,http://arxiv.org/pdf/2412.15965v1,"eess.SP, cs.IT, math.IT, math.OC"
On Adversarial Robustness: A Neural Architecture Search perspective,"Chaitanya Devaguptapu, Devansh Agarwal, Gaurav Mittal, Pulkit Gopalani, Vineeth N Balasubramanian","Adversarial robustness of deep learning models has gained much traction in the last few years. Various attacks and defenses are proposed to improve the adversarial robustness of modern-day deep learning architectures. While all these approaches help improve the robustness, one promising direction for improving adversarial robustness is unexplored, i.e., the complex topology of the neural network architecture. In this work, we address the following question: Can the complex topology of a neural network give adversarial robustness without any form of adversarial training?. We answer this empirically by experimenting with different hand-crafted and NAS-based architectures. Our findings show that, for small-scale attacks, NAS-based architectures are more robust for small-scale datasets and simple tasks than hand-crafted architectures. However, as the size of the dataset or the complexity of task increases, hand-crafted architectures are more robust than NAS-based architectures. Our work is the first large-scale study to understand adversarial robustness purely from an architectural perspective. Our study shows that random sampling in the search space of DARTS (a popular NAS method) with simple ensembling can improve the robustness to PGD attack by nearly~12\%. We show that NAS, which is popular for achieving SoTA accuracy, can provide adversarial accuracy as a free add-on without any form of adversarial training. Our results show that leveraging the search space of NAS methods with methods like ensembles can be an excellent way to achieve adversarial robustness without any form of adversarial training. We also introduce a metric that can be used to calculate the trade-off between clean accuracy and adversarial robustness. Code and pre-trained models will be made available at \url{https://github.com/tdchaitanya/nas-robustness}",2020-07-16T16:07:10Z,2021-08-26T09:01:16Z,http://arxiv.org/abs/2007.08428v4,http://arxiv.org/pdf/2007.08428v4,"cs.LG, cs.CR, cs.CV, stat.ML"
Stronger NAS with Weaker Predictors,"Junru Wu, Xiyang Dai, Dongdong Chen, Yinpeng Chen, Mengchen Liu, Ye Yu, Zhangyang Wang, Zicheng Liu, Mei Chen, Lu Yuan","Neural Architecture Search (NAS) often trains and evaluates a large number of architectures. Recent predictor-based NAS approaches attempt to alleviate such heavy computation costs with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Given limited samples, these predictors, however, are far from accurate to locate top architectures due to the difficulty of fitting the huge search space. This paper reflects on a simple yet crucial question: if our final goal is to find the best architecture, do we really need to model the whole space well?. We propose a paradigm shift from fitting the whole architecture space using one strong predictor, to progressively fitting a search path towards the high-performance sub-space through a set of weaker predictors. As a key property of the weak predictors, their probabilities of sampling better architectures keep increasing. Hence we only sample a few well-performed architectures guided by the previously learned predictor and estimate a new better weak predictor. This embarrassingly easy framework, dubbed WeakNAS, produces coarse-to-fine iteration to gradually refine the ranking of sampling space. Extensive experiments demonstrate that WeakNAS costs fewer samples to find top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared to state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all with notable margins, e.g., requiring at least 7.5x less samples to find global optimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost performance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the ImageNet MobileNet Search Space. The code is available at https://github.com/VITA-Group/WeakNAS.",2021-02-21T01:58:43Z,2021-11-03T08:51:36Z,http://arxiv.org/abs/2102.10490v3,http://arxiv.org/pdf/2102.10490v3,"cs.LG, cs.CV, stat.ML"
NASTransfer: Analyzing Architecture Transferability in Large Scale   Neural Architecture Search,"Rameswar Panda, Michele Merler, Mayoore Jaiswal, Hui Wu, Kandan Ramakrishnan, Ulrich Finkler, Chun-Fu Chen, Minsik Cho, David Kung, Rogerio Feris, Bishwaranjan Bhattacharjee","Neural Architecture Search (NAS) is an open and challenging problem in machine learning. While NAS offers great promise, the prohibitive computational demand of most of the existing NAS methods makes it difficult to directly search the architectures on large-scale tasks. The typical way of conducting large scale NAS is to search for an architectural building block on a small dataset (either using a proxy set from the large dataset or a completely different small scale dataset) and then transfer the block to a larger dataset. Despite a number of recent results that show the promise of transfer from proxy datasets, a comprehensive evaluation of different NAS methods studying the impact of different source datasets has not yet been addressed. In this work, we propose to analyze the architecture transferability of different NAS methods by performing a series of experiments on large scale benchmarks such as ImageNet1K and ImageNet22K. We find that: (i) The size and domain of the proxy set does not seem to influence architecture performance on the target dataset. On average, transfer performance of architectures searched using completely different small datasets (e.g., CIFAR10) perform similarly to the architectures searched directly on proxy target datasets. However, design of proxy sets has considerable impact on rankings of different NAS methods. (ii) While different NAS methods show similar performance on a source dataset (e.g., CIFAR10), they significantly differ on the transfer performance to a large dataset (e.g., ImageNet1K). (iii) Even on large datasets, random sampling baseline is very competitive, but the choice of the appropriate combination of proxy set and search strategy can provide significant improvement over it. We believe that our extensive empirical analysis will prove useful for future design of NAS algorithms.",2020-06-23T20:28:42Z,2021-02-12T02:55:35Z,http://arxiv.org/abs/2006.13314v2,http://arxiv.org/pdf/2006.13314v2,"cs.CV, cs.LG, cs.NE, 68T05, I.2.6; I.4"
A Classification of $G$-invariant Shallow Neural Networks,"Devanshu Agrawal, James Ostrowski","When trying to fit a deep neural network (DNN) to a $G$-invariant target function with $G$ a group, it only makes sense to constrain the DNN to be $G$-invariant as well. However, there can be many different ways to do this, thus raising the problem of ``$G$-invariant neural architecture design'': What is the optimal $G$-invariant architecture for a given problem? Before we can consider the optimization problem itself, we must understand the search space, the architectures in it, and how they relate to one another. In this paper, we take a first step towards this goal; we prove a theorem that gives a classification of all $G$-invariant single-hidden-layer or ``shallow'' neural network ($G$-SNN) architectures with ReLU activation for any finite orthogonal group $G$, and we prove a second theorem that characterizes the inclusion maps or ``network morphisms'' between the architectures that can be leveraged during neural architecture search (NAS). The proof is based on a correspondence of every $G$-SNN to a signed permutation representation of $G$ acting on the hidden neurons; the classification is equivalently given in terms of the first cohomology classes of $G$, thus admitting a topological interpretation. The $G$-SNN architectures corresponding to nontrivial cohomology classes have, to our knowledge, never been explicitly identified in the literature previously. Using a code implementation, we enumerate the $G$-SNN architectures for some example groups $G$ and visualize their structure. Finally, we prove that architectures corresponding to inequivalent cohomology classes coincide in function space only when their weight matrices are zero, and we discuss the implications of this for NAS.",2022-05-18T21:18:16Z,2023-01-07T14:14:29Z,http://arxiv.org/abs/2205.09219v5,http://arxiv.org/pdf/2205.09219v5,"cs.LG, stat.ML"
ChangeMamba: Remote Sensing Change Detection With Spatiotemporal State   Space Model,"Hongruixuan Chen, Jian Song, Chengxi Han, Junshi Xia, Naoto Yokoya","Convolutional neural networks (CNN) and Transformers have made impressive progress in the field of remote sensing change detection (CD). However, both architectures have inherent shortcomings: CNN are constrained by a limited receptive field that may hinder their ability to capture broader spatial contexts, while Transformers are computationally intensive, making them costly to train and deploy on large datasets. Recently, the Mamba architecture, based on state space models, has shown remarkable performance in a series of natural language processing tasks, which can effectively compensate for the shortcomings of the above two architectures. In this paper, we explore for the first time the potential of the Mamba architecture for remote sensing CD tasks. We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD), semantic change detection (SCD), and building damage assessment (BDA), respectively. All three frameworks adopt the cutting-edge Visual Mamba architecture as the encoder, which allows full learning of global spatial contextual information from the input images. For the change decoder, which is available in all three architectures, we propose three spatio-temporal relationship modeling mechanisms, which can be naturally combined with the Mamba architecture and fully utilize its attribute to achieve spatio-temporal interaction of multi-temporal features, thereby obtaining accurate change information. On five benchmark datasets, our proposed frameworks outperform current CNN- and Transformer-based approaches without using any complex training strategies or tricks, fully demonstrating the potential of the Mamba architecture in CD tasks. Further experiments show that our architecture is quite robust to degraded data. The source code will be available in https://github.com/ChenHongruixuan/MambaCD",2024-04-04T13:06:25Z,2024-12-30T06:28:34Z,http://arxiv.org/abs/2404.03425v7,http://arxiv.org/pdf/2404.03425v7,"eess.IV, cs.AI, cs.CV"
A Blueprint for Building Serverless Applications on the Net,"A. I. Khan, R. Spindler","A peer-to-peer application architecture is proposed that has the potential to eliminate the back-end servers for hosting services on the Internet. The proposed application architecture has been modeled as a distributed system for delivering an Internet service. The service thus created, though chaotic and fraught with uncertainties, would be highly scalable and capable of achieving unprecedented levels of robustness and viability with the increase in the number of the users. The core issues relating to the architecture, such as service discovery, distributed application architecture components, and inter-application communications, have been analysed. It is shown that the communications for the coordination of various functions, among the cooperating instances of the application, may be optimised using a divide-and-conquer strategy. Finally, the areas where future work needs to be directed have been identified.",2001-07-05T07:03:48Z,2001-07-05T07:03:48Z,http://arxiv.org/abs/cs/0107009v1,http://arxiv.org/pdf/cs/0107009v1,"cs.DC, cs.NI, C.2.4; C.1.4; D.2.11"
Fedora: An Architecture for Complex Objects and their Relationships,"Carl Lagoze, Sandy Payette, Edwin Shin, Chris Wilper","The Fedora architecture is an extensible framework for the storage, management, and dissemination of complex objects and the relationships among them. Fedora accommodates the aggregation of local and distributed content into digital objects and the association of services with objects. This al-lows an object to have several accessible representations, some of them dy-namically produced. The architecture includes a generic RDF-based relation-ship model that represents relationships among objects and their components. Queries against these relationships are supported by an RDF triple store. The architecture is implemented as a web service, with all aspects of the complex object architecture and related management functions exposed through REST and SOAP interfaces. The implementation is available as open-source soft-ware, providing the foundation for a variety of end-user applications for digital libraries, archives, institutional repositories, and learning object systems.",2005-01-07T17:57:05Z,2005-08-23T13:26:04Z,http://arxiv.org/abs/cs/0501012v6,http://arxiv.org/pdf/cs/0501012v6,"cs.DL, cs.MM, H.3.7"
An Application of the Mobile Transient Internet Architecture to IP   Mobility and Inter-Operability,"Joud Khoury, Henry N Jerez, Nicolas Nehme-Antoun, Chaouki Abdallah","We introduce an application of a mobile transient network architecture on top of the current Internet. This paper is an application extension to a conceptual mobile network architecture. It attempts to specifically reinforce some of the powerful notions exposed by the architecture from an application perspective. Of these notions, we explore the network expansion layer, an overlay of components and services, that enables a persistent identification network and other required services. The overlay abstraction introduces several benefits of which mobility and communication across heterogenous network structures are of interest to this paper. We present implementations of several components and protocols including gateways, Agents and the Open Device Access Protocol. Our present identification network implementation exploits the current implementation of the Handle System through the use of distributed, global and persistent identifiers called handles. Handles are used to identify and locate devices and services abstracting any physical location or network association from the communicating ends. A communication framework is finally demonstrated that would allow for mobile devices on the public Internet to have persistent identifiers and thus be persistently accessible either directly or indirectly. This application expands IP inter-operability beyond its current boundaries.",2006-10-13T22:12:59Z,2006-10-13T22:12:59Z,http://arxiv.org/abs/cs/0610087v1,http://arxiv.org/pdf/cs/0610087v1,"cs.NI, C.2.6"
Static Address Generation Easing: a Design Methodology for Parallel   Interleaver Architectures,"Cyrille Chavet, Philippe Coussy, Eric Martin, Pascal Urard","For high throughput applications, turbo-like iterative decoders are implemented with parallel architectures. However, to be efficient parallel architectures require to avoid collision accesses i.e. concurrent read/write accesses should not target the same memory block. This consideration applies to the two main classes of turbo-like codes which are Low Density Parity Check (LDPC) and Turbo-Codes. In this paper we propose a methodology which finds a collision-free mapping of the variables in the memory banks and which optimizes the resulting interleaving architecture. Finally, we show through a pedagogical example the interest of our approach compared to state-of-the-art techniques.",2010-02-21T18:51:31Z,2010-02-21T18:51:31Z,http://arxiv.org/abs/1002.3990v1,http://arxiv.org/pdf/1002.3990v1,"cs.AR, cs.IT, math.IT"
Magnetohydrodynamics on Heterogeneous architectures: a performance   comparison,"Bijia Pang, Ue-li Pen, Michael Perrone","We present magneto-hydrodynamic simulation results for heterogeneous systems. Heterogeneous architectures combine high floating point performance many-core units hosted in conventional server nodes. Examples include Graphics Processing Units (GPU's) and Cell. They have potentially large gains in performance, at modest power and monetary cost. We implemented a magneto-hydrodynamic (MHD) simulation code on a variety of heterogeneous and multi-core architectures --- multi-core x86, Cell, Nvidia and ATI GPU --- in different languages, FORTRAN, C, Cell, CUDA and OpenCL. We present initial performance results for these systems. To our knowledge, this is the widest comparison of heterogeneous systems for MHD simulations. We review the different challenges faced in each architecture, and potential bottlenecks. We conclude that substantial gains in performance over traditional systems are possible, and in particular that is possible to extract a greater percentage of peak theoretical performance from some systems when compared to x86 architectures.",2010-04-10T04:14:15Z,2010-04-10T04:14:15Z,http://arxiv.org/abs/1004.1680v1,http://arxiv.org/pdf/1004.1680v1,"cs.PF, astro-ph.IM, physics.comp-ph"
A Real-Time Model-Based Reinforcement Learning Architecture for Robot   Control,"Todd Hester, Michael Quinlan, Peter Stone","Reinforcement Learning (RL) is a method for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few actions, while continually taking those actions in real-time. Existing model-based RL methods learn in relatively few actions, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.",2011-05-09T18:17:20Z,2011-05-21T14:15:28Z,http://arxiv.org/abs/1105.1749v2,http://arxiv.org/pdf/1105.1749v2,"cs.AI, cs.RO, cs.SE, D.2.11; I.2.6"
A Conceptual Framework to Analyze Enterprise Business Solutions from a   Software Architecture Perspective,Basem Y. Alkazemi,"The architectural aspects of software systems are not always explicitly exposed to customers when a product is presented to them by software vendors. Therefore, customers might be put at a major risk if new emerging business needs come to light that require modification of some of the core business processes within their organizations. So they might need to replace their existing systems or re-architect old ones to comply with new architectural standards. This paper describes a proposed framework that helps organizations to build a comprehensive view of their system architecture prior to dealing with vendors. Consequently, every organization can have a reference model that facilitates negotiation and communication with software vendors. The paper applies the proposed framework to an organization in the region of Saudi Arabia to validate its applicability and generates an architectural design for their software systems.",2012-10-14T04:10:32Z,2012-10-14T04:10:32Z,http://arxiv.org/abs/1210.3756v1,http://arxiv.org/pdf/1210.3756v1,"cs.SE, 68N99"
A New Approach for Quality Management in Pervasive Computing   Environments,"Alti Adel, Roose Phillipe","This paper provides an extension of MDA called Context-aware Quality Model Driven Architecture (CQ-MDA) which can be used for quality control in pervasive computing environments. The proposed CQ-MDA approach based on ContextualArchRQMM (Contextual ARCHitecture Quality Requirement MetaModel), being an extension to the MDA, allows for considering quality and resources-awareness while conducting the design process. The contributions of this paper are a meta-model for architecture quality control of context-aware applications and a model driven approach to separate architecture concerns from context and quality concerns and to configure reconfigurable software architectures of distributed systems. To demonstrate the utility of our approach, we use a videoconference system.",2013-06-17T19:24:55Z,2013-06-17T19:24:55Z,http://arxiv.org/abs/1306.3959v1,http://arxiv.org/pdf/1306.3959v1,"cs.SE, H.5.1; D.2"
Parallel Programming Model for the Epiphany Many-Core Coprocessor Using   Threaded MPI,"James A. Ross, David A. Richie, Song J. Park, Dale R. Shires","The Adapteva Epiphany many-core architecture comprises a 2D tiled mesh Network-on-Chip (NoC) of low-power RISC cores with minimal uncore functionality. It offers high computational energy efficiency for both integer and floating point calculations as well as parallel scalability. Yet despite the interesting architectural features, a compelling programming model has not been presented to date. This paper demonstrates an efficient parallel programming model for the Epiphany architecture based on the Message Passing Interface (MPI) standard. Using MPI exploits the similarities between the Epiphany architecture and a conventional parallel distributed cluster of serial cores. Our approach enables MPI codes to execute on the RISC array processor with little modification and achieve high performance. We report benchmark results for the threaded MPI implementation of four algorithms (dense matrix-matrix multiplication, N-body particle interaction, a five-point 2D stencil update, and 2D FFT) and highlight the importance of fast inter-core communication for the architecture.",2015-06-17T19:39:41Z,2015-06-17T19:39:41Z,http://arxiv.org/abs/1506.05442v1,http://arxiv.org/pdf/1506.05442v1,"cs.DC, C.1.4; D.1.3"
Processing In-memory realization using Quantum Dot Cellular Automata,"P. P. Chougule, B. Sen, T. D. Dongale","The present manuscript deals with the realization of Processing In-memory (PIM) computing architecture using Quantum Dot Cellular Automata (QCA) and Akers array. The PIM computing architecture becomes popular due to its effective framework for storage and computation of data in a single unit. Here, we illustrate two input NAND and NOR gate with the help of QCA based Akers Array as a case study. The QCA flip-flop is used as a primitive cell to design PIM architecture. The results suggested that both the gate have minimum power dissipation. The polarization results of proposed architecture suggested that the signals are in good control. The foot print of the primitive cell equals to 0.04 micron^2, which is smaller than conventional CMOS primitive cell. The combination of QCA and Akers array provides many additional benefits over the conventional architecture like reduction in the power consumption and feature size, furthermore, it also improves the computational speed.",2016-07-13T11:37:04Z,2016-07-13T11:37:04Z,http://arxiv.org/abs/1607.05065v1,http://arxiv.org/pdf/1607.05065v1,"cs.ET, 03G12, 81P10, 81P68, B.3.4; B.3.2; B.3.0"
QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures,Tapabrata Ghosh,"We present QuickNet, a fast and accurate network architecture that is both faster and significantly more accurate than other fast deep architectures like SqueezeNet. Furthermore, it uses less parameters than previous networks, making it more memory efficient. We do this by making two major modifications to the reference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable convolutions and 2) The use of parametric rectified linear units. We make the observation that parametric rectified linear units are computationally equivalent to leaky rectified linear units at test time and the observation that separable convolutions can be interpreted as a compressed Inception network (Chollet, 2016). Using these observations, we derive a network architecture, which we call QuickNet, that is both faster and more accurate than previous models. Our architecture provides at least four major advantages: (1) A smaller model size, which is more tenable on memory constrained systems; (2) A significantly faster network which is more tenable on computationally constrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10 Dataset which outperforms all but one result published so far, although we note that our works are orthogonal approaches and can be combined (4) Orthogonality to previous model compression approaches allowing for further speed gains to be realized.",2017-01-09T18:29:07Z,2017-01-12T07:44:17Z,http://arxiv.org/abs/1701.02291v2,http://arxiv.org/pdf/1701.02291v2,"cs.LG, stat.ML"
RaPro: A Novel 5G Rapid Prototyping System Architecture,"Xi Yang, Zhichao Huang, Bin Han, Senjie Zhang, Chao-Kai Wen, Feifei Gao, Shi Jin","We propose a novel fifth-generation (5G) rapid prototyping (RaPro) system architecture by combining FPGA-privileged modules from a software defined radio (or FPGA-coprocessor) and high-level programming language for advanced algorithms from multi-core general purpose processors. The proposed system architecture exhibits excellent flexibility and scalability in the development of a 5G prototyping system. As a proof of concept, a multi-user full-dimension multiple-input and multiple-output system is established based on the proposed architecture. Experimental results demonstrate the superiority of the proposed architecture in large-scale antenna and wideband communication systems.",2017-04-15T03:43:18Z,2017-04-15T03:43:18Z,http://arxiv.org/abs/1704.04573v1,http://arxiv.org/pdf/1704.04573v1,"cs.IT, math.IT"
Parasitic Bipolar Leakage in III-V FETs: Impact of Substrate   Architecture,"Borna Obradovic, Titash Rakshit, Wei-E Wang, Dennis Lin, Niamh Waldron, Nadine Collaert, Mark S. Rodder","InGaAs-based Gate-all-Around (GAA) FETs with moderate to high In content are shown experimentally and theoretically to be unsuitable for low-leakage advanced CMOS nodes. The primary cause for this is the large leakage penalty induced by the Parasitic Bipolar Effect (PBE), which is seen to be particularly difficult to remedy in GAA architectures. Experimental evidence of PBE in In70Ga30As GAA FETs is demonstrated, along with a simulation-based analysis of the PBE behavior. The impact of PBE is investigated by simulation for alternative device architectures, such as bulk FinFETs and FinFETs-on-insulator. PBE is found to be non-negligible in all standard InGaAs FET designs. Practical PBE metrics are introduced and the design of a substrate architecture for PBE suppression is elucidated. Finally, it is concluded that the GAA architecture is not suitable for low-leakage InGaAs FETs; a bulk FinFET is better suited for the role.",2017-05-17T18:55:23Z,2017-05-17T18:55:23Z,http://arxiv.org/abs/1705.06731v1,http://arxiv.org/pdf/1705.06731v1,"physics.app-ph, cond-mat.mes-hall"
Investigating the Impact of CNN Depth on Neonatal Seizure Detection   Performance,"Alison O'Shea, Gordon Lightbody, Geraldine Boylan, Andriy Temko","This study presents a novel, deep, fully convolutional architecture which is optimized for the task of EEG-based neonatal seizure detection. Architectures of different depths were designed and tested; varying network depth impacts convolutional receptive fields and the corresponding learned feature complexity. Two deep convolutional networks are compared with a shallow SVM-based neonatal seizure detector, which relies on the extraction of hand-crafted features. On a large clinical dataset, of over 800 hours of multichannel unedited EEG, containing 1389 seizure events, the deep 11-layer architecture significantly outperforms the shallower architectures, improving the AUC90 from 82.6% to 86.8%. Combining the end-to-end deep architecture with the feature-based shallow SVM further improves the AUC90 to 87.6%. The fusion of classifiers of different depths gives greatly improved performance and reduced variability, making the combined classifier more clinically reliable.",2018-06-08T09:34:22Z,2018-06-08T09:34:22Z,http://arxiv.org/abs/1806.03044v1,http://arxiv.org/pdf/1806.03044v1,"stat.ML, cs.LG"
Auto-Meta: Automated Gradient Based Meta Learner Search,"Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, Jiwon Kim","Fully automating machine learning pipelines is one of the key challenges of current artificial intelligence research, since practical machine learning often requires costly and time-consuming human-powered processes such as model design, algorithm development, and hyperparameter tuning. In this paper, we verify that automated architecture search synergizes with the effect of gradient-based meta learning. We adopt the progressive neural architecture search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal architectures for meta-learners. The gradient based meta-learner whose architecture was automatically found achieved state-of-the-art results on the 5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy, which is $11.54\%$ improvement over the result obtained by the first gradient-based meta-learner called MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is the first successful neural architecture search implementation in the context of meta learning.",2018-06-11T04:28:02Z,2018-12-10T19:02:53Z,http://arxiv.org/abs/1806.06927v2,http://arxiv.org/pdf/1806.06927v2,"cs.LG, cs.AI, cs.CV, stat.ML"
Shallow vs deep learning architectures for white matter lesion   segmentation in the early stages of multiple sclerosis,"Francesco La Rosa, Mário João Fartaria, Tobias Kober, Jonas Richiardi, Cristina Granziera, Jean-Philippe Thiran, Meritxell Bach Cuadra","In this work, we present a comparison of a shallow and a deep learning architecture for the automated segmentation of white matter lesions in MR images of multiple sclerosis patients. In particular, we train and test both methods on early stage disease patients, to verify their performance in challenging conditions, more similar to a clinical setting than what is typically provided in multiple sclerosis segmentation challenges. Furthermore, we evaluate a prototype naive combination of the two methods, which refines the final segmentation. All methods were trained on 32 patients, and the evaluation was performed on a pure test set of 73 cases. Results show low lesion-wise false positives (30%) for the deep learning architecture, whereas the shallow architecture yields the best Dice coefficient (63%) and volume difference (19%). Combining both shallow and deep architectures further improves the lesion-wise metrics (69% and 26% lesion-wise true and false positive rate, respectively).",2018-09-10T08:50:34Z,2018-09-10T08:50:34Z,http://arxiv.org/abs/1809.03185v1,http://arxiv.org/pdf/1809.03185v1,"cs.LG, cs.CV, stat.ML"
An Optical Frontend for a Convolutional Neural Network,"Shane Colburn, Yi Chu, Eli Shlizerman, Arka Majumdar","The parallelism of optics and the miniaturization of optical components using nanophotonic structures, such as metasurfaces present a compelling alternative to electronic implementations of convolutional neural networks. The lack of a low-power optical nonlinearity, however, requires slow and energy-inefficient conversions between the electronic and optical domains. Here, we design an architecture which utilizes a single electrical to optical conversion by designing a free-space optical frontend unit that implements the linear operations of the first layer with the subsequent layers realized electronically. Speed and power analysis of the architecture indicates that the hybrid photonic-electronic architecture outperforms sole electronic architecture for large image sizes and kernels. Benchmarking of the photonic-electronic architecture on a modified version of AlexNet achieves a classification accuracy of 87% on images from the Kaggle Cats and Dogs challenge database.",2018-12-23T06:46:19Z,2019-01-14T02:16:45Z,http://arxiv.org/abs/1901.03661v2,http://arxiv.org/pdf/1901.03661v2,"cs.CV, cs.ET, cs.LG, physics.optics"
Robust Deep Multi-Modal Sensor Fusion using Fusion Weight Regularization   and Target Learning,"Myung Seok Shim, Chenye Zhao, Yang Li, Xuchong Zhang, Wenrui Zhang, Peng Li","Sensor fusion has wide applications in many domains including health care and autonomous systems. While the advent of deep learning has enabled promising multi-modal fusion of high-level features and end-to-end sensor fusion solutions, existing deep learning based sensor fusion techniques including deep gating architectures are not always resilient, leading to the issue of fusion weight inconsistency. We propose deep multi-modal sensor fusion architectures with enhanced robustness particularly under the presence of sensor failures. At the core of our gating architectures are fusion weight regularization and fusion target learning operating on auxiliary unimodal sensing networks appended to the main fusion model. The proposed regularized gating architectures outperform the existing deep learning architectures with and without gating under both clean and corrupted sensory inputs resulted from sensor failures. The demonstrated improvements are particularly pronounced when one or more multiple sensory modalities are corrupted.",2019-01-29T23:32:20Z,2021-04-22T02:38:26Z,http://arxiv.org/abs/1901.10610v3,http://arxiv.org/pdf/1901.10610v3,"cs.LG, stat.ML"
Capacity allocation analysis of neural networks: A tool for principled   architecture design,Jonathan Donier,"Designing neural network architectures is a task that lies somewhere between science and art. For a given task, some architectures are eventually preferred over others, based on a mix of intuition, experience, experimentation and luck. For many tasks, the final word is attributed to the loss function, while for some others a further perceptual evaluation is necessary to assess and compare performance across models. In this paper, we introduce the concept of capacity allocation analysis, with the aim of shedding some light on what network architectures focus their modelling capacity on, when used on a given task. We focus more particularly on spatial capacity allocation, which analyzes a posteriori the effective number of parameters that a given model has allocated for modelling dependencies on a given point or region in the input space, in linear settings. We use this framework to perform a quantitative comparison between some classical architectures on various synthetic tasks. Finally, we consider how capacity allocation might translate in non-linear settings.",2019-02-12T16:43:36Z,2019-02-12T16:43:36Z,http://arxiv.org/abs/1902.04485v1,http://arxiv.org/pdf/1902.04485v1,"cs.LG, stat.ML"
Improving Neural Architecture Search Image Classifiers via Ensemble   Learning,"Vladimir Macko, Charles Weill, Hanna Mazzawi, Javier Gonzalvo","Finding the best neural network architecture requires significant time, resources, and human expertise. These challenges are partially addressed by neural architecture search (NAS) which is able to find the best convolutional layer or cell that is then used as a building block for the network. However, once a good building block is found, manual design is still required to assemble the final architecture as a combination of multiple blocks under a predefined parameter budget constraint. A common solution is to stack these blocks into a single tower and adjust the width and depth to fill the parameter budget. However, these single tower architectures may not be optimal. Instead, in this paper we present the AdaNAS algorithm, that uses ensemble techniques to compose a neural network as an ensemble of smaller networks automatically. Additionally, we introduce a novel technique based on knowledge distillation to iteratively train the smaller networks using the previous ensemble as a teacher. Our experiments demonstrate that ensembles of networks improve accuracy upon a single neural network while keeping the same number of parameters. Our models achieve comparable results with the state-of-the-art on CIFAR-10 and sets a new state-of-the-art on CIFAR-100.",2019-03-14T20:17:33Z,2019-03-14T20:17:33Z,http://arxiv.org/abs/1903.06236v1,http://arxiv.org/pdf/1903.06236v1,"cs.LG, stat.ML"
Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional   Networks,"Sitao Luan, Mingde Zhao, Xiao-Wen Chang, Doina Precup","Recently, neural network based approaches have achieved significant improvement for solving large, complex, graph-structured problems. However, their bottlenecks still need to be addressed, and the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper, we theoretically analyze how existing Graph Convolutional Networks (GCNs) have limited expressive power due to the constraint of the activation functions and their architectures. We generalize spectral graph convolution and deep GCN in block Krylov subspace forms and devise two architectures, both with the potential to be scaled deeper but each making use of the multi-scale information in different ways. We further show that the equivalence of these two architectures can be established under certain conditions. On several node classification tasks, with or without the help of validation, the two new architectures achieve better performance compared to many state-of-the-art methods.",2019-06-05T17:59:39Z,2019-09-08T16:22:01Z,http://arxiv.org/abs/1906.02174v3,http://arxiv.org/pdf/1906.02174v3,"cs.LG, cs.AI, stat.ML"
A Partially Reversible U-Net for Memory-Efficient Volumetric Image   Segmentation,"Robin Brügger, Christian F. Baumgartner, Ender Konukoglu","One of the key drawbacks of 3D convolutional neural networks for segmentation is their memory footprint, which necessitates compromises in the network architecture in order to fit into a given memory budget. Motivated by the RevNet for image classification, we propose a partially reversible U-Net architecture that reduces memory consumption substantially. The reversible architecture allows us to exactly recover each layer's outputs from the subsequent layer's ones, eliminating the need to store activations for backpropagation. This alleviates the biggest memory bottleneck and enables very deep (theoretically infinitely deep) 3D architectures. On the BraTS challenge dataset, we demonstrate substantial memory savings. We further show that the freed memory can be used for processing the whole field-of-view (FOV) instead of patches. Increasing network depth led to higher segmentation accuracy while growing the memory footprint only by a very small fraction, thanks to the partially reversible architecture.",2019-06-14T12:04:46Z,2019-06-20T15:02:02Z,http://arxiv.org/abs/1906.06148v2,http://arxiv.org/pdf/1906.06148v2,"cs.CV, eess.IV"
XNAS: Neural Architecture Search with Expert Advice,"Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, Lihi Zelnik-Manor","This paper introduces a novel optimization method for differential neural architecture search, based on the theory of prediction with expert advice. Its optimization criterion is well fitted for an architecture-selection, i.e., it minimizes the regret incurred by a sub-optimal selection of operations. Unlike previous search relaxations, that require hard pruning of architectures, our method is designed to dynamically wipe out inferior architectures and enhance superior ones. It achieves an optimal worst-case regret bound and suggests the use of multiple learning-rates, based on the amount of information carried by the backward gradients. Experiments show that our algorithm achieves a strong performance over several image classification datasets. Specifically, it obtains an error rate of 1.6% for CIFAR-10, 24% for ImageNet under mobile settings, and achieves state-of-the-art results on three additional datasets.",2019-06-19T12:00:00Z,2019-06-19T12:00:00Z,http://arxiv.org/abs/1906.08031v1,http://arxiv.org/pdf/1906.08031v1,"cs.LG, cs.CV, math.OC, stat.ML"
An Underparametrized Deep Decoder Architecture for Graph Signals,"Samuel Rey, Antonio G. Marques, Santiago Segarra","While deep convolutional architectures have achieved remarkable results in a gamut of supervised applications dealing with images and speech, recent works show that deep untrained non-convolutional architectures can also outperform state-of-the-art methods in several tasks such as image compression and denoising. Motivated by the fact that many contemporary datasets have an irregular structure different from a 1D/2D grid, this paper generalizes untrained and underparametrized non-convolutional architectures to signals defined over irregular domains represented by graphs. The proposed architecture consists of a succession of layers, each of them implementing an upsampling operator, a linear feature combination, and a scalar nonlinearity. A novel element is the incorporation of upsampling operators accounting for the structure of the supporting graph, which is achieved by considering a systematic graph coarsening approach based on hierarchical clustering. The numerical results carried out in synthetic and real-world datasets showcase that the reconstruction performance can improve drastically if the information of the supporting graph topology is taken into account.",2019-08-02T14:21:43Z,2020-01-14T12:30:33Z,http://arxiv.org/abs/1908.00878v2,http://arxiv.org/pdf/1908.00878v2,"eess.SP, cs.LG"
Feature Partitioning for Efficient Multi-Task Architectures,"Alejandro Newell, Lu Jiang, Chong Wang, Li-Jia Li, Jia Deng","Multi-task learning holds the promise of less data, parameters, and time than training of separate models. We propose a method to automatically search over multi-task architectures while taking resource constraints into consideration. We propose a search space that compactly represents different parameter sharing strategies. This provides more effective coverage and sampling of the space of multi-task architectures. We also present a method for quick evaluation of different architectures by using feature distillation. Together these contributions allow us to quickly optimize for efficient multi-task models. We benchmark on Visual Decathlon, demonstrating that we can automatically search for and identify multi-task architectures that effectively make trade-offs between task resource requirements while achieving a high level of final performance.",2019-08-12T19:06:32Z,2019-08-12T19:06:32Z,http://arxiv.org/abs/1908.04339v1,http://arxiv.org/pdf/1908.04339v1,"cs.LG, cs.CV, stat.ML"
Language models and Automated Essay Scoring,"Pedro Uria Rodriguez, Amir Jafari, Christopher M. Ormerod","In this paper, we present a new comparative study on automatic essay scoring (AES). The current state-of-the-art natural language processing (NLP) neural network architectures are used in this work to achieve above human-level accuracy on the publicly available Kaggle AES dataset. We compare two powerful language models, BERT and XLNet, and describe all the layers and network architectures in these models. We elucidate the network architectures of BERT and XLNet using clear notation and diagrams and explain the advantages of transformer architectures over traditional recurrent neural network architectures. Linear algebra notation is used to clarify the functions of transformers and attention mechanisms. We compare the results with more traditional methods, such as bag of words (BOW) and long short term memory (LSTM) networks.",2019-09-18T18:50:18Z,2019-09-18T18:50:18Z,http://arxiv.org/abs/1909.09482v1,http://arxiv.org/pdf/1909.09482v1,"cs.CL, cs.LG, stat.ML"
Neural Architecture Search Over a Graph Search Space,"Stanisław Jastrzębski, Quentin de Laroussilhe, Mingxing Tan, Xiao Ma, Neil Houlsby, Andrea Gesmundo","Neural Architecture Search (NAS) enabled the discovery of state-of-the-art architectures in many domains. However, the success of NAS depends on the definition of the search space. Current search spaces are defined as a static sequence of decisions and a set of available actions for each decision. Each possible sequence of actions defines an architecture. We propose a more expressive class of search space: directed graphs. In our formalism, each decision is a vertex and each action is an edge. This allows us to model iterative and branching architecture design decisions. We demonstrate in simulation, and on image classification experiments, basic iterative and branching search structures, and show that the graph representation improves sample efficiency.",2018-12-27T09:04:17Z,2019-07-31T16:19:31Z,http://arxiv.org/abs/1812.10666v2,http://arxiv.org/pdf/1812.10666v2,"cs.LG, stat.ML"
Meta-learning Convolutional Neural Architectures for Multi-target   Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset,"Martin Mundt, Sagnik Majumder, Sreenivas Murali, Panagiotis Panetsos, Visvanathan Ramesh","Recognition of defects in concrete infrastructure, especially in bridges, is a costly and time consuming crucial first step in the assessment of the structural integrity. Large variation in appearance of the concrete material, changing illumination and weather conditions, a variety of possible surface markings as well as the possibility for different types of defects to overlap, make it a challenging real-world task. In this work we introduce the novel COncrete DEfect BRidge IMage dataset (CODEBRIM) for multi-target classification of five commonly appearing concrete defects. We investigate and compare two reinforcement learning based meta-learning approaches, MetaQNN and efficient neural architecture search, to find suitable convolutional neural network architectures for this challenging multi-class multi-target task. We show that learned architectures have fewer overall parameters in addition to yielding better multi-target accuracy in comparison to popular neural architectures from the literature evaluated in the context of our application.",2019-04-02T13:08:33Z,2019-04-02T13:08:33Z,http://arxiv.org/abs/1904.08486v1,http://arxiv.org/pdf/1904.08486v1,"cs.CV, cs.LG, stat.ML"
EmbraceNet: A robust deep learning architecture for multimodal   classification,"Jun-Ho Choi, Jong-Seok Lee","Classification using multimodal data arises in many machine learning applications. It is crucial not only to model cross-modal relationship effectively but also to ensure robustness against loss of part of data or modalities. In this paper, we propose a novel deep learning-based multimodal fusion architecture for classification tasks, which guarantees compatibility with any kind of learning models, deals with cross-modal information carefully, and prevents performance degradation due to partial absence of data. We employ two datasets for multimodal classification tasks, build models based on our architecture and other state-of-the-art models, and analyze their performance on various situations. The results show that our architecture outperforms the other multimodal fusion architectures when some parts of data are not available.",2019-04-19T04:46:29Z,2019-04-19T04:46:29Z,http://arxiv.org/abs/1904.09078v1,http://arxiv.org/pdf/1904.09078v1,"cs.LG, stat.ML"
Towards a Software Architecture Maturity Model for Improving   Ultra-Large-Scale Systems Interoperability,"Shervin Ostadzadeh, Fereidoon Shams","For the last two decades, software architecture has been adopted as one of the main viable solutions to address the ever-increasing demands in the design and development of software systems. Nevertheless, the rapidly growing utilization of communication networks and interconnections among software systems have introduced some critical challenges, which need to be handled in order to fully unleash the potential of these systems. In this respect, Ultra-Large-Scale (ULS) systems, generally considered as a system of systems, have gained considerable attention, since their scale is incomparable to the traditional systems. The scale of ULS systems makes drastic changes in various aspects of system development. As a result, it requires that we broaden our understanding of software architectures and the ways we structure them. In this paper, we investigate the lack of an architectural maturity model framework for ULS system interoperability, and propose an architectural maturity model framework to improve ULS system interoperability.",2014-01-22T18:56:42Z,2014-01-22T18:56:42Z,http://arxiv.org/abs/1401.5752v1,http://arxiv.org/pdf/1401.5752v1,"cs.SE, D.2.9; D.2.11; D.2.12"
NADS-Net: A Nimble Architecture for Driver and Seat Belt Detection via   Convolutional Neural Networks,"Sehyun Chun, Nima Hamidi Ghalehjegh, Joseph B. Choi, Chris W. Schwarz, John G. Gaspar, Daniel V. McGehee, Stephen S. Baek","A new convolutional neural network (CNN) architecture for 2D driver/passenger pose estimation and seat belt detection is proposed in this paper. The new architecture is more nimble and thus more suitable for in-vehicle monitoring tasks compared to other generic pose estimation algorithms. The new architecture, named NADS-Net, utilizes the feature pyramid network (FPN) backbone with multiple detection heads to achieve the optimal performance for driver/passenger state detection tasks. The new architecture is validated on a new data set containing video clips of 100 drivers in 50 driving sessions that are collected for this study. The detection performance is analyzed under different demographic, appearance, and illumination conditions. The results presented in this paper may provide meaningful insights for the autonomous driving research community and automotive industry for future algorithm development and data collection.",2019-10-08T21:16:28Z,2019-10-08T21:16:28Z,http://arxiv.org/abs/1910.03695v1,http://arxiv.org/pdf/1910.03695v1,"cs.CV, cs.LG, I.4; I.2.1"
Performance evaluation of an integrated photonic convolutional neural   network based on delay buffering and wavelength division multiplexing,"Shaofu Xu, Jing Wang, Weiwen Zou","Photonic technologies have shown a promising way to build high-speed and high-energy-efficiency neural network accelerators. In previously presented photonic neural networks, architectures are mainly designed for fully-connected layers. When convolutional layers are executed in such neural networks, the large-scale electrooptic modulation array heavily increases the energy dissipation on chip. To increase the energy efficiency, here we show an integrated photonic architecture specifically for convolutional layer calculations. Optical delay lines replace electronics to execute data manipulations on optical chip, reducing the scale of electro-optic modulation array. Consequently, the energy dissipation of these parts is mitigated. Powered by wavelength division multiplexing, the footprint of delay lines is significantly reduced compared with previous art, thus being practical to fabricate. We evaluate the potential performance of the proposed architecture with respect to component flaws in practical fabrications. According to the results, with well-controlled system insertion loss, energy efficiency of the proposed architecture would surpass previously presented works and the state-of-art electronic processors. We anticipate the proposed architecture is beneficial for future fast and energy-efficient convolutional neural network accelerators.",2019-10-25T08:05:09Z,2020-02-28T09:40:50Z,http://arxiv.org/abs/1910.12635v2,http://arxiv.org/pdf/1910.12635v2,"eess.SP, physics.optics"
Design Trade-offs for Decentralized Baseband Processing in Massive   MU-MIMO Systems,"Kaipeng Li, James McNaney, Chance Tarver, Oscar Castañeda, Charles Jeon, Joseph R. Cavallaro, Christoph Studer","Massive multi-user (MU) multiple-input multiple-output (MIMO) provides high spectral efficiency by means of spatial multiplexing and fine-grained beamforming. However, conventional base-station (BS) architectures for systems with hundreds of antennas that rely on centralized baseband processing inevitably suffer from (i) excessive interconnect data rates between radio-frequency circuitry and processing fabrics, and (ii) prohibitive complexity at the centralized baseband processor. Recently, decentralized baseband processing (DBP) architectures and algorithms have been proposed, which mitigate the interconnect bandwidth and complexity bottlenecks. This paper systematically explores the design trade-offs between error-rate performance, computational complexity, and data transfer latency of DBP architectures under different system configurations and channel conditions. Considering architecture, algorithm, and numerical precision aspects, we provide practical guidelines to select the DBP architecture and algorithm that are able to realize the full benefits of massive MU-MIMO in the uplink and downlink.",2019-12-10T01:05:04Z,2019-12-15T04:37:55Z,http://arxiv.org/abs/1912.04437v2,http://arxiv.org/pdf/1912.04437v2,"cs.IT, eess.SP, math.IT"
Parkinson's Disease Detection Using Ensemble Architecture from MR Images,"Tahjid Ashfaque Mostafa, Irene Cheng","Parkinson's Disease(PD) is one of the major nervous system disorders that affect people over 60. PD can cause cognitive impairments. In this work, we explore various approaches to identify Parkinson's using Magnetic Resonance (MR) T1 images of the brain. We experiment with ensemble architectures combining some winning Convolutional Neural Network models of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and propose two architectures. We find that detection accuracy increases drastically when we focus on the Gray Matter (GM) and White Matter (WM) regions from the MR images instead of using whole MR images. We achieved an average accuracy of 94.7\% using smoothed GM and WM extracts and one of our proposed architectures. We also perform occlusion analysis and determine which brain areas are relevant in the architecture decision making process.",2020-07-01T18:03:23Z,2020-07-01T18:03:23Z,http://arxiv.org/abs/2007.00682v1,http://arxiv.org/pdf/2007.00682v1,"eess.IV, cs.LG"
Parkinson's Disease Detection with Ensemble Architectures based on   ILSVRC Models,"Tahjid Ashfaque Mostafa, Irene Cheng","In this work, we explore various neural network architectures using Magnetic Resonance (MR) T1 images of the brain to identify Parkinson's Disease (PD), which is one of the most common neurodegenerative and movement disorders. We propose three ensemble architectures combining some winning Convolutional Neural Network models of ImageNet Large Scale Visual Recognition Challenge (ILSVRC). All of our proposed architectures outperform existing approaches to detect PD from MR images, achieving upto 95\% detection accuracy. We also find that when we construct our ensemble architecture using models pretrained on the ImageNet dataset unrelated to PD, the detection performance is significantly better compared to models without any prior training. Our finding suggests a promising direction when no or insufficient training data is available.",2020-07-23T05:40:47Z,2020-07-23T05:40:47Z,http://arxiv.org/abs/2007.12496v1,http://arxiv.org/pdf/2007.12496v1,"eess.IV, cs.CV"
Towards Learning Convolutions from Scratch,Behnam Neyshabur,"Convolution is one of the most essential components of architectures used in computer vision. As machine learning moves towards reducing the expert bias and learning it from data, a natural next step seems to be learning convolution-like structures from scratch. This, however, has proven elusive. For example, current state-of-the-art architecture search algorithms use convolution as one of the existing modules rather than learning it from data. In an attempt to understand the inductive bias that gives rise to convolutions, we investigate minimum description length as a guiding principle and show that in some settings, it can indeed be indicative of the performance of architectures. To find architectures with small description length, we propose $\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on fully-connected networks for image classification tasks, learns architectures with local connections and achieves state-of-the-art accuracies for training fully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%) bridging the gap between fully-connected and convolutional nets.",2020-07-27T16:13:13Z,2020-07-27T16:13:13Z,http://arxiv.org/abs/2007.13657v1,http://arxiv.org/pdf/2007.13657v1,"cs.LG, cs.CV, stat.ML"
Efficient OCT Image Segmentation Using Neural Architecture Search,"Saba Heidari Gheshlaghi, Omid Dehzangi, Ali Dabouei, Annahita Amireskandari, Ali Rezai, Nasser M Nasrabadi","In this work, we propose a Neural Architecture Search (NAS) for retinal layer segmentation in Optical Coherence Tomography (OCT) scans. We incorporate the Unet architecture in the NAS framework as its backbone for the segmentation of the retinal layers in our collected and pre-processed OCT image dataset. At the pre-processing stage, we conduct super resolution and image processing techniques on the raw OCT scans to improve the quality of the raw images. For our search strategy, different primitive operations are suggested to find the down- & up-sampling cell blocks, and the binary gate method is applied to make the search strategy practical for the task in hand. We empirically evaluated our method on our in-house OCT dataset. The experimental results demonstrate that the self-adapting NAS-Unet architecture substantially outperformed the competitive human-designed architecture by achieving 95.4% in mean Intersection over Union metric and 78.7% in Dice similarity coefficient.",2020-07-28T02:48:07Z,2020-07-28T02:48:07Z,http://arxiv.org/abs/2007.14790v1,http://arxiv.org/pdf/2007.14790v1,"eess.IV, cs.CV"
Growing Efficient Deep Networks by Structured Continuous Sparsification,"Xin Yuan, Pedro Savarese, Michael Maire","We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\%$ inference FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\%$ top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.",2020-07-30T10:03:47Z,2023-06-06T03:20:50Z,http://arxiv.org/abs/2007.15353v2,http://arxiv.org/pdf/2007.15353v2,"cs.LG, stat.ML"
FC-GAGA: Fully Connected Gated Graph Architecture for Spatio-Temporal   Traffic Forecasting,"Boris N. Oreshkin, Arezou Amini, Lucy Coyle, Mark J. Coates","Forecasting of multivariate time-series is an important problem that has applications in traffic management, cellular network configuration, and quantitative finance. A special case of the problem arises when there is a graph available that captures the relationships between the time-series. In this paper we propose a novel learning architecture that achieves performance competitive with or better than the best existing algorithms, without requiring knowledge of the graph. The key element of our proposed architecture is the learnable fully connected hard graph gating mechanism that enables the use of the state-of-the-art and highly computationally efficient fully connected time-series forecasting architecture in traffic forecasting applications. Experimental results for two public traffic network datasets illustrate the value of our approach, and ablation studies confirm the importance of each element of the architecture. The code is available here: https://github.com/boreshkinai/fc-gaga.",2020-07-30T15:35:15Z,2020-12-14T19:41:19Z,http://arxiv.org/abs/2007.15531v2,http://arxiv.org/pdf/2007.15531v2,"cs.LG, stat.ML"
HMCNAS: Neural Architecture Search using Hidden Markov Chains and   Bayesian Optimization,"Vasco Lopes, Luís A. Alexandre","Neural Architecture Search has achieved state-of-the-art performance in a variety of tasks, out-performing human-designed networks. However, many assumptions, that require human definition, related with the problems being solved or the models generated are still needed: final model architectures, number of layers to be sampled, forced operations, small search spaces, which ultimately contributes to having models with higher performances at the cost of inducing bias into the system. In this paper, we propose HMCNAS, which is composed of two novel components: i) a method that leverages information about human-designed models to autonomously generate a complex search space, and ii) an Evolutionary Algorithm with Bayesian Optimization that is capable of generating competitive CNNs from scratch, without relying on human-defined parameters or small search spaces. The experimental results show that the proposed approach results in competitive architectures obtained in a very short time. HMCNAS provides a step towards generalizing NAS, by providing a way to create competitive models, without requiring any human knowledge about the specific task.",2020-07-31T16:04:08Z,2020-07-31T16:04:08Z,http://arxiv.org/abs/2007.16149v1,http://arxiv.org/pdf/2007.16149v1,"cs.LG, cs.CV, cs.NE, stat.ML"
Hierarchical Representations for Efficient Architecture Search,"Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu","We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.",2017-11-01T16:46:27Z,2018-02-22T22:31:30Z,http://arxiv.org/abs/1711.00436v2,http://arxiv.org/pdf/1711.00436v2,"cs.LG, cs.CV, cs.NE, stat.ML"
Introducing Graph Smoothness Loss for Training Deep Learning   Architectures,"Myriam Bontonou, Carlos Lassance, Ghouthi Boukli Hacene, Vincent Gripon, Jian Tang, Antonio Ortega","We introduce a novel loss function for training deep learning architectures to perform classification. It consists in minimizing the smoothness of label signals on similarity graphs built at the output of the architecture. Equivalently, it can be seen as maximizing the distances between the network function images of training inputs from distinct classes. As such, only distances between pairs of examples in distinct classes are taken into account in the process, and the training does not prevent inputs from the same class to be mapped to distant locations in the output domain. We show that this loss leads to similar performance in classification as architectures trained using the classical cross-entropy, while offering interesting degrees of freedom and properties. We also demonstrate the interest of the proposed loss to increase robustness of trained architectures to deviations of the inputs.",2019-05-01T13:09:04Z,2019-05-01T13:09:04Z,http://arxiv.org/abs/1905.00301v1,http://arxiv.org/pdf/1905.00301v1,"cs.LG, stat.ML"
Differentiable Architecture Search with Ensemble Gumbel-Softmax,"Jianlong Chang, Xinbang Zhang, Yiwen Guo, Gaofeng Meng, Shiming Xiang, Chunhong Pan","For network architecture search (NAS), it is crucial but challenging to simultaneously guarantee both effectiveness and efficiency. Towards achieving this goal, we develop a differentiable NAS solution, where the search space includes arbitrary feed-forward network consisting of the predefined number of connections. Benefiting from a proposed ensemble Gumbel-Softmax estimator, our method optimizes both the architecture of a deep network and its parameters in the same round of backward propagation, yielding an end-to-end mechanism of searching network architectures. Extensive experiments on a variety of popular datasets strongly evidence that our method is capable of discovering high-performance architectures, while guaranteeing the requisite efficiency during searching.",2019-05-06T01:47:17Z,2019-05-06T01:47:17Z,http://arxiv.org/abs/1905.01786v1,http://arxiv.org/pdf/1905.01786v1,"cs.LG, cs.CV, stat.ML"
DARC: Differentiable ARchitecture Compression,"Shashank Singh, Ashish Khetan, Zohar Karnin","In many learning situations, resources at inference time are significantly more constrained than resources at training time. This paper studies a general paradigm, called Differentiable ARchitecture Compression (DARC), that combines model compression and architecture search to learn models that are resource-efficient at inference time. Given a resource-intensive base architecture, DARC utilizes the training data to learn which sub-components can be replaced by cheaper alternatives. The high-level technique can be applied to any neural architecture, and we report experiments on state-of-the-art convolutional neural networks for image classification. For a WideResNet with $97.2\%$ accuracy on CIFAR-10, we improve single-sample inference speed by $2.28\times$ and memory footprint by $5.64\times$, with no accuracy loss. For a ResNet with $79.15\%$ Top1 accuracy on ImageNet, we improve batch inference speed by $1.29\times$ and memory footprint by $3.57\times$ with $1\%$ accuracy loss. We also give theoretical Rademacher complexity bounds in simplified cases, showing how DARC avoids overfitting despite over-parameterization.",2019-05-20T15:30:06Z,2019-05-20T15:30:06Z,http://arxiv.org/abs/1905.08170v1,http://arxiv.org/pdf/1905.08170v1,"cs.LG, cs.CV, stat.ML"
Not All Features Are Equal: Feature Leveling Deep Neural Networks for   Better Interpretation,"Yingjing Lu, Runde Yang","Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions",2019-05-24T02:53:45Z,2019-05-29T21:01:43Z,http://arxiv.org/abs/1905.10009v2,http://arxiv.org/pdf/1905.10009v2,"cs.LG, stat.ML"
Optical Stochastic Computing Architectures Using Photonic Crystal   Nanocavities,"Hassnaa El-Derhalli, Lea Constans, Sebastien Le Beux, Alfredo De Rossi, Fabrice Raineri, Sofiene Tahar","Stochastic computing allows a drastic reduction in hardware complexity using serial processing of bit streams. While the induced high computing latency can be overcome using integrated optics technology, the design of realistic optical stochastic computing architectures calls for energy efficient switching devices. Photonics Crystal (PhC) nanocavities are $\mu m^2$ scale devices offering 100fJ switching operation under picoseconds-scale switching speed. Fabrication process allows controlling the Quality factor of each nanocavity resonance, leading to opportunities to implement architectures involving cascaded gates and multi-wavelength signaling. In this report, we investigate the design of cascaded gates architecture using nanocavities in the context of stochastic computing. We propose a transmission model considering key nanocavity device parameters, such as Quality factors, resonance wavelength and switching efficiency. The model is calibrated with experimental measurements. We propose the design of XOR gate and multiplexer. We illustrate the use of the gates to design an edge detection filter. System-level exploration of laser power, bit-stream length and bit-error rate is carried out for the processing of gray-scale images. The results show that the proposed architecture leads to 8.5nJ/pixel energy consumption and 512ns/pixel processing time.",2021-02-03T13:42:58Z,2021-02-03T13:42:58Z,http://arxiv.org/abs/2102.02064v1,http://arxiv.org/pdf/2102.02064v1,"cs.ET, physics.optics"
"Discrete Event, Continuous Time RNNs","Michael C. Mozer, Denis Kazakov, Robert V. Lindsey","We investigate recurrent neural network architectures for event-sequence processing. Event sequences, characterized by discrete observations stamped with continuous-valued times of occurrence, are challenging due to the potentially wide dynamic range of relevant time scales as well as interactions between time scales. We describe four forms of inductive bias that should benefit architectures for event sequences: temporal locality, position and scale homogeneity, and scale interdependence. We extend the popular gated recurrent unit (GRU) architecture to incorporate these biases via intrinsic temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by interpreting the gates of a GRU as selecting a time scale of memory, and the CT-GRU generalizes the GRU by incorporating multiple time scales of memory and performing context-dependent selection of time scales for information storage and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas they serve as generic additional inputs to the GRU. Despite the very different manner in which the two models consider time, their performance on eleven data sets we examined is essentially identical. Our surprising results point both to the robustness of GRU and LSTM architectures for handling continuous time, and to the potency of incorporating continuous dynamics into neural architectures.",2017-10-11T15:20:51Z,2017-10-11T15:20:51Z,http://arxiv.org/abs/1710.04110v1,http://arxiv.org/pdf/1710.04110v1,"cs.NE, cs.LG, I.2.6"
MIMO Graph Filters for Convolutional Neural Networks,"Fernando Gama, Antonio G. Marques, Alejandro Ribeiro, Geert Leus","Superior performance and ease of implementation have fostered the adoption of Convolutional Neural Networks (CNNs) for a wide array of inference and reconstruction tasks. CNNs implement three basic blocks: convolution, pooling and pointwise nonlinearity. Since the two first operations are well-defined only on regular-structured data such as audio or images, application of CNNs to contemporary datasets where the information is defined in irregular domains is challenging. This paper investigates CNNs architectures to operate on signals whose support can be modeled using a graph. Architectures that replace the regular convolution with a so-called linear shift-invariant graph filter have been recently proposed. This paper goes one step further and, under the framework of multiple-input multiple-output (MIMO) graph filters, imposes additional structure on the adopted graph filters, to obtain three new (more parsimonious) architectures. The proposed architectures result in a lower number of model parameters, reducing the computational complexity, facilitating the training, and mitigating the risk of overfitting. Simulations show that the proposed simpler architectures achieve similar performance as more complex models.",2018-03-06T15:18:56Z,2018-03-06T15:18:56Z,http://arxiv.org/abs/1803.02247v1,http://arxiv.org/pdf/1803.02247v1,"cs.LG, eess.SP, stat.ML"
Quality Attributes on Quantum Computing Platforms,Balwinder Sodhi,"As the practical Quantum Computing Platforms (QCPs) rapidly become a reality, it is desirable to harness their true potential in software applications. Thus it becomes important to determine the implications of QCPs for software architecture. In this paper we present the in-depth examination of state-of-the-art QCPs for identifying all such characteristics of a QCP that are relevant from software architecture perspective. Lack of a native quantum operating system, a hard dependency on quantum algorithms, the lower level of programming abstractions are few, out of many, examples of QCP characteristics which may affect architecture of quantum software applications. Key contributions of this paper include identifying: i) The general architecture of a QCP, ii) The programming model which is typically used when developing software for a QCP, iii) Architecturally significant characteristics of QCPs and iv) The impact of these characteristics on various Quality Attributes (QAs). We show that except performance and scalability, most of the other QAs (e.g. maintainability, testability, reliability etc.) are adversely affected by different characteristics of a QCP.",2018-03-15T05:28:10Z,2018-03-15T05:28:10Z,http://arxiv.org/abs/1803.07407v1,http://arxiv.org/pdf/1803.07407v1,"cs.SE, cs.ET, quant-ph"
Fast FPGA emulation of analog dynamics in digitally-driven systems,"Steven Herbst, Byong Chan Lim, Mark Horowitz","In this paper, we propose an architecture for FPGA emulation of mixed-signal systems that achieves high accuracy at a high throughput. We represent the analog output of a block as a superposition of step responses to changes in its analog input, and the output is evaluated only when needed by the digital subsystem. Our architecture is therefore intended for digitally-driven systems; that is, those in which the inputs of analog dynamical blocks change only on digital clock edges. We implemented a high-speed link transceiver design using the proposed architecture on a Xilinx FPGA. This design demonstrates how our approach breaks the link between simulation rate and time resolution that is characteristic of prior approaches. The emulator is flexible, allowing for the real-time adjustment of analog dynamics, clock jitter, and various design parameters. We demonstrate that our architecture achieves 1% accuracy while running 3 orders of magnitude faster than a comparable high-performance CPU simulation.",2020-02-06T02:24:15Z,2020-02-06T02:24:15Z,http://arxiv.org/abs/2002.02072v1,http://arxiv.org/pdf/2002.02072v1,"cs.AR, eess.SP"
ForecastNet: A Time-Variant Deep Feed-Forward Neural Network   Architecture for Multi-Step-Ahead Time-Series Forecasting,"Joel Janek Dabrowski, YiFan Zhang, Ashfaqur Rahman","Recurrent and convolutional neural networks are the most common architectures used for time series forecasting in deep learning literature. These networks use parameter sharing by repeating a set of fixed architectures with fixed parameters over time or space. The result is that the overall architecture is time-invariant (shift-invariant in the spatial domain) or stationary. We argue that time-invariance can reduce the capacity to perform multi-step-ahead forecasting, where modelling the dynamics at a range of scales and resolutions is required. We propose ForecastNet which uses a deep feed-forward architecture to provide a time-variant model. An additional novelty of ForecastNet is interleaved outputs, which we show assist in mitigating vanishing gradients. ForecastNet is demonstrated to outperform statistical and deep learning benchmark models on several datasets.",2020-02-11T01:03:33Z,2020-06-27T23:24:54Z,http://arxiv.org/abs/2002.04155v2,http://arxiv.org/pdf/2002.04155v2,"cs.LG, stat.ML"
Follow the Neurally-Perturbed Leader for Adversarial Training,Ari Azarafrooz,"Game-theoretic models of learning are a powerful set of models that optimize multi-objective architectures. Among these models are zero-sum architectures that have inspired adversarial learning frameworks. An important shortcoming of these zeros-sum architectures is that gradient-based training leads to weak convergence and cyclic dynamics.   We propose a novel follow the leader training algorithm for zeros-sum architectures that guarantees convergence to mixed Nash equilibrium without cyclic behaviors. It is a special type of follow the perturbed leader algorithm where perturbations are the result of a neural mediating agent.   We validate our theoretical results by applying this training algorithm to games with convex and non-convex loss as well as generative adversarial architectures. Moreover, we customize the implementation of this algorithm for adversarial imitation learning applications. At every step of the training, the mediator agent perturbs the observations with generated codes. As a result of these mediating codes, the proposed algorithm is also efficient for learning in environments with various factors of variations. We validate our assertion by using a procedurally generated game environment as well as synthetic data. Github implementation is available.",2020-02-16T00:09:02Z,2020-06-08T04:54:53Z,http://arxiv.org/abs/2002.06476v2,http://arxiv.org/pdf/2002.06476v2,"cs.LG, cs.GT, stat.ML"
Stealing Black-Box Functionality Using The Deep Neural Tree Architecture,"Daniel Teitelman, Itay Naeh, Shie Mannor","This paper makes a substantial step towards cloning the functionality of black-box models by introducing a Machine learning (ML) architecture named Deep Neural Trees (DNTs). This new architecture can learn to separate different tasks of the black-box model, and clone its task-specific behavior. We propose to train the DNT using an active learning algorithm to obtain faster and more sample-efficient training. In contrast to prior work, we study a complex ""victim"" black-box model based solely on input-output interactions, while at the same time the attacker and the victim model may have completely different internal architectures. The attacker is a ML based algorithm whereas the victim is a generally unknown module, such as a multi-purpose digital chip, complex analog circuit, mechanical system, software logic or a hybrid of these. The trained DNT module not only can function as the attacked module, but also provides some level of explainability to the cloned model due to the tree-like nature of the proposed architecture.",2020-02-23T09:04:30Z,2020-02-23T09:04:30Z,http://arxiv.org/abs/2002.09864v1,http://arxiv.org/pdf/2002.09864v1,"cs.LG, cs.CR, stat.ML"
Multiresolution Convolutional Autoencoders,"Yuying Liu, Colin Ponce, Steven L. Brunton, J. Nathan Kutz","We propose a multi-resolution convolutional autoencoder (MrCAE) architecture that integrates and leverages three highly successful mathematical architectures: (i) multigrid methods, (ii) convolutional autoencoders and (iii) transfer learning. The method provides an adaptive, hierarchical architecture that capitalizes on a progressive training approach for multiscale spatio-temporal data. This framework allows for inputs across multiple scales: starting from a compact (small number of weights) network architecture and low-resolution data, our network progressively deepens and widens itself in a principled manner to encode new information in the higher resolution data based on its current performance of reconstruction. Basic transfer learning techniques are applied to ensure information learned from previous training steps can be rapidly transferred to the larger network. As a result, the network can dynamically capture different scaled features at different depths of the network. The performance gains of this adaptive multiscale architecture are illustrated through a sequence of numerical experiments on synthetic examples and real-world spatial-temporal data.",2020-04-10T08:31:59Z,2020-04-10T08:31:59Z,http://arxiv.org/abs/2004.04946v1,http://arxiv.org/pdf/2004.04946v1,"cs.LG, cs.NA, eess.IV, math.NA, stat.ML"
Intelligent Slicing of Radio Resource Control Layer for Cellular IoT:   Design and Implementation,"Lian Cao, Rongpeng Li, Jon Crowcroft, Zhifeng Zhao, Honggang Zhang","The cellular internet of things (CIoT) has become an important branch to cater various applications of IoT devices. Within CIoT, the radio resource control (RRC) layer is responsible for fundamental functionalities such as connection control and bearer establishment in radio access network (RAN). The emergence of various IoT scenarios and diversified service requirements have made both RAN slicing and intelligent control imperative requirement in RRC layer. This paper focuses on enhancing standardized capabilities of CIoT RRC layer, by designing and implementing a new architecture which accommodate RRC slicing and intelligent controller. The architecture aims to realize functionalities of creating, modifying, and deleting slices in RRC layer, while the intelligent controller is added to satisfy various and dynamic service requirements of different IoT devices smartly. The proposed architecture is further implemented on an open-source software platform OpenAirInterface (OAI), on top of which the effectiveness of RRC slicing is validated and one proof-of-concept case to adopt reinforcement learning to dynamically tune discontinuous reception parameters therein is presented. Simulation results have demonstrated the effectiveness of the proposed intelligent RRC slicing architecture.",2020-04-15T08:28:06Z,2020-04-15T08:28:06Z,http://arxiv.org/abs/2004.06935v1,http://arxiv.org/pdf/2004.06935v1,"cs.IT, cs.NI, math.IT"
A 5G NR based System Architecture for Real-Time Control with Batteryless   RFID Sensors,Peng Hu,"The fifth-generation wireless networking (5G) technologies have been developed to meet various time-critical use cases with ultra-reliable, low-latency and massive machine-type communications which are indispensable for tactile Internet applications. Recent advancements in very low-cost and batteryless radio-frequency identification (RFID) sensors have given promises of deploying a massive amount of such sensors for real-time sensing and control applications on a 5G New Radio (NR) network. However, the system design and performance of such applications have not been well studied. This paper proposes a novel system architecture for the representative batteryless RFID touch sensors in generic real-time control applications in a 5G NR mmWave environment. We will discuss the solution using edge computing nodes on the 5G NR base station to the implementation of the proposed system architecture. The real-time performance evaluation with the comparison of the Long-Term Evolution (LTE) networks has shown the effectiveness of the proposed system architecture.",2020-04-15T15:06:27Z,2020-08-13T00:15:09Z,http://arxiv.org/abs/2004.07135v2,http://arxiv.org/pdf/2004.07135v2,"cs.NI, cs.SY, eess.SY"
Spectrally Consistent UNet for High Fidelity Image Transformations,"Demetris Marnerides, Thomas Bashford-Rogers, Kurt Debattista","Convolutional Neural Networks (CNNs) are the current de-facto models used for many imaging tasks due to their high learning capacity as well as their architectural qualities. The ubiquitous UNet architecture provides an efficient and multi-scale solution that combines local and global information. Despite the success of UNet architectures, the use of upsampling layers can cause artefacts. In this work, a method for assessing the structural biases of UNets and the effects these have on the outputs is presented, characterising their impact in the Fourier domain. A new upsampling module is proposed, based on a novel use of the Guided Image Filter, that provides spectrally consistent outputs when used in a UNet architecture, forming the Guided UNet (GUNet). The GUNet architecture is applied and evaluated for example applications of inverse tone mapping/dynamic range expansion and colourisation from grey-scale images and is shown to provide higher fidelity outputs.",2020-04-22T17:04:02Z,2020-09-29T09:32:09Z,http://arxiv.org/abs/2004.10696v2,http://arxiv.org/pdf/2004.10696v2,"eess.IV, cs.GR, cs.LG, stat.ML"
Efficient Neural Architecture for Text-to-Image Synthesis,"Douglas M. Souza, Jônatas Wehrmann, Duncan D. Ruiz","Text-to-image synthesis is the task of generating images from text descriptions. Image generation, by itself, is a challenging task. When we combine image generation and text, we bring complexity to a new level: we need to combine data from two different modalities. Most of recent works in text-to-image synthesis follow a similar approach when it comes to neural architectures. Due to aforementioned difficulties, plus the inherent difficulty of training GANs at high resolutions, most methods have adopted a multi-stage training strategy. In this paper we shift the architectural paradigm currently used in text-to-image methods and show that an effective neural architecture can achieve state-of-the-art performance using a single stage training with a single generator and a single discriminator. We do so by applying deep residual networks along with a novel sentence interpolation strategy that enables learning a smooth conditional space. Finally, our work points a new direction for text-to-image research, which has not experimented with novel neural architectures recently.",2020-04-23T19:33:40Z,2020-04-23T19:33:40Z,http://arxiv.org/abs/2004.11437v1,http://arxiv.org/pdf/2004.11437v1,"cs.LG, stat.ML"
A Quantum Router Architecture for High-Fidelity Entanglement Flows in   Quantum Networks,"Yuan Lee, Eric Bersin, Axel Dahlberg, Stephanie Wehner, Dirk Englund","The past decade has seen tremendous progress in experimentally realizing the building blocks of quantum repeaters. Repeater architectures with multiplexed quantum memories have been proposed to increase entanglement distribution rates, but an open challenge is to maintain entanglement fidelity over long-distance links. Here, we address this with a quantum router architecture comprising many quantum memories connected in a photonic switchboard to broker entanglement flows across quantum networks. We compute the rate and fidelity of entanglement distribution under this architecture using an event-based simulator, finding that the router improves the entanglement fidelity as multiplexing depth increases without a significant drop in the entanglement distribution rate. Specifically, the router permits channel-loss-invariant fidelity, i.e. the same fidelity achievable with lossless links. Furthermore, this scheme automatically prioritizes entanglement flows across the full network without requiring global network information. The proposed architecture uses present-day photonic technology, opening a path to near-term deployable multi-node quantum networks.",2020-05-04T21:20:33Z,2022-10-22T15:21:50Z,http://arxiv.org/abs/2005.01852v3,http://arxiv.org/pdf/2005.01852v3,"quant-ph, physics.app-ph"
Scalable and Secure Architecture for Distributed IoT Systems,"Najmeddine Dhieb, Hakim Ghazzai, Hichem Besbes, Yehia Massoud","Internet-of-things (IoT) is perpetually revolutionizing our daily life and rapidly transforming physical objects into an ubiquitous connected ecosystem. Due to their massive deployment and moderate security levels, those devices face a lot of security, management, and control challenges. Their classical centralized architecture is still cloaking vulnerabilities and anomalies that can be exploited by hackers for spying, eavesdropping, and taking control of the network. In this paper, we propose to improve the IoT architecture with additional security features using Artificial Intelligence (AI) and blockchain technology. We propose a novel architecture based on permissioned blockchain technology in order to build a scalable and decentralized end-to-end secure IoT system. Furthermore, we enhance the IoT system security with an AI-component at the gateway level to detect and classify suspected activities, malware, and cyber-attacks using machine learning techniques. Simulations and practical implementation show that the proposed architecture delivers high performance against cyber-attacks.",2020-04-20T23:50:43Z,2020-04-20T23:50:43Z,http://arxiv.org/abs/2005.02456v1,http://arxiv.org/pdf/2005.02456v1,"cs.CR, cs.LG, cs.SY, eess.SY"
Medical Image Segmentation Using a U-Net type of Architecture,"Eshal Zahra, Bostan Ali, Wajahat Siddique","Deep convolutional neural networks have been proven to be very effective in image related analysis and tasks, such as image segmentation, image classification, image generation, etc. Recently many sophisticated CNN based architectures have been proposed for the purpose of image segmentation. Some of these newly designed networks are used for the specific purpose of medical image segmentation, models like V-Net, U-Net and their variants. It has been shown that U-Net produces very promising results in the domain of medical image segmentation.However, in this paper, we argue that the architecture of U-Net, when combined with a supervised training strategy at the bottleneck layer, can produce comparable results with the original U-Net architecture. More specifically, we introduce a fully supervised FC layers based pixel-wise loss at the bottleneck of the encoder branch of U-Net. The two layer based FC sub-net will train the bottleneck representation to contain more semantic information, which will be used by the decoder layers to predict the final segmentation map. The FC layer based sub-net is trained by employing the pixel-wise cross entropy loss, while the U-Net architectures trained by using L1 loss.",2020-05-11T16:10:18Z,2020-05-11T16:10:18Z,http://arxiv.org/abs/2005.05218v1,http://arxiv.org/pdf/2005.05218v1,"eess.IV, cs.CV"
Incorporating Image Gradients as Secondary Input Associated with Input   Image to Improve the Performance of the CNN Model,"Vijay Pandey, Shashi Bhushan Jha","CNN is very popular neural network architecture in modern days. It is primarily most used tool for vision related task to extract the important features from the given image. Moreover, CNN works as a filter to extract the important features using convolutional operation in distinct layers. In existing CNN architectures, to train the network on given input, only single form of given input is fed to the network. In this paper, new architecture has been proposed where given input is passed in more than one form to the network simultaneously by sharing the layers with both forms of input. We incorporate image gradient as second form of the input associated with the original input image and allowing both inputs to flow in the network using same number of parameters to improve the performance of the model for better generalization. The results of the proposed CNN architecture, applying on diverse set of datasets such as MNIST, CIFAR10 and CIFAR100 show superior result compared to the benchmark CNN architecture considering inputs in single form.",2020-06-05T14:01:52Z,2020-06-05T14:01:52Z,http://arxiv.org/abs/2006.04570v1,http://arxiv.org/pdf/2006.04570v1,"cs.CV, cs.LG, eess.IV"
Relay Aided Intelligent Reconfigurable Surfaces: Achieving the Potential   Without So Many Antennas,"Xiaoyan Ying, Umut Demirhan, Ahmed Alkhateeb","This paper proposes a novel relay-aided intelligent reconfigurable surface (IRS) architecture for future wireless communication systems. The proposed architecture, which consists of two side-by-side intelligent surfaces connected via a full-duplex relay, has the potential of achieving the promising gains of intelligent surfaces while requiring much smaller numbers of reflecting elements. Consequently, the proposed IRS architecture needs significantly less channel estimation and beam training overhead and provides higher robustness compared to classical IRS approaches. Further, thanks to dividing the IRS reflection process over two surfaces, the position and orientation of these surfaces can be optimized to extend the wireless communication coverage and enhance the system performance. In this paper, the achievable rates and required numbers of elements using the proposed relay-aided IRS architecture are first analytically characterized and then evaluated using numerical simulations. The results show that the proposed architecture can achieve the data rate targets with much smaller numbers of elements compared to typical IRS solutions, which highlights a promising path towards the practical deployment of these intelligent surfaces.",2020-06-11T17:37:44Z,2020-06-11T17:37:44Z,http://arxiv.org/abs/2006.06644v1,http://arxiv.org/pdf/2006.06644v1,"cs.IT, eess.SP, math.IT"
NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language   Processing,"Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim Fedorov, Evgeny Burnaev","Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure more reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We believe that our results have high potential of usage for both NAS and NLP communities.",2020-06-12T12:19:06Z,2020-06-12T12:19:06Z,http://arxiv.org/abs/2006.07116v1,http://arxiv.org/pdf/2006.07116v1,"cs.LG, cs.CL, stat.ML"
Optimal Transport Kernels for Sequential and Parallel Neural   Architecture Search,"Vu Nguyen, Tam Le, Makoto Yamada, Michael A Osborne","Neural architecture search (NAS) automates the design of deep neural networks. One of the main challenges in searching complex and non-continuous architectures is to compare the similarity of networks that the conventional Euclidean metric may fail to capture. Optimal transport (OT) is resilient to such complex structure by considering the minimal cost for transporting a network into another. However, the OT is generally not negative definite which may limit its ability to build the positive-definite kernels required in many kernel-dependent frameworks. Building upon tree-Wasserstein (TW), which is a negative definite variant of OT, we develop a novel discrepancy for neural architectures, and demonstrate it within a Gaussian process surrogate model for the sequential NAS settings. Furthermore, we derive a novel parallel NAS, using quality k-determinantal point process on the GP posterior, to select diverse and high-performing architectures from a discrete set of candidates. Empirically, we demonstrate that our TW-based approaches outperform other baselines in both sequential and parallel NAS.",2020-06-13T08:44:41Z,2021-06-10T06:55:22Z,http://arxiv.org/abs/2006.07593v3,http://arxiv.org/pdf/2006.07593v3,"cs.LG, cs.NE, stat.ML"
Ginkgo: A Modern Linear Operator Algebra Framework for High Performance   Computing,"Hartwig Anzt, Terry Cojean, Goran Flegar, Fritz Göbel, Thomas Grützmacher, Pratik Nayak, Tobias Ribizel, Yuhsiang Mike Tsai, Enrique S. Quintana-Ortí","In this paper, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo's design principle abstracts all functionality as ""linear operators"", motivating the notation of a ""linear operator algebra library"". Ginkgo's current focus is oriented towards providing sparse linear algebra functionality for high performance GPU architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific back ends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo's usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo's high performance on state-of-the-art GPU architectures.",2020-06-30T14:42:48Z,2020-07-01T08:31:08Z,http://arxiv.org/abs/2006.16852v2,http://arxiv.org/pdf/2006.16852v2,"cs.MS, D.2; G.1.3; G.4"
Achieving Full Grating-Lobe-Free Field-of-View with Low-Complexity   Co-prime Photonic Beamforming Transceivers,"Aroutin Khachaturian, Reza Fatemi, Ali Hajimiri","Integrated photonic active beamforming can significantly reduce the size and cost of coherent imagers for LiDAR and medical imaging applications. In current architectures, the complexity of photonic and electronic circuitry linearly increases with the desired imaging resolution. We propose a novel photonic transceiver architecture based on co-prime sampling techniques that breaks this trade-off and achieves the full (radiating-element-limited) field-of-view (FOV) for a 2D aperture with a single-frequency laser. Using only order-of-N radiating elements, this architecture achieves beamwidth and side-lobe level (SLL) performance equivalent to a transceiver with order-of-N-squared elements with half-wavelength spacing. Furthermore, we incorporate a pulse amplitude modulation (PAM) row-column drive methodology to reduce the number of required electrical drivers for this architecture from order of N to order of square root of N. A silicon photonics implementation of this architecture using two 64-element apertures, one for transmitting and one for receiving, requires only 34 PAM electrical drivers and achieves a transceiver SLL of -11.3dB with 1026 total resolvable spots, and 0.6 degree beamwidth within a 23x16.3 degree FOV.",2021-08-17T21:23:14Z,2021-08-17T21:23:14Z,http://arxiv.org/abs/2108.10223v1,http://arxiv.org/pdf/2108.10223v1,"eess.SP, physics.optics"
A Comparison of Deep Learning Architectures for Optical Galaxy   Morphology Classification,"Ezra Fielding, Clement N. Nyirenda, Mattia Vaccari","The classification of galaxy morphology plays a crucial role in understanding galaxy formation and evolution. Traditionally, this process is done manually. The emergence of deep learning techniques has given room for the automation of this process. As such, this paper offers a comparison of deep learning architectures to determine which is best suited for optical galaxy morphology classification. Adapting the model training method proposed by Walmsley et al in 2021, the Zoobot Python library is used to train models to predict Galaxy Zoo DECaLS decision tree responses, made by volunteers, using EfficientNet B0, DenseNet121 and ResNet50 as core model architectures. The predicted results are then used to generate accuracy metrics per decision tree question to determine architecture performance. DenseNet121 was found to produce the best results, in terms of accuracy, with a reasonable training time. In future, further testing with more deep learning architectures could prove beneficial.",2021-11-08T09:19:32Z,2021-11-08T09:19:32Z,http://arxiv.org/abs/2111.04353v1,http://arxiv.org/pdf/2111.04353v1,"cs.LG, astro-ph.GA, astro-ph.IM"
Guided Sampling-based Evolutionary Deep Neural Network for Intelligent   Fault Diagnosis,"Arun K. Sharma, Nishchal K. Verma","The diagnostic performance of most of the deep learning models is greatly affected by the selection of model architecture and hyperparameters. Manual selection of model architecture is not feasible as training and evaluating the different architectures of deep learning models is a time-consuming process. Therefore, we have proposed a novel framework of evolutionary deep neural network which uses policy gradient to guide the evolution of DNN architecture towards maximum diagnostic accuracy. We have formulated a policy gradient-based controller which generates an action to sample the new model architecture at every generation such that the optimality is obtained quickly. The fitness of the best model obtained is used as a reward to update the policy parameters. Also, the best model obtained is transferred to the next generation for quick model evaluation in the NSGA-II evolutionary framework. Thus, the algorithm gets the benefits of fast non-dominated sorting as well as quick model evaluation. The effectiveness of the proposed framework has been validated on three datasets: the Air Compressor dataset, Case Western Reserve University dataset, and Paderborn university dataset.",2021-11-12T18:59:45Z,2022-02-23T05:22:23Z,http://arxiv.org/abs/2111.06885v3,http://arxiv.org/pdf/2111.06885v3,"cs.NE, cs.SY, eess.SY"
Predicting Succinylation Sites in Proteins with Improved Deep Learning   Architecture,"Olusola Odeyomi, Gergely Zaruba","Post-translational modifications (PTMs) in proteins occur after the process of translation. PTMs account for many cellular processes such as deoxyribonucleic acid (DNA) repair, cell signaling and cell death. One of the recent PTMs is succinylation. Succinylation modifies lysine residue from $-1$ to $+1$. Locating succinylation sites using experimental methods, such as mass spectrometry is very laborious. Hence, computational methods are favored using machine learning techniques. This paper proposes a deep learning architecture to predict succinylation sites. The performance of the proposed architecture is compared to the state-of-the-art deep learning architecture and other traditional machine learning techniques for succinylation. It is shown from the performance metrics that the proposed architecture provides a good trade-off between speed of computation and classification accuracy.",2021-12-27T16:15:34Z,2021-12-27T16:15:34Z,http://arxiv.org/abs/2201.11215v1,http://arxiv.org/pdf/2201.11215v1,"q-bio.BM, cs.LG"
An Analytical Framework for Control Synthesis of Cyber-Physical Systems   with Safety Guarantee,"Luyao Niu, Abdullah Al Maruf, Andrew Clark, J. Sukarno Mertoguno, Radha Poovendran","Cyber-physical systems (CPS) are required to operate safely under fault and malicious attacks. The simplex architecture and the recently proposed cyber resilient architectures, e.g., Byzantine fault tolerant++ (BFT++), provide safety for CPS under faults and malicious cyber attacks, respectively. However, these existing architectures make use of different timing parameters and implementations to provide safety, and are seemingly unrelated. In this paper, we propose an analytical framework to represent the simplex, BFT++ and other practical cyber resilient architectures (CRAs). We construct a hybrid system that models CPS adopting any of these architectures. We derive sufficient conditions via our proposed framework under which a control policy is guaranteed to be safe. We present an algorithm to synthesize the control policy. We validate the proposed framework using a case study on lateral control of a Boeing 747, and demonstrate that our proposed approach ensures safety of the system.",2022-04-01T15:15:05Z,2022-04-01T15:15:05Z,http://arxiv.org/abs/2204.00514v1,http://arxiv.org/pdf/2204.00514v1,"eess.SY, cs.SY"
Towards Refactoring of DMARF and GIPSY Case Studies -- a Team 8   SOEN6471-S14 Project Report,"Nitish Agrawal, Rachit Naidu, Sadhana Viswanathan, Vikram Wankhede, Zakaria Nasereldine, Zohaib S. Kiyani","Of the factors that determines the quality of a software system is its design and architecture. Having a good and clear design and architecture allows the system to evolve (plan and add new features), be easier to comprehend, easier to develop, easier to maintain; and in conclusion increase the life time of the, and being more competitive in its market. In the following paper we study the architecture of two different systems: GIPSY and DMARF. This paper provides a general overview of these two systems. What are these two systems, purpose, architecture, and their design patterns? Classes with week architecture and design, and code smells were also identified and some refactorings were suggested and implemented. Several tools were used throughout the paper for several purpose. LOGICSCOPE, JDeodoant, McCabe were used to identify classes with weak designs and code smells. Other tools and plugins were also used to identify class designs and relationships between classes such as ObjectAid (Eclipse plugin).",2014-12-23T18:09:27Z,2014-12-23T18:09:27Z,http://arxiv.org/abs/1412.7535v1,http://arxiv.org/pdf/1412.7535v1,"cs.SE, D.2; K.6; H.5.2"
Genetic Architect: Discovering Genomic Structure with Learned Neural   Architectures,"Laura Deming, Sasha Targ, Nate Sauder, Diogo Almeida, Chun Jimmie Ye","Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges.",2016-05-23T19:43:08Z,2016-05-23T19:43:08Z,http://arxiv.org/abs/1605.07156v1,http://arxiv.org/pdf/1605.07156v1,"cs.LG, cs.AI, cs.NE, stat.ML"
Stochastic Adaptive Neural Architecture Search for Keyword Spotting,"Tom Véniat, Olivier Schwander, Ludovic Denoyer","The problem of keyword spotting i.e. identifying keywords in a real-time audio stream is mainly solved by applying a neural network over successive sliding windows. Due to the difficulty of the task, baseline models are usually large, resulting in a high computational cost and energy consumption level. We propose a new method called SANAS (Stochastic Adaptive Neural Architecture Search) which is able to adapt the architecture of the neural network on-the-fly at inference time such that small architectures will be used when the stream is easy to process (silence, low noise, ...) and bigger networks will be used when the task becomes more difficult. We show that this adaptive model can be learned end-to-end by optimizing a trade-off between the prediction performance and the average computational cost per unit of time. Experiments on the Speech Commands dataset show that this approach leads to a high recognition level while being much faster (and/or energy saving) than classical approaches where the network architecture is static.",2018-11-16T11:08:26Z,2018-11-16T11:08:26Z,http://arxiv.org/abs/1811.06753v1,http://arxiv.org/pdf/1811.06753v1,"cs.LG, eess.AS, stat.ML"
Stochastic Deep Networks,"Gwendoline de Bie, Gabriel Peyré, Marco Cuturi","Machine learning is increasingly targeting areas where input data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concept of random vectors, namely probability measures or more simply point clouds of varying cardinality. Using deep architectures on measures poses, however, many challenging issues. Indeed, deep architectures are originally designed to handle fixedlength vectors, or, using recursive mechanisms, ordered sequences thereof. In sharp contrast, measures describe a varying number of weighted observations with no particular order. We propose in this work a deep framework designed to handle crucial aspects of measures, namely permutation invariances, variations in weights and cardinality. Architectures derived from this pipeline can (i) map measures to measures - using the concept of push-forward operators; (ii) bridge the gap between measures and Euclidean spaces - through integration steps. This allows to design discriminative networks (to classify or reduce the dimensionality of input measures), generative architectures (to synthesize measures) and recurrent pipelines (to predict measure dynamics). We provide a theoretical analysis of these building blocks, review our architectures' approximation abilities and robustness w.r.t. perturbation, and try them on various discriminative and generative tasks.",2018-11-19T00:11:06Z,2019-02-20T09:25:41Z,http://arxiv.org/abs/1811.07429v2,http://arxiv.org/pdf/1811.07429v2,"stat.ML, cs.LG"
Towards Robust Neural Networks with Lipschitz Continuity,"Muhammad Usama, Dong Eui Chang","Deep neural networks have shown remarkable performance across a wide range of vision-based tasks, particularly due to the availability of large-scale datasets for training and better architectures. However, data seen in the real world are often affected by distortions that not accounted for by the training datasets. In this paper, we address the challenge of robustness and stability of neural networks and propose a general training method that can be used to make the existing neural network architectures more robust and stable to input visual perturbations while using only available datasets for training. Proposed training method is convenient to use as it does not require data augmentation or changes in the network architecture. We provide theoretical proof as well as empirical evidence for the efficiency of the proposed training method by performing experiments with existing neural network architectures and demonstrate that same architecture when trained with the proposed training method perform better than when trained with conventional training approach in the presence of noisy datasets.",2018-11-22T03:42:17Z,2018-11-22T03:42:17Z,http://arxiv.org/abs/1811.09008v1,http://arxiv.org/pdf/1811.09008v1,"cs.LG, stat.ML"
Evolutionary-Neural Hybrid Agents for Architecture Search,"Krzysztof Maziarz, Mingxing Tan, Andrey Khorlin, Marin Georgiev, Andrea Gesmundo","Neural Architecture Search has shown potential to automate the design of neural networks. Deep Reinforcement Learning based agents can learn complex architectural patterns, as well as explore a vast and compositional search space. On the other hand, evolutionary algorithms offer higher sample efficiency, which is critical for such a resource intensive application. In order to capture the best of both worlds, we propose a class of Evolutionary-Neural hybrid agents (Evo-NAS). We show that the Evo-NAS agent outperforms both neural and evolutionary agents when applied to architecture search for a suite of text and image classification benchmarks. On a high-complexity architecture search space for image classification, the Evo-NAS agent surpasses the accuracy achieved by commonly used agents with only 1/3 of the search cost.",2018-11-24T13:00:47Z,2020-02-15T13:25:42Z,http://arxiv.org/abs/1811.09828v4,http://arxiv.org/pdf/1811.09828v4,"cs.LG, cs.NE, stat.ML"
XferNAS: Transfer Neural Architecture Search,Martin Wistuba,"The term Neural Architecture Search (NAS) refers to the automatic optimization of network architectures for a new, previously unknown task. Since testing an architecture is computationally very expensive, many optimizers need days or even weeks to find suitable architectures. However, this search time can be significantly reduced if knowledge from previous searches on different tasks is reused. In this work, we propose a generally applicable framework that introduces only minor changes to existing optimizers to leverage this feature. As an example, we select an existing optimizer and demonstrate the complexity of the integration of the framework as well as its impact. In experiments on CIFAR-10 and CIFAR-100, we observe a reduction in the search time from 200 to only 6 GPU days, a speed up by a factor of 33. In addition, we observe new records of 1.99 and 14.06 for NAS optimizers on the CIFAR benchmarks, respectively. In a separate study, we analyze the impact of the amount of source and target data. Empirically, we demonstrate that the proposed framework generally gives better results and, in the worst case, is just as good as the unmodified optimizer.",2019-07-18T22:05:49Z,2019-07-18T22:05:49Z,http://arxiv.org/abs/1907.08307v1,http://arxiv.org/pdf/1907.08307v1,"cs.LG, cs.CV, cs.NE, stat.ML"
MemNet: Memory-Efficiency Guided Neural Architecture Search with   Augment-Trim learning,"Peiye Liu, Bo Wu, Huadong Ma, Mingoo Seok","Recent studies on automatic neural architectures search have demonstrated significant performance, competitive to or even better than hand-crafted neural architectures. However, most of the existing network architecture tend to use residual, parallel structures and concatenation block between shallow and deep features to construct a large network. This requires large amounts of memory for storing both weights and feature maps. This is challenging for mobile and embedded devices since they may not have enough memory to perform inference with the designed large network model. To close this gap, we propose MemNet, an augment-trim learning-based neural network search framework that optimizes not only performance but also memory requirement. Specifically, it employs memory consumption based ranking score which forces an upper bound on memory consumption for navigating the search process. Experiment results show that, as compared to the state-of-the-art efficient designing methods, MemNet can find an architecture which can achieve competitive accuracy and save an average of 24.17% on the total memory needed.",2019-07-22T20:49:53Z,2020-06-10T20:12:57Z,http://arxiv.org/abs/1907.09569v2,http://arxiv.org/pdf/1907.09569v2,"cs.LG, cs.CV, stat.ML"
Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural   Computer Architecture,"Daniel Tanneberg, Elmar Rueckert, Jan Peters","A key feature of intelligent behavior is the ability to learn abstract strategies that transfer to unfamiliar problems. Therefore, we present a novel architecture, based on memory-augmented networks, that is inspired by the von Neumann and Harvard architectures of modern computers. This architecture enables the learning of abstract algorithmic solutions via Evolution Strategies in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle and robotic manipulation tasks, we show that the architecture can learn algorithmic solutions with strong generalization and abstraction: scaling to arbitrary task configurations and complexities, and being independent of both the data representation and the task domain.",2019-10-30T17:02:13Z,2020-06-03T11:21:39Z,http://arxiv.org/abs/1911.00926v2,http://arxiv.org/pdf/1911.00926v2,"cs.NE, cs.AI, cs.LG, stat.ML"
Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture   Search,"Xiangxiang Chu, Tianbao Zhou, Bo Zhang, Jixiang Li","Differentiable Architecture Search (DARTS) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair DARTS where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation's architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet. Our code is available on https://github.com/xiaomi-automl/fairdarts .",2019-11-27T13:10:25Z,2020-07-16T01:16:52Z,http://arxiv.org/abs/1911.12126v4,http://arxiv.org/pdf/1911.12126v4,"cs.LG, cs.AI, cs.CV, stat.ML"
Processing Distribution and Architecture Tradeoff for Large Intelligent   Surface Implementation,"Jesus Rodriguez Sanchez, Ove Edfors, Fredrik Rusek, Liang Liu","The Large Intelligent Surface (LIS) concept has emerged recently as a new paradigm for wireless communication, remote sensing and positioning. It consists of a continuous radiating surface placed relatively close to the users, which is able to communicate with users by independent transmission and reception (replacing base stations). Despite of its potential, there are a lot of challenges from an implementation point of view, with the interconnection data-rate and computational complexity being the most relevant. Distributed processing techniques and hierarchical architectures are expected to play a vital role addressing this while ensuring scalability. In this paper we perform algorithm-architecture codesign and analyze the hardware requirements and architecture trade-offs for a discrete LIS to perform uplink detection. By doing this, we expect to give concrete case studies and guidelines for efficient implementation of LIS systems.",2020-01-14T17:54:08Z,2020-06-07T09:38:42Z,http://arxiv.org/abs/2001.04937v2,http://arxiv.org/pdf/2001.04937v2,"eess.SP, cs.AR, cs.DC"
Evolutionary Neural Architecture Search for Retinal Vessel Segmentation,"Zhun Fan, Jiahong Wei, Guijie Zhu, Jiajie Mo, Wenji Li","The accurate retinal vessel segmentation (RVS) is of great significance to assist doctors in the diagnosis of ophthalmology diseases and other systemic diseases. Manually designing a valid neural network architecture for retinal vessel segmentation requires high expertise and a large workload. In order to improve the performance of vessel segmentation and reduce the workload of manually designing neural network, we propose novel approach which applies neural architecture search (NAS) to optimize an encoder-decoder architecture for retinal vessel segmentation. A modified evolutionary algorithm is used to evolve the architectures of encoder-decoder framework with limited computing resources. The evolved model obtained by the proposed approach achieves top performance among all compared methods on the three datasets, namely DRIVE, STARE and CHASE_DB1, but with much fewer parameters. Moreover, the results of cross-training show that the evolved model is with considerable scalability, which indicates a great potential for clinical disease diagnosis.",2020-01-18T15:07:26Z,2020-03-18T05:58:25Z,http://arxiv.org/abs/2001.06678v3,http://arxiv.org/pdf/2001.06678v3,"eess.IV, cs.CV"
A Power-Efficient Binary-Weight Spiking Neural Network Architecture for   Real-Time Object Classification,"Pai-Yu Tan, Po-Yao Chuang, Yen-Ting Lin, Cheng-Wen Wu, Juin-Ming Lu","Neural network hardware is considered an essential part of future edge devices. In this paper, we propose a binary-weight spiking neural network (BW-SNN) hardware architecture for low-power real-time object classification on edge platforms. This design stores a full neural network on-chip, and hence requires no off-chip bandwidth. The proposed systolic array maximizes data reuse for a typical convolutional layer. A 5-layer convolutional BW-SNN hardware is implemented in 90nm CMOS. Compared with state-of-the-art designs, the area cost and energy per classification are reduced by 7$\times$ and 23$\times$, respectively, while also achieving a higher accuracy on the MNIST benchmark. This is also a pioneering SNN hardware architecture that supports advanced CNN architectures.",2020-03-12T11:25:00Z,2020-03-12T11:25:00Z,http://arxiv.org/abs/2003.06310v1,http://arxiv.org/pdf/2003.06310v1,"eess.SP, cs.AR, cs.CV, cs.NE"
Enhanced MRI Reconstruction Network using Neural Architecture Search,"Qiaoying Huang, Dong Yang, Yikun Xian, Pengxiang Wu, Jingru Yi, Hui Qu, Dimitris Metaxas","The accurate reconstruction of under-sampled magnetic resonance imaging (MRI) data using modern deep learning technology, requires significant effort to design the necessary complex neural network architectures. The cascaded network architecture for MRI reconstruction has been widely used, while it suffers from the ""vanishing gradient"" problem when the network becomes deep. In addition, homogeneous architecture degrades the representation capacity of the network. In this work, we present an enhanced MRI reconstruction network using a residual in residual basic block. For each cell in the basic block, we use the differentiable neural architecture search (NAS) technique to automatically choose the optimal operation among eight variants of the dense block. This new heterogeneous network is evaluated on two publicly available datasets and outperforms all current state-of-the-art methods, which demonstrates the effectiveness of our proposed method.",2020-08-19T03:44:31Z,2020-08-19T03:44:31Z,http://arxiv.org/abs/2008.08248v1,http://arxiv.org/pdf/2008.08248v1,"eess.IV, cs.CV, cs.LG"
Simplifying Architecture Search for Graph Neural Network,"Huan Zhao, Lanning Wei, Quanming Yao","Recent years have witnessed the popularity of Graph Neural Networks (GNN) in various scenarios. To obtain optimal data-specific GNN architectures, researchers turn to neural architecture search (NAS) methods, which have made impressive progress in discovering effective architectures in convolutional neural networks. Two preliminary works, GraphNAS and Auto-GNN, have made first attempt to apply NAS methods to GNN. Despite the promising results, there are several drawbacks in expressive capability and search efficiency of GraphNAS and Auto-GNN due to the designed search space. To overcome these drawbacks, we propose the SNAG framework (Simplified Neural Architecture search for Graph neural networks), consisting of a novel search space and a reinforcement learning based search algorithm. Extensive experiments on real-world datasets demonstrate the effectiveness of the SNAG framework compared to human-designed GNNs and NAS methods, including GraphNAS and Auto-GNN.",2020-08-26T16:24:03Z,2020-09-06T12:06:14Z,http://arxiv.org/abs/2008.11652v2,http://arxiv.org/pdf/2008.11652v2,"cs.LG, stat.ML"
High-Bandwidth Spatial Equalization for mmWave Massive MU-MIMO with   Processing-In-Memory,"Oscar Castañeda, Sven Jacobsson, Giuseppe Durisi, Tom Goldstein, Christoph Studer","All-digital basestation (BS) architectures enable superior spectral efficiency compared to hybrid solutions in massive multi-user MIMO systems. However, supporting large bandwidths with all-digital architectures at mmWave frequencies is challenging as traditional baseband processing would result in excessively high power consumption and large silicon area. The recently-proposed concept of finite-alphabet equalization is able to address both of these issues by using equalization matrices that contain low-resolution entries to lower the power and complexity of high-throughput matrix-vector products in hardware. In this paper, we explore two different finite-alphabet equalization hardware implementations that tightly integrate the memory and processing elements: (i) a parallel array of multiply-accumulate (MAC) units and (ii) a bit-serial processing-in-memory (PIM) architecture. Our all-digital VLSI implementation results in 28nm CMOS show that the bit-serial PIM architecture reduces the area and power consumption up to a factor of 2x and 3x, respectively, when compared to a parallel MAC array that operates at the same throughput.",2020-09-08T17:30:15Z,2020-09-08T17:30:15Z,http://arxiv.org/abs/2009.03874v1,http://arxiv.org/pdf/2009.03874v1,"eess.SP, cs.AR"
Exploring a Double Full-Stack Communications-Enabled Architecture for   Multi-Core Quantum Computers,"Santiago Rodrigo, Sergi Abadal, Eduard Alarcón, Carmen G. Almudever","Being a very promising technology, with impressive advances in the recent years, it is still unclear how quantum computing will scale to satisfy the requirements of its most powerful applications. Although continued progress in the fabrication and control of qubits is required, quantum computing scalability will depend as well on a comprehensive architectural design considering a multi-core approach as an alternative to the traditional monolithic version, hence including a communications perspective. However, this goes beyond introducing mere interconnects. Rather, it implies consolidating the full communications stack in the quantum computer architecture. In this paper, we propose a double full-stack architecture encompassing quantum computation and quantum communications, which we use to address the monolithic versus multi-core question with a structured design methodology. For that, we revisit the different quantum computing layers to capture and model their essence by highlighting the open design variables and performance metrics. Using behavioral models and actual measurements from existing quantum computers, the results of simulations suggest that multi-core architectures may effectively unleash the full quantum computer potential.",2020-09-17T09:58:26Z,2020-09-17T09:58:26Z,http://arxiv.org/abs/2009.08186v1,http://arxiv.org/pdf/2009.08186v1,"quant-ph, cs.ET"
Context Aware 3D UNet for Brain Tumor Segmentation,"Parvez Ahmad, Saqib Qamar, Linlin Shen, Adnan Saeed","Deep convolutional neural network (CNN) achieves remarkable performance for medical image analysis. UNet is the primary source in the performance of 3D CNN architectures for medical imaging tasks, including brain tumor segmentation. The skip connection in the UNet architecture concatenates features from both encoder and decoder paths to extract multi-contextual information from image data. The multi-scaled features play an essential role in brain tumor segmentation. However, the limited use of features can degrade the performance of the UNet approach for segmentation. In this paper, we propose a modified UNet architecture for brain tumor segmentation. In the proposed architecture, we used densely connected blocks in both encoder and decoder paths to extract multi-contextual information from the concept of feature reusability. In addition, residual-inception blocks (RIB) are used to extract the local and global information by merging features of different kernel sizes. We validate the proposed architecture on the multi-modal brain tumor segmentation challenge (BRATS) 2020 testing dataset. The dice (DSC) scores of the whole tumor (WT), tumor core (TC), and enhancing tumor (ET) are 89.12%, 84.74%, and 79.12%, respectively.",2020-10-25T10:32:25Z,2020-11-27T13:57:26Z,http://arxiv.org/abs/2010.13082v2,http://arxiv.org/pdf/2010.13082v2,"eess.IV, cs.CV"
SIMDive: Approximate SIMD Soft Multiplier-Divider for FPGAs with Tunable   Accuracy,"Zahra Ebrahimi, Salim Ullah, Akash Kumar","The ever-increasing quest for data-level parallelism and variable precision in ubiquitous multimedia and Deep Neural Network (DNN) applications has motivated the use of Single Instruction, Multiple Data (SIMD) architectures. To alleviate energy as their main resource constraint, approximate computing has re-emerged,albeit mainly specialized for their Application-Specific Integrated Circuit (ASIC) implementations. This paper, presents for the first time, an SIMD architecture based on novel multiplier and divider with tunable accuracy, targeted for Field-Programmable Gate Arrays (FPGAs). The proposed hybrid architecture implements Mitchell's algorithms and supports precision variability from 8 to 32 bits. Experimental results obtained from Vivado, multimedia and DNN applications indicate superiority of proposed architecture (both SISD and SIMD) over accurate and state-of-the-art approximate counterparts. In particular, the proposed SISD divider outperforms the accurate Intellectual Property (IP) divider provided by Xilinx with 4x higher speed and 4.6x less energy and tolerating only < 0.8% error. Moreover, the proposed SIMD multiplier-divider supersede accurate SIMD multiplier by achieving up to 26%, 45%, 36%, and 56% improvement in area, throughput, power, and energy, respectively.",2020-11-02T17:40:44Z,2020-11-02T17:40:44Z,http://arxiv.org/abs/2011.01148v1,http://arxiv.org/pdf/2011.01148v1,"cs.AR, cs.LG, eess.IV"
Asynchronous Deep Model Reference Adaptive Control,"Girish Joshi, Jasvir Virdi, Girish Chowdhary","In this paper, we present Asynchronous implementation of Deep Neural Network-based Model Reference Adaptive Control (DMRAC). We evaluate this new neuro-adaptive control architecture through flight tests on a small quadcopter. We demonstrate that a single DMRAC controller can handle significant nonlinearities due to severe system faults and deliberate wind disturbances while executing high-bandwidth attitude control. We also show that the architecture has long-term learning abilities across different flight regimes, and can generalize to fly different flight trajectories than those on which it was trained. These results demonstrating the efficacy of this architecture for high bandwidth closed-loop attitude control of unstable and nonlinear robots operating in adverse situations. To achieve these results, we designed a software+communication architecture to ensure online real-time inference of the deep network on a high-bandwidth computation-limited platform. We expect that this architecture will benefit other deep learning in the closed-loop experiments on robots.",2020-11-04T14:58:58Z,2020-11-04T14:58:58Z,http://arxiv.org/abs/2011.02920v1,http://arxiv.org/pdf/2011.02920v1,"cs.RO, cs.SY, eess.SY"
A Temporal Neural Network Architecture for Online Learning,James E. Smith,"A long-standing proposition is that by emulating the operation of the brain's neocortex, a spiking neural network (SNN) can achieve similar desirable features: flexible learning, speed, and efficiency. Temporal neural networks (TNNs) are SNNs that communicate and process information encoded as relative spike times (in contrast to spike rates). A TNN architecture is proposed, and, as a proof-of-concept, TNN operation is demonstrated within the larger context of online supervised classification. First, through unsupervised learning, a TNN partitions input patterns into clusters based on similarity. The TNN then passes a cluster identifier to a simple online supervised decoder which finishes the classification task. The TNN learning process adjusts synaptic weights by using only signals local to each synapse, and clustering behavior emerges globally. The system architecture is described at an abstraction level analogous to the gate and register transfer levels in conventional digital design. Besides features of the overall architecture, several TNN components are new to this work. Although not addressed directly, the overall research objective is a direct hardware implementation of TNNs. Consequently, all the architecture elements are simple, and processing is done at very low precision.",2020-11-27T17:15:29Z,2021-02-22T22:29:32Z,http://arxiv.org/abs/2011.13844v2,http://arxiv.org/pdf/2011.13844v2,"cs.NE, cs.AI, cs.LG, C.3; I.2.6; I.5.3"
How Does a Neural Network's Architecture Impact Its Robustness to Noisy   Labels?,"Jingling Li, Mozhi Zhang, Keyulu Xu, John P. Dickerson, Jimmy Ba","Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works -- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations -- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.",2020-12-23T18:58:05Z,2021-11-28T04:57:07Z,http://arxiv.org/abs/2012.12896v2,http://arxiv.org/pdf/2012.12896v2,"cs.LG, cs.CV, stat.ML"
Multi-rate attention architecture for fast streamable Text-to-speech   spectrum modeling,"Qing He, Zhiping Xiu, Thilo Koehler, Jilong Wu","Typical high quality text-to-speech (TTS) systems today use a two-stage architecture, with a spectrum model stage that generates spectral frames and a vocoder stage that generates the actual audio. High-quality spectrum models usually incorporate the encoder-decoder architecture with self-attention or bi-directional long short-term (BLSTM) units. While these models can produce high quality speech, they often incur O($L$) increase in both latency and real-time factor (RTF) with respect to input length $L$. In other words, longer inputs leads to longer delay and slower synthesis speed, limiting its use in real-time applications. In this paper, we propose a multi-rate attention architecture that breaks the latency and RTF bottlenecks by computing a compact representation during encoding and recurrently generating the attention vector in a streaming manner during decoding. The proposed architecture achieves high audio quality (MOS of 4.31 compared to groundtruth 4.48), low latency, and low RTF at the same time. Meanwhile, both latency and RTF of the proposed system stay constant regardless of input lengths, making it ideal for real-time applications.",2021-04-01T18:15:30Z,2021-04-01T18:15:30Z,http://arxiv.org/abs/2104.00705v1,http://arxiv.org/pdf/2104.00705v1,"cs.SD, cs.AI, eess.AS"
Partially-Connected Differentiable Architecture Search for Deepfake and   Spoofing Detection,"Wanying Ge, Michele Panariello, Jose Patino, Massimiliano Todisco, Nicholas Evans","This paper reports the first successful application of a differentiable architecture search (DARTS) approach to the deepfake and spoofing detection problems. An example of neural architecture search, DARTS operates upon a continuous, differentiable search space which enables both the architecture and parameters to be optimised via gradient descent. Solutions based on partially-connected DARTS use random channel masking in the search space to reduce GPU time and automatically learn and optimise complex neural architectures composed of convolutional operations and residual blocks. Despite being learned quickly with little human effort, the resulting networks are competitive with the best performing systems reported in the literature. Some are also far less complex, containing 85% fewer parameters than a Res2Net competitor.",2021-04-07T13:53:20Z,2021-06-30T11:03:10Z,http://arxiv.org/abs/2104.03123v2,http://arxiv.org/pdf/2104.03123v2,"cs.LG, cs.SD, eess.AS"
Heart-Darts: Classification of Heartbeats Using Differentiable   Architecture Search,"Jindi Lv, Qing Ye, Yanan Sun, Juan Zhao, Jiancheng Lv","Arrhythmia is a cardiovascular disease that manifests irregular heartbeats. In arrhythmia detection, the electrocardiogram (ECG) signal is an important diagnostic technique. However, manually evaluating ECG signals is a complicated and time-consuming task. With the application of convolutional neural networks (CNNs), the evaluation process has been accelerated and the performance is improved. It is noteworthy that the performance of CNNs heavily depends on their architecture design, which is a complex process grounded on expert experience and trial-and-error. In this paper, we propose a novel approach, Heart-Darts, to efficiently classify the ECG signals by automatically designing the CNN model with the differentiable architecture search (i.e., Darts, a cell-based neural architecture search method). Specifically, we initially search a cell architecture by Darts and then customize a novel CNN model for ECG classification based on the obtained cells. To investigate the efficiency of the proposed method, we evaluate the constructed model on the MIT-BIH arrhythmia database. Additionally, the extensibility of the proposed CNN model is validated on two other new databases. Extensive experimental results demonstrate that the proposed method outperforms several state-of-the-art CNN models in ECG classification in terms of both performance and generalization capability.",2021-05-03T08:57:48Z,2021-05-03T08:57:48Z,http://arxiv.org/abs/2105.00693v1,http://arxiv.org/pdf/2105.00693v1,"eess.SP, cs.LG"
Neural network architectures using min-plus algebra for solving certain   high dimensional optimal control problems and Hamilton-Jacobi PDEs,"Jérôme Darbon, Peter M. Dower, Tingwei Meng","Solving high dimensional optimal control problems and corresponding Hamilton-Jacobi PDEs are important but challenging problems in control engineering. In this paper, we propose two abstract neural network architectures which are respectively used to compute the value function and the optimal control for certain class of high dimensional optimal control problems. We provide the mathematical analysis for the two abstract architectures. We also show several numerical results computed using the deep neural network implementations of these abstract architectures. A preliminary implementation of our proposed neural network architecture on FPGAs shows promising speed up compared to CPUs. This work paves the way to leverage efficient dedicated hardware designed for neural networks to solve high dimensional optimal control problems and Hamilton-Jacobi PDEs.",2021-05-07T15:40:23Z,2023-03-29T19:46:59Z,http://arxiv.org/abs/2105.03336v2,http://arxiv.org/pdf/2105.03336v2,"math.OC, cs.LG"
Surrogate Modeling of Fluid Dynamics with a Multigrid Inspired Neural   Network Architecture,"Quang Tuyen Le, Chin Chun Ooi","Algebraic or geometric multigrid methods are commonly used in numerical solvers as they are a multi-resolution method able to handle problems with multiple scales. In this work, we propose a modification to the commonly-used U-Net neural network architecture that is inspired by the principles of multigrid methods, referred to here as U-Net-MG. We then demonstrate that this proposed U-Net-MG architecture can successfully reduce the test prediction errors relative to the conventional U-Net architecture when modeling a set of fluid dynamic problems. In total, we demonstrate an improvement in the prediction of velocity and pressure fields for the canonical fluid dynamics cases of flow past a stationary cylinder, flow past 2 cylinders in out-of-phase motion, and flow past an oscillating airfoil in both the propulsion and energy harvesting modes. In general, while both the U-Net and U-Net-MG models can model the systems well with test RMSEs of less than 1%, the use of the U-Net-MG architecture can further reduce RMSEs by between 20% and 70%.",2021-05-09T07:04:30Z,2021-05-09T07:04:30Z,http://arxiv.org/abs/2105.03854v1,http://arxiv.org/pdf/2105.03854v1,"physics.flu-dyn, cs.LG, physics.comp-ph"
Dynamically Grown Generative Adversarial Networks,"Lanlan Liu, Yuting Zhang, Jia Deng, Stefano Soatto","Recent work introduced progressive network growing as a promising way to ease the training for large GANs, but the model design and architecture-growing strategy still remain under-explored and needs manual design for different image data. In this paper, we propose a method to dynamically grow a GAN during training, optimizing the network architecture and its parameters together with automation. The method embeds architecture search techniques as an interleaving step with gradient-based training to periodically seek the optimal architecture-growing strategy for the generator and discriminator. It enjoys the benefits of both eased training because of progressive growing and improved performance because of broader architecture design space. Experimental results demonstrate new state-of-the-art of image generation. Observations in the search procedure also provide constructive insights into the GAN model design such as generator-discriminator balance and convolutional layer choices.",2021-06-16T01:25:51Z,2021-06-16T01:25:51Z,http://arxiv.org/abs/2106.08505v1,http://arxiv.org/pdf/2106.08505v1,"cs.CV, cs.LG, eess.IV"
Poisoning the Search Space in Neural Architecture Search,"Robert Wu, Nayan Saxena, Rohan Jain","Deep learning has proven to be a highly effective problem-solving tool for object detection and image segmentation across various domains such as healthcare and autonomous driving. At the heart of this performance lies neural architecture design which relies heavily on domain knowledge and prior experience on the researchers' behalf. More recently, this process of finding the most optimal architectures, given an initial search space of possible operations, was automated by Neural Architecture Search (NAS). In this paper, we evaluate the robustness of one such algorithm known as Efficient NAS (ENAS) against data agnostic poisoning attacks on the original search space with carefully designed ineffective operations. By evaluating algorithm performance on the CIFAR-10 dataset, we empirically demonstrate how our novel search space poisoning (SSP) approach and multiple-instance poisoning attacks exploit design flaws in the ENAS controller to result in inflated prediction error rates for child networks. Our results provide insights into the challenges to surmount in using NAS for more adversarially robust architecture search.",2021-06-28T05:45:57Z,2021-06-28T05:45:57Z,http://arxiv.org/abs/2106.14406v1,http://arxiv.org/pdf/2106.14406v1,"cs.LG, cs.CR, cs.NE, stat.ML"
A Generative Model for Raw Audio Using Transformer Architectures,"Prateek Verma, Chris Chafe","This paper proposes a novel way of doing audio synthesis at the waveform level using Transformer architectures. We propose a deep neural network for generating waveforms, similar to wavenet. This is fully probabilistic, auto-regressive, and causal, i.e. each sample generated depends only on the previously observed samples. Our approach outperforms a widely used wavenet architecture by up to 9% on a similar dataset for predicting the next step. Using the attention mechanism, we enable the architecture to learn which audio samples are important for the prediction of the future sample. We show how causal transformer generative models can be used for raw waveform synthesis. We also show that this performance can be improved by another 2% by conditioning samples over a wider context. The flexibility of the current model to synthesize audio from latent representations suggests a large number of potential applications. The novel approach of using generative transformer architectures for raw audio synthesis is, however, still far away from generating any meaningful music, without using latent codes/meta-data to aid the generation process.",2021-06-30T13:05:31Z,2021-07-08T15:28:02Z,http://arxiv.org/abs/2106.16036v3,http://arxiv.org/pdf/2106.16036v3,"cs.SD, cs.AI, cs.LG, cs.MM, eess.AS"
Comparison between autosar platforms with functional safety for   automotive software architectures,"Youssef Elkharaz, Saad Motahhir, Abdelaziz Elghzizal","In the next Vehicle generations, connected and highly developed driving cars will have an important impact on the networking architecture and the interconnection between ECUs(Electronic Control Unit). The automotive industry begins to develop new and efficient strategies to improve the performance of the global system. AUTOSAR organization as part of this industry tries to present plenary solutions especially software architectures for new technologies in this field. Thus, in this paper, we present the aspects of new E/E architectures with upcoming technologies. We discuss a new solution presented by AUTOSAR organization to implement new software requirements for next generation cars. This solution aims to provide a safe environment for the features that require complex data processing and to communicate with AUTOSAR and non AUTOSAR Platforms. We summarize a comprehensive comparison between AUTOSAR adaptive and AUTOSAR classic in terms of functionality and application area. We provide functional Safety preliminaries for the global E/E architectures.",2021-08-31T22:37:30Z,2021-09-18T16:02:18Z,http://arxiv.org/abs/2109.00099v2,http://arxiv.org/pdf/2109.00099v2,"eess.SY, cs.SY"
Optimal Mapping for Near-Term Quantum Architectures based on Rydberg   Atoms,"Sebastian Brandhofer, Hans Peter Büchler, Ilia Polian","Quantum algorithms promise quadratic or exponential speedups for applications in cryptography, chemistry and material sciences. The topologies of today's quantum computers offer limited connectivity, leading to significant overheads for implementing such quantum algorithms. One-dimensional topology displacements that remedy these limits have been recently demonstrated for architectures based on Rydberg atoms, and they are possible in principle in photonic and ion trap architectures. We present the first optimal quantum circuit-to-architecture mapping algorithm that exploits such one-dimensional topology displacements. We benchmark our method on quantum circuits with up to 15 qubits and investigate the improvements compared with conventional mapping based on inserting swap gates into the quantum circuits. Depending on underlying technology parameters, our approach can decrease the quantum circuit depth by up to 58% and increase the fidelity by up to 29%. We also study runtime and fidelity requirements on one-dimensional displacements and swap gates to derive conditions under which one-dimensional topology displacements provide benefits.",2021-09-09T11:33:43Z,2021-09-09T11:33:43Z,http://arxiv.org/abs/2109.04179v1,http://arxiv.org/pdf/2109.04179v1,"quant-ph, cs.AR"
ReconfigISP: Reconfigurable Camera Image Processing Pipeline,"Ke Yu, Zexian Li, Yue Peng, Chen Change Loy, Jinwei Gu","Image Signal Processor (ISP) is a crucial component in digital cameras that transforms sensor signals into images for us to perceive and understand. Existing ISP designs always adopt a fixed architecture, e.g., several sequential modules connected in a rigid order. Such a fixed ISP architecture may be suboptimal for real-world applications, where camera sensors, scenes and tasks are diverse. In this study, we propose a novel Reconfigurable ISP (ReconfigISP) whose architecture and parameters can be automatically tailored to specific data and tasks. In particular, we implement several ISP modules, and enable backpropagation for each module by training a differentiable proxy, hence allowing us to leverage the popular differentiable neural architecture search and effectively search for the optimal ISP architecture. A proxy tuning mechanism is adopted to maintain the accuracy of proxy networks in all cases. Extensive experiments conducted on image restoration and object detection, with different sensors, light conditions and efficiency constraints, validate the effectiveness of ReconfigISP. Only hundreds of parameters need tuning for every task.",2021-09-10T09:56:43Z,2021-09-10T09:56:43Z,http://arxiv.org/abs/2109.04760v1,http://arxiv.org/pdf/2109.04760v1,"eess.IV, cs.CV"
Temporal Knowledge Distillation for On-device Audio Classification,"Kwanghee Choi, Martin Kersner, Jacob Morton, Buru Chang","Improving the performance of on-device audio classification models remains a challenge given the computational limits of the mobile environment. Many studies leverage knowledge distillation to boost predictive performance by transferring the knowledge from large models to on-device models. However, most lack a mechanism to distill the essence of the temporal information, which is crucial to audio classification tasks, or similar architecture is often required. In this paper, we propose a new knowledge distillation method designed to incorporate the temporal knowledge embedded in attention weights of large transformer-based models into on-device models. Our distillation method is applicable to various types of architectures, including the non-attention-based architectures such as CNNs or RNNs, while retaining the original network architecture during inference. Through extensive experiments on both an audio event detection dataset and a noisy keyword spotting dataset, we show that our proposed method improves the predictive performance across diverse on-device architectures.",2021-10-27T02:29:54Z,2022-02-05T15:44:59Z,http://arxiv.org/abs/2110.14131v2,http://arxiv.org/pdf/2110.14131v2,"cs.SD, cs.LG, eess.AS"
The thermally-coupled imager: A scalable readout architecture for   superconducting nanowire single photon detectors,"Adam N. McCaughan, Yao Zhai, Boris Korzh, Jason P. Allmaras, Bakhrom G. Oripov, Matthew D. Shaw, Sae Woo Nam","Although superconducting nanowire single-photon detectors (SNSPDs) are a promising technology for quantum optics, metrology, and astronomy, they currently lack a readout architecture that is scalable to the megapixel regime and beyond. In this work, we have designed and demonstrated such an architecture for SNSPDs, called the thermally-coupled imager (TCI). The TCI uses a combination of time-of-flight delay lines and thermal coupling to create a scalable architecture that can scale to large array sizes, allows neighboring detectors to operate independently, and requires only four microwave readout lines to operate no matter the size of the array. We give an overview of how the architecture functions, and demonstrate a proof-of-concept $32\times32$ imaging array. The array was able to image a free-space focused spot at 373 nm, count at 9.6 Mcps, and resolve photon location with greater than 99.83\% distinguishability.",2021-12-09T05:19:46Z,2021-12-09T05:19:46Z,http://arxiv.org/abs/2112.04705v1,http://arxiv.org/pdf/2112.04705v1,"cond-mat.supr-con, physics.ins-det, quant-ph"
Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures,"Cesar Carranza, Daniel Llamocca, Marios Pattichis","The manuscript describes fast and scalable architectures and associated algorithms for computing convolutions and cross-correlations. The basic idea is to map 2D convolutions and cross-correlations to a collection of 1D convolutions and cross-correlations in the transform domain. This is accomplished through the use of the Discrete Periodic Radon Transform (DPRT) for general kernels and the use of SVD-LU decompositions for low-rank kernels. The approach uses scalable architectures that can be fitted into modern FPGA and Zynq-SOC devices. Based on different types of available resources, for $P\times P$ blocks, 2D convolutions and cross-correlations can be computed in just $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a trade-off between performance and required numbers and types of resources. We provide implementations of the proposed architectures using modern programmable devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required resources, we show that the proposed approaches significantly outperform current methods.",2021-12-24T22:34:51Z,2021-12-24T22:34:51Z,http://arxiv.org/abs/2112.13150v1,http://arxiv.org/pdf/2112.13150v1,"cs.AR, cs.CV, cs.DC, eess.IV, eess.SP"
An Initial Description of Capabilities and Constraints for a   Computational Auditory System (an Artificial Ear) for Cognitive Architectures,"Frank E. Ritter, Mathieu Brener","We present an initial set of factors, features, and constraints for developing a Computational Auditory System (CAS, aka less formally an artificial ear, AE) for use by cognitive architectures. We start to define a CAS and what tasks it should be able to perform. We then outline the features of a CAS for use by a cognitive architecture and factors that influence its performance. We conclude with an update on what has been created so far and insights on how to create and use a CAS in a cognitive architecture and include a set of functionalities for an artificial ear.",2022-02-10T21:23:17Z,2022-02-10T21:23:17Z,http://arxiv.org/abs/2202.05332v1,http://arxiv.org/pdf/2202.05332v1,"cs.SD, eess.AS"
Neural Architecture Search for Speech Emotion Recognition,"Xixin Wu, Shoukang Hu, Zhiyong Wu, Xunying Liu, Helen Meng","Deep neural networks have brought significant advancements to speech emotion recognition (SER). However, the architecture design in SER is mainly based on expert knowledge and empirical (trial-and-error) evaluations, which is time-consuming and resource intensive. In this paper, we propose to apply neural architecture search (NAS) techniques to automatically configure the SER models. To accelerate the candidate architecture optimization, we propose a uniform path dropout strategy to encourage all candidate architecture operations to be equally optimized. Experimental results of two different neural structures on IEMOCAP show that NAS can improve SER performance (54.89\% to 56.28\%) while maintaining model parameter sizes. The proposed dropout strategy also shows superiority over the previous approaches.",2022-03-31T10:16:10Z,2022-03-31T10:16:10Z,http://arxiv.org/abs/2203.16928v1,http://arxiv.org/pdf/2203.16928v1,"cs.SD, cs.CL, cs.LG, eess.AS"
A Safety Assurable Human-Inspired Perception Architecture,"Rick Salay, Krzysztof Czarnecki","Although artificial intelligence-based perception (AIP) using deep neural networks (DNN) has achieved near human level performance, its well-known limitations are obstacles to the safety assurance needed in autonomous applications. These include vulnerability to adversarial inputs, inability to handle novel inputs and non-interpretability. While research in addressing these limitations is active, in this paper, we argue that a fundamentally different approach is needed to address them. Inspired by dual process models of human cognition, where Type 1 thinking is fast and non-conscious while Type 2 thinking is slow and based on conscious reasoning, we propose a dual process architecture for safe AIP. We review research on how humans address the simplest non-trivial perception problem, image classification, and sketch a corresponding AIP architecture for this task. We argue that this architecture can provide a systematic way of addressing the limitations of AIP using DNNs and an approach to assurance of human-level performance and beyond. We conclude by discussing what components of the architecture may already be addressed by existing work and what remains future work.",2022-05-10T15:41:35Z,2022-06-18T14:14:02Z,http://arxiv.org/abs/2205.07862v2,http://arxiv.org/pdf/2205.07862v2,"cs.LG, cs.AI, cs.RO, cs.SY, eess.SY"
Analyzing Tree Architectures in Ensembles via Neural Tangent Kernel,"Ryuichi Kanoh, Mahito Sugiyama","A soft tree is an actively studied variant of a decision tree that updates splitting rules using the gradient method. Although soft trees can take various architectures, their impact is not theoretically well known. In this paper, we formulate and analyze the Neural Tangent Kernel (NTK) induced by soft tree ensembles for arbitrary tree architectures. This kernel leads to the remarkable finding that only the number of leaves at each depth is relevant for the tree architecture in ensemble learning with an infinite number of trees. In other words, if the number of leaves at each depth is fixed, the training behavior in function space and the generalization performance are exactly the same across different tree architectures, even if they are not isomorphic. We also show that the NTK of asymmetric trees like decision lists does not degenerate when they get infinitely deep. This is in contrast to the perfect binary trees, whose NTK is known to degenerate and leads to worse generalization performance for deeper trees.",2022-05-25T16:49:29Z,2023-02-07T08:08:05Z,http://arxiv.org/abs/2205.12904v2,http://arxiv.org/pdf/2205.12904v2,"cs.LG, stat.ML"
Stacked unsupervised learning with a network architecture found by   supervised meta-learning,"Kyle Luther, H. Sebastian Seung","Stacked unsupervised learning (SUL) seems more biologically plausible than backpropagation, because learning is local to each layer. But SUL has fallen far short of backpropagation in practical applications, undermining the idea that SUL can explain how brains learn. Here we show an SUL algorithm that can perform completely unsupervised clustering of MNIST digits with comparable accuracy relative to unsupervised algorithms based on backpropagation. Our algorithm is exceeded only by self-supervised methods requiring training data augmentation by geometric distortions. The only prior knowledge in our unsupervised algorithm is implicit in the network architecture. Multiple convolutional ""energy layers"" contain a sum-of-squares nonlinearity, inspired by ""energy models"" of primary visual cortex. Convolutional kernels are learned with a fast minibatch implementation of the K-Subspaces algorithm. High accuracy requires preprocessing with an initial whitening layer, representations that are less sparse during inference than learning, and rescaling for gain control. The hyperparameters of the network architecture are found by supervised meta-learning, which optimizes unsupervised clustering accuracy. We regard such dependence of unsupervised learning on prior knowledge implicit in network architecture as biologically plausible, and analogous to the dependence of brain architecture on evolutionary history.",2022-06-06T16:17:20Z,2022-06-06T16:17:20Z,http://arxiv.org/abs/2206.02716v1,http://arxiv.org/pdf/2206.02716v1,"cs.NE, q-bio.NC"
A Functional Architecture for 6G Special Purpose Industrial IoT Networks,"{Nurul Huda Mahmood, Gilberto Berardinelli, Emil J. Khatib, Ramin Hashemi, Carlos de Lima, Matti Latva-aho","Future industrial applications will encompass compelling new use cases requiring stringent performance guarantees over multiple key performance indicators (KPI) such as reliability, dependability, latency, time synchronization, security, etc. Achieving such stringent and diverse service requirements necessitates the design of a special-purpose Industrial Internet of Things (IIoT) network comprising a multitude of specialized functionalities and technological enablers. This article proposes an innovative architecture for such a special-purpose 6G IIoT network incorporating seven functional building blocks categorized into: special-purpose functionalities and enabling technologies. The former consists of Wireless Environment Control, Traffic/Channel Prediction, Proactive Resource Management and End-to-End Optimization functions; whereas the latter includes Synchronization and Coordination, Machine Learning and Artificial Intelligence Algorithms, and Auxiliary Functions. The proposed architecture aims at providing a resource-efficient and holistic solution for the complex and dynamically challenging requirements imposed by future 6G industrial use cases. Selected test scenarios are provided and assessed to illustrate cross-functional collaboration and demonstrate the applicability of the proposed architecture in a wireless IIoT network.",2022-07-01T08:36:44Z,2022-07-01T08:36:44Z,http://arxiv.org/abs/2207.00264v1,http://arxiv.org/pdf/2207.00264v1,"cs.NI, eess.SP"
Hardware architecture for high throughput event visual data filtering   with matrix of IIR filters algorithm,"Marcin Kowalczyk, Tomasz Kryjak","Neuromorphic vision is a rapidly growing field with numerous applications in the perception systems of autonomous vehicles. Unfortunately, due to the sensors working principle, there is a significant amount of noise in the event stream. In this paper we present a novel algorithm based on an IIR filter matrix for filtering this type of noise and a hardware architecture that allows its acceleration using an SoC FPGA. Our method has a very good filtering efficiency for uncorrelated noise - over 99% of noisy events are removed. It has been tested for several event data sets with added random noise. We designed the hardware architecture in such a way as to reduce the utilisation of the FPGA's internal BRAM resources. This enabled a very low latency and a throughput of up to 385.8 MEPS million events per second.The proposed hardware architecture was verified in simulation and in hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the Mercury+ XU9 module with the Mercury+ ST1 base board.",2022-07-02T15:18:53Z,2022-07-02T15:18:53Z,http://arxiv.org/abs/2207.00860v1,http://arxiv.org/pdf/2207.00860v1,"cs.CV, cs.AR, eess.IV, eess.SP"
Evolutionary Quantum Architecture Search for Parametrized Quantum   Circuits,"Li Ding, Lee Spector","Recent advancements in quantum computing have shown promising computational advantages in many problem areas. As one of those areas with increasing attention, hybrid quantum-classical machine learning systems have demonstrated the capability to solve various data-driven learning tasks. Recent works show that parameterized quantum circuits (PQCs) can be used to solve challenging reinforcement learning (RL) tasks with provable learning advantages. While existing works yield potentials of PQC-based methods, the design choices of PQC architectures and their influences on the learning tasks are generally underexplored. In this work, we introduce EQAS-PQC, an evolutionary quantum architecture search framework for PQC-based models, which uses a population-based genetic algorithm to evolve PQC architectures by exploring the search space of quantum operations. Experimental results show that our method can significantly improve the performance of hybrid quantum-classical models in solving benchmark reinforcement problems. We also model the probability distributions of quantum operations in top-performing architectures to identify essential design choices that are critical to the performance.",2022-08-23T19:47:37Z,2022-08-23T19:47:37Z,http://arxiv.org/abs/2208.11167v1,http://arxiv.org/pdf/2208.11167v1,"cs.NE, quant-ph"
Image Classification using Sequence of Pixels,Gajraj Kuldeep,"This study compares sequential image classification methods based on recurrent neural networks. We describe methods based on recurrent neural networks such as Long-Short-Term memory(LSTM), bidirectional Long-Short-Term memory(BiLSTM) architectures, etc. We also review the state-of-the-art sequential image classification architectures. We mainly focus on LSTM, BiLSTM, temporal convolution network, and independent recurrent neural network architecture in the study. It is known that RNN lacks in learning long-term dependencies in the input sequence. We use a simple feature construction method using orthogonal Ramanujan periodic transform on the input sequence. Experiments demonstrate that if these features are given to LSTM or BiLSTM networks, the performance increases drastically.   Our focus in this study is to increase the training accuracy simultaneously reducing the training time for the LSTM and BiLSTM architecture, but not on pushing the state-of-the-art results, so we use simple LSTM/BiLSTM architecture. We compare sequential input with the constructed feature as input to single layer LSTM and BiLSTM network for MNIST and CIFAR datasets. We observe that sequential input to the LSTM network with 128 hidden unit training for five epochs results in training accuracy of 33% whereas constructed features as input to the same LSTM network results in training accuracy of 90% with 1/3 lesser time.",2022-09-23T09:42:44Z,2022-09-23T09:42:44Z,http://arxiv.org/abs/2209.11495v1,http://arxiv.org/pdf/2209.11495v1,"eess.IV, cs.LG"
An unsupervised latent/output physics-informed convolutional-LSTM   network for solving partial differential equations using peridynamic   differential operator,"A. Mavi, A. C. Bekar, E. Haghighat, E. Madenci","This study presents a novel unsupervised convolutional Neural Network (NN) architecture with nonlocal interactions for solving Partial Differential Equations (PDEs). The nonlocal Peridynamic Differential Operator (PDDO) is employed as a convolutional filter for evaluating derivatives the field variable. The NN captures the time-dynamics in smaller latent space through encoder-decoder layers with a Convolutional Long-short Term Memory (ConvLSTM) layer between them. The ConvLSTM architecture is modified by employing a novel activation function to improve the predictive capability of the learning architecture for physics with periodic behavior. The physics is invoked in the form of governing equations at the output of the NN and in the latent (reduced) space. By considering a few benchmark PDEs, we demonstrate the training performance and extrapolation capability of this novel NN architecture by comparing against Physics Informed Neural Networks (PINN) type solvers. It is more capable of extrapolating the solution for future timesteps than the other existing architectures.",2022-10-21T18:09:23Z,2022-10-21T18:09:23Z,http://arxiv.org/abs/2210.12177v1,http://arxiv.org/pdf/2210.12177v1,"cs.LG, cs.NA, math.NA"
Construction of Hierarchical Neural Architecture Search Spaces based on   Context-free Grammars,"Simon Schrodi, Danny Stoll, Binxin Ru, Rhea Sukthanker, Thomas Brox, Frank Hutter","The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Code is available at https://github.com/automl/hierarchical_nas_construction.",2022-11-03T14:23:00Z,2023-12-08T14:09:11Z,http://arxiv.org/abs/2211.01842v3,http://arxiv.org/pdf/2211.01842v3,"cs.LG, cs.AI, cs.CV, stat.ML"
A study and comparison of COordinate Rotation DIgital Computer (CORDIC)   architectures,"Neha K Nawandar, Vishal R Satpute","Most of the digital signal processing applications performs operations like multiplication, addition, square-root calculation, solving linear equations etc. The physical implementation of these operations consumes a lot of hardware and, software implementation consumes large memory. Even if they are implemented in hardware, they do not provide high speed, and due to this reason, even today the software implementation dominates hardware. For realizing operations from basic to very complex ones with less hardware, a Co-ordinate Rotation Digital Computer (CORDIC) proves beneficial. It is capable of performing mathematical operations right from addition to highly complex functions with the help of arithmetic unit and shifters only. This paper gives a brief overview of various existing CORDIC architectures, their working principle, application domain and a comparison of these architectures. Different designs are available as per the target, i.e. high accuracy and precision, low area, low latency, hardware efficient, low power, reconfigurability, etc. that can be used as per the application in which the architecture needs to be employed.",2022-11-08T07:25:36Z,2022-11-08T07:25:36Z,http://arxiv.org/abs/2211.04053v1,http://arxiv.org/pdf/2211.04053v1,"cs.AR, eess.IV"
"Performance Evaluation of Vanilla, Residual, and Dense 2D U-Net   Architectures for Skull Stripping of Augmented 3D T1-weighted MRI Head Scans","Anway S. Pimpalkar, Rashmika K. Patole, Ketaki D. Kamble, Mahesh H. Shindikar","Skull Stripping is a requisite preliminary step in most diagnostic neuroimaging applications. Manual Skull Stripping methods define the gold standard for the domain but are time-consuming and challenging to integrate into processing pipelines with a high number of data samples. Automated methods are an active area of research for head MRI segmentation, especially deep learning methods such as U-Net architecture implementations. This study compares Vanilla, Residual, and Dense 2D U-Net architectures for Skull Stripping. The Dense 2D U-Net architecture outperforms the Vanilla and Residual counterparts by achieving an accuracy of 99.75% on a test dataset. It is observed that dense interconnections in a U-Net encourage feature reuse across layers of the architecture and allow for shallower models with the strengths of a deeper network.",2022-11-29T20:11:11Z,2023-01-21T05:29:40Z,http://arxiv.org/abs/2211.16570v2,http://arxiv.org/pdf/2211.16570v2,"eess.IV, cs.CV"
Spectral Efficiency and Scalability Analysis for Multi-Level Cooperative   Cell-Free Massive MIMO Systems,"Jiamin Li, Xiaoyu Sun, Pengcheng Zhu, Dongming Wang, Xiaohu You","This paper proposes a multi-level cooperative architecture to balance the spectral efficiency and scalability of cell-free massive multiple-input multiple-output (MIMO) systems. In the proposed architecture, spatial expansion units (SEUs) are introduced to avoid a large amount of computation at the access points (APs) and increase the degree of cooperation among APs. We first derive the closed-form expressions of the uplink user achievable rates under multi-level cooperative architecture with maximal ratio combination (MRC) and zero-forcing (ZF) receivers. The accuracy of the closed-form expressions is verified. Moreover, numerical results have demonstrated that the proposed multi-level cooperative architecture achieves a better trade-off between spectral efficiency and scalability than other forms of cell-free massive MIMO architectures.",2023-02-16T06:14:47Z,2023-02-16T06:14:47Z,http://arxiv.org/abs/2302.08107v1,http://arxiv.org/pdf/2302.08107v1,"cs.IT, eess.SP, math.IT"
From Audio to Symbolic Encoding,"Shenli Yuan, Lingjie Kong, Jiushuang Guo","Automatic music transcription (AMT) aims to convert raw audio to symbolic music representation. As a fundamental problem of music information retrieval (MIR), AMT is considered a difficult task even for trained human experts due to overlap of multiple harmonics in the acoustic signal. On the other hand, speech recognition, as one of the most popular tasks in natural language processing, aims to translate human spoken language to texts. Based on the similar nature of AMT and speech recognition (as they both deal with tasks of translating audio signal to symbolic encoding), this paper investigated whether a generic neural network architecture could possibly work on both tasks. In this paper, we introduced our new neural network architecture built on top of the current state-of-the-art Onsets and Frames, and compared the performances of its multiple variations on AMT task. We also tested our architecture with the task of speech recognition. For AMT, our models were able to produce better results compared to the model trained using the state-of-art architecture; however, although similar architecture was able to be trained on the speech recognition task, it did not generate very ideal result compared to other task-specific models.",2023-02-26T20:15:00Z,2023-02-26T20:15:00Z,http://arxiv.org/abs/2302.13401v1,http://arxiv.org/pdf/2302.13401v1,"cs.SD, cs.IR, cs.LG, eess.AS"
Torque Control with Joints Position and Velocity Limits Avoidance,"Venus Pasandi, Daniele Pucci","The design of a control architecture for providing the desired motion along with the realization of the joint limitation of a robotic system is still an open challenge in control and robotics. This paper presents a torque control architecture for fully actuated manipulators for tracking the desired time-varying trajectory while ensuring the joints position and velocity limits. The presented architecture stems from the parametrization of the feasible joints position and velocity space by exogenous states. The proposed parametrization transforms the control problem with constrained states to an un-constrained one by replacing the joints position and velocity with the exogenous states. With the help of Lyapunov-based arguments, we prove that the proposed control architecture ensures the stability and convergence of the desired joint trajectory along with the joints position and velocity limits avoidance. We validate the performance of proposed architecture through various simulations on a simple two-degree-of-freedom manipulator and the humanoid robot iCub.",2023-03-30T09:30:26Z,2023-03-30T09:30:26Z,http://arxiv.org/abs/2303.17252v1,http://arxiv.org/pdf/2303.17252v1,"cs.RO, math.DS"
STM-UNet: An Efficient U-shaped Architecture Based on Swin Transformer   and Multi-scale MLP for Medical Image Segmentation,"Lei Shi, Tianyu Gao, Zheng Zhang, Junxing Zhang","Automated medical image segmentation can assist doctors to diagnose faster and more accurate. Deep learning based models for medical image segmentation have made great progress in recent years. However, the existing models fail to effectively leverage Transformer and MLP for improving U-shaped architecture efficiently. In addition, the multi-scale features of the MLP have not been fully extracted in the bottleneck of U-shaped architecture. In this paper, we propose an efficient U-shaped architecture based on Swin Transformer and multi-scale MLP, namely STM-UNet. Specifically, the Swin Transformer block is added to skip connection of STM-UNet in form of residual connection, which can enhance the modeling ability of global features and long-range dependency. Meanwhile, a novel PCAS-MLP with parallel convolution module is designed and placed into the bottleneck of our architecture to contribute to the improvement of segmentation performance. The experimental results on ISIC 2016 and ISIC 2018 demonstrate the effectiveness of our proposed method. Our method also outperforms several state-of-the-art methods in terms of IoU and Dice. Our method has achieved a better trade-off between high segmentation accuracy and low model complexity.",2023-04-25T07:18:40Z,2023-04-25T07:18:40Z,http://arxiv.org/abs/2304.12615v1,http://arxiv.org/pdf/2304.12615v1,"eess.IV, cs.CV"
Neural Architecture Search for Parameter-Efficient Fine-tuning of Large   Pre-trained Language Models,"Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan, Greg Ver Steeg","Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.",2023-05-26T03:01:07Z,2023-05-26T03:01:07Z,http://arxiv.org/abs/2305.16597v1,http://arxiv.org/pdf/2305.16597v1,"cs.CL, cs.AI, cs.LG, I.2.7"
Machine learning for option pricing: an empirical investigation of   network architectures,"Laurens Van Mieghem, Antonis Papapantoleon, Jonas Papazoglou-Hennig","We consider the supervised learning problem of learning the price of an option or the implied volatility given appropriate input data (model parameters) and corresponding output data (option prices or implied volatilities). The majority of articles in this literature considers a (plain) feed forward neural network architecture in order to connect the neurons used for learning the function mapping inputs to outputs. In this article, motivated by methods in image classification and recent advances in machine learning methods for PDEs, we investigate empirically whether and how the choice of network architecture affects the accuracy and training time of a machine learning algorithm. We find that for option pricing problems, where we focus on the Black--Scholes and the Heston model, the generalized highway network architecture outperforms all other variants, when considering the mean squared error and the training time as criteria. Moreover, for the computation of the implied volatility, after a necessary transformation, a variant of the DGM architecture outperforms all other variants, when considering again the mean squared error and the training time as criteria.",2023-07-14T23:27:43Z,2023-07-14T23:27:43Z,http://arxiv.org/abs/2307.07657v1,http://arxiv.org/pdf/2307.07657v1,"q-fin.CP, cs.LG, 91G20, 91G60, 68T07"
CERMET: Coding for Energy Reduction with Multiple Encryption Techniques   -- $It's\ easy\ being\ green$,"Jongchan Woo, Vipindev Adat Vasudevan, Benjamin Kim, Alejandro Cohen, Rafael G. L. D'Oliveira, Thomas Stahlbuhk, Muriel Médard","This paper presents CERMET, an energy-efficient hardware architecture designed for hardware-constrained cryptosystems. CERMET employs a base cryptosystem in conjunction with network coding to provide both information-theoretic and computational security while reducing energy consumption per bit. This paper introduces the hardware architecture for the system and explores various optimizations to enhance its performance. The universality of the approach is demonstrated by designing the architecture to accommodate both asymmetric and symmetric cryptosystems. The analysis reveals that the benefits of this proposed approach are multifold, reducing energy per bit and area without compromising security or throughput. The optimized hardware architectures can achieve below 1 pJ/bit operations for AES-256. Furthermore, for a public key cryptosystem based on Elliptic Curve Cryptography (ECC), a remarkable 14.6X reduction in energy per bit and a 9.3X reduction in area are observed, bringing it to less than 1 nJ/bit.",2023-08-09T16:47:35Z,2023-08-09T16:47:35Z,http://arxiv.org/abs/2308.05063v1,http://arxiv.org/pdf/2308.05063v1,"cs.CR, cs.AR, cs.IT, cs.SY, eess.SY, math.IT"
A Control Architecture for Entanglement Generation Switches in Quantum   Networks,"Scarlett Gauthier, Gayane Vardoyan, Stephanie Wehner","Entanglement between quantum network nodes is often produced using intermediary devices - such as heralding stations - as a resource. When scaling quantum networks to many nodes, requiring a dedicated intermediary device for every pair of nodes introduces high costs. Here, we propose a cost-effective architecture to connect many quantum network nodes via a central quantum network hub called an Entanglement Generation Switch (EGS). The EGS allows multiple quantum nodes to be connected at a fixed resource cost, by sharing the resources needed to make entanglement. We propose an algorithm called the Rate Control Protocol (RCP) which moderates the level of competition for access to the hub's resources between sets of users. We proceed to prove a convergence theorem for rates yielded by the algorithm. To derive the algorithm we work in the framework of Network Utility Maximization (NUM) and make use of the theory of Lagrange multipliers and Lagrangian duality. Our EGS architecture lays the groundwork for developing control architectures compatible with other types of quantum network hubs as well as system models of greater complexity.",2023-09-05T10:06:48Z,2023-09-05T10:06:48Z,http://arxiv.org/abs/2309.02098v1,http://arxiv.org/pdf/2309.02098v1,"quant-ph, cs.NI"
Advancing Text-to-GLOSS Neural Translation Using a Novel Hyper-parameter   Optimization Technique,"Younes Ouargani, Noussaima El Khattabi","In this paper, we investigate the use of transformers for Neural Machine Translation of text-to-GLOSS for Deaf and Hard-of-Hearing communication. Due to the scarcity of available data and limited resources for text-to-GLOSS translation, we treat the problem as a low-resource language task. We use our novel hyper-parameter exploration technique to explore a variety of architectural parameters and build an optimal transformer-based architecture specifically tailored for text-to-GLOSS translation. The study aims to improve the accuracy and fluency of Neural Machine Translation generated GLOSS. This is achieved by examining various architectural parameters including layer count, attention heads, embedding dimension, dropout, and label smoothing to identify the optimal architecture for improving text-to-GLOSS translation performance. The experiments conducted on the PHOENIX14T dataset reveal that the optimal transformer architecture outperforms previous work on the same dataset. The best model reaches a ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score of 55.18% and a BLEU-1 (BiLingual Evaluation Understudy 1) score of 63.6%, outperforming state-of-the-art results on the BLEU1 and ROUGE score by 8.42 and 0.63 respectively.",2023-09-05T11:59:31Z,2023-09-05T11:59:31Z,http://arxiv.org/abs/2309.02162v1,http://arxiv.org/pdf/2309.02162v1,"cs.CL, I.2.7"
Brief Architectural Survey of Biopotential Recording Front-Ends since   the 1970s,"Taeju Lee, Minkyu Je","Measuring the bioelectric signals is one of the key functions in wearable healthcare devices and implantable medical devices. The use of wearable healthcare devices has made continuous and immediate monitoring of personal health status possible. Implantable medical devices have played an important role throughout the fields of neuroscience, brain-machine (or brain-computer) interface, and rehabilitation technology. Over the last five decades, the bioelectric signals have been observed through a variety of biopotential recording front-ends, along with advances in semiconductor technology scaling and circuit techniques. Also, for reliable and continuous signal acquisition, the front-end architectures have evolved while maintaining low power and low noise performance. In this article, the architecture history of the biopotential recording front-ends developed since the 1970s is surveyed, and overall key circuit techniques are discussed. Depending on the bioelectric signals being measured, appropriate front-end architecture needs to be chosen, and the characteristics and challenges of each architecture are also covered in this article.",2023-09-20T19:57:32Z,2023-09-20T19:57:32Z,http://arxiv.org/abs/2309.11612v1,http://arxiv.org/pdf/2309.11612v1,"eess.SY, cs.HC, cs.SY, eess.SP"
On the Disconnect Between Theory and Practice of Neural Networks: Limits   of the NTK Perspective,"Jonathan Wenger, Felix Dangel, Agustinus Kristiadi","The neural tangent kernel (NTK) has garnered significant attention as a theoretical framework for describing the behavior of large-scale neural networks. Kernel methods are theoretically well-understood and as a result enjoy algorithmic benefits, which can be demonstrated to hold in wide synthetic neural network architectures. These advantages include faster optimization, reliable uncertainty quantification and improved continual learning. However, current results quantifying the rate of convergence to the kernel regime suggest that exploiting these benefits requires architectures that are orders of magnitude wider than they are deep. This assumption raises concerns that architectures used in practice do not exhibit behaviors as predicted by the NTK. Here, we supplement previous work on the NTK by empirically investigating whether the limiting regime predicts practically relevant behavior of large-width architectures. Our results demonstrate that this is not the case across multiple domains. This observed disconnect between theory and practice further calls into question to what degree NTK theory should inform architectural and algorithmic choices.",2023-09-29T20:51:24Z,2024-05-28T20:59:41Z,http://arxiv.org/abs/2310.00137v2,http://arxiv.org/pdf/2310.00137v2,"cs.LG, stat.ML"
HAPS in the Non-Terrestrial Network Nexus: Prospective Architectures and   Performance Insights,"Zhengying Lou, Baha Eddine Youcef Belmekki, Mohamed-Slim Alouini","High altitude platform stations (HAPS) have recently emerged as a new key stratospheric player in non-terrestrial networks (NTN) alongside satellites and low-altitude platforms. In this paper, we present the main communication links between HAPS and other NTN platforms, their advantages, and their challenges. Then, prospective network architectures in which HAPS plays an indispensable role in the future NTNs are presented such as ad-hoc, cell-free, and integrated access and backhaul. To showcase the importance of HAPS in the NTN, we provide comprehensive performance insights when using HAPS in the prospective architectures with the most suitable communication link. The insights show the HAPS' ability to interconnect the NTN nexus as well as their versatility by incorporating different metrics into the analysis such as routing latency, energy efficiency, coverage probability, and channel capacity. Depending on the architecture, HAPS will play different roles in NTN, such as a UAV network center, satellite relay, and ground network extension. Finally, the performance gain provided by HAPS usage in NTN is further highlighted by comparing the results when no HAPS are used.",2023-10-14T20:20:03Z,2023-10-14T20:20:03Z,http://arxiv.org/abs/2310.09659v1,http://arxiv.org/pdf/2310.09659v1,"cs.NI, eess.SP"
CycleGANAS: Differentiable Neural Architecture Search for CycleGAN,"Taegun An, Changhee Joo","We develop a Neural Architecture Search (NAS) framework for CycleGAN that carries out unpaired image-to-image translation task. Extending previous NAS techniques for Generative Adversarial Networks (GANs) to CycleGAN is not straightforward due to the task difference and greater search space. We design architectures that consist of a stack of simple ResNet-based cells and develop a search method that effectively explore the large search space. We show that our framework, called CycleGANAS, not only effectively discovers high-performance architectures that either match or surpass the performance of the original CycleGAN, but also successfully address the data imbalance by individual architecture search for each translation direction. To our best knowledge, it is the first NAS result for CycleGAN and shed light on NAS for more complex structures.",2023-11-13T08:56:56Z,2023-11-13T08:56:56Z,http://arxiv.org/abs/2311.07162v1,http://arxiv.org/pdf/2311.07162v1,"cs.CV, cs.LG, eess.IV"
Graph Metanetworks for Processing Diverse Neural Architectures,"Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas","Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",2023-12-07T18:21:52Z,2023-12-29T22:55:45Z,http://arxiv.org/abs/2312.04501v2,http://arxiv.org/pdf/2312.04501v2,"cs.LG, cs.AI, stat.ML"
Accelerating Seed Location Filtering in DNA Read Mapping Using a   Commercial Compute-in-SRAM Architecture,"Courtney Golden, Dan Ilan, Nicholas Cebry, Christopher Batten","DNA sequence alignment is an important workload in computational genomics. Reference-guided DNA assembly involves aligning many read sequences against candidate locations in a long reference genome. To reduce the computational load of this alignment, candidate locations can be pre-filtered using simpler alignment algorithms like edit distance. Prior work has explored accelerating filtering on simulated compute-in-DRAM, due to the massive parallelism of compute-in-memory architectures. In this paper, we present work-in-progress on accelerating filtering using a commercial compute-in-SRAM accelerator. We leverage the recently released Gemini accelerator platform from GSI Technology, which is the first, to our knowledge, commercial-scale compute-in-SRAM system. We accelerate the Myers' bit-parallel edit distance algorithm, producing average speedups of 14.1x over single-core CPU performance. Individual query/candidate alignments produce speedups of up to 24.1x. These early results suggest this novel architecture is well-suited to accelerating the filtering step of sequence-to-sequence DNA alignment.",2024-01-22T04:44:50Z,2024-01-22T04:44:50Z,http://arxiv.org/abs/2401.11685v1,http://arxiv.org/pdf/2401.11685v1,"cs.AR, q-bio.GN"
Partially Stochastic Infinitely Deep Bayesian Neural Networks,"Sergio Calvo-Ordonez, Matthieu Meunier, Francesco Piatti, Yuantao Shi","In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the computational efficiency of existing architectures at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational complexity. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators. Lastly, empirical evaluations across multiple tasks show that our proposed architectures achieve better downstream task performance and uncertainty quantification than their counterparts while being significantly more efficient. The code can be found at \url{https://github.com/Sergio20f/part_stoch_inf_deep}",2024-02-05T20:15:19Z,2024-07-13T17:27:30Z,http://arxiv.org/abs/2402.03495v4,http://arxiv.org/pdf/2402.03495v4,"cs.LG, math.PR"
Qubit-Wise Architecture Search Method for Variational Quantum Circuits,"Jialin Chen, Zhiqiang Cai, Ke Xu, Di Wu, Wei Cao","Considering the noise level limit, one crucial aspect for quantum machine learning is to design a high-performing variational quantum circuit architecture with small number of quantum gates. As the classical neural architecture search (NAS), quantum architecture search methods (QAS) employ methods like reinforcement learning, evolutionary algorithms and supernet optimiza-tion to improve the search efficiency. In this paper, we propose a novel qubit-wise architec-ture search (QWAS) method, which progres-sively search one-qubit configuration per stage, and combine with Monte Carlo Tree Search al-gorithm to find good quantum architectures by partitioning the search space into several good and bad subregions. The numerical experimental results indicate that our proposed method can balance the exploration and exploitation of cir-cuit performance and size in some real-world tasks, such as MNIST, Fashion and MOSI. As far as we know, QWAS achieves the state-of-art re-sults of all tasks in the terms of accuracy and circuit size.",2024-03-07T07:08:57Z,2024-03-07T07:08:57Z,http://arxiv.org/abs/2403.04268v1,http://arxiv.org/pdf/2403.04268v1,"quant-ph, cs.LG"
"Robust NAS under adversarial training: benchmark, theory, and beyond","Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G Chrysos, Volkan Cevher","Recent developments in neural architecture search (NAS) emphasize the significance of considering robust architectures against malicious data. However, there is a notable absence of benchmark evaluations and theoretical guarantees for searching these robust architectures, especially when adversarial training is considered. In this work, we aim to address these two challenges, making twofold contributions. First, we release a comprehensive data set that encompasses both clean accuracy and robust accuracy for a vast array of adversarially trained networks from the NAS-Bench-201 search space on image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep learning theory, we establish a generalization theory for searching architecture in terms of clean accuracy and robust accuracy under multi-objective adversarial training. We firmly believe that our benchmark and theoretical insights will significantly benefit the NAS community through reliable reproducibility, efficient assessment, and theoretical foundation, particularly in the pursuit of robust architectures.",2024-03-19T20:10:23Z,2024-03-19T20:10:23Z,http://arxiv.org/abs/2403.13134v1,http://arxiv.org/pdf/2403.13134v1,"cs.LG, cs.AI, stat.ML"
Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep   Learning and Explainable AI Analysis,"Ekramul Haque, Kamrul Hasan, Imtiaz Ahmed, Md. Sahabul Alam, Tariqul Islam","In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs), the utmost importance lies in guaranteeing resilient and lucid security measures. This study highlights the necessity of implementing a Zero Trust Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs), hence departing from conventional perimeter defences that may expose vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous and continuous process of authenticating all network entities and communications. The accuracy of our methodology in detecting and identifying unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio Frequency (RF) signals within a Deep Learning framework, a unique method. Precise identification is crucial in Zero Trust Architecture (ZTA), as it determines network access. In addition, the use of eXplainable Artificial Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) contributes to the improvement of the model's transparency and interpretability. Adherence to Zero Trust Architecture (ZTA) standards guarantees that the classifications of unmanned aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security within the UAV field.",2024-03-25T18:32:22Z,2024-03-25T18:32:22Z,http://arxiv.org/abs/2403.17093v1,http://arxiv.org/pdf/2403.17093v1,"cs.LG, eess.SP"
Efficient NAS with FaDE on Hierarchical Spaces,"Simon Neumeyer, Julian Stier, Michael Granitzer","Neural architecture search (NAS) is a challenging problem. Hierarchical search spaces allow for cheap evaluations of neural network sub modules to serve as surrogate for architecture evaluations. Yet, sometimes the hierarchy is too restrictive or the surrogate fails to generalize. We present FaDE which uses differentiable architecture search to obtain relative performance predictions on finite regions of a hierarchical NAS space. The relative nature of these ranks calls for a memory-less, batch-wise outer search algorithm for which we use an evolutionary algorithm with pseudo-gradient descent. FaDE is especially suited on deep hierarchical, respectively multi-cell search spaces, which it can explore by linear instead of exponential cost and therefore eliminates the need for a proxy search space.   Our experiments show that firstly, FaDE-ranks on finite regions of the search space correlate with corresponding architecture performances and secondly, the ranks can empower a pseudo-gradient evolutionary search on the complete neural architecture search space.",2024-04-24T21:33:17Z,2024-04-24T21:33:17Z,http://arxiv.org/abs/2404.16218v1,http://arxiv.org/pdf/2404.16218v1,"cs.NE, cs.AI, cs.LG, I.2.6"
An Implementation and Analysis of a Practical Quantum Link Architecture   Utilizing Entangled Photon Sources,"Kento Samuel Soon, Michal Hajdušek, Shota Nagayama, Naphan Benchasattabuse, Kentaro Teramoto, Ryosuke Satoh, Rodney Van Meter","Quantum repeater networks play a crucial role in distributing entanglement. Various link architectures have been proposed to facilitate the creation of Bell pairs between distant nodes, with entangled photon sources emerging as a primary technology for building quantum networks. Our work advances the Memory-Source-Memory (MSM) link architecture, addressing the absence of practical implementation details. We conduct numerical simulations using the Quantum Internet Simulation Package (QuISP) to analyze the performance of the MSM link and contrast it with other link architectures. We observe a saturation effect in the MSM link, where additional quantum resources do not affect the Bell pair generation rate of the link. By introducing a theoretical model, we explain the origin of this effect and characterize the parameter region where it occurs. Our work bridges theoretical insights with practical implementation, which is crucial for robust and scalable quantum networks.",2024-05-16T07:38:47Z,2024-05-16T07:38:47Z,http://arxiv.org/abs/2405.09861v1,http://arxiv.org/pdf/2405.09861v1,"quant-ph, cs.NI"
Regularity-Conforming Neural Networks (ReCoNNs) for solving Partial   Differential Equations,"Jamie M. Taylor, David Pardo, Judit Muñoz-Matute","Whilst the Universal Approximation Theorem guarantees the existence of approximations to Sobolev functions -- the natural function spaces for PDEs -- by Neural Networks (NNs) of sufficient size, low-regularity solutions may lead to poor approximations in practice. For example, classical fully-connected feed-forward NNs fail to approximate continuous functions whose gradient is discontinuous when employing strong formulations like in Physics Informed Neural Networks (PINNs). In this article, we propose the use of regularity-conforming neural networks, where a priori information on the regularity of solutions to PDEs can be employed to construct proper architectures. We illustrate the potential of such architectures via a two-dimensional (2D) transmission problem, where the solution may admit discontinuities in the gradient across interfaces, as well as power-like singularities at certain points. In particular, we formulate the weak transmission problem in a PINNs-like strong formulation with interface and continuity conditions. Such architectures are partially explainable; discontinuities are explicitly described, allowing the introduction of novel terms into the loss function. We demonstrate via several model problems in one and two dimensions the advantages of using regularity-conforming architectures in contrast to classical architectures. The ideas presented in this article easily extend to problems in higher dimensions.",2024-05-23T02:31:53Z,2024-05-23T02:31:53Z,http://arxiv.org/abs/2405.14110v1,http://arxiv.org/pdf/2405.14110v1,"math.NA, cs.NA"
A Scalable FPGA Architecture for Quantum Computing Simulation,Lee A. Belfore II,"A quantum computing simulation provides the opportunity to explore the behaviors of quantum circuits, study the properties of quantum gates, and develop quantum computing algorithms. Simulating quantum circuits requires geometric time and space complexities, impacting the size of the quantum circuit that can be simulated as well as the respective time required to simulate a particular circuit. Applying the parallelism inherent in the simulation and crafting custom architectures, larger quantum circuits can be simulated. A scalable accelerator architecture is proposed to provide a high performance, highly parallel, accelerator. Among the challenges of creating a scalable architecture is managing parallelism, efficiently routing quantum state components for gate evaluation, and measurement. An example is demonstrated on an Intel Agilex field programmable gate array (FPGA).",2024-07-08T21:48:28Z,2024-07-08T21:48:28Z,http://arxiv.org/abs/2407.06415v1,http://arxiv.org/pdf/2407.06415v1,"quant-ph, cs.AR"
NeuralMultiling: A Novel Neural Architecture Search for Smartphone based   Multilingual Speaker Verification,"Aravinda Reddy PN, Raghavendra Ramachandra, K. Sreenivasa Rao, Pabitra Mitra","Multilingual speaker verification introduces the challenge of verifying a speaker in multiple languages. Existing systems were built using i-vector/x-vector approaches along with Bi-LSTMs, which were trained to discriminate speakers, irrespective of the language. Instead of exploring the design space manually, we propose a neural architecture search for multilingual speaker verification suitable for mobile devices, called \textbf{NeuralMultiling}. First, our algorithm searches for an optimal operational combination of neural cells with different architectures for normal cells and reduction cells and then derives a CNN model by stacking neural cells. Using the derived architecture, we performed two different studies:1) language agnostic condition and 2) interoperability between languages and devices on the publicly available Multilingual Audio-Visual Smartphone (MAVS) dataset. The experimental results suggest that the derived architecture significantly outperforms the existing Autospeech method by a 5-6\% reduction in the Equal Error Rate (EER) with fewer model parameters.",2024-08-08T10:49:17Z,2024-08-08T10:49:17Z,http://arxiv.org/abs/2408.04362v1,http://arxiv.org/pdf/2408.04362v1,"cs.SD, eess.AS"
HYDRA: Hybrid Data Multiplexing and Run-time Layer Configurable DNN   Accelerator,"Sonu Kumar, Komal Gupta, Gopal Raut, Mukul Lokhande, Santosh Kumar Vishvakarma","Deep neural networks (DNNs) offer plenty of challenges in executing efficient computation at edge nodes, primarily due to the huge hardware resource demands. The article proposes HYDRA, hybrid data multiplexing, and runtime layer configurable DNN accelerators to overcome the drawbacks. The work proposes a layer-multiplexed approach, which further reuses a single activation function within the execution of a single layer with improved Fused-Multiply-Accumulate (FMA). The proposed approach works in iterative mode to reuse the same hardware and execute different layers in a configurable fashion. The proposed architectures achieve reductions over 90% of power consumption and resource utilization improvements of state-of-the-art works, with 35.21 TOPSW. The proposed architecture reduces the area overhead (N-1) times required in bandwidth, AF and layer architecture. This work shows HYDRA architecture supports optimal DNN computations while improving performance on resource-constrained edge devices.",2024-09-08T05:10:02Z,2024-09-08T05:10:02Z,http://arxiv.org/abs/2409.04976v1,http://arxiv.org/pdf/2409.04976v1,"cs.AR, cs.AI, cs.CV, eess.IV"
Inverse Design of Copolymers Including Stoichiometry and Chain   Architecture,"Gabriel Vogel, Jana M. Weber","The demand for innovative synthetic polymers with improved properties is high, but their structural complexity and vast design space hinder rapid discovery. Machine learning-guided molecular design is a promising approach to accelerate polymer discovery. However, the scarcity of labeled polymer data and the complex hierarchical structure of synthetic polymers make generative design particularly challenging. We advance the current state-of-the-art approaches to generate not only repeating units, but monomer ensembles including their stoichiometry and chain architecture. We build upon a recent polymer representation that includes stoichiometries and chain architectures of monomer ensembles and develop a novel variational autoencoder (VAE) architecture encoding a graph and decoding a string. Using a semi-supervised setup, we enable the handling of partly labelled datasets which can be benefitial for domains with a small corpus of labelled data. Our model learns a continuous, well organized latent space (LS) that enables de-novo generation of copolymer structures including different monomer stoichiometries and chain architectures. In an inverse design case study, we demonstrate our model for in-silico discovery of novel conjugated copolymer photocatalysts for hydrogen production using optimization of the polymer's electron affinity and ionization potential in the latent space.",2024-09-30T15:37:39Z,2024-09-30T15:37:39Z,http://arxiv.org/abs/2410.02824v1,http://arxiv.org/pdf/2410.02824v1,"cond-mat.soft, cs.LG"
Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for   Audio-Visual Deepfake Detection,"Aravinda Reddy PN, Raghavendra Ramachandra, Krothapalli Sreenivasa Rao, Pabitra Mitra Vinod Rathod","Deepfakes pose a critical threat to biometric authentication systems by generating highly realistic synthetic media. Existing multimodal deepfake detectors often struggle to adapt to diverse data and rely on simple fusion methods. To address these challenges, we propose Gumbel-Rao Monte Carlo Bi-modal Neural Architecture Search (GRMC-BMNAS), a novel architecture search framework that employs Gumbel-Rao Monte Carlo sampling to optimize multimodal fusion. It refines the Straight through Gumbel Softmax (STGS) method by reducing variance with Rao-Blackwellization, stabilizing network training. Using a two-level search approach, the framework optimizes the network architecture, parameters, and performance. Crucial features are efficiently identified from backbone networks, while within the cell structure, a weighted fusion operation integrates information from various sources. By varying parameters such as temperature and number of Monte carlo samples yields an architecture that maximizes classification performance and better generalisation capability. Experimental results on the FakeAVCeleb and SWAN-DF datasets demonstrate an impressive AUC percentage of 95.4\%, achieved with minimal model parameters.",2024-10-09T04:37:35Z,2024-10-09T04:37:35Z,http://arxiv.org/abs/2410.06543v1,http://arxiv.org/pdf/2410.06543v1,"cs.CR, cs.SD, eess.AS"
Perspectives on 6G Architectures,"Rainer Liebhart, Mansoor Shafi, Harsh Tataria, Gajan Shivanandan, Devaki Chandramouli","Mobile communications have been undergoing a generational change every ten years. While 5G network deployments are maturing, significant efforts are being made to standardize 6G, which is expected to be commercially introduced by 2030. This paper provides unique perspectives on the 6G network (radio and core) architecture(s) from the anticipated 6G use cases to meet the necessary performance requirements. To cater for the key 6G use cases, the 6G architecture must integrate different network-level functions in a multiplicity of virtual cloud environments, leveraging the advancements of distributed processing, artificial intelligence, and securely integrating different sub-networks e.g., terrestrial, and non-terrestrial networks into the overall 6G network. This paper characterizes the impact of 6G architectures from a deployment perspective with backwards compatibility in mind.",2024-11-28T00:53:35Z,2024-11-28T00:53:35Z,http://arxiv.org/abs/2411.18836v1,http://arxiv.org/pdf/2411.18836v1,"cs.NI, cs.IT, math.IT"
Zonal Architecture Development with evolution of Artificial Intelligence,"Sneha Sudhir Shetiya, Vikas Vyas, Shreyas Renukuntla","This paper explains how traditional centralized architectures are transitioning to distributed zonal approaches to address challenges in scalability, reliability, performance, and cost-effectiveness. The role of edge computing and neural networks in enabling sophisticated sensor fusion and decision-making capabilities for autonomous vehicles is examined. Additionally, this paper discusses the impact of zonal architectures on vehicle diagnostics, power distribution, and smart power management systems. Key design considerations for implementing effective zonal architectures are presented, along with an overview of current challenges and future directions. The objective of this paper is to provide a comprehensive understanding of how zonal architectures are shaping the future of automotive technology, particularly in the context of self-driving vehicles and artificial intelligence integration.",2024-11-18T03:15:44Z,2024-11-18T03:15:44Z,http://arxiv.org/abs/2412.01840v1,http://arxiv.org/pdf/2412.01840v1,"cs.NE, cs.AI, cs.SY, eess.SY"
Hamiltonian-based neural networks for systems under nonholonomic   constraints,"Ignacio Puiggros T., A. Srikantha Phani","There has been increasing interest in methodologies that incorporate physics priors into neural network architectures to enhance their modeling capabilities. A family of these methodologies that has gained traction are Hamiltonian neural networks (HNN) and their variations. These architectures explicitly encode Hamiltonian mechanics both in their structure and loss function. Although Hamiltonian systems under nonholonomic constraints are in general not Hamiltonian, it is possible to formulate them in pseudo-Hamiltonian form, equipped with a Lie bracket which is almost Poisson. This opens the possibility of using some principles of HNNs in systems under nonholonomic constraints. The goal of the present work is to develop a modified Hamiltonian neural network architecture capable of modeling Hamiltonian systems under holonomic and nonholonomic constraints. A three-network parallel architecture is proposed to simultaneously learn the Hamiltonian of the system, the constraints, and their associated multipliers. A rolling disk and a ball on a spinning table are considered as canonical examples to assess the performance of the proposed Hamiltonian architecture. The experiments are then repeated with a noisy training set to study modeling performance under more realistic conditions.",2024-12-04T04:08:51Z,2024-12-04T04:08:51Z,http://arxiv.org/abs/2412.03018v1,http://arxiv.org/pdf/2412.03018v1,"physics.class-ph, cs.LG"
Associative memory inspires improvements for in-context learning using a   novel attention residual stream architecture,"Thomas F Burns, Tomoki Fukai, Christopher J Earls","Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.",2024-12-19T17:55:42Z,2024-12-19T17:55:42Z,http://arxiv.org/abs/2412.15113v1,http://arxiv.org/pdf/2412.15113v1,"cs.NE, cs.AI, cs.CL, 92B20, 68T01, 68T37, 68T50, I.2; I.5; I.7; J.2; J.3"
Boosting Cross-Architectural Emulation Performance by Foregoing the   Intermediate Representation Model,Amy Iris Parker,"As more applications utilize virtualization and emulation to run mission-critical tasks, the performance requirements of emulated and virtualized platforms continue to rise. Hardware virtualization is not universally available for all systems, and is incapable of emulating CPU architectures, requiring software emulation to be used. QEMU, the premier cross-architecture emulator for Linux and some BSD systems, currently uses dynamic binary translation (DBT) through intermediate representations using its Tiny Code Generator (TCG) model. While using intermediate representations of translated code allows QEMU to quickly add new host and guest architectures, it creates additional steps in the emulation pipeline which decrease performance. We construct a proof of concept emulator to demonstrate the slowdown caused by the usage of intermediate representations in TCG; this emulator performed up to 35x faster than QEMU with TCG, indicating substantial room for improvement in QEMU's design. We propose an expansion of QEMU's two-tier engine system (Linux KVM versus TCG) to include a middle tier using direct binary translation for commonly paired architectures such as RISC-V, x86, and ARM. This approach provides a slidable trade-off between development effort and performance depending on the needs of end users.",2025-01-06T23:15:26Z,2025-01-06T23:15:26Z,http://arxiv.org/abs/2501.03427v1,http://arxiv.org/pdf/2501.03427v1,"cs.PF, cs.OS, D.4.8; D.4.7; C.1.0"
Hybrid-Quantum Neural Architecture Search for The Proximal Policy   Optimization Algorithm,Moustafa Zada,"Recent studies in quantum machine learning advocated the use of hybrid models to assist with the limitations of the currently existing Noisy Intermediate Scale Quantum (NISQ) devices, but what was missing from most of them was the explanations and interpretations of the choices that were made to pick those exact architectures and the differentiation between good and bad hybrid architectures, this research attempts to tackle that gap in the literature by using the Regularized Evolution algorithm to search for the optimal hybrid classical-quantum architecture for the Proximal Policy Optimization (PPO) algorithm, a well-known reinforcement learning algorithm, ultimately the classical models dominated the leaderboard with the best hybrid model coming in eleventh place among all unique models, while we also try to explain the factors that contributed to such results,and for some models to behave better than others in hope to grasp a better intuition about what we should consider good practices for designing an efficient hybrid architecture.",2025-01-18T06:39:05Z,2025-01-18T06:39:05Z,http://arxiv.org/abs/2501.10673v1,http://arxiv.org/pdf/2501.10673v1,"quant-ph, cs.LG, cs.NE"
Fat-Tree QRAM: A High-Bandwidth Shared Quantum Random Access Memory for   Parallel Queries,"Shifan Xu, Alvin Lu, Yongshan Ding","Quantum Random Access Memory (QRAM) is a crucial architectural component for querying classical or quantum data in superposition, enabling algorithms with wide-ranging applications in quantum arithmetic, quantum chemistry, machine learning, and quantum cryptography. In this work, we introduce Fat-Tree QRAM, a novel query architecture capable of pipelining multiple quantum queries simultaneously while maintaining desirable scalings in query speed and fidelity. Specifically, Fat-Tree QRAM performs $O(\log (N))$ independent queries in $O(\log (N))$ time using $O(N)$ qubits, offering immense parallelism benefits over traditional QRAM architectures. To demonstrate its experimental feasibility, we propose modular and on-chip implementations of Fat-Tree QRAM based on superconducting circuits and analyze their performance and fidelity under realistic parameters. Furthermore, a query scheduling protocol is presented to maximize hardware utilization and access the underlying data at an optimal rate. These results suggest that Fat-Tree QRAM is an attractive architecture in a shared memory system for practical quantum computing.",2025-02-10T18:47:16Z,2025-02-10T18:47:16Z,http://arxiv.org/abs/2502.06767v1,http://arxiv.org/pdf/2502.06767v1,"quant-ph, cs.AR"
Enhancing the Utility of Higher-Order Information in Relational Learning,"Raphael Pellegrin, Lukas Fesser, Melanie Weber","Higher-order information is crucial for relational learning in many domains where relationships extend beyond pairwise interactions. Hypergraphs provide a natural framework for modeling such relationships, which has motivated recent extensions of graph neural network architectures to hypergraphs. However, comparisons between hypergraph architectures and standard graph-level models remain limited. In this work, we systematically evaluate a selection of hypergraph-level and graph-level architectures, to determine their effectiveness in leveraging higher-order information in relational learning. Our results show that graph-level architectures applied to hypergraph expansions often outperform hypergraph-level ones, even on inputs that are naturally parametrized as hypergraphs. As an alternative approach for leveraging higher-order information, we propose hypergraph-level encodings based on classical hypergraph characteristics. While these encodings do not significantly improve hypergraph architectures, they yield substantial performance gains when combined with graph-level models. Our theoretical analysis shows that hypergraph-level encodings provably increase the representational power of message-passing graph neural networks beyond that of their graph-level counterparts.",2025-02-13T18:28:17Z,2025-02-13T18:28:17Z,http://arxiv.org/abs/2502.09570v1,http://arxiv.org/pdf/2502.09570v1,"cs.LG, stat.ML"
Topological Neural Networks over the Air,"Simone Fiorellino, Claudio Battiloro, Paolo Di Lorenzo","Topological neural networks (TNNs) are information processing architectures that model representations from data lying over topological spaces (e.g., simplicial or cell complexes) and allow for decentralized implementation through localized communications over different neighborhoods. Existing TNN architectures have not yet been considered in realistic communication scenarios, where channel effects typically introduce disturbances such as fading and noise. This paper aims to propose a novel TNN design, operating on regular cell complexes, that performs over-the-air computation, incorporating the wireless communication model into its architecture. Specifically, during training and inference, the proposed method considers channel impairments such as fading and noise in the topological convolutional filtering operation, which takes place over different signal orders and neighborhoods. Numerical results illustrate the architecture's robustness to channel impairments during testing and the superior performance with respect to existing architectures, which are either communication-agnostic or graph-based.",2025-02-14T10:45:36Z,2025-02-14T10:45:36Z,http://arxiv.org/abs/2502.10070v1,http://arxiv.org/pdf/2502.10070v1,"cs.IT, cs.LG, math.IT"
Cognitive Neural Architecture Search Reveals Hierarchical Entailment,"Lukas Kuhn, Sari Saba-Sadiya, Gemma Roig","Recent research has suggested that the brain is more shallow than previously thought, challenging the traditionally assumed hierarchical structure of the ventral visual pathway. Here, we demonstrate that optimizing convolutional network architectures for brain-alignment via evolutionary neural architecture search results in models with clear representational hierarchies. Despite having random weights, the identified models achieve brain-alignment scores surpassing even those of pretrained classification models - as measured by both regression and representational similarity analysis. Furthermore, through traditional supervised training, architectures optimized for alignment with late ventral regions become competitive classification models. These findings suggest that hierarchical structure is a fundamental mechanism of primate visual processing. Finally, this work demonstrates the potential of neural architecture search as a framework for computational cognitive neuroscience research that could reduce the field's reliance on manually designed convolutional networks.",2025-02-16T14:13:04Z,2025-02-16T14:13:04Z,http://arxiv.org/abs/2502.11141v1,http://arxiv.org/pdf/2502.11141v1,"cs.NE, cs.AI, q-bio.QM"
Research on Architectures for Integrated Speech/Language Systems in   Verbmobil,"Günther Görz, Marcus Kesseler, Jörg Spilker, Hans Weber","The German joint research project Verbmobil (VM) aims at the development of a speech to speech translation system. This paper reports on research done in our group which belongs to Verbmobil's subproject on system architectures (TP15). Our specific research areas are the construction of parsers for spontaneous speech, investigations in the parallelization of parsing and to contribute to the development of a flexible communication architecture with distributed control.",1996-06-25T15:20:14Z,1996-06-25T15:20:14Z,http://arxiv.org/abs/cmp-lg/9606031v1,http://arxiv.org/pdf/cmp-lg/9606031v1,"cmp-lg, cs.CL"
Three-Tiered Specification of Micro-Architectures,"Vasu Alagar, Ralf Laemmel","A three-tiered specification approach is developed to formally specify collections of collaborating objects, say micro-architectures. (i) The structural properties to be maintained in the collaboration are specified in the lowest tier. (ii) The behaviour of the object methods in the classes is specified in the middle tier. (iii) The interaction of the objects in the micro-architecture is specified in the third tier. The specification approach is based on Larch and accompanying notations and tools. The approach enables the unambiguous and complete specification of reusable collections of collaborating objects. The layered, formal approach is compared to other approaches including the mainstream UML approach.",2002-05-19T14:46:34Z,2002-05-19T14:46:34Z,http://arxiv.org/abs/cs/0205052v1,http://arxiv.org/pdf/cs/0205052v1,"cs.SE, cs.PL, D.2.4; D.2.10; D.2.11; D.2.13"
Application Architecture for Spoken Language Resources in Organisational   Settings,"Rodney J. Clarke, Dali Dong, Philip C. Windridge","Special technologies need to be used to take advantage of, and overcome, the challenges associated with acquiring, transforming, storing, processing, and distributing spoken language resources in organisations. This paper introduces an application architecture consisting of tools and supporting utilities for indexing and transcription, and describes how these tools, together with downstream processing and distribution systems, can be integrated into a workflow. Two sample applications for this architecture are outlined- the analysis of decision-making processes in organisations and the deployment of systems development methods by designers in the field.",2003-10-29T20:13:30Z,2003-10-29T20:13:30Z,http://arxiv.org/abs/cs/0310058v1,http://arxiv.org/pdf/cs/0310058v1,"cs.CL, I.2.7"
2 P2P or Not 2 P2P?,"Mema Roussopoulos, Mary Baker, David S. H. Rosenthal, TJ Giuli, Petros Maniatis, Jeff Mogul","In the hope of stimulating discussion, we present a heuristic decision tree that designers can use to judge the likely suitability of a P2P architecture for their applications. It is based on the characteristics of a wide range of P2P systems from the literature, both proposed and deployed.",2003-11-14T22:36:39Z,2003-11-14T22:36:39Z,http://arxiv.org/abs/cs/0311017v1,http://arxiv.org/pdf/cs/0311017v1,"cs.NI, cs.AR, C.2.4"
Visualising the structure of architectural open spaces based on shape   analysis,"Sanjay Rana, Mike Batty","This paper proposes the application of some well known two-dimensional geometrical shape descriptors for the visualisation of the structure of architectural open spaces. The paper demonstrates the use of visibility measures such as distance to obstacles and amount of visible space to calculate shape descriptors such as convexity and skeleton of the open space. The aim of the paper is to indicate a simple, objective and quantifiable approach to understand the structure of open spaces otherwise impossible due to the complex construction of built structures.",2004-04-22T13:42:48Z,2004-04-22T13:42:48Z,http://arxiv.org/abs/cs/0404046v1,http://arxiv.org/pdf/cs/0404046v1,"cs.CV, cs.CG, cs.DS, I.3.5;I.4.8;I.5.2"
Decoding the Golden Code: a VLSI design,"Barbara Cerato, Guido Masera, Emanuele Viterbo","The recently proposed Golden code is an optimal space-time block code for 2 X 2 multiple-input multiple-output (MIMO) systems. The aim of this work is the design of a VLSI decoder for a MIMO system coded with the Golden code. The architecture is based on a rearrangement of the sphere decoding algorithm that achieves maximum-likelihood (ML) decoding performance. Compared to other approaces, the proposed solution exhibits an inherent flexibility in terms of modulation schemes QAM modulation size and this makes our architecture particularly suitable for adaptive modulation schemes.",2007-11-15T11:55:30Z,2007-11-15T11:55:30Z,http://arxiv.org/abs/0711.2383v1,http://arxiv.org/pdf/0711.2383v1,"cs.AR, B.7.1"
"Cognitive Architecture for Direction of Attention Founded on Subliminal   Memory Searches, Pseudorandom and Nonstop",J. R. Burger,"By way of explaining how a brain works logically, human associative memory is modeled with logical and memory neurons, corresponding to standard digital circuits. The resulting cognitive architecture incorporates basic psychological elements such as short term and long term memory. Novel to the architecture are memory searches using cues chosen pseudorandomly from short term memory. Recalls alternated with sensory images, many tens per second, are analyzed subliminally as an ongoing process, to determine a direction of attention in short term memory.",2008-05-20T17:37:31Z,2008-05-20T17:37:31Z,http://arxiv.org/abs/0805.3126v1,http://arxiv.org/pdf/0805.3126v1,"cs.AI, cs.NE, I.2.0; C.1.3"
Synergetics and Its Application to Literature and Architecture,"V. P. Maslov, T. V. Maslova","A series of phenomena pertaining to economics, quantum physics, language, literary criticism, and especially architecture is studied from the standpoint of synergetics (the study of self-organizing complex systems). It turns out that a whole series of concrete formulas describing these phenomena is identical in these different situations. This is the case of formulas relating to the Bose-Einstein distribution of particles and the distribution of words from a frequency dictionary. This also allows to apply a ""quantized"" from of the Zipf law to the problem of the authorship of 'Quiet Flows the Don' and to the""blending in"" of new architectural structures in an existing environment.",2008-06-25T18:08:31Z,2008-06-25T18:08:31Z,http://arxiv.org/abs/0806.4164v1,http://arxiv.org/pdf/0806.4164v1,"nlin.AO, physics.soc-ph"
Architecture for communication with a fidelity criterion in unknown   networks,"Mukul Agarwal, Sanjoy Mitter","We prove that in order to communicate independent sources (this is the unicast problem) between various users over an unknown medium to within various distortion levels, it is sufficient to consider source-channel separation based architectures: architectures which first compress the sources to within the corresponding distortion levels followed by reliable communication over the unknown medium. We are reducing the problem of universal rate-distortion communication of independent sources over a network to the universal reliable communication problem over networks. This is a reductionist view. We are not solving the reliable communication problem in networks.",2010-02-05T17:21:42Z,2011-01-21T03:57:20Z,http://arxiv.org/abs/1002.1300v2,http://arxiv.org/pdf/1002.1300v2,"cs.IT, math.IT"
Multiplierless Modules for Forward and Backward Integer Wavelet   Transform,Vasil Kolev,This article is about the architecture of a lossless wavelet filter bank with reprogrammable logic. It is based on second generation of wavelets with a reduced of number of operations. A new basic structure for parallel architecture and modules to forward and backward integer discrete wavelet transform is proposed.,2010-10-19T21:58:14Z,2021-08-18T22:41:16Z,http://arxiv.org/abs/1010.4059v3,http://arxiv.org/pdf/1010.4059v3,"cs.AR, cs.CV, 68-xx, 42Cxx, 65Txx, 54Hxx, 94Axx, 94-XX, 47AXX,, B.1; B.2; B.4; B.7; C.1; C.5; E.3; E.4; G.1; I.4; I.5; I.6; J.3;
  J.6; J.7"
Design for a Darwinian Brain: Part 1. Philosophy and Neuroscience,Chrisantha Fernando,"Physical symbol systems are needed for open-ended cognition. A good way to understand physical symbol systems is by comparison of thought to chemistry. Both have systematicity, productivity and compositionality. The state of the art in cognitive architectures for open-ended cognition is critically assessed. I conclude that a cognitive architecture that evolves symbol structures in the brain is a promising candidate to explain open-ended cognition. Part 2 of the paper presents such a cognitive architecture.",2013-03-28T18:45:52Z,2013-03-28T18:45:52Z,http://arxiv.org/abs/1303.7200v1,http://arxiv.org/pdf/1303.7200v1,"cs.AI, q-bio.NC"
Resnet in Resnet: Generalizing Residual Architectures,"Sasha Targ, Diogo Almeida, Kevin Lyman","Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.",2016-03-25T20:55:40Z,2016-03-25T20:55:40Z,http://arxiv.org/abs/1603.08029v1,http://arxiv.org/pdf/1603.08029v1,"cs.LG, cs.CV, cs.NE, stat.ML"
The Statistical Recurrent Unit,"Junier B. Oliva, Barnabas Poczos, Jeff Schneider","Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters in a Bayesian optimization scheme for both synthetic and real-world tasks.",2017-03-01T16:50:54Z,2017-03-01T16:50:54Z,http://arxiv.org/abs/1703.00381v1,http://arxiv.org/pdf/1703.00381v1,"cs.LG, cs.AI, stat.ML"
On The Automated Planning And Design Of SMATV Systems,Radu Arsinte,"The paper presents some theoretical and practical considerations regarding the TV information distribution in local (small and medium) networks, using different technologies and architectures. The SMATV concept is chosen to be presented extensively. The most important design formulae are presented with a software package supporting the network planner to design and optimize the network. A case study is realized, using standard components in SMATV, for a 5 floor building. The study proved that it is possible to design and optimize the entire network, without realizing first a costly experimental setup. It is also possible to run different architectures, optimizing also the costs of the final solution of network.",2017-01-30T13:18:28Z,2017-01-30T13:18:28Z,http://arxiv.org/abs/1703.01331v1,http://arxiv.org/pdf/1703.01331v1,"cs.NI, cs.MM, 78.06, C.2.5"
Diagonal RNNs in Symbolic Music Modeling,"Y. Cem Subakan, Paris Smaragdis","In this paper, we propose a new Recurrent Neural Network (RNN) architecture. The novelty is simple: We use diagonal recurrent matrices instead of full. This results in better test likelihood and faster convergence compared to regular full RNNs in most of our experiments. We show the benefits of using diagonal recurrent matrices with popularly used LSTM and GRU architectures as well as with the vanilla RNN architecture, on four standard symbolic music datasets.",2017-04-18T16:47:38Z,2017-04-19T23:36:18Z,http://arxiv.org/abs/1704.05420v2,http://arxiv.org/pdf/1704.05420v2,"cs.NE, cs.LG, stat.ML"
Bayesian Learning of Neural Network Architectures,"Georgi Dikov, Patrick van der Smagt, Justin Bayer","In this paper we propose a Bayesian method for estimating architectural parameters of neural networks, namely layer size and network depth. We do this by learning concrete distributions over these parameters. Our results show that regular networks with a learnt structure can generalise better on small datasets, while fully stochastic networks can be more robust to parameter initialisation. The proposed method relies on standard neural variational learning and, unlike randomised architecture search, does not require a retraining of the model, thus keeping the computational overhead at minimum.",2019-01-14T18:07:18Z,2019-01-27T12:09:50Z,http://arxiv.org/abs/1901.04436v2,http://arxiv.org/pdf/1901.04436v2,"stat.ML, cs.LG"
Continual Learning in Practice,"Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, Neil Lawrence","This paper describes a reference architecture for self-maintaining systems that can learn continually, as data arrives. In environments where data evolves, we need architectures that manage Machine Learning (ML) models in production, adapt to shifting data distributions, cope with outliers, retrain when necessary, and adapt to new tasks. This represents continual AutoML or Automatically Adaptive Machine Learning. We describe the challenges and proposes a reference architecture.",2019-03-12T20:41:36Z,2019-03-18T14:44:53Z,http://arxiv.org/abs/1903.05202v2,http://arxiv.org/pdf/1903.05202v2,"stat.ML, cs.LG"
Clustering and Classification Networks,Jin-mo Choi,"In this paper, we will describe a network architecture that demonstrates high performance on various sizes of datasets. To do this, we will perform an architecture search by dividing the fully connected layer into three levels in the existing network architecture. The first step is to learn existing CNN layer and existing fully connected layer for 1 epoch. The second step is clustering similar classes by applying L1 distance to the result of Softmax. The third step is to reclassify using clustering class masks. We accomplished the result of state-of-the-art by performing the above three steps sequentially or recursively. The technology recorded an error of 11.56% on Cifar-100.",2019-06-20T15:59:22Z,2019-06-20T15:59:22Z,http://arxiv.org/abs/1906.08714v1,http://arxiv.org/pdf/1906.08714v1,"cs.LG, cs.CV, stat.ML"
Universal Transforming Geometric Network,Jin Li,"The recurrent geometric network (RGN), the first end-to-end differentiable neural architecture for protein structure prediction, is a competitive alternative to existing models. However, the RGN's use of recurrent neural networks (RNNs) as internal representations results in long training time and unstable gradients. And because of its sequential nature, it is less effective at learning global dependencies among amino acids than existing transformer architectures. We propose the Universal Transforming Geometric Network (UTGN), an end-to-end differentiable model that uses the encoder portion of the Universal Transformer architecture as an alternative for internal representations. Our experiments show that compared to RGN, UTGN achieve a $1.7$ \si{\angstrom} improvement on the free modeling portion and a $0.7$ \si{\angstrom} improvement on the template based modeling of the CASP12 competition.",2019-08-02T07:14:08Z,2019-08-02T07:14:08Z,http://arxiv.org/abs/1908.00723v1,http://arxiv.org/pdf/1908.00723v1,"q-bio.BM, cs.LG"
Best Practices for Scientific Research on Neural Architecture Search,"Marius Lindauer, Frank Hutter","Finding a well-performing architecture is often tedious for both DL practitioners and researchers, leading to tremendous interest in the automation of this task by means of neural architecture search (NAS). Although the community has made major strides in developing better NAS methods, the quality of scientific empirical evaluations in the young field of NAS is still lacking behind that of other areas of machine learning. To address this issue, we describe a set of possible issues and ways to avoid them, leading to the NAS best practices checklist available at http://automl.org/nas_checklist.pdf.",2019-09-05T14:39:27Z,2020-11-03T08:52:42Z,http://arxiv.org/abs/1909.02453v3,http://arxiv.org/pdf/1909.02453v3,"cs.LG, stat.ML"
Microstructure synthesis using style-based generative adversarial   network,"Daria Fokina, Ekaterina Muravleva, George Ovchinnikov, Ivan Oseledets","Work considers the usage of StyleGAN architecture for the task of microstructure synthesis. The task is the following: given number of samples of structure we try to generate similar samples at the same time preserving its properties. Since the considered architecture is not able to produce samples of sizes larger than the training images, we propose to use image quilting to merge fixed-sized samples. One of the key features of the considered architecture is that it uses multiple image resolutions. We also investigate the necessity of such an approach.",2019-09-16T07:50:17Z,2019-09-16T07:50:17Z,http://arxiv.org/abs/1909.07042v1,http://arxiv.org/pdf/1909.07042v1,"eess.IV, cs.NA, math.NA"
Deep Model Reference Adaptive Control,"Girish Joshi, Girish Chowdhary","We present a new neuroadaptive architecture: Deep Neural Network based Model Reference Adaptive Control (DMRAC). Our architecture utilizes the power of deep neural network representations for modeling significant nonlinearities while marrying it with the boundedness guarantees that characterize MRAC based controllers. We demonstrate through simulations and analysis that DMRAC can subsume previously studied learning based MRAC methods, such as concurrent learning and GP-MRAC. This makes DMRAC a highly powerful architecture for high-performance control of nonlinear systems with long-term learning properties.",2019-09-18T17:45:22Z,2019-09-18T17:45:22Z,http://arxiv.org/abs/1909.08602v1,http://arxiv.org/pdf/1909.08602v1,"cs.LG, cs.SY, eess.SY"
"Deep, Skinny Neural Networks are not Universal Approximators",Jesse Johnson,"In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.",2018-09-30T14:55:41Z,2018-09-30T14:55:41Z,http://arxiv.org/abs/1810.00393v1,http://arxiv.org/pdf/1810.00393v1,"cs.LG, stat.ML"
Rethinking Layer-wise Feature Amounts in Convolutional Neural Network   Architectures,"Martin Mundt, Sagnik Majumder, Tobias Weis, Visvanathan Ramesh","We characterize convolutional neural networks with respect to the relative amount of features per layer. Using a skew normal distribution as a parametrized framework, we investigate the common assumption of monotonously increasing feature-counts with higher layers of architecture designs. Our evaluation on models with VGG-type layers on the MNIST, Fashion-MNIST and CIFAR-10 image classification benchmarks provides evidence that motivates rethinking of our common assumption: architectures that favor larger early layers seem to yield better accuracy.",2018-12-14T09:28:05Z,2018-12-14T09:28:05Z,http://arxiv.org/abs/1812.05836v1,http://arxiv.org/pdf/1812.05836v1,"cs.LG, cs.CV, stat.ML"
VALAN: Vision and Language Agent Navigation,"Larry Lansing, Vihan Jain, Harsh Mehta, Haoshuo Huang, Eugene Ie","VALAN is a lightweight and scalable software framework for deep reinforcement learning based on the SEED RL architecture. The framework facilitates the development and evaluation of embodied agents for solving grounded language understanding tasks, such as Vision-and-Language Navigation and Vision-and-Dialog Navigation, in photo-realistic environments, such as Matterport3D and Google StreetView. We have added a minimal set of abstractions on top of SEED RL allowing us to generalize the architecture to solve a variety of other RL problems. In this article, we will describe VALAN's software abstraction and architecture, and also present an example of using VALAN to design agents for instruction-conditioned indoor navigation.",2019-12-06T17:29:43Z,2019-12-06T17:29:43Z,http://arxiv.org/abs/1912.03241v1,http://arxiv.org/pdf/1912.03241v1,"cs.LG, stat.ML"
Wasserstein Routed Capsule Networks,"Alexander Fuchs, Franz Pernkopf","Capsule networks offer interesting properties and provide an alternative to today's deep neural network architectures. However, recent approaches have failed to consistently achieve competitive results across different image datasets. We propose a new parameter efficient capsule architecture, that is able to tackle complex tasks by using neural networks trained with an approximate Wasserstein objective to dynamically select capsules throughout the entire architecture. This approach focuses on implementing a robust routing scheme, which can deliver improved results using little overhead. We perform several ablation studies verifying the proposed concepts and show that our network is able to substantially outperform other capsule approaches by over 1.2 % on CIFAR-10, using fewer parameters.",2020-07-22T14:38:05Z,2020-07-22T14:38:05Z,http://arxiv.org/abs/2007.11465v1,http://arxiv.org/pdf/2007.11465v1,"cs.LG, cs.CV, stat.ML, I.2.10"
A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic   Platforms,"Wilkie Olin-Ammentorp, Yury Sokolov, Maxim Bazhenov","Reinforcement learning (RL) is a foundation of learning in biological systems and provides a framework to address numerous challenges with real-world artificial intelligence applications. Efficient implementations of RL techniques could allow for agents deployed in edge-use cases to gain novel abilities, such as improved navigation, understanding complex situations and critical decision making. Towards this goal, we describe a flexible architecture to carry out reinforcement learning on neuromorphic platforms. This architecture was implemented using an Intel neuromorphic processor and demonstrated solving a variety of tasks using spiking dynamics. Our study proposes a usable energy efficient solution for real-world RL applications and demonstrates applicability of the neuromorphic platforms for RL problems.",2021-03-05T01:54:22Z,2021-03-05T01:54:22Z,http://arxiv.org/abs/2103.04780v1,http://arxiv.org/pdf/2103.04780v1,"cs.LG, cs.AI, I.2"
Modeling and simulation of nuclear architecture reorganization process   using a phase field approach,"Qing Cheng, Pourya Delafrouz, Jie Liang, Chun Liu, Jie Shen","We develop a special phase field/diffusive interface method to model the nuclear architecture reorganization process. In particular, we use a Lagrange multiplier approach in the phase field model to preserve the specific physical and geometrical constraints for the biological events. We develop several efficient and robust linear and weakly nonlinear schemes for this new model. To validate the model and numerical methods, we present ample numerical simulations which in particular reproduce several processes of nuclear architecture reorganization from the experiment literature.",2021-03-12T03:13:56Z,2021-03-12T03:13:56Z,http://arxiv.org/abs/2103.09658v1,http://arxiv.org/pdf/2103.09658v1,"math.NA, cs.NA"
Failure-Tolerant Contract-Based Design of an Automated Valet Parking   System using a Directive-Response Architecture,"Josefine Graebener, Tung Phan-Minh, Jiaqi Yan, Qiming Zhao, Richard M. Murray","Increased complexity in cyber-physical systems calls for modular system design methodologies that guarantee correct and reliable behavior, both in normal operations and in the presence of failures. This paper aims to extend the contract-based design approach using a directive-response architecture to enable reactivity to failure scenarios. The architecture is demonstrated on a modular automated valet parking (AVP) system. The contracts for the different components in the AVP system are explicitly defined, implemented, and validated against a Python implementation.",2021-03-24T01:32:17Z,2021-03-24T01:32:17Z,http://arxiv.org/abs/2103.12919v1,http://arxiv.org/pdf/2103.12919v1,"eess.SY, cs.FL, cs.SY"
Five Disruptive Technology Directions for 5G,"Federico Boccardi, Robert W. Heath Jr., Angel Lozano, Thomas L. Marzetta, Petar Popovski","New research directions will lead to fundamental changes in the design of future 5th generation (5G) cellular networks. This paper describes five technologies that could lead to both architectural and component disruptive design changes: device-centric architectures, millimeter Wave, Massive-MIMO, smarter devices, and native support to machine-2-machine. The key ideas for each technology are described, along with their potential impact on 5G and the research challenges that remain.",2013-12-01T14:55:07Z,2013-12-01T14:55:07Z,http://arxiv.org/abs/1312.0229v1,http://arxiv.org/pdf/1312.0229v1,"cs.NI, cs.IT, math.IT"
Adaptive algebraic multigrid on SIMD architectures,"Simon Heybrock, Matthias Rottmann, Peter Georg, Tilo Wettig","We present details of our implementation of the Wuppertal adaptive algebraic multigrid code DD-$\alpha$AMG on SIMD architectures, with particular emphasis on the Intel Xeon Phi processor (KNC) used in QPACE 2. As a smoother, the algorithm uses a domain-decomposition-based solver code previously developed for the KNC in Regensburg. We optimized the remaining parts of the multigrid code and conclude that it is a very good target for SIMD architectures. Some of the remaining bottlenecks can be eliminated by vectorizing over multiple test vectors in the setup, which is discussed in the contribution of Daniel Richtmann.",2015-12-14T20:27:07Z,2015-12-14T20:27:07Z,http://arxiv.org/abs/1512.04506v1,http://arxiv.org/pdf/1512.04506v1,"physics.comp-ph, hep-lat"
Block Neural Network Avoids Catastrophic Forgetting When Learning   Multiple Task,"Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov","In the present work we propose a Deep Feed Forward network architecture which can be trained according to a sequential learning paradigm, where tasks of increasing difficulty are learned sequentially, yet avoiding catastrophic forgetting. The proposed architecture can re-use the features learned on previous tasks in a new task when the old tasks and the new one are related. The architecture needs fewer computational resources (neurons and connections) and less data for learning the new task than a network trained from scratch",2017-11-28T09:47:51Z,2017-11-28T09:47:51Z,http://arxiv.org/abs/1711.10204v1,http://arxiv.org/pdf/1711.10204v1,"cs.NE, cs.LG, stat.ML"
Dynamic Cell Structure via Recursive-Recurrent Neural Networks,"Xin Qian, Matthew Kennedy, Diego Klabjan","In a recurrent setting, conventional approaches to neural architecture search find and fix a general model for all data samples and time steps. We propose a novel algorithm that can dynamically search for the structure of cells in a recurrent neural network model. Based on a combination of recurrent and recursive neural networks, our algorithm is able to construct customized cell structures for each data sample and time step, allowing for a more efficient architecture search than existing models. Experiments on three common datasets show that the algorithm discovers high-performance cell architectures and achieves better prediction accuracy compared to the GRU structure for language modelling and sentiment analysis.",2019-05-25T07:14:05Z,2019-05-25T07:14:05Z,http://arxiv.org/abs/1905.10540v1,http://arxiv.org/pdf/1905.10540v1,"cs.LG, cs.NE, stat.ML"
OrderNet: Ordering by Example,Robert Porter,In this paper we introduce a new neural architecture for sorting unordered sequences where the correct sequence order is not easily defined but must rather be inferred from training data. We refer to this architecture as OrderNet and describe how it was constructed to be naturally permutation equivariant while still allowing for rich interactions of elements of the input set. We evaluate the capabilities of our architecture by training it to approximate solutions for the Traveling Salesman Problem and find that it outperforms previously studied supervised techniques in its ability to generalize to longer sequences than it was trained with. We further demonstrate the capability by reconstructing the order of sentences with scrambled word order.,2019-05-27T23:09:35Z,2019-05-27T23:09:35Z,http://arxiv.org/abs/1905.11536v1,http://arxiv.org/pdf/1905.11536v1,"cs.LG, stat.ML"
Resilient Architectures for Free Space Optical Wireless Interconnection   Systems,"Sanaa Hamid Mohamed, Osama Zwaid Alsulami, Taisir E. H. El-Gorashi, Mohammed T. Alresheedi, Jaafar M. H. Elmirghani","In this paper, we propose the use of two Passive Optical Network (PON)-based network architectures to connect free-space Optical Wireless Communication (OWC) Access Points (APs) within a room with multiple users. We optimize through a Mixed Linear Integer Programming (MILP) model the assignment of mobile OWC users to more than one AP to improve the resilience of the fronthaul network, i.e the OWC system and the wired network linked to APs, and study the impact of users distribution and channel characteristics.",2021-02-10T19:08:49Z,2021-02-10T19:08:49Z,http://arxiv.org/abs/2102.05694v1,http://arxiv.org/pdf/2102.05694v1,"cs.NI, eess.SP"
Attention on Attention: Architectures for Visual Question Answering   (VQA),"Jasdeep Singh, Vincent Ying, Alex Nutkiewicz","Visual Question Answering (VQA) is an increasingly popular topic in deep learning research, requiring coordination of natural language processing and computer vision modules into a single architecture. We build upon the model which placed first in the VQA Challenge by developing thirteen new attention mechanisms and introducing a simplified classifier. We performed 300 GPU hours of extensive hyperparameter and architecture searches and were able to achieve an evaluation score of 64.78%, outperforming the existing state-of-the-art single model's validation score of 63.15%.",2018-03-21T03:05:58Z,2018-03-21T03:05:58Z,http://arxiv.org/abs/1803.07724v1,http://arxiv.org/pdf/1803.07724v1,"cs.CL, cs.AI, cs.CV, 68Txx"
Information Theory as a Means of Determining the Main Factors Affecting   the Processors Architecture,"Anton Rakitskiy, Boris Ryabko","In this article we are investigating the computers development process in the past decades in order to identify the factors that influence it the most. We describe such factors and use them to predict the direction of further development. To solve these problems, we use the concept of the Computer Capacity, which allows us to estimate the performance of computers theoretically, relying only on the description of its architecture.",2020-02-17T21:53:45Z,2020-02-17T21:53:45Z,http://arxiv.org/abs/2002.07271v1,http://arxiv.org/pdf/2002.07271v1,"cs.IT, cs.AR, math.IT"
A Resilient AWGR and Server Based PON Data Centre Architecture,"Randa A. Thabit, Taisir E. H. El-Gorashi, Jaafar M. H. Elmirghani",This paper studies the resilience of an AWGR and server based PON DCN architecture against link failure scenarios and proposes a modified design for improved resilience. A MILP model is developed to evaluate the performance of the modified design considering different failure scenarios. The results show a limited increase in power consumption and a large increase in delay under failure scenarios compared to the normal state.,2020-04-10T02:04:49Z,2020-04-10T02:04:49Z,http://arxiv.org/abs/2004.04880v1,http://arxiv.org/pdf/2004.04880v1,"cs.NI, eess.SP"
Energy Efficient Software Matching in Distributed Vehicular Fog Based   Architecture with Cloud and Fixed Fog Nodes,"Rui Ma, Amal A. Alahmadi, Taisir E. H. El-Gorashi, Jaafar M. H. Elmirghani","The rapid development of vehicles on-board units and the proliferation of autonomous vehicles in modern cities create a potential for a new fog computing paradigm, referred to as vehicular fog computing (VFC). In this paper, we propose an architecture that integrates a vehicular fog (VF) composed of vehicles clustered in a parking lot with a fixed fog node at the access network and the central cloud. We investigate the problem of energy efficient software matching in the VF considering different approaches to deploy software packages in vehicles.",2020-04-27T12:47:50Z,2020-04-27T12:47:50Z,http://arxiv.org/abs/2004.12747v1,http://arxiv.org/pdf/2004.12747v1,"cs.NI, eess.SP"
$\mathcal{L}_1$-$\mathcal{GP}$: $\mathcal{L}_1$ Adaptive Control with   Bayesian Learning,"Aditya Gahlawat, Pan Zhao, Andrew Patterson, Naira Hovakimyan, Evangelos A. Theodorou","We present $\mathcal{L}_1$-$\mathcal{GP}$, an architecture based on $\mathcal{L}_1$ adaptive control and Gaussian Process Regression (GPR) for safe simultaneous control and learning. On one hand, the $\mathcal{L}_1$ adaptive control provides stability and transient performance guarantees, which allows for GPR to efficiently and safely learn the uncertain dynamics. On the other hand, the learned dynamics can be conveniently incorporated into the $\mathcal{L}_1$ control architecture without sacrificing robustness and tracking performance. Subsequently, the learned dynamics can lead to less conservative designs for performance/robustness tradeoff. We illustrate the efficacy of the proposed architecture via numerical simulations.",2020-04-30T06:04:31Z,2020-04-30T06:04:31Z,http://arxiv.org/abs/2004.14594v1,http://arxiv.org/pdf/2004.14594v1,"eess.SY, cs.SY"
Pay as You Go: A Generic Crypto Tolling Architecture,"Paulo Bartolomeu, Emanuel Vieira, Joaquim Ferreira","The imminent pervasive adoption of vehicular communication, based on dedicated short-range technology (ETSI ITS G5 or IEEE WAVE), 5G, or both, will foster a richer service ecosystem for vehicular applications. The appearance of new cryptography based solutions envisaging digital identity and currency exchange are set to stem new approaches for existing and future challenges. This paper presents a novel tolling architecture that harnesses the availability of 5G C-V2X connectivity for open road tolling using smartphones, IOTA as the digital currency and Hyperledger Indy for identity validation. An experimental feasibility analysis is used to validate the proposed architecture for secure, private and convenient electronic toll payment.",2020-05-06T14:52:09Z,2020-05-06T14:52:09Z,http://arxiv.org/abs/2005.02875v1,http://arxiv.org/pdf/2005.02875v1,"cs.CR, cs.ET, cs.SY, eess.SY"
A Multiscale Graph Convolutional Network Using Hierarchical Clustering,"Alex Lipov, Pietro Liò","The information contained in hierarchical topology, intrinsic to many networks, is currently underutilised. A novel architecture is explored which exploits this information through a multiscale decomposition. A dendrogram is produced by a Girvan-Newman hierarchical clustering algorithm. It is segmented and fed through graph convolutional layers, allowing the architecture to learn multiple scale latent space representations of the network, from fine to coarse grained. The architecture is tested on a benchmark citation network, demonstrating competitive performance. Given the abundance of hierarchical networks, possible applications include quantum molecular property prediction, protein interface prediction and multiscale computational substrates for partial differential equations.",2020-06-22T18:13:03Z,2020-06-22T18:13:03Z,http://arxiv.org/abs/2006.12542v1,http://arxiv.org/pdf/2006.12542v1,"cs.LG, stat.ML"
Deep Convolutional GANs for Car Image Generation,Dong Hui Kim,"In this paper, we investigate the application of deep convolutional GANs on car image generation. We improve upon the commonly used DCGAN architecture by implementing Wasserstein loss to decrease mode collapse and introducing dropout at the end of the discrimiantor to introduce stochasticity. Furthermore, we introduce convolutional layers at the end of the generator to improve expressiveness and smooth noise. All of these improvements upon the DCGAN architecture comprise our proposal of the novel BoolGAN architecture, which is able to decrease the FID from 195.922 (baseline) to 165.966.",2020-06-24T06:56:56Z,2020-06-24T06:56:56Z,http://arxiv.org/abs/2006.14380v1,http://arxiv.org/pdf/2006.14380v1,"cs.CV, eess.IV"
Transformation Invariant Cancerous Tissue Classification Using Spatially   Transformed DenseNet,"Omar Mahdi, Ali Bou Nassif","In this work, we introduce a spatially transformed DenseNet architecture for transformation invariant classification of cancer tissue. Our architecture increases the accuracy of the base DenseNet architecture while adding the ability to operate in a transformation invariant way while simultaneously being simpler than other models that try to provide some form of invariance.",2022-04-23T13:12:50Z,2022-04-23T13:12:50Z,http://arxiv.org/abs/2204.11066v1,http://arxiv.org/pdf/2204.11066v1,"eess.IV, cs.CV, cs.LG"
Exploring Ancient Architectural Designs with Cellular Automata,Hokky Situngkir,"The paper discusses the utilization of three-dimensional cellular automata employing the two-dimensional totalistic cellular automata to simulate how simple rules could emerge a highly complex architectural designs of some Indonesian heritages. A detailed discussion is brought to see the simple rules applied in Borobudur Temple, the largest ancient Buddhist temple in the country with very complex detailed designs within. The simulation confirms some previous findings related to measurement of the temple as well as some other ancient buildings in Indonesia. This happens to open further exploitation of the explanatory power presented by cellular automata for complex architectural designs built by civilization not having any supporting sophisticated tools, even standard measurement systems.",2015-08-13T17:46:07Z,2015-08-13T17:46:07Z,http://arxiv.org/abs/1508.03610v1,http://arxiv.org/pdf/1508.03610v1,"cs.CY, nlin.CG"
Generating Multi-Categorical Samples with Generative Adversarial   Networks,"Ramiro Camino, Christian Hammerschmidt, Radu State","We propose a method to train generative adversarial networks on mutivariate feature vectors representing multiple categorical values. In contrast to the continuous domain, where GAN-based methods have delivered considerable results, GANs struggle to perform equally well on discrete data. We propose and compare several architectures based on multiple (Gumbel) softmax output layers taking into account the structure of the data. We evaluate the performance of our architecture on datasets with different sparsity, number of features, ranges of categorical values, and dependencies among the features. Our proposed architecture and method outperforms existing models.",2018-07-03T14:26:57Z,2018-07-04T15:10:32Z,http://arxiv.org/abs/1807.01202v2,http://arxiv.org/pdf/1807.01202v2,"stat.ML, cs.LG"
Anomaly Detection for Water Treatment System based on Neural Network   with Automatic Architecture Optimization,"Dmitry Shalyga, Pavel Filonov, Andrey Lavrentyev","We continue to develop our neural network (NN) based forecasting approach to anomaly detection (AD) using the Secure Water Treatment (SWaT) industrial control system (ICS) testbed dataset. We propose genetic algorithms (GA) to find the best NN architecture for a given dataset, using the NAB metric to assess the quality of different architectures. The drawbacks of the F1-metric are analyzed. Several techniques are proposed to improve the quality of AD: exponentially weighted smoothing, mean p-powered error measure, individual error weight for each variable, disjoint prediction windows. Based on the techniques used, an approach to anomaly interpretation is introduced.",2018-07-19T08:22:21Z,2018-07-19T08:22:21Z,http://arxiv.org/abs/1807.07282v1,http://arxiv.org/pdf/1807.07282v1,"cs.LG, stat.ML"
Radius-margin bounds for deep neural networks,"Mayank Sharma, Jayadeva, Sumit Soman","Explaining the unreasonable effectiveness of deep learning has eluded researchers around the globe. Various authors have described multiple metrics to evaluate the capacity of deep architectures. In this paper, we allude to the radius margin bounds described for a support vector machine (SVM) with hinge loss, apply the same to the deep feed-forward architectures and derive the Vapnik-Chervonenkis (VC) bounds which are different from the earlier bounds proposed in terms of number of weights of the network. In doing so, we also relate the effectiveness of techniques like Dropout and Dropconnect in bringing down the capacity of the network. Finally, we describe the effect of maximizing the input as well as the output margin to achieve an input noise-robust deep architecture.",2018-11-03T07:56:16Z,2018-11-03T07:56:16Z,http://arxiv.org/abs/1811.01171v1,http://arxiv.org/pdf/1811.01171v1,"cs.LG, stat.ML"
Music theme recognition using CNN and self-attention,"Manoj Sukhavasi, Sainath Adapa","We present an efficient architecture to detect mood/themes in music tracks on autotagging-moodtheme subset of the MTG-Jamendo dataset. Our approach consists of two blocks, a CNN block based on MobileNetV2 architecture and a self-attention block from Transformer architecture to capture long term temporal characteristics. We show that our proposed model produces a significant improvement over the baseline model. Our model (team name: AMLAG) achieves 4th place on PR-AUC-macro Leaderboard in MediaEval 2019: Emotion and Theme Recognition in Music Using Jamendo.",2019-11-16T14:53:01Z,2019-11-16T14:53:01Z,http://arxiv.org/abs/1911.07041v1,http://arxiv.org/pdf/1911.07041v1,"cs.SD, cs.LG, eess.AS"
A 75kb SRAM in 65nm CMOS for In-Memory Computing Based Neuromorphic   Image Denoising,"Sumon Kumar Bose, Vivek Mohan, Arindam Basu","This paper presents an in-memory computing (IMC) architecture for image denoising. The proposed SRAM based in-memory processing framework works in tandem with approximate computing on a binary image generated from neuromorphic vision sensors. Implemented in TSMC 65nm process, the proposed architecture enables approximately 2000X energy savings (approximately 222X from IMC) compared to a digital implementation when tested with the video recordings from a DAVIS sensor and achieves a peak throughput of 1.25-1.66 frames/us.",2020-03-23T14:36:12Z,2020-03-23T14:36:12Z,http://arxiv.org/abs/2003.10300v1,http://arxiv.org/pdf/2003.10300v1,"eess.IV, cs.AR, eess.SP"
Unsupervised Competitive Hardware Learning Rule for Spintronic   Clustering Architecture,"Alvaro Velasquez, Christopher H. Bennett, Naimul Hassan, Wesley H. Brigner, Otitoaleke G. Akinola, Jean Anne C. Incorvia, Matthew J. Marinella, Joseph S. Friedman",We propose a hardware learning rule for unsupervised clustering within a novel spintronic computing architecture. The proposed approach leverages the three-terminal structure of domain-wall magnetic tunnel junction devices to establish a feedback loop that serves to train such devices when they are used as synapses in a neuromorphic computing architecture.,2020-03-24T21:25:53Z,2020-03-24T21:25:53Z,http://arxiv.org/abs/2003.11120v1,http://arxiv.org/pdf/2003.11120v1,"cs.NE, cs.ET, physics.app-ph"
Low-complexity Architecture for AR(1) Inference,"A. Borges Jr., R. J. Cintra, D. F. G. Coelho, V. S. Dimitrov","In this Letter, we propose a low-complexity estimator for the correlation coefficient based on the signed $\operatorname{AR}(1)$ process. The introduced approximation is suitable for implementation in low-power hardware architectures. Monte Carlo simulations reveal that the proposed estimator performs comparably to the competing methods in literature with maximum error in order of $10^{-2}$. However, the hardware implementation of the introduced method presents considerable advantages in several relevant metrics, offering more than 95% reduction in dynamic power and doubling the maximum operating frequency when compared to the reference method.",2020-08-21T18:16:37Z,2020-08-21T18:16:37Z,http://arxiv.org/abs/2008.09633v1,http://arxiv.org/pdf/2008.09633v1,"eess.SP, cs.AR, stat.CO, stat.ME"
Floating-Point Multiplication Using Neuromorphic Computing,"Karn Dubey, Urja Kothari, Shrisha Rao","Neuromorphic computing describes the use of VLSI systems to mimic neuro-biological architectures and is also looked at as a promising alternative to the traditional von Neumann architecture. Any new computing architecture would need a system that can perform floating-point arithmetic. In this paper, we describe a neuromorphic system that performs IEEE 754-compliant floating-point multiplication. The complex process of multiplication is divided into smaller sub-tasks performed by components Exponent Adder, Bias Subtractor, Mantissa Multiplier and Sign OF/UF. We study the effect of the number of neurons per bit on accuracy and bit error rate, and estimate the optimal number of neurons needed for each component.",2020-08-30T19:07:14Z,2020-08-30T19:07:14Z,http://arxiv.org/abs/2008.13245v1,http://arxiv.org/pdf/2008.13245v1,"cs.ET, cs.NE, 94C05, 68T05, C.1.3"
Hardware Implementation of Fano Decoder for Polarization-adjusted   Convolutional (PAC) Codes,Amir Mozammel,"This brief proposes a hardware implementation architecture for Fano decoding of polarization-adjusted convolutional (PAC) codes. This architecture uses a novel branch metric unit specific to PAC codes. The proposed decoder is tested on FPGA, and its performance is evaluated on ASIC using TSMC 28 nm 0.72 V library. The decoder can be clocked at 500 MHz and reach an average information throughput of 38 Mb/s at 3.5 dB signal-to-noise ratio for a block length of 128 and a code rate of 1/2.",2020-11-19T13:46:09Z,2021-06-29T13:52:52Z,http://arxiv.org/abs/2011.09819v3,http://arxiv.org/pdf/2011.09819v3,"cs.IT, cs.AR, math.IT"
Benchmarking Invertible Architectures on Inverse Problems,"Jakob Kruse, Lynton Ardizzone, Carsten Rother, Ullrich Köthe","Recent work demonstrated that flow-based invertible neural networks are promising tools for solving ambiguous inverse problems. Following up on this, we investigate how ten invertible architectures and related models fare on two intuitive, low-dimensional benchmark problems, obtaining the best results with coupling layers and simple autoencoders. We hope that our initial efforts inspire other researchers to evaluate their invertible architectures in the same setting and put forth additional benchmarks, so our evaluation may eventually grow into an official community challenge.",2021-01-26T13:10:37Z,2021-06-22T10:53:42Z,http://arxiv.org/abs/2101.10763v3,http://arxiv.org/pdf/2101.10763v3,"cs.LG, 68T01"
Transient and Asymptotic Properties of Robust Adaptive Controllers in   the Presence of Non-Coercive Lyapunov Functions,"Aditya A. Paranjape, Vivek Natarajan, Supratim Ghosh","Adaptive control architectures often make use of Lyapunov functions to design adaptive laws. We are specifically interested in adaptive control methods, such as the well-known L1 adaptive architecture, which employ a parameter observer for this purpose. In such architectures, the observation error plays a critical role in determining analytical bounds on the tracking error as well as robustness. In this paper, we show how the non-existence of coercive Lyapunov operators can impact the analytical bounds, and with it the performance and the robustness of such adaptive systems.",2021-04-22T16:31:04Z,2021-04-22T16:31:04Z,http://arxiv.org/abs/2104.11161v1,http://arxiv.org/pdf/2104.11161v1,"eess.SY, cs.SY, 93C20, 93C25, 93C40, 93D05"
Sparsity-Probe: Analysis tool for Deep Learning Models,"Ido Ben-Shaul, Shai Dekel","We propose a probe for the analysis of deep learning architectures that is based on machine learning and approximation theoretical principles. Given a deep learning architecture and a training set, during or after training, the Sparsity Probe allows to analyze the performance of intermediate layers by quantifying the geometrical features of representations of the training set. We show how the Sparsity Probe enables measuring the contribution of adding depth to a given architecture, to detect under-performing layers, etc., all this without any auxiliary test data set.",2021-05-14T14:24:20Z,2021-05-14T14:24:20Z,http://arxiv.org/abs/2105.06849v1,http://arxiv.org/pdf/2105.06849v1,"cs.LG, math.FA"
Neural Architecture Search via Bregman Iterations,"Leon Bungert, Tim Roith, Daniel Tenbrinck, Martin Burger","We propose a novel strategy for Neural Architecture Search (NAS) based on Bregman iterations. Starting from a sparse neural network our gradient-based one-shot algorithm gradually adds relevant parameters in an inverse scale space manner. This allows the network to choose the best architecture in the search space which makes it well-designed for a given task, e.g., by adding neurons or skip connections. We demonstrate that using our approach one can unveil, for instance, residual autoencoders for denoising, deblurring, and classification tasks. Code is available at https://github.com/TimRoith/BregmanLearning.",2021-06-04T13:37:47Z,2021-06-04T13:37:47Z,http://arxiv.org/abs/2106.02479v1,http://arxiv.org/pdf/2106.02479v1,"cs.LG, cs.NE, math.OC, 65K10, 68T05, 90C26, I.2.6; F.2.1; G.1.6"
Arrow: A RISC-V Vector Accelerator for Machine Learning Inference,"Imad Al Assir, Mohamad El Iskandarani, Hadi Rayan Al Sandid, Mazen A. R. Saghir","In this paper we present Arrow, a configurable hardware accelerator architecture that implements a subset of the RISC-V v0.9 vector ISA extension aimed at edge machine learning inference. Our experimental results show that an Arrow co-processor can execute a suite of vector and matrix benchmarks fundamental to machine learning inference 2 - 78x faster than a scalar RISC processor while consuming 20% - 99% less energy when implemented in a Xilinx XC7A200T-1SBG484C FPGA.",2021-07-15T07:36:01Z,2021-07-15T07:36:01Z,http://arxiv.org/abs/2107.07169v1,http://arxiv.org/pdf/2107.07169v1,"cs.AR, B.5.1; C.1.4; C.3; C.4"
Practice Problems for Hardware Engineers,Shahin Nazarian,"This book is to help undergraduate and graduate students of electrical and computer engineering disciplines with their job interviews. It may also be used as a practice resource while taking courses in VLSI, logic and computer architecture design. The first edition consists of more than 150 problems and their solutions which the author has used in his VLSI, logic, and architectures courses while teaching at USC. The author wishes this book to be available free of charge, subject to the copyright policy on page 3.",2021-10-13T06:41:10Z,2021-10-15T00:31:04Z,http://arxiv.org/abs/2110.06526v3,http://arxiv.org/pdf/2110.06526v3,"cs.AR, B.0"
Overview of Quantum Key Distribution Technique within IPsec Architecture,"Emir Dervisevic, Miralem Mehic",Quantum Key Distribution (QKD) is an approach for establishing symmetrical binary keys between distant users in an information-theoretically secure way. In this paper we provide an overview of existing solutions that integrate QKD within the most popular architecture for establishing secure communications in modern IP (Internet Protocol) networks - IPsec (Internet Protocol security). The provided overview can be used to further design the integration of QKD within the IPsec architecture striving for a standardized solution.,2021-12-24T16:51:47Z,2021-12-24T16:51:47Z,http://arxiv.org/abs/2112.13105v1,http://arxiv.org/pdf/2112.13105v1,"cs.CR, quant-ph"
Transformer-based encoder-encoder architecture for Spoken Term Detection,"Jan Švec, Luboš Šmídl, Jan Lehečka","The paper presents a method for spoken term detection based on the Transformer architecture. We propose the encoder-encoder architecture employing two BERT-like encoders with additional modifications, including convolutional and upsampling layers, attention masking, and shared parameters. The encoders project a recognized hypothesis and a searched term into a shared embedding space, where the score of the putative hit is computed using the calibrated dot product. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the proposed system outperformed a baseline method based on deep LSTMs on the English and Czech STD datasets based on USC Shoah Foundation Visual History Archive (MALACH).",2022-11-02T13:03:15Z,2022-11-02T13:03:15Z,http://arxiv.org/abs/2211.01089v1,http://arxiv.org/pdf/2211.01089v1,"cs.CL, cs.SD, eess.AS"
Architecture and Knowledge Representation for Composable Inductive   Programming,"Edward McDaid, Sarah McDaid","We present an update on the current architecture of the Zoea knowledge-based, Composable Inductive Programming system. The Zoea compiler is built using a modern variant of the black-board architecture. Zoea integrates a large number of knowledge sources that encode different aspects of programming language and software development expertise. We describe the use of synthetic test cases as a ubiquitous form of knowledge and hypothesis representation that sup-ports a variety of reasoning strategies. Some future plans are also outlined.",2022-12-22T17:02:19Z,2022-12-22T17:02:19Z,http://arxiv.org/abs/2212.12320v1,http://arxiv.org/pdf/2212.12320v1,"cs.PL, cs.AI, D.1.2; D.2.3; D.3.2; D.3.4; F.3.1; I.2.2; I.2.4; I.2.5; I.2.11"
Lossless Microarray Image Compression by Hardware Array Compactor,"Anahita Banaei, Shadrokh Samavi, Ebrahim Nasr Esfahani","Microarray technology is a new and powerful tool for the concurrent monitoring of a large number of gene expressions. Each microarray experiment produces hundreds of images. Each digital image requires a large storage space. Hence, real-time processing of these images and transmission of them necessitates efficient and custom-made lossless compression schemes. In this paper, we offer a new architecture for the lossless compression of microarray images. In this architecture, we have used dedicated hardware for the separation of foreground pixels from background ones. By separating these pixels and using pipeline architecture, a higher lossless compression ratio has been achieved as compared to other existing methods.",2023-03-18T20:01:37Z,2023-03-18T20:01:37Z,http://arxiv.org/abs/2303.10489v1,http://arxiv.org/pdf/2303.10489v1,"eess.IV, cs.CV"
Inter-temperature Bandwidth Reduction in Cryogenic QAOA Machines,"Yosuke Ueno, Yuna Tomida, Teruo Tanimoto, Masamitsu Tanaka, Yutaka Tabuchi, Koji Inoue, Hiroshi Nakamura","The bandwidth limit between cryogenic and room-temperature environments is a critical bottleneck in superconducting noisy intermediate-scale quantum computers. This paper presents the first trial of algorithm-aware system-level optimization to solve this issue by targeting the quantum approximate optimization algorithm. Our counter-based cryogenic architecture using single-flux quantum logic shows exponential bandwidth reduction and decreases heat inflow and peripheral power consumption of inter-temperature cables, which contributes to the scalability of superconducting quantum computers.",2023-10-02T20:51:53Z,2023-10-02T20:51:53Z,http://arxiv.org/abs/2310.01630v1,http://arxiv.org/pdf/2310.01630v1,"quant-ph, cs.AR"
DeepThought: An Architecture for Autonomous Self-motivated Systems,"Arlindo L. Oliveira, Tiago Domingos, Mário Figueiredo, Pedro U. Lima","The ability of large language models (LLMs) to engage in credible dialogues with humans, taking into account the training data and the context of the conversation, has raised discussions about their ability to exhibit intrinsic motivations, agency, or even some degree of consciousness. We argue that the internal architecture of LLMs and their finite and volatile state cannot support any of these properties. By combining insights from complementary learning systems, global neuronal workspace, and attention schema theories, we propose to integrate LLMs and other deep learning systems into an architecture for cognitive language agents able to exhibit properties akin to agency, self-motivation, even some features of meta-cognition.",2023-11-14T21:20:23Z,2023-11-14T21:20:23Z,http://arxiv.org/abs/2311.08547v1,http://arxiv.org/pdf/2311.08547v1,"cs.AI, I.2"
Deep Neural Networks: A Formulation Via Non-Archimedean Analysis,W. A. Zúñiga-Galindo,We introduce a new class of deep neural networks (DNNs) with multilayered tree-like architectures. The architectures are codified using numbers from the ring of integers of non-Archimdean local fields. These rings have a natural hierarchical organization as infinite rooted trees. Natural morphisms on these rings allow us to construct finite multilayered architectures. The new DNNs are robust universal approximators of real-valued functions defined on the mentioned rings. We also show that the DNNs are robust universal approximators of real-valued square-integrable functions defined in the unit interval.,2024-01-31T14:49:44Z,2024-01-31T14:49:44Z,http://arxiv.org/abs/2402.00094v1,http://arxiv.org/pdf/2402.00094v1,"cs.NE, cs.AI, cs.LG, Primary 68T07, 65D15, Secondary 41A30, 11S85"
Exploiting Heterogeneity in the Decentralised Control of Platoons,Richard Pates,"This paper investigates the use of decentralised control architectures with heterogeneous dynamics for improving performance in large-scale systems. Our focus is on two well-known decentralised approaches; the 'predecessor following' and 'bidirectional architectures' for vehicle platooning. The former, utilising homogeneous control dynamics, is known to face exponential growth in disturbance amplification throughout the platoon, resulting in poor scalability properties. We demonstrate that by incorporating heterogeneous control system dynamics, this limitation disappears entirely, even under bandwidth constraints. Furthermore, we reveal that introducing heterogeneity in the bidirectional architecture allows the platoon's behaviour to be rendered independent of its length, allowing for highly scalable performance.",2024-06-11T13:56:12Z,2024-06-11T13:56:12Z,http://arxiv.org/abs/2406.07271v1,http://arxiv.org/pdf/2406.07271v1,"eess.SY, cs.SY, 93D15"
A Simple Architecture for Enterprise Large Language Model Applications   based on Role based security and Clearance Levels using Retrieval-Augmented   Generation or Mixture of Experts,"Atilla Özgür, Yılmaz Uygun","This study proposes a simple architecture for Enterprise application for Large Language Models (LLMs) for role based security and NATO clearance levels. Our proposal aims to address the limitations of current LLMs in handling security and information access. The proposed architecture could be used while utilizing Retrieval-Augmented Generation (RAG) and fine tuning of Mixture of experts models (MoE). It could be used only with RAG, or only with MoE or with both of them. Using roles and security clearance level of the user, documents in RAG and experts in MoE are filtered. This way information leakage is prevented.",2024-07-09T09:46:23Z,2024-07-09T09:46:23Z,http://arxiv.org/abs/2407.06718v1,http://arxiv.org/pdf/2407.06718v1,"cs.AI, D.2.11; I.2.7"
StreamAAD: Decoding Spatial Auditory Attention with a Streaming   Architecture,"Zelin Qiu, Dingding Yao, Junfeng Li","In this paper, we present our approach for the Track 1 of the Chinese Auditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most existing spatial auditory attention decoding (Sp-AAD) methods employ an isolated window architecture, focusing solely on global invariant features without considering relationships between different decision windows, which can lead to suboptimal performance. To address this issue, we propose a novel streaming decoding architecture, termed StreamAAD. In StreamAAD, decision windows are input to the network as a sequential stream and decoded in order, allowing for the modeling of inter-window relationships. Additionally, we employ a model ensemble strategy, achieving significant better performance than the baseline, ranking First in the challenge.",2024-08-24T08:54:26Z,2024-08-24T08:54:26Z,http://arxiv.org/abs/2408.13522v1,http://arxiv.org/pdf/2408.13522v1,"cs.SD, eess.AS"
Photonic Quantum Computers,M. AbuGhanem,"In the pursuit of scalable and fault-tolerant quantum computing architectures, photonic-based quantum computers have emerged as a leading frontier. This article provides a comprehensive overview of advancements in photonic quantum computing, developed by leading industry players, examining current performance, architectural designs, and strategies for developing large-scale, fault-tolerant photonic quantum computers. It also highlights recent groundbreaking experiments that leverage the unique advantages of photonic technologies, underscoring their transformative potential. This review captures a pivotal moment of photonic quantum computing in the noisy intermediate-scale quantum (NISQ) era, offering insights into how photonic quantum computers might reshape the future of quantum computing.",2024-09-12T17:16:38Z,2024-09-12T17:16:38Z,http://arxiv.org/abs/2409.08229v1,http://arxiv.org/pdf/2409.08229v1,"quant-ph, cs.AI, cs.AR"
Harnessing and modulating chaos to sample from neural generative models,"Rishidev Chaudhuri, Vivek Handebagh","Chaos is generic in strongly-coupled recurrent networks of model neurons, and thought to be an easily accessible dynamical regime in the brain. While neural chaos is typically seen as an impediment to robust computation, we show how such chaos might play a functional role in allowing the brain to learn and sample from generative models. We construct architectures that combine a classic model of neural chaos either with a canonical generative modeling architecture or with energy-based models of neural memory. We show that these architectures have appealing properties for sampling, including easy biologically-plausible control of sampling rates via overall gain modulation.",2024-09-26T22:52:26Z,2024-09-26T22:52:26Z,http://arxiv.org/abs/2409.18329v1,http://arxiv.org/pdf/2409.18329v1,"q-bio.NC, cs.NE, nlin.CD"
A Mathematical Explanation of UNet,"Xue-Cheng Tai, Hao Liu, Raymond H. Chan, Lingfeng Li","The UNet architecture has transformed image segmentation. UNet's versatility and accuracy have driven its widespread adoption, significantly advancing fields reliant on machine learning problems with images. In this work, we give a clear and concise mathematical explanation of UNet. We explain what is the meaning and function of each of the components of UNet. We will show that UNet is solving a control problem. We decompose the control variables using multigrid methods. Then, operator-splitting techniques is used to solve the problem, whose architecture exactly recovers the UNet architecture. Our result shows that UNet is a one-step operator-splitting algorithm for the control problem.",2024-10-06T10:07:52Z,2024-10-06T10:07:52Z,http://arxiv.org/abs/2410.04434v1,http://arxiv.org/pdf/2410.04434v1,"cs.CV, 68U10, 94A08"
On Training of Kolmogorov-Arnold Networks,Shairoz Sohail,"Kolmogorov-Arnold Networks have recently been introduced as a flexible alternative to multi-layer Perceptron architectures. In this paper, we examine the training dynamics of different KAN architectures and compare them with corresponding MLP formulations. We train with a variety of different initialization schemes, optimizers, and learning rates, as well as utilize back propagation free approaches like the HSIC Bottleneck. We find that (when judged by test accuracy) KANs are an effective alternative to MLP architectures on high-dimensional datasets and have somewhat better parameter efficiency, but suffer from more unstable training dynamics. Finally, we provide recommendations for improving training stability of larger KAN models.",2024-11-08T02:57:59Z,2024-11-08T02:57:59Z,http://arxiv.org/abs/2411.05296v1,http://arxiv.org/pdf/2411.05296v1,"cs.LG, cs.AI, I.2.4"
Architecture Proposal for 6G Systems Integrating Sensing and   Communication,"Peter Gersing, Mark Doll, Joerg Huschke, Oliver Holschke","Integrating sensing functionality into 6G communication networks requires some changes to existing components as well as new entities processing the radar sensing signals received by the communication antennas. This whitepaper provides a comprehensive overview of the 6G design proposal for ISaC (Integrated Sensing and Communication). The whitepaper has been created by the architecture group of the KOMSENS-6G project. It represents an intermediate state of the work, as the KOMSENS-6G project is still ongoing. The proposal should serve as a basis for further discussions and alignment across innovative 6G projects.",2024-11-15T12:28:22Z,2024-11-15T12:28:22Z,http://arxiv.org/abs/2411.10138v1,http://arxiv.org/pdf/2411.10138v1,"cs.NI, 68M10, C.2.1"
Bidirectional Mamba state-space model for anomalous diffusion,"Maxime Lavaud, Yosef Shokeeb, Juliette Lacherez, Yacine Amarouchene, Thomas Salez","Characterizing anomalous diffusion is crucial in order to understand the evolution of complex stochastic systems, from molecular interactions to cellular dynamics. In this work, we characterize the performances regarding such a task of Bi-Mamba, a novel state-space deep-learning architecture articulated with a bidirectional scan mechanism. Our implementation is tested on the AnDi-2 challenge datasets among others. Designed for regression tasks, the Bi-Mamba architecture infers efficiently the effective diffusion coefficient and anomalous exponent from single, short trajectories. As such, our results indicate the potential practical use of the Bi-Mamba architecture for anomalousdiffusion characterization.",2024-12-10T08:29:38Z,2024-12-10T08:29:38Z,http://arxiv.org/abs/2412.07299v1,http://arxiv.org/pdf/2412.07299v1,"cond-mat.soft, physics.bio-ph, physics.optics, stat.ML"
Coexistence Options and Performance Analysis of 100 Gbit/s Coherent PON   in Brownfield DWDM Networks,"Gabriele Di Rosa, Martin Kuipers, Jim Zou, Ognjen Jovanovic, Jörg-Peter Elbers","We study system architectures for the coexistence of future coherent PON and DWDM networks. Considering deployed optical filters, we observe filtering penalties < 1dB at a laser frequency accuracy < 12GHz when using a cost-effective architecture.",2024-12-20T10:02:48Z,2024-12-20T10:02:48Z,http://arxiv.org/abs/2412.15743v1,http://arxiv.org/pdf/2412.15743v1,"eess.SP, cs.NI"
Absence of censoring inequalities in random quantum circuits,"Daniel Belkin, James Allen, Bryan K. Clark","Ref. 1 asked whether deleting gates from a random quantum circuit architecture can ever make the architecture a better approximate $t$-design. We show that it can. In particular, we construct a family of architectures such that the approximate $2$-design depth decreases when certain gates are deleted. We also give some intuition for this construction and discuss the relevance of this result to the approximate $t$-design depth of the 1D brickwork. Deleting gates always decreases scrambledness in the short run, but can sometimes cause it to increase in the long run. Finally, we give analogous results for spectral gaps and when deleting edges of interaction graphs.",2025-02-21T23:17:26Z,2025-02-21T23:17:26Z,http://arxiv.org/abs/2502.15995v1,http://arxiv.org/pdf/2502.15995v1,"quant-ph, math-ph, math.MP"
Contact-Based Architecture for Resource Discovery (CARD) in Large Scale   MANets,"Saurabh Garg, Priyatham Pamu, Nitin Nahata, Ahmed Helmy","In this paper we propose a novel architecture, CARD, for resource discovery in large scale Mobile Ad hoc Networks (MANets) which, may scale up to thousands of nodes and may span wide geographical regions. Unlike previously proposed schemes, our architecture avoids expensive mechanisms such as global flooding as well as complex coordination between nodes to form a hierarchy. CARD is also independent of any external source of information such as GPS. In our architecture nodes within a limited number of hops from each node form the neighborhood of that node. Resources within the neighborhood can be readily accessed with the help of a proactive scheme within the neighborhood. For accessing resources beyond the neighborhood, each node also maintains a few distant nodes called contacts. Contacts help in creating a small world in the network and provide an efficient way to query for resources beyond the neighborhood. As the number of contacts of a node increases, the network view (reachability) of the node increases. Paths to contacts are validated periodically to adapt to mobility. We present mechanisms for contact selection and maintenance that attempt to increase reachability while minimizing overhead. Our simulation results show a clear trade-off between increase in reachability on one hand, and contact selection and maintenance overhead on the other. Our results suggest that CARD can be configured to provide a desirable reachability distribution for different network sizes. Comparisons with other schemes for resource discovery, such as flooding and bordercasting, show our architecture to be much more efficient and scalable.",2002-08-16T17:21:13Z,2002-08-16T17:21:13Z,http://arxiv.org/abs/cs/0208024v1,http://arxiv.org/pdf/cs/0208024v1,"cs.NI, C.2.1; C.2.2"
SU(2) Lattice Gauge Theory Simulations on Fermi GPUs,"Nuno Cardoso, Pedro Bicudo","In this work we explore the performance of CUDA in quenched lattice SU(2) simulations. CUDA, NVIDIA Compute Unified Device Architecture, is a hardware and software architecture developed by NVIDIA for computing on the GPU. We present an analysis and performance comparison between the GPU and CPU in single and double precision. Analyses with multiple GPUs and two different architectures (G200 and Fermi architectures) are also presented. In order to obtain a high performance, the code must be optimized for the GPU architecture, i.e., an implementation that exploits the memory hierarchy of the CUDA programming model.   We produce codes for the Monte Carlo generation of SU(2) lattice gauge configurations, for the mean plaquette, for the Polyakov Loop at finite T and for the Wilson loop. We also present results for the potential using many configurations ($50\ 000$) without smearing and almost $2\ 000$ configurations with APE smearing. With two Fermi GPUs we have achieved an excellent performance of $200 \times$ the speed over one CPU, in single precision, around 110 Gflops/s. We also find that, using the Fermi architecture, double precision computations for the static quark-antiquark potential are not much slower (less than $2 \times$ slower) than single precision computations.",2010-10-23T00:44:58Z,2011-03-11T17:16:41Z,http://arxiv.org/abs/1010.4834v2,http://arxiv.org/pdf/1010.4834v2,"hep-lat, physics.comp-ph"
Cross-point architecture for spin transfer torque magnetic random access   memory,"Weisheng Zhao, Sumanta Chaudhuri, Celso Accoto, Jacques-Olivier Klein, Claude Chappert, Pascale Mazoyer","Spin transfer torque magnetic random access memory (STT-MRAM) is considered as one of the most promising candidates to build up a true universal memory thanks to its fast write/read speed, infinite endurance and non-volatility. However the conventional access architecture based on 1 transistor + 1 memory cell limits its storage density as the selection transistor should be large enough to ensure the write current higher than the critical current for the STT operation. This paper describes a design of cross-point architecture for STT-MRAM. The mean area per word corresponds to only two transistors, which are shared by a number of bits (e.g. 64). This leads to significant improvement of data density (e.g. 1.75 F2/bit). Special techniques are also presented to address the sneak currents and low speed issues of conventional cross-point architecture, which are difficult to surmount and few efficient design solutions have been reported in the literature. By using a STT-MRAM SPICE model including precise experimental parameters and STMicroelectronics 65 nm technology, some chip characteristic results such as cell area, data access speed and power have been calculated or simulated to demonstrate the expected performances of this new memory architecture.",2012-02-08T17:50:33Z,2012-02-08T17:50:33Z,http://arxiv.org/abs/1202.1782v1,http://arxiv.org/pdf/1202.1782v1,"cs.ET, cs.AR, physics.class-ph"
Towards a Unified Architecture for in-RDBMS Analytics,"Xixuan Feng, Arun Kumar, Ben Recht, Christopher Ré","The increasing use of statistical data analysis in enterprise applications has created an arms race among database vendors to offer ever more sophisticated in-database analytics. One challenge in this race is that each new statistical technique must be implemented from scratch in the RDBMS, which leads to a lengthy and complex development process. We argue that the root cause for this overhead is the lack of a unified architecture for in-database analytics. Our main contribution in this work is to take a step towards such a unified architecture. A key benefit of our unified architecture is that performance optimizations for analytics techniques can be studied generically instead of an ad hoc, per-technique fashion. In particular, our technical contributions are theoretical and empirical studies of two key factors that we found impact performance: the order data is stored, and parallelization of computations on a single-node multicore RDBMS. We demonstrate the feasibility of our architecture by integrating several popular analytics techniques into two commercial and one open-source RDBMS. Our architecture requires changes to only a few dozen lines of code to integrate a new statistical technique. We then compare our approach with the native analytics tools offered by the commercial RDBMSes on various analytics tasks, and validate that our approach achieves competitive or higher performance, while still achieving the same quality.",2012-03-12T18:07:58Z,2012-03-14T18:21:13Z,http://arxiv.org/abs/1203.2574v2,http://arxiv.org/pdf/1203.2574v2,"cs.DB, 97R50, H.2.m"
Network Coding as a WiMAX Link Reliability Mechanism,"S. Teerapittayanon, K. Fouli, M. Medard, M. -J. Montpetit, X. Shi, I. Seskar, A. Gosain","We design and implement a network-coding-enabled reliability architecture for next generation wireless networks. Our network coding (NC) architecture uses a flexible thread-based design, with each encoder-decoder instance applying systematic intra-session random linear network coding as a packet erasure code at the IP layer, to ensure the fast and reliable transfer of information between wireless nodes.   Using Global Environment for Network Innovations (GENI) WiMAX platforms, a series of point-to-point transmission experiments were conducted to compare the performance of the NC architecture to that of the Automatic Repeated reQuest (ARQ) and Hybrid ARQ (HARQ) mechanisms. At the application layer, Iperf and UDP-based File Transfer Protocol (UFTP) are used to measure throughput, packet loss and file transfer delay. In our selected scenarios, the proposed architecture is able to decrease packet loss from around 11-32% to nearly 0%; compared to HARQ and joint HARQ/ARQ mechanisms, the NC architecture offers up to 5.9 times gain in throughput and 5.5 times reduction in end-to-end file transfer delay. Our experiments show that network coding as a packet erasure code in the upper layers of the protocol stack has the potential to reduce the need for joint HARQ/ARQ schemes in the PHY/MAC layers, thus offering insights into cross-layer designs of efficient next generation wireless networks.",2012-08-23T14:26:07Z,2013-08-01T20:48:26Z,http://arxiv.org/abs/1208.4766v2,http://arxiv.org/pdf/1208.4766v2,"cs.NI, H.4.3; H.1.1"
Adding Gradient Noise Improves Learning for Very Deep Networks,"Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens","Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures.",2015-11-21T01:11:29Z,2015-11-21T01:11:29Z,http://arxiv.org/abs/1511.06807v1,http://arxiv.org/pdf/1511.06807v1,"stat.ML, cs.LG"
Unsupervised Basis Function Adaptation for Reinforcement Learning,"Edward Barker, Charl Ras","When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture on-line and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice.",2017-03-23T05:23:34Z,2019-02-16T23:14:40Z,http://arxiv.org/abs/1703.07940v3,http://arxiv.org/pdf/1703.07940v3,"cs.LG, cs.AI, stat.ML"
The capacity of feedforward neural networks,"Pierre Baldi, Roman Vershynin","A long standing open problem in the theory of neural networks is the development of quantitative methods to estimate and compare the capabilities of different architectures. Here we define the capacity of an architecture by the binary logarithm of the number of functions it can compute, as the synaptic weights are varied. The capacity provides an upper bound on the number of bits that can be extracted from the training data and stored in the architecture during learning. We study the capacity of layered, fully-connected, architectures of linear threshold neurons with $L$ layers of size $n_1,n_2, \ldots, n_L$ and show that in essence the capacity is given by a cubic polynomial in the layer sizes: $C(n_1,\ldots, n_L)=\sum_{k=1}^{L-1} \min(n_1,\ldots,n_k)n_kn_{k+1}$, where layers that are smaller than all previous layers act as bottlenecks. In proving the main result, we also develop new techniques (multiplexing, enrichment, and stacking) as well as new bounds on the capacity of finite sets. We use the main result to identify architectures with maximal or minimal capacity under a number of natural constraints. This leads to the notion of structural regularization for deep architectures. While in general, everything else being equal, shallow networks compute more functions than deep networks, the functions computed by deep networks are more regular and ""interesting"".",2019-01-02T16:05:28Z,2019-03-27T21:06:06Z,http://arxiv.org/abs/1901.00434v2,http://arxiv.org/pdf/1901.00434v2,"cs.LG, cs.NE, math.CO, stat.ML, 68Q32, 06E30, 92B20"
Effect of Various Regularizers on Model Complexities of Neural Networks   in Presence of Input Noise,"Mayank Sharma, Aayush Yadav, Sumit Soman, Jayadeva","Deep neural networks are over-parameterized, which implies that the number of parameters are much larger than the number of samples used to train the network. Even in such a regime deep architectures do not overfit. This phenomenon is an active area of research and many theories have been proposed trying to understand this peculiar observation. These include the Vapnik Chervonenkis (VC) dimension bounds and Rademacher complexity bounds which show that the capacity of the network is characterized by the norm of weights rather than the number of parameters. However, the effect of input noise on these measures for shallow and deep architectures has not been studied. In this paper, we analyze the effects of various regularization schemes on the complexity of a neural network which we characterize with the loss, $L_2$ norm of the weights, Rademacher complexities (Directly Approximately Regularizing Complexity-DARC1), VC dimension based Low Complexity Neural Network (LCNN) when subject to varying degrees of Gaussian input noise. We show that $L_2$ regularization leads to a simpler hypothesis class and better generalization followed by DARC1 regularizer, both for shallow as well as deeper architectures. Jacobian regularizer works well for shallow architectures with high level of input noises. Spectral normalization attains highest test set accuracies both for shallow and deeper architectures. We also show that Dropout alone does not perform well in presence of input noise. Finally, we show that deeper architectures are robust to input noise as opposed to their shallow counterparts.",2019-01-31T16:31:30Z,2019-01-31T16:31:30Z,http://arxiv.org/abs/1901.11458v1,http://arxiv.org/pdf/1901.11458v1,"cs.LG, stat.ML"
Impact of Fully Connected Layers on Performance of Convolutional Neural   Networks for Image Classification,"S. H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, Snehasis Mukherjee","The Convolutional Neural Networks (CNNs), in domains like computer vision, mostly reduced the need for handcrafted features due to its ability to learn the problem-specific features from the raw input data. However, the selection of dataset-specific CNN architecture, which mostly performed by either experience or expertise is a time-consuming and error-prone process. To automate the process of learning a CNN architecture, this paper attempts at finding the relationship between Fully Connected (FC) layers with some of the characteristics of the datasets. The CNN architectures, and recently datasets also, are categorized as deep, shallow, wide, etc. This paper tries to formalize these terms along with answering the following questions. (i) What is the impact of deeper/shallow architectures on the performance of the CNN w.r.t. FC layers?, (ii) How the deeper/wider datasets influence the performance of CNN w.r.t. FC layers?, and (iii) Which kind of architecture (deeper/ shallower) is better suitable for which kind of (deeper/ wider) datasets. To address these findings, we have performed experiments with three CNN architectures having different depths. The experiments are conducted by varying the number of FC layers. We used four widely used datasets including CIFAR-10, CIFAR-100, Tiny ImageNet, and CRCHistoPhenotypes to justify our findings in the context of the image classification problem. The source code of this research is available at https://github.com/shabbeersh/Impact-of-FC-layers.",2019-01-21T07:42:26Z,2019-11-19T05:28:01Z,http://arxiv.org/abs/1902.02771v3,http://arxiv.org/pdf/1902.02771v3,"cs.CV, cs.LG, cs.NE, eess.IV"
Evaluating the Search Phase of Neural Architecture Search,"Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, Mathieu Salzmann","Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used weight sharing strategy degrades the ranking of the NAS candidates to the point of not reflecting their true performance, thus reducing the effectiveness of the search process. We believe that our evaluation framework will be key to designing NAS strategies that consistently discover architectures superior to random ones.",2019-02-21T17:11:56Z,2019-11-22T17:07:59Z,http://arxiv.org/abs/1902.08142v3,http://arxiv.org/pdf/1902.08142v3,"cs.LG, stat.ML"
SNAS: Stochastic Neural Architecture Search,"Sirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin","We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets. We have released our implementation at https://github.com/SNAS-Series/SNAS-Series.",2018-12-24T14:13:16Z,2020-04-01T00:44:35Z,http://arxiv.org/abs/1812.09926v3,http://arxiv.org/pdf/1812.09926v3,"cs.LG, cs.AI, stat.ML"
Audio Source Separation via Multi-Scale Learning with Dilated Dense   U-Nets,"Vivek Sivaraman Narayanaswamy, Sameeksha Katoch, Jayaraman J. Thiagarajan, Huan Song, Andreas Spanias","Modern audio source separation techniques rely on optimizing sequence model architectures such as, 1D-CNNs, on mixture recordings to generalize well to unseen mixtures. Specifically, recent focus is on time-domain based architectures such as Wave-U-Net which exploit temporal context by extracting multi-scale features. However, the optimality of the feature extraction process in these architectures has not been well investigated. In this paper, we examine and recommend critical architectural changes that forge an optimal multi-scale feature extraction process. To this end, we replace regular $1-$D convolutions with adaptive dilated convolutions that have innate capability of capturing increased context by using large temporal receptive fields. We also investigate the impact of dense connections on the extraction process that encourage feature reuse and better gradient flow. The dense connections between the downsampling and upsampling paths of a U-Net architecture capture multi-resolution information leading to improved temporal modelling. We evaluate the proposed approaches on the MUSDB test dataset. In addition to providing an improved performance over the state-of-the-art, we also provide insights on the impact of different architectural choices on complex data-driven solutions for source separation.",2019-04-08T16:13:16Z,2019-04-08T16:13:16Z,http://arxiv.org/abs/1904.04161v1,http://arxiv.org/pdf/1904.04161v1,"cs.LG, cs.SD, eess.AS, stat.ML"
AdversarialNAS: Adversarial Neural Architecture Search for GANs,"Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, Shuicheng Yan","Neural Architecture Search (NAS) that aims to automate the procedure of architecture design has achieved promising results in many computer vision fields. In this paper, we propose an AdversarialNAS method specially tailored for Generative Adversarial Networks (GANs) to search for a superior generative model on the task of unconditional image generation. The AdversarialNAS is the first method that can search the architectures of generator and discriminator simultaneously in a differentiable manner. During searching, the designed adversarial search algorithm does not need to comput any extra metric to evaluate the performance of the searched architecture, and the search paradigm considers the relevance between the two network architectures and improves their mutual balance. Therefore, AdversarialNAS is very efficient and only takes 1 GPU day to search for a superior generative model in the proposed large search space ($10^{38}$). Experiments demonstrate the effectiveness and superiority of our method. The discovered generative model sets a new state-of-the-art FID score of $10.87$ and highly competitive Inception Score of $8.74$ on CIFAR-10. Its transferability is also proven by setting new state-of-the-art FID score of $26.98$ and Inception score of $9.63$ on STL-10. Code is at: \url{https://github.com/chengaopro/AdversarialNAS}.",2019-12-04T15:02:03Z,2020-04-08T14:01:02Z,http://arxiv.org/abs/1912.02037v2,http://arxiv.org/pdf/1912.02037v2,"cs.CV, eess.IV"
A Study on Encodings for Neural Architecture Search,"Colin White, Willie Neiswanger, Sam Nolen, Yash Savani","Neural architecture search (NAS) has been extensively studied in the past few years. A popular approach is to represent each neural architecture in the search space as a directed acyclic graph (DAG), and then search over all DAGs by encoding the adjacency matrix and list of operations as a set of hyperparameters. Recent work has demonstrated that even small changes to the way each architecture is encoded can have a significant effect on the performance of NAS algorithms.   In this work, we present the first formal study on the effect of architecture encodings for NAS, including a theoretical grounding and an empirical study. First we formally define architecture encodings and give a theoretical characterization on the scalability of the encodings we study Then we identify the main encoding-dependent subroutines which NAS algorithms employ, running experiments to show which encodings work best with each subroutine for many popular algorithms. The experiments act as an ablation study for prior work, disentangling the algorithmic and encoding-based contributions, as well as a guideline for future work. Our results demonstrate that NAS encodings are an important design decision which can have a significant impact on overall performance. Our code is available at https://github.com/naszilla/nas-encodings.",2020-07-09T17:52:11Z,2021-02-17T23:04:02Z,http://arxiv.org/abs/2007.04965v2,http://arxiv.org/pdf/2007.04965v2,"cs.LG, cs.NE, stat.ML"
High-Throughput VLSI Architecture for GRAND,"Syed Mohsin Abbas, Thibaud Tonnellier, Furkan Ercan, Warren J. Gross","Guessing Random Additive Noise Decoding (GRAND) is a recently proposed universal decoding algorithm for linear error correcting codes. Since GRAND does not depend on the structure of the code, it can be used for any code encountered in contemporary communication standards or may even be used for random linear network coding. This property makes this new algorithm particularly appealing. Instead of trying to decode the received vector, GRAND attempts to identify the noise that corrupted the codeword. To that end, GRAND relies on the generation of test error patterns that are successively applied to the received vector. In this paper, we propose the first hardware architecture for the GRAND algorithm. Considering GRAND with ABandonment (GRANDAB) that limits the number of test patterns, the proposed architecture only needs $2+\sum_{i=2}^{n} \left\lfloor\frac{i}{2}\right\rfloor$ time steps to perform the $\sum_{i=1}^3 \binom{n}{i}$ queries required when $\text{AB}=3$. For a code length of $128$, our proposed hardware architecture demonstrates only a fraction ($1.2\%$) of the total number of performed queries as time steps. Synthesis result using TSMC 65nm CMOS technology shows that average throughputs of $32$ Gbps to $64$ Gbps can be achieved at an SNR of $10$ dB for a code length of $128$ and code rates rate higher than $0.75$, transmitted over an AWGN channel. Comparisons with a decoder tailored for a $(79,64)$ BCH code show that the proposed architecture can achieve a slightly higher average throughput at high SNRs, while obtaining the same decoding performance.",2020-07-14T19:44:42Z,2020-07-14T19:44:42Z,http://arxiv.org/abs/2007.07328v1,http://arxiv.org/pdf/2007.07328v1,"cs.IT, cs.AR, math.IT"
Design Principles for Packet Deparsers on FPGAs,"Thomas Luinaud, Jeferson Santiago da Silva, J. M. Pierre Langlois, Yvon Savaria","The P4 language has drastically changed the networking field as it allows to quickly describe and implement new networking applications. Although a large variety of applications can be described with the P4 language, current programmable switch architectures impose significant constraints on P4 programs. To address this shortcoming, FPGAs have been explored as potential targets for P4 applications. P4 applications are described using three abstractions: a packet parser, match-action tables, and a packet deparser, which reassembles the output packet with the result of the match-action tables. While implementations of packet parsers and match-action tables on FPGAs have been widely covered in the literature, no general design principles have been presented for the packet deparser. Indeed, implementing a high-speed and efficient deparser on FPGAs remains an open issue because it requires a large amount of interconnections and the architecture must be tailored to a P4 program. As a result, in several works where a P4 application is implemented on FPGAs, the deparser consumes a significant proportion of chip resources. Hence, in this paper, we address this issue by presenting design principles for efficient and high-speed deparsers on FPGAs. As an artifact, we introduce a tool that generates an efficient vendor-agnostic deparser architecture from a P4 program. Our design has been validated and simulated with a cocotb-based framework. The resulting architecture is implemented on Xilinx Ultrascale+ FPGAs and supports a throughput of more than 200 Gbps while reducing resource usage by almost 10$\times$ compared to other solutions.",2021-03-13T16:58:09Z,2021-03-13T16:58:09Z,http://arxiv.org/abs/2103.07750v1,http://arxiv.org/pdf/2103.07750v1,"cs.AR, cs.NI, B.5.1"
MANAS: Multi-Scale and Multi-Level Neural Architecture Search for   Low-Dose CT Denoising,"Zexin Lu, Wenjun Xia, Yongqiang Huang, Hongming Shan, Hu Chen, Jiliu Zhou, Yi Zhang","Lowering the radiation dose in computed tomography (CT) can greatly reduce the potential risk to public health. However, the reconstructed images from the dose-reduced CT or low-dose CT (LDCT) suffer from severe noise, compromising the subsequent diagnosis and analysis. Recently, convolutional neural networks have achieved promising results in removing noise from LDCT images; the network architectures used are either handcrafted or built on top of conventional networks such as ResNet and U-Net. Recent advance on neural network architecture search (NAS) has proved that the network architecture has a dramatic effect on the model performance, which indicates that current network architectures for LDCT may be sub-optimal. Therefore, in this paper, we make the first attempt to apply NAS to LDCT and propose a multi-scale and multi-level NAS for LDCT denoising, termed MANAS. On the one hand, the proposed MANAS fuses features extracted by different scale cells to capture multi-scale image structural details. On the other hand, the proposed MANAS can search a hybrid cell- and network-level structure for better performance. Extensively experimental results on three different dose levels demonstrate that the proposed MANAS can achieve better performance in terms of preserving image structural details than several state-of-the-art methods. In addition, we also validate the effectiveness of the multi-scale and multi-level architecture for LDCT denoising.",2021-03-24T05:41:01Z,2021-03-24T05:41:01Z,http://arxiv.org/abs/2103.12995v1,http://arxiv.org/pdf/2103.12995v1,"physics.med-ph, cs.CV"
Hello Edge: Keyword Spotting on Microcontrollers,"Yundong Zhang, Naveen Suda, Liangzhen Lai, Vikas Chandra","Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4%, which is ~10% higher than the DNN model with similar number of parameters.",2017-11-20T03:19:03Z,2018-02-14T19:24:55Z,http://arxiv.org/abs/1711.07128v3,http://arxiv.org/pdf/1711.07128v3,"cs.SD, cs.CL, cs.LG, cs.NE, eess.AS"
Transfer Learning to Learn with Multitask Neural Model Search,"Catherine Wong, Andrea Gesmundo","Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks. In this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.",2017-10-30T05:32:50Z,2017-10-30T05:32:50Z,http://arxiv.org/abs/1710.10776v1,http://arxiv.org/pdf/1710.10776v1,"cs.AI, cs.LG, stat.ML"
Deep Neural Network Architectures for Modulation Classification,"Xiaoyu Liu, Diyu Yang, Aly El Gamal","In this work, we investigate the value of employing deep learning for the task of wireless signal modulation recognition. Recently in [1], a framework has been introduced by generating a dataset using GNU radio that mimics the imperfections in a real wireless channel, and uses 10 different modulation types. Further, a convolutional neural network (CNN) architecture was developed and shown to deliver performance that exceeds that of expert-based approaches. Here, we follow the framework of [1] and find deep neural network architectures that deliver higher accuracy than the state of the art. We tested the architecture of [1] and found it to achieve an accuracy of approximately 75% of correctly recognizing the modulation type. We first tune the CNN architecture of [1] and find a design with four convolutional layers and two dense layers that gives an accuracy of approximately 83.8% at high SNR. We then develop architectures based on the recently introduced ideas of Residual Networks (ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR accuracies of approximately 83.5% and 86.6%, respectively. Finally, we introduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to achieve an accuracy of approximately 88.5% at high SNR.",2017-12-01T18:56:37Z,2018-01-05T12:54:51Z,http://arxiv.org/abs/1712.00443v3,http://arxiv.org/pdf/1712.00443v3,"cs.LG, stat.ML"
Near-Optimal Hardware Design for Convolutional Neural Networks,Byungik Ahn,"Recently, the demand of low-power deep-learning hardware for industrial applications has been increasing. Most existing artificial intelligence (AI) chips have evolved to rely on new chip technologies rather than on radically new hardware architectures, to maintain their generality. This study proposes a novel, special-purpose, and high-efficiency hardware architecture for convolutional neural networks. The proposed architecture maximizes the utilization of multipliers by designing the computational circuit with the same structure as that of the computational flow of the model, rather than mapping computations to fixed hardware. In addition, a specially designed filter circuit simultaneously provides all the data of the receptive field, using only one memory read operation during each clock cycle; this allows the computation circuit to operate seamlessly without idle cycles. Our reference system based on the proposed architecture uses 97% of the peak-multiplication capability in actual computations required by the computation model throughout the computation period. In addition, overhead components are minimized so that the proportion of the resources constituting the non-multiplier components is smaller than that constituting the multiplier components, which are indispensable for the computational model. The efficiency of the proposed architecture is close to an ideally efficient system that cannot be improved further in terms of the performance-to-resource ratio. An implementation based on the proposed hardware architecture has been applied in commercial AI products.",2020-02-06T09:15:03Z,2020-02-06T09:15:03Z,http://arxiv.org/abs/2002.05526v1,http://arxiv.org/pdf/2002.05526v1,"cs.LG, eess.SP"
Bridging the Gap: FPGAs as Programmable Switches,"Thomas Luinaud, Thibaut Stimpfling, Jeferson Santiago da Silva, Yvon Savaria, J. M. Pierre Langlois","The emergence of P4, a domain specific language, coupled to PISA, a domain specific architecture, is revolutionizing the networking field. P4 allows to describe how packets are processed by a programmable data plane, spanning ASICs and CPUs, implementing PISA. Because the processing flexibility can be limited on ASICs, while the CPUs performance for networking tasks lag behind, recent works have proposed to implement PISA on FPGAs. However, little effort has been dedicated to analyze whether FPGAs are good candidates to implement PISA. In this work, we take a step back and evaluate the micro-architecture efficiency of various PISA blocks. We demonstrate, supported by a theoretical and experimental analysis, that the performance of a few PISA blocks is severely limited by the current FPGA architectures. Specifically, we show that match tables and programmable packet schedulers represent the main performance bottlenecks for FPGA-based programmable switches. Thus, we explore two avenues to alleviate these shortcomings. First, we identify network applications well tailored to current FPGAs. Second, to support a wider range of networking applications, we propose modifications to the FPGA architectures which can also be of interest out of the networking field.",2020-04-16T16:15:51Z,2020-04-16T16:15:51Z,http://arxiv.org/abs/2004.07733v1,http://arxiv.org/pdf/2004.07733v1,"cs.AR, cs.NI, B.5.1"
CAPODAZ: A Containerised Authorisation and Policy-driven Architecture   using Microservices,"Dimitrios Kallergis, Zacharenia Garofalaki, Georgios Katsikogiannis, Christos Douligeris","The microservices architectural approach has important benefits regarding the agile applications' development and the delivery of complex solutions. However, to convey the information and share the data amongst services in a verifiable and stateless way, there is a need to enable appropriate access control methods and authorisations. In this paper, we study the use of policy-driven authorisations with independent fine-grained microservices in the case of a real-world machine-to-machine (M2M) scenario using a hybrid cloud-based infrastructure and Internet of Things (IoT) services. We also model the authentication flows which facilitate the message exchanges between the involved entities, and we propose a containerised authorisation and policy-driven architecture (CAPODAZ) using the microservices paradigm. The proposed architecture implements a policy-based management framework and integrates in an on-going work regarding a Cloud-IoT intelligent transportation service. For the in-depth quantitative evaluation, we treat multiple distributions of users' populations and assess the proposed architecture against other similar microservices. The numerical results based on the experimental data show that there exists significant performance preponderance in terms of latency, throughput and successful requests.",2020-04-21T20:16:34Z,2020-04-30T10:18:07Z,http://arxiv.org/abs/2004.10276v2,http://arxiv.org/pdf/2004.10276v2,"cs.NI, cs.CR, cs.DC, 68M14, C.2.4; C.2.1; D.4.6; H.3.4"
Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR   in Transfer Learning,"Zhiping Zeng, Van Tung Pham, Haihua Xu, Yerbolat Khassanov, Eng Siong Chng, Chongjia Ni, Bin Ma","In this work, we study leveraging extra text data to improve low-resource end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend our prior work [1], and propose a hybrid Transformer-LSTM based architecture. This architecture not only takes advantage of the highly effective encoding capacity of the Transformer network but also benefits from extra text data due to the LSTM-based independent language model network. We conduct experiments on our in-house Malay corpus which contains limited labeled data and a large amount of extra text. Results show that the proposed architecture outperforms the previous LSTM-based architecture [1] by 24.2% relative word error rate (WER) when both are trained using limited labeled data. Starting from this, we obtain further 25.4% relative WER reduction by transfer learning from another resource-rich language. Moreover, we obtain additional 13.6% relative WER reduction by boosting the LSTM decoder of the transferred model with the extra text data. Overall, our best model outperforms the vanilla Transformer ASR by 11.9% relative WER. Last but not least, the proposed hybrid architecture offers much faster inference compared to both LSTM and Transformer architectures.",2020-05-21T00:56:42Z,2020-05-28T09:35:42Z,http://arxiv.org/abs/2005.10407v2,http://arxiv.org/pdf/2005.10407v2,"eess.AS, cs.LG, cs.SD"
Efficient Architecture Search for Continual Learning,"Qiang Gao, Zhipeng Luo, Diego Klabjan","Continual learning with neural networks is an important learning framework in AI that aims to learn a sequence of tasks well. However, it is often confronted with three challenges: (1) overcome the catastrophic forgetting problem, (2) adapt the current network to new tasks, and meanwhile (3) control its model complexity. To reach these goals, we propose a novel approach named as Continual Learning with Efficient Architecture Search, or CLEAS in short. CLEAS works closely with neural architecture search (NAS) which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer), and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows one to find a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.",2020-06-07T02:59:29Z,2020-06-09T04:54:11Z,http://arxiv.org/abs/2006.04027v2,http://arxiv.org/pdf/2006.04027v2,"cs.LG, cs.AI, stat.ML"
A Comprehensive Performance Analysis for mm-Wave Massive MIMO Hybrid   Beamforming under PA Nonlinearities,"Murat Babek Salman, Gokhan Muzaffer Guvensen","In this paper, we develop a framework to investigate the performances of different hybrid beamforming architectures for massive multiple input multiple output (MIMO) systems impaired by power amplifier (PA) nonlinearities. Indirect learning architecture based on feedback after anti-beamforming is adopted in design of digital pre-distortion (DPD) in order to compensate the nonlinear distortion caused by PA. In addition, we propose a novel analog beamformer design for partially connected architecture based on generalized eigen-beamformer (GEB) approach. In literature, the effects of nonlinear PA's on the out-of-band (OOB) radiation and achieved signal-to-interfence-plus-noise ratio (SINR) are investigated. However, these studies are limitted to fully digital or partially connected hybrid beamforming architectures while deploying Bussgang decompostion on a PA basis without considering the array architecture type in performance analysis. In this study, we derived an analytical bit-error-rate (BER) expression based on spatio-temporal Bussgang decompostion in matrix form, and mismatched decoding capacity via Generalized Mutual Information (GMI) is obtained under PA nonlinearity for different hybrid Massive MIMO architectures. Analytical results show that the nonlinear distortion significantly affects the system performance, and DPD can reduce these effects to some extend. Finally, obtained analytical BER expression is verified via numerical results.",2020-06-29T10:53:11Z,2021-02-26T13:27:41Z,http://arxiv.org/abs/2006.15930v4,http://arxiv.org/pdf/2006.15930v4,"cs.IT, eess.SP, math.IT"
ML-Assisted UE Positioning: Performance Analysis and 5G Architecture   Enhancements,"M. Majid Butt, Anna Pantelidou, István Z. Kovács","Artificial intelligence and data-driven networks will be integral part of 6G systems. In this article, we comprehensively discuss implementation challenges and need for architectural changes in 5G radio access networks for integrating machine learning (ML) solutions. As an example use case, we investigate user equipment (UE) positioning assisted by deep learning (DL) in 5G and beyond networks. As compared to state of the art positioning algorithms used in today's networks, radio signal fingerprinting and machine learning (ML) assisted positioning requires smaller additional feedback overhead; and the positioning estimates are made directly inside the radio access network (RAN), thereby assisting in radio resource management. In this regard, we study ML-assisted positioning methods and evaluate their performance using system level simulations for an outdoor scenario. The study is based on the use of raytracing tool, a 3GPP 5G NR compliant system level simulator and DL framework to estimate positioning accuracy of the UE. We evaluate and compare performance of various DL models and show mean positioning error in the range of 1-1.5m for a 2-hidden layer DL architecture with appropriate feature-modeling. Building on our performance analysis, we discuss pros and cons of various architectures to implement ML solutions for future networks and draw conclusions on the most suitable architecture.",2021-08-25T17:52:58Z,2021-08-25T17:52:58Z,http://arxiv.org/abs/2108.11365v1,http://arxiv.org/pdf/2108.11365v1,"cs.NI, eess.SP"
Algorithm and Hardware Co-design for Reconfigurable CNN Accelerator,"Hongxiang Fan, Martin Ferianc, Zhiqiang Que, He Li, Shuanglong Liu, Xinyu Niu, Wayne Luk","Recent advances in algorithm-hardware co-design for deep neural networks (DNNs) have demonstrated their potential in automatically designing neural architectures and hardware designs. Nevertheless, it is still a challenging optimization problem due to the expensive training cost and the time-consuming hardware implementation, which makes the exploration on the vast design space of neural architecture and hardware design intractable. In this paper, we demonstrate that our proposed approach is capable of locating designs on the Pareto frontier. This capability is enabled by a novel three-phase co-design framework, with the following new features: (a) decoupling DNN training from the design space exploration of hardware architecture and neural architecture, (b) providing a hardware-friendly neural architecture space by considering hardware characteristics in constructing the search cells, (c) adopting Gaussian process to predict accuracy, latency and power consumption to avoid time-consuming synthesis and place-and-route processes. In comparison with the manually-designed ResNet101, InceptionV2 and MobileNetV2, we can achieve up to 5% higher accuracy with up to 3x speed up on the ImageNet dataset. Compared with other state-of-the-art co-design frameworks, our found network and hardware configuration can achieve 2% ~ 6% higher accuracy, 2x ~ 26x smaller latency and 8.5x higher energy efficiency.",2021-11-24T20:37:50Z,2021-11-24T20:37:50Z,http://arxiv.org/abs/2111.12787v1,http://arxiv.org/pdf/2111.12787v1,"cs.LG, cs.AR, cs.SY, eess.SY"
TabNAS: Rejection Sampling for Neural Architecture Search on Tabular   Datasets,"Chengrun Yang, Gabriel Bender, Hanxiao Liu, Pieter-Jan Kindermans, Madeleine Udell, Yifeng Lu, Quoc Le, Da Huang","The best neural architecture for a given machine learning problem depends on many factors: not only the complexity and structure of the dataset, but also on resource constraints including latency, compute, energy consumption, etc. Neural architecture search (NAS) for tabular datasets is an important but under-explored problem. Previous NAS algorithms designed for image search spaces incorporate resource constraints directly into the reinforcement learning (RL) rewards. However, for NAS on tabular datasets, this protocol often discovers suboptimal architectures. This paper develops TabNAS, a new and more effective approach to handle resource constraints in tabular NAS using an RL controller motivated by the idea of rejection sampling. TabNAS immediately discards any architecture that violates the resource constraints without training or learning from that architecture. TabNAS uses a Monte-Carlo-based correction to the RL policy gradient update to account for this extra filtering step. Results on several tabular datasets demonstrate the superiority of TabNAS over previous reward-shaping methods: it finds better models that obey the constraints.",2022-04-15T19:03:25Z,2022-10-20T04:07:27Z,http://arxiv.org/abs/2204.07615v4,http://arxiv.org/pdf/2204.07615v4,"cs.LG, stat.ML"
PRE-NAS: Predictor-assisted Evolutionary Neural Architecture Search,"Yameng Peng, Andy Song, Vic Ciesielski, Haytham M. Fayek, Xiaojun Chang","Neural architecture search (NAS) aims to automate architecture engineering in neural networks. This often requires a high computational overhead to evaluate a number of candidate networks from the set of all possible networks in the search space during the search. Prediction of the networks' performance can alleviate this high computational overhead by mitigating the need for evaluating every candidate network. Developing such a predictor typically requires a large number of evaluated architectures which may be difficult to obtain. We address this challenge by proposing a novel evolutionary-based NAS strategy, Predictor-assisted E-NAS (PRE-NAS), which can perform well even with an extremely small number of evaluated architectures. PRE-NAS leverages new evolutionary search strategies and integrates high-fidelity weight inheritance over generations. Unlike one-shot strategies, which may suffer from bias in the evaluation due to weight sharing, offspring candidates in PRE-NAS are topologically homogeneous, which circumvents bias and leads to more accurate predictions. Extensive experiments on NAS-Bench-201 and DARTS search spaces show that PRE-NAS can outperform state-of-the-art NAS methods. With only a single GPU searching for 0.6 days, competitive architecture can be found by PRE-NAS which achieves 2.40% and 24% test error rates on CIFAR-10 and ImageNet respectively.",2022-04-27T06:40:39Z,2022-04-27T06:40:39Z,http://arxiv.org/abs/2204.12726v1,http://arxiv.org/pdf/2204.12726v1,"cs.CV, I.2; I.4"
CONDENSE: A Reconfigurable Knowledge Acquisition Architecture for Future   5G IoT,"Dejan Vukobratovic, Dusan Jakovetic, Vitaly Skachek, Dragana Bajovic, Dino Sejdinovic, Gunes Karabulut Kurt, Camilla Hollanti, Ingo Fischer","In forthcoming years, the Internet of Things (IoT) will connect billions of smart devices generating and uploading a deluge of data to the cloud. If successfully extracted, the knowledge buried in the data can significantly improve the quality of life and foster economic growth. However, a critical bottleneck for realising the efficient IoT is the pressure it puts on the existing communication infrastructures, requiring transfer of enormous data volumes. Aiming at addressing this problem, we propose a novel architecture dubbed Condense, which integrates the IoT-communication infrastructure into data analysis. This is achieved via the generic concept of network function computation: Instead of merely transferring data from the IoT sources to the cloud, the communication infrastructure should actively participate in the data analysis by carefully designed en-route processing. We define the Condense architecture, its basic layers, and the interactions among its constituent modules. Further, from the implementation side, we describe how Condense can be integrated into the 3rd Generation Partnership Project (3GPP) Machine Type Communications (MTC) architecture, as well as the prospects of making it a practically viable technology in a short time frame, relying on Network Function Virtualization (NFV) and Software Defined Networking (SDN). Finally, from the theoretical side, we survey the relevant literature on computing ""atomic"" functions in both analog and digital domains, as well as on function decomposition over networks, highlighting challenges, insights, and future directions for exploiting these techniques within practical 3GPP MTC architecture.",2016-09-12T12:23:03Z,2016-09-12T12:23:03Z,http://arxiv.org/abs/1609.03363v1,http://arxiv.org/pdf/1609.03363v1,"cs.IT, cs.NI, math.IT"
Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device   Text-Independent Speaker Verification,"Sobhan Soleymani, Ali Dabouei, Seyed Mehdi Iranmanesh, Hadi Kazemi, Jeremy Dawson, Nasser M. Nasrabadi","In this paper a novel cross-device text-independent speaker verification architecture is proposed. Majority of the state-of-the-art deep architectures that are used for speaker verification tasks consider Mel-frequency cepstral coefficients. In contrast, our proposed Siamese convolutional neural network architecture uses Mel-frequency spectrogram coefficients to benefit from the dependency of the adjacent spectro-temporal features. Moreover, although spectro-temporal features have proved to be highly reliable in speaker verification models, they only represent some aspects of short-term acoustic level traits of the speaker's voice. However, the human voice consists of several linguistic levels such as acoustic, lexicon, prosody, and phonetics, that can be utilized in speaker verification models. To compensate for these inherited shortcomings in spectro-temporal features, we propose to enhance the proposed Siamese convolutional neural network architecture by deploying a multilayer perceptron network to incorporate the prosodic, jitter, and shimmer features. The proposed end-to-end verification architecture performs feature extraction and verification simultaneously. This proposed architecture displays significant improvement over classical signal processing approaches and deep algorithms for forensic cross-device speaker verification.",2018-07-31T19:21:59Z,2018-07-31T19:21:59Z,http://arxiv.org/abs/1808.01026v1,http://arxiv.org/pdf/1808.01026v1,"eess.AS, cs.CV, cs.LG, cs.SD"
Integrated Offline and Online Optimization-Based Control in a   Base-Parallel Architecture,"Anahita Jamshidnejad, Gabriel Gomes, Alexandre M. Bayen, Bart De Schutter","We propose an integrated control architecture to address the gap that currently exists for efficient real-time implementation of MPC-based control approaches for highly nonlinear systems with fast dynamics and a large number of control constraints. The proposed architecture contains two types of controllers: base controllers that are tuned or optimized offline, and parallel controllers that solve an optimization-based control problem online. The control inputs computed by the base controllers provide starting points for the optimization problem of the parallel controllers, which operate in parallel within a limited time budget that does not exceed the control sampling time. The resulting control system is very flexible and its architecture can easily be modified or changed online, e.g., by adding or eliminating controllers, for online improvement of the performance of the controlled system. In a case study, the proposed control architecture is implemented for highway traffic, which is characterized by nonlinear, fast dynamics with multiple control constraints, to minimize the overall travel time of the vehicles, while increasing their total traveled distance within the fixed simulation time window. The results of the simulation show the excellent real-time (i.e., within the given time budget) performance of the proposed control architecture, with the least realized value of the overall cost function. Moreover, among the online control approaches considered for the case study, the average cost per vehicle for the base-parallel control approach is the closest to the online MPC-based controllers, which have excellent performance but may involve computation times that exceed the given time budget.",2019-07-11T19:48:42Z,2019-07-11T19:48:42Z,http://arxiv.org/abs/1907.05464v1,http://arxiv.org/pdf/1907.05464v1,"eess.SY, cs.SY, 49-XX"
Efficient Uncertainty Modeling for System Design via Mixed Integer   Programming,"Zichang He, Weilong Cui, Chunfeng Cui, Timothy Sherwood, Zheng Zhang","The post-Moore era casts a shadow of uncertainty on many aspects of computer system design. Managing that uncertainty requires new algorithmic tools to make quantitative assessments. While prior uncertainty quantification methods, such as generalized polynomial chaos (gPC), show how to work precisely under the uncertainty inherent to physical devices, these approaches focus solely on variables from a continuous domain. However, as one moves up the system stack to the architecture level many parameters are constrained to a discrete (integer) domain. This paper proposes an efficient and accurate uncertainty modeling technique, named mixed generalized polynomial chaos (M-gPC), for architectural uncertainty analysis. The M-gPC technique extends the generalized polynomial chaos (gPC) theory originally developed in the uncertainty quantification community, such that it can efficiently handle the mixed-type (i.e., both continuous and discrete) uncertainties in computer architecture design. Specifically, we employ some stochastic basis functions to capture the architecture-level impact caused by uncertain parameters in a simulator. We also develop a novel mixed-integer programming method to select a small number of uncertain parameter samples for detailed simulations. With a few highly informative simulation samples, an accurate surrogate model is constructed in place of cycle-level simulators for various architectural uncertainty analysis. In the chip-multiprocessor (CMP) model, we are able to estimate the propagated uncertainties with only 95 samples whereas Monte Carlo requires 5*10^4 samples to achieve the similar accuracy. We also demonstrate the efficiency and effectiveness of our method on a detailed DRAM subsystem.",2019-07-10T23:50:01Z,2019-10-21T02:56:35Z,http://arxiv.org/abs/1907.05700v2,http://arxiv.org/pdf/1907.05700v2,"eess.SP, cs.AR, cs.NA, math.NA"
On Neural Architecture Search for Resource-Constrained Hardware   Platforms,"Qing Lu, Weiwen Jiang, Xiaowei Xu, Yiyu Shi, Jingtong Hu","In the recent past, the success of Neural Architecture Search (NAS) has enabled researchers to broadly explore the design space using learning-based methods. Apart from finding better neural network architectures, the idea of automation has also inspired to improve their implementations on hardware. While some practices of hardware machine-learning automation have achieved remarkable performance, the traditional design concept is still followed: a network architecture is first structured with excellent test accuracy, and then compressed and optimized to fit into a target platform. Such a design flow will easily lead to inferior local-optimal solutions. To address this problem, we propose a new framework to jointly explore the space of neural architecture, hardware implementation, and quantization. Our objective is to find a quantized architecture with the highest accuracy that is implementable on given hardware specifications. We employ FPGAs to implement and test our designs with limited loop-up tables (LUTs) and required throughput. Compared to the separate design/searching methods, our framework has demonstrated much better performance under strict specifications and generated designs of higher accuracy by 18\% to 68\% in the task of classifying CIFAR10 images. With 30,000 LUTs, a light-weight design is found to achieve 82.98\% accuracy and 1293 images/second throughput, compared to which, under the same constraints, the traditional method even fails to find a valid solution.",2019-10-31T21:02:23Z,2019-10-31T21:02:23Z,http://arxiv.org/abs/1911.00105v1,http://arxiv.org/pdf/1911.00105v1,"cs.LG, cs.NE, eess.SP"
Bridging the Gap between Sample-based and One-shot Neural Architecture   Search with BONAS,"Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T. Kwok, Tong Zhang","Neural Architecture Search (NAS) has shown great potentials in finding better neural network designs. Sample-based NAS is the most reliable approach which aims at exploring the search space and evaluating the most promising architectures. However, it is computationally very costly. As a remedy, the one-shot approach has emerged as a popular technique for accelerating NAS using weight-sharing. However, due to the weight-sharing of vastly different networks, the one-shot approach is less reliable than the sample-based approach. In this work, we propose BONAS (Bayesian Optimized Neural Architecture Search), a sample-based NAS framework which is accelerated using weight-sharing to evaluate multiple related architectures simultaneously. Specifically, we apply Graph Convolutional Network predictor as a surrogate model for Bayesian Optimization to select multiple related candidate models in each iteration. We then apply weight-sharing to train multiple candidate models simultaneously. This approach not only accelerates the traditional sample-based approach significantly, but also keeps its reliability. This is because weight-sharing among related architectures are more reliable than those in the one-shot approach. Extensive experiments are conducted to verify the effectiveness of our method over many competing algorithms.",2019-11-21T08:29:00Z,2020-11-25T02:13:43Z,http://arxiv.org/abs/1911.09336v4,http://arxiv.org/pdf/1911.09336v4,"cs.LG, stat.ML"
Meta-Learning of Neural Architectures for Few-Shot Learning,"Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, Frank Hutter","The recent progress in neural architecture search (NAS) has allowed scaling the automated design of neural architectures to real-world domains, such as object detection and semantic segmentation. However, one prerequisite for the application of NAS are large amounts of labeled data and compute resources. This renders its application challenging in few-shot learning scenarios, where many related tasks need to be learned, each with limited amounts of data and compute time. Thus, few-shot learning is typically done with a fixed neural architecture. To improve upon this, we propose MetaNAS, the first method which fully integrates NAS with gradient-based meta-learning. MetaNAS optimizes a meta-architecture along with the meta-weights during meta-training. During meta-testing, architectures can be adapted to a novel task with a few steps of the task optimizer, that is: task adaptation becomes computationally cheap and requires only little data per task. Moreover, MetaNAS is agnostic in that it can be used with arbitrary model-agnostic meta-learning algorithms and arbitrary gradient-based NAS methods. %We present encouraging results for MetaNAS with a combination of DARTS and REPTILE on few-shot classification benchmarks. Empirical results on standard few-shot classification benchmarks show that MetaNAS with a combination of DARTS and REPTILE yields state-of-the-art results.",2019-11-25T17:45:39Z,2021-06-14T09:33:52Z,http://arxiv.org/abs/1911.11090v3,http://arxiv.org/pdf/1911.11090v3,"cs.LG, stat.ML"
Isochronous Architecture-Based Voltage-Active Power Droop for   Multi-Inverter Systems,"Sourav Patel, Soham Chakraborty, Blake Lundstrom, Srinivasa Salapaka, Murti V. Salapaka","Advanced microgrids consisting of distributed energy resources interfaced with multi-inverter systems are becoming more common. Consequently, the effectiveness of voltage and frequency regulation in microgrids using conventional droop-based methodologies is challenged by uncertainty in the sizeand schedule of loads. This article proposes an isochronous architecture of parallel inverters with only voltage-active power droop (VP-D) control for improving active power sharing as well as plug-and-play of multi-inverter based distributed energyresources (DERs). In spite of not employing explicit control for frequency regulation, this architecture allows even sharing of reactive power while maintaining reduced circulating currents between inverters. The performance is achieved even when there are mismatches between commanded reference and power demanded from the actual load in the network. The isochronous architecture is implemented by employing a global positioning system (GPS) to disseminate timing signals that enable the microgrid to maintain nominal system frequency in the entire network. This enables direct control of active power through voltage source inverter (VSI) output voltage regulation, even in the presence of system disturbances. A small signal eigenvalue analysis of a multi-inverter system near the steady-state operating point is presented to evaluate the stability of the multi-inverter system with the proposed VP-D control. Simulation studies and hardware experiments on an 1.2 kVA prototype are conducted. The effectiveness of the proposed architecture towards active and reactive power sharing between inverters with load scenarios are demonstrated. Results of the hardware experiments corroborate the viability of the proposed VP-D control architecture.",2020-03-12T20:33:36Z,2020-03-12T20:33:36Z,http://arxiv.org/abs/2003.06009v1,http://arxiv.org/pdf/2003.06009v1,"eess.SY, cs.SY"
DCNAS: Densely Connected Neural Architecture Search for Semantic Image   Segmentation,"Xiong Zhang, Hongmin Xu, Hong Mo, Jianchao Tan, Cheng Yang, Lei Wang, Wenqi Ren","Neural Architecture Search (NAS) has shown great potentials in automatically designing scalable network architectures for dense image predictions. However, existing NAS algorithms usually compromise on restricted search space and search on proxy task to meet the achievable computational demands. To allow as wide as possible network architectures and avoid the gap between target and proxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which directly searches the optimal network structures for the multi-scale representations of visual information, over a large-scale target dataset. Specifically, by connecting cells with each other using learnable weights, we introduce a densely connected search space to cover an abundance of mainstream network designs. Moreover, by combining both path-level and channel-level sampling strategies, we design a fusion module to reduce the memory consumption of ample search space. We demonstrate that the architecture obtained from our DCNAS algorithm achieves state-of-the-art performances on public semantic image segmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC 2012. We also retain leading performances when evaluating the architecture on the more challenging ADE20K and Pascal Context dataset.",2020-03-26T13:21:33Z,2021-03-27T15:03:47Z,http://arxiv.org/abs/2003.11883v2,http://arxiv.org/pdf/2003.11883v2,"cs.CV, cs.LG, eess.IV"
NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search,"Chen Wei, Chuang Niu, Yiping Tang, Yue Wang, Haihong Hu, Jimin Liang","Neural architecture search (NAS) is a promising method for automatically design neural architectures. NAS adopts a search strategy to explore the predefined search space to find outstanding performance architecture with the minimum searching costs. Bayesian optimization and evolutionary algorithms are two commonly used search strategies, but they suffer from computationally expensive, challenge to implement or inefficient exploration ability. In this paper, we propose a neural predictor guided evolutionary algorithm to enhance the exploration ability of EA for NAS (NPENAS) and design two kinds of neural predictors. The first predictor is defined from Bayesian optimization and we propose a graph-based uncertainty estimation network as a surrogate model that is easy to implement and computationally efficient. The second predictor is a graph-based neural network that directly outputs the performance prediction of the input neural architecture. The NPENAS using the two neural predictors are denoted as NPENAS-BO and NPENAS-NP respectively. In addition, we introduce a new random architecture sampling method to overcome the drawbacks of the existing sampling method. Extensive experiments demonstrate the superiority of NPENAS. Quantitative results on three NAS search spaces indicate that both NPENAS-BO and NPENAS-NP outperform most existing NAS algorithms, with NPENAS-BO achieving state-of-the-art performance on NASBench-201 and NPENAS-NP on NASBench-101 and DARTS, respectively.",2020-03-28T17:56:31Z,2020-09-10T06:22:06Z,http://arxiv.org/abs/2003.12857v3,http://arxiv.org/pdf/2003.12857v3,"cs.LG, cs.AI, cs.CV, cs.NE, stat.ML"
MTL-NAS: Task-Agnostic Neural Architecture Search towards   General-Purpose Multi-Task Learning,"Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, Wei Liu","We propose to incorporate neural architecture search (NAS) into general-purpose multi-task learning (GP-MTL). Existing NAS methods typically define different search spaces according to different tasks. In order to adapt to different task combinations (i.e., task sets), we disentangle the GP-MTL networks into single-task backbones (optionally encode the task priors), and a hierarchical and layerwise features sharing/fusing scheme across them. This enables us to design a novel and general task-agnostic search space, which inserts cross-task edges (i.e., feature fusion connections) into fixed single-task network backbones. Moreover, we also propose a novel single-shot gradient-based search algorithm that closes the performance gap between the searched architectures and the final evaluation architecture. This is realized with a minimum entropy regularization on the architecture weights during the search phase, which makes the architecture weights converge to near-discrete values and therefore achieves a single model. As a result, our searched model can be directly used for evaluation without (re-)training from scratch. We perform extensive experiments using different single-task backbones on various task sets, demonstrating the promising performance obtained by exploiting the hierarchical and layerwise features, as well as the desirable generalizability to different i) task sets and ii) single-task backbones. The code of our paper is available at https://github.com/bhpfelix/MTLNAS.",2020-03-31T09:49:14Z,2020-03-31T09:49:14Z,http://arxiv.org/abs/2003.14058v1,http://arxiv.org/pdf/2003.14058v1,"cs.LG, cs.CV, stat.ML"
MultiCheXNet: A Multi-Task Learning Deep Network For Pneumonia-like   Diseases Diagnosis From X-ray Scans,"Abdullah Tarek Farag, Ahmed Raafat Abd El-Wahab, Mahmoud Nada, Mohamed Yasser Abd El-Hakeem, Omar Sayed Mahmoud, Reem Khaled Rashwan, Ahmad El Sallab","We present MultiCheXNet, an end-to-end Multi-task learning model, that is able to take advantage of different X-rays data sets of Pneumonia-like diseases in one neural architecture, performing three tasks at the same time; diagnosis, segmentation and localization. The common encoder in our architecture can capture useful common features present in the different tasks. The common encoder has another advantage of efficient computations, which speeds up the inference time compared to separate models. The specialized decoders heads can then capture the task-specific features. We employ teacher forcing to address the issue of negative samples that hurt the segmentation and localization performance. Finally,we employ transfer learning to fine tune the classifier on unseen pneumonia-like diseases. The MTL architecture can be trained on joint or dis-joint labeled data sets. The training of the architecture follows a carefully designed protocol, that pre trains different sub-models on specialized datasets, before being integrated in the joint MTL model. Our experimental setup involves variety of data sets, where the baseline performance of the 3 tasks is compared to the MTL architecture performance. Moreover, we evaluate the transfer learning mode to COVID-19 data set,both from individual classifier model, and from MTL architecture classification head.",2020-08-05T07:45:24Z,2020-08-05T07:45:24Z,http://arxiv.org/abs/2008.01973v1,http://arxiv.org/pdf/2008.01973v1,"cs.CV, cs.LG, eess.IV"
Auto-Panoptic: Cooperative Multi-Component Architecture Search for   Panoptic Segmentation,"Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang, Liang Lin","Panoptic segmentation is posed as a new popular test-bed for the state-of-the-art holistic scene understanding methods with the requirement of simultaneously segmenting both foreground things and background stuff. The state-of-the-art panoptic segmentation network exhibits high structural complexity in different network components, i.e. backbone, proposal-based foreground branch, segmentation-based background branch, and feature fusion module across branches, which heavily relies on expert knowledge and tedious trials. In this work, we propose an efficient, cooperative and highly automated framework to simultaneously search for all main components including backbone, segmentation branches, and feature fusion module in a unified panoptic segmentation pipeline based on the prevailing one-shot Network Architecture Search (NAS) paradigm. Notably, we extend the common single-task NAS into the multi-component scenario by taking the advantage of the newly proposed intra-modular search space and problem-oriented inter-modular search space, which helps us to obtain an optimal network architecture that not only performs well in both instance segmentation and semantic segmentation tasks but also be aware of the reciprocal relations between foreground things and background stuff classes. To relieve the vast computation burden incurred by applying NAS to complicated network architectures, we present a novel path-priority greedy search policy to find a robust, transferrable architecture with significantly reduced searching overhead. Our searched architecture, namely Auto-Panoptic, achieves the new state-of-the-art on the challenging COCO and ADE20K benchmarks. Moreover, extensive experiments are conducted to demonstrate the effectiveness of path-priority policy and transferability of Auto-Panoptic across different datasets. Codes and models are available at: https://github.com/Jacobew/AutoPanoptic.",2020-10-30T08:34:35Z,2020-10-30T08:34:35Z,http://arxiv.org/abs/2010.16119v1,http://arxiv.org/pdf/2010.16119v1,"cs.CV, cs.LG, eess.IV"
An Evolution of CNN Object Classifiers on Low-Resolution Images,"Md. Mohsin Kabir, Abu Quwsar Ohi, Md. Saifur Rahman, M. F. Mridha","Object classification is a significant task in computer vision. It has become an effective research area as an important aspect of image processing and the building block of image localization, detection, and scene parsing. Object classification from low-quality images is difficult for the variance of object colors, aspect ratios, and cluttered backgrounds. The field of object classification has seen remarkable advancements, with the development of deep convolutional neural networks (DCNNs). Deep neural networks have been demonstrated as very powerful systems for facing the challenge of object classification from high-resolution images, but deploying such object classification networks on the embedded device remains challenging due to the high computational and memory requirements. Using high-quality images often causes high computational and memory complexity, whereas low-quality images can solve this issue. Hence, in this paper, we investigate an optimal architecture that accurately classifies low-quality images using DCNNs architectures. To validate different baselines on lowquality images, we perform experiments using webcam captured image datasets of 10 different objects. In this research work, we evaluate the proposed architecture by implementing popular CNN architectures. The experimental results validate that the MobileNet architecture delivers better than most of the available CNN architectures for low-resolution webcam image datasets.",2021-01-03T18:44:23Z,2021-01-03T18:44:23Z,http://arxiv.org/abs/2101.00686v1,http://arxiv.org/pdf/2101.00686v1,"cs.CV, cs.LG, eess.IV"
Joint Inter-path and Intra-path Multiplexing for Terahertz Widely-spaced   Multi-subarray Hybrid Beamforming Systems,"Longfei Yan, Yuhang Chen, Chong Han, Jinhong Yuan","Terahertz (THz) communications with multi-GHz bandwidth are envisioned as a key technology for 6G systems. Ultra-massive (UM) MIMO with hybrid beamforming architectures are widely investigated to provide a high array gain to overcome the huge propagation loss. However, most of the existing hybrid beamforming architectures can only utilize the multiplexing offered by the multipath components, i.e., inter-path multiplexing, which is very limited due to the spatially sparse THz channel. In this paper, a widely-spaced multi-subarray (WSMS) hybrid beamforming architecture is proposed, which improves the multiplexing gain by exploiting a new type of intra-path multiplexing provided by the spherical-wave propagation among k widely-spaced subarrays, in addition to the inter-path multiplexing. The resulting multiplexing gain of WSMS architecture is k times of the existing architectures. To harness WSMS hybrid beamforming, a novel design problem is formulated by optimizing the number of subarrays, subarray spacing, and hybrid beamforming matrices to maximize the spectral efficiency, which is decomposed into two subproblems. An optimal closed-form solution is derived for the first hybrid beamforming subproblem, while a dominant-line-of-sight-relaxation algorithm is proposed for the second array configuration subproblem. Extensive simulation results demonstrate that the WSMS architecture and proposed algorithms substantially enhance the spectral efficiency and energy efficiency.",2021-01-20T02:42:57Z,2021-06-08T11:45:40Z,http://arxiv.org/abs/2101.07936v2,http://arxiv.org/pdf/2101.07936v2,"cs.IT, math.IT"
Hardware Architecture of Embedded Inference Accelerator and Analysis of   Algorithms for Depthwise and Large-Kernel Convolutions,"Tse-Wei Chen, Wei Tao, Deyu Wang, Dongchao Wen, Kinya Osa, Masami Kato","In order to handle modern convolutional neural networks (CNNs) efficiently, a hardware architecture of CNN inference accelerator is proposed to handle depthwise convolutions and regular convolutions, which are both essential building blocks for embedded-computer-vision algorithms. Different from related works, the proposed architecture can support filter kernels with different sizes with high flexibility since it does not require extra costs for intra-kernel parallelism, and it can generate convolution results faster than the architecture of the related works. The experimental results show the importance of supporting depthwise convolutions and dilated convolutions with the proposed hardware architecture. In addition to depthwise convolutions with large-kernels, a new structure called DDC layer, which includes the combination of depthwise convolutions and dilated convolutions, is also analyzed in this paper. For face detection, the computational costs decrease by 30%, and the model size decreases by 20% when the DDC layers are applied to the network. For image classification, the accuracy is increased by 1% by simply replacing $3 \times 3$ filters with $5 \times 5$ filters in depthwise convolutions.",2021-04-29T05:45:16Z,2021-04-29T05:45:16Z,http://arxiv.org/abs/2104.14125v1,http://arxiv.org/pdf/2104.14125v1,"cs.CV, cs.AR, eess.IV"
Audio Transformers:Transformer Architectures For Large Scale Audio   Understanding. Adieu Convolutions,"Prateek Verma, Jonathan Berger","Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean aver-age precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional net-work designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant band-width filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation.",2021-05-01T19:38:30Z,2021-05-01T19:38:30Z,http://arxiv.org/abs/2105.00335v1,http://arxiv.org/pdf/2105.00335v1,"cs.SD, cs.AI, cs.LG, cs.MM, eess.AS"
Efficient Deep Learning Architectures for Fast Identification of   Bacterial Strains in Resource-Constrained Devices,"R. Gallardo García, S. Jarquín Rodríguez, B. Beltrán Martínez, C. Hernández Gracidas, R. Martínez Torres","This work presents twelve fine-tuned deep learning architectures to solve the bacterial classification problem over the Digital Image of Bacterial Species Dataset. The base architectures were mainly published as mobile or efficient solutions to the ImageNet challenge, and all experiments presented in this work consisted of making several modifications to the original designs, in order to make them able to solve the bacterial classification problem by using fine-tuning and transfer learning techniques. This work also proposes a novel data augmentation technique for this dataset, which is based on the idea of artificial zooming, strongly increasing the performance of every tested architecture, even doubling it in some cases. In order to get robust and complete evaluations, all experiments were performed with 10-fold cross-validation and evaluated with five different metrics: top-1 and top-5 accuracy, precision, recall, and F1 score. This paper presents a complete comparison of the twelve different architectures, cross-validated with the original and the augmented version of the dataset, the results are also compared with several literature methods. Overall, eight of the eleven architectures surpassed the 0.95 scores in top-1 accuracy with our data augmentation method, being 0.9738 the highest top-1 accuracy. The impact of the data augmentation technique is reported with relative improvement scores.",2021-06-11T16:59:22Z,2021-06-11T16:59:22Z,http://arxiv.org/abs/2106.06505v1,http://arxiv.org/pdf/2106.06505v1,"cs.CV, 68T07 (Primary), 68U10 (Secondary), I.4; J.3"
Accelerating Recurrent Neural Networks for Gravitational Wave   Experiments,"Zhiqiang Que, Erwei Wang, Umar Marikar, Eric Moreno, Jennifer Ngadiuba, Hamza Javed, Bartłomiej Borzyszkowski, Thea Aarrestad, Vladimir Loncar, Sioni Summers, Maurizio Pierini, Peter Y Cheung, Wayne Luk","This paper presents novel reconfigurable architectures for reducing the latency of recurrent neural networks (RNNs) that are used for detecting gravitational waves. Gravitational interferometers such as the LIGO detectors capture cosmic events such as black hole mergers which happen at unknown times and of varying durations, producing time-series data. We have developed a new architecture capable of accelerating RNN inference for analyzing time-series data from LIGO detectors. This architecture is based on optimizing the initiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory) network, by identifying appropriate reuse factors for each layer. A customizable template for this architecture has been designed, which enables the generation of low-latency FPGA designs with efficient resource utilization using high-level synthesis tools. The proposed approach has been evaluated based on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA. Experimental results show that with balanced II, the number of DSPs can be reduced up to 42% while achieving the same IIs. When compared to other FPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower latency.",2021-06-26T20:44:02Z,2021-06-26T20:44:02Z,http://arxiv.org/abs/2106.14089v1,http://arxiv.org/pdf/2106.14089v1,"cs.LG, cs.AR, physics.ins-det"
An optimised deep spiking neural network architecture without gradients,"Yeshwanth Bethi, Ying Xu, Gregory Cohen, Andre van Schaik, Saeed Afshar","We present an end-to-end trainable modular event-driven neural architecture that uses local synaptic and threshold adaptation rules to perform transformations between arbitrary spatio-temporal spike patterns. The architecture represents a highly abstracted model of existing Spiking Neural Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking neural network Architecture (ODESA) can simultaneously learn hierarchical spatio-temporal features at multiple arbitrary time scales. ODESA performs online learning without the use of error back-propagation or the calculation of gradients. Through the use of simple local adaptive selection thresholds at each node, the network rapidly learns to appropriately allocate its neuronal resources at each layer for any given problem without using a real-valued error measure. These adaptive selection thresholds are the central feature of ODESA, ensuring network stability and remarkable robustness to noise as well as to the selection of initial system parameters. Network activations are inherently sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We evaluate the architecture on existing spatio-temporal datasets, including the spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based on International Morse Code that we created. These tests demonstrate the hierarchical spatio-temporal learning capabilities of ODESA. Through these tests, we demonstrate ODESA can optimally solve practical and highly challenging hierarchical spatio-temporal learning tasks with the minimum possible number of computing nodes.",2021-09-27T05:59:12Z,2022-05-02T20:45:29Z,http://arxiv.org/abs/2109.12813v3,http://arxiv.org/pdf/2109.12813v3,"cs.NE, cs.CV, I.2.6; I.5.1"
Attention is All You Need? Good Embeddings with Statistics are   enough:Large Scale Audio Understanding without Transformers/ Convolutions/   BERTs/ Mixers/ Attention/ RNNs or ....,Prateek Verma,"This paper presents a way of doing large scale audio understanding without traditional state of the art neural architectures. Ever since the introduction of deep learning for understanding audio signals in the past decade, convolutional architectures have been able to achieve state of the art results surpassing traditional hand-crafted features. In the recent past, there has been a similar shift away from traditional convolutional and recurrent neural networks towards purely end-to-end Transformer architectures. We, in this work, explore an approach, based on Bag-of-Words model. Our approach does not have any convolutions, recurrence, attention, transformers or other approaches such as BERT. We utilize micro and macro level clustered vanilla embeddings, and use a MLP head for classification. We only use feed-forward encoder-decoder models to get the bottlenecks of spectral envelops, spectral patches and slices as well as multi-resolution spectra. A classification head (a feed-forward layer), similar to the approach in SimCLR is trained on a learned representation. Using simple codes learned on latent representations, we show how we surpass traditional convolutional neural network architectures, and come strikingly close to outperforming powerful Transformer architectures. This work hopefully would pave way for exciting advancements in the field of representation learning without massive, end-to-end neural architectures.",2021-10-07T05:00:26Z,2022-01-30T02:25:10Z,http://arxiv.org/abs/2110.03183v5,http://arxiv.org/pdf/2110.03183v5,"cs.SD, cs.AI, cs.IR, cs.LG, cs.MM, eess.AS"
Natural Computational Architectures for Cognitive Info-Communication,Gordana Dodig-Crnkovic,"Recent comprehensive overview of 40 years of research in cognitive architectures, (Kotseruba and Tsotsos 2020), evaluates modelling of the core cognitive abilities in humans, but only marginally addresses biologically plausible approaches based on natural computation. This mini review presents a set of perspectives and approaches which have shaped the development of biologically inspired computational models in the recent past that can lead to the development of biologically more realistic cognitive architectures. For describing continuum of natural cognitive architectures, from basal cellular to human-level cognition, we use evolutionary info-computational framework, where natural/ physical/ morphological computation leads to evolution of increasingly complex cognitive systems. Forty years ago, when the first cognitive architectures have been proposed, understanding of cognition, embodiment and evolution was different. So was the state of the art of information physics, bioinformatics, information chemistry, computational neuroscience, complexity theory, self-organization, theory of evolution, information and computation. Novel developments support a constructive interdisciplinary framework for cognitive architectures in the context of computing nature, where interactions between constituents at different levels of organization lead to complexification of agency and increased cognitive capacities. We identify several important research questions for further investigation that can increase understanding of cognition in nature and inspire new developments of cognitive technologies. Recently, basal cell cognition attracted a lot of interest for its possible applications in medicine, new computing technologies, as well as micro- and nanorobotics.",2021-10-01T18:01:16Z,2021-10-01T18:01:16Z,http://arxiv.org/abs/2110.06339v1,http://arxiv.org/pdf/2110.06339v1,"q-bio.NC, cs.AI"
Assessing the Impact of Attention and Self-Attention Mechanisms on the   Classification of Skin Lesions,"Rafael Pedro, Arlindo L. Oliveira","Attention mechanisms have raised significant interest in the research community, since they promise significant improvements in the performance of neural network architectures. However, in any specific problem, we still lack a principled way to choose specific mechanisms and hyper-parameters that lead to guaranteed improvements. More recently, self-attention has been proposed and widely used in transformer-like architectures, leading to significant breakthroughs in some applications. In this work we focus on two forms of attention mechanisms: attention modules and self-attention. Attention modules are used to reweight the features of each layer input tensor. Different modules have different ways to perform this reweighting in fully connected or convolutional layers. The attention models studied are completely modular and in this work they will be used with the popular ResNet architecture. Self-Attention, originally proposed in the area of Natural Language Processing makes it possible to relate all the items in an input sequence. Self-Attention is becoming increasingly popular in Computer Vision, where it is sometimes combined with convolutional layers, although some recent architectures do away entirely with convolutions. In this work, we study and perform an objective comparison of a number of different attention mechanisms in a specific computer vision task, the classification of samples in the widely used Skin Cancer MNIST dataset. The results show that attention modules do sometimes improve the performance of convolutional neural network architectures, but also that this improvement, although noticeable and statistically significant, is not consistent in different settings. The results obtained with self-attention mechanisms, on the other hand, show consistent and significant improvements, leading to the best results even in architectures with a reduced number of parameters.",2021-12-23T18:02:48Z,2021-12-23T18:02:48Z,http://arxiv.org/abs/2112.12748v1,http://arxiv.org/pdf/2112.12748v1,"cs.CV, cs.AI, cs.LG, I.5.4"
Towards Tailored Models on Private AIoT Devices: Federated Direct Neural   Architecture Search,"Chunhui Zhang, Xiaoming Yuan, Qianyun Zhang, Guangxu Zhu, Lei Cheng, Ning Zhang","Neural networks often encounter various stringent resource constraints while deploying on edge devices. To tackle these problems with less human efforts, automated machine learning becomes popular in finding various neural architectures that fit diverse Artificial Intelligence of Things (AIoT) scenarios. Recently, to prevent the leakage of private information while enable automated machine intelligence, there is an emerging trend to integrate federated learning and neural architecture search (NAS). Although promising as it may seem, the coupling of difficulties from both tenets makes the algorithm development quite challenging. In particular, how to efficiently search the optimal neural architecture directly from massive non-independent and identically distributed (non-IID) data among AIoT devices in a federated manner is a hard nut to crack. In this paper, to tackle this challenge, by leveraging the advances in ProxylessNAS, we propose a Federated Direct Neural Architecture Search (FDNAS) framework that allows for hardware-friendly NAS from non- IID data across devices. To further adapt to both various data distributions and different types of devices with heterogeneous embedded hardware platforms, inspired by meta-learning, a Cluster Federated Direct Neural Architecture Search (CFDNAS) framework is proposed to achieve device-aware NAS, in the sense that each device can learn a tailored deep learning model for its particular data distribution and hardware constraint. Extensive experiments on non-IID datasets have shown the state-of-the-art accuracy-efficiency trade-offs achieved by the proposed solution in the presence of both data and device heterogeneity.",2022-02-23T13:10:01Z,2022-02-23T13:10:01Z,http://arxiv.org/abs/2202.11490v1,http://arxiv.org/pdf/2202.11490v1,"cs.LG, cs.DC, eess.SP"
UNet Architectures in Multiplanar Volumetric Segmentation -- Validated   on Three Knee MRI Cohorts,"Sandeep Singh Sengara, Christopher Meulengrachtb, Mikael Ploug Boesenb, Anders Føhrby Overgaardb, Henrik Gudbergsenb, Janus Damm Nybingb, Erik Bjørnager Dam","UNet has become the gold standard method for segmenting 2D medical images that any new method must be validated against. However, in recent years, several variations of the seminal UNet have been proposed with promising results. However, there is no clear consensus on the generalisability of these architectures, and UNet currently remains the methodological gold standard. The purpose of this study was to evaluate some of the most promising UNet-inspired architectures for 3D segmentation. For the segmentation of 3D scans, UNet-inspired methods are also dominant, but there is a larger variety across applications. By evaluating the architectures in a different dimensionality, embedded in a different method, and for a different task, we aimed to evaluate if any of these UNet-alternatives are promising as a new gold standard that generalizes even better than UNet. Specifically, we investigated the architectures as the central 2D segmentation core in the Multi-Planar Unet 3D segmentation method that previously demonstrated excellent generalization in the MICCAI Segmentation Decathlon. Generalisability can be demonstrated if a promising UNet-variant consistently outperforms UNet in this setting. For this purpose, we evaluated four architectures for cartilage segmentation from three different cohorts with knee MRIs.",2022-03-15T18:45:58Z,2022-03-15T18:45:58Z,http://arxiv.org/abs/2203.08194v1,http://arxiv.org/pdf/2203.08194v1,"eess.IV, cs.CV, cs.LG"
"Beyond Diagonal Reconfigurable Intelligent Surfaces: From Transmitting   and Reflecting Modes to Single-, Group-, and Fully-Connected Architectures","Hongyu Li, Shanpu Shen, Bruno Clerckx","Reconfigurable intelligent surfaces (RISs) are envisioned as a promising technology for future wireless communications. With various hardware realizations, RISs can work under different modes (reflective/transmissive/hybrid) or have different architectures (single/group/fully-connected). However, most existing research focused on single-connected reflective RISs, mathematically characterized by diagonal phase shift matrices, while there is a lack of a comprehensive study for RISs unifying different modes/architectures. In this paper, we solve this issue by analyzing and proposing a general RIS-aided communication model. Specifically, we establish an RIS model not limited to diagonal phase shift matrices, a novel branch referred to as beyond diagonal RIS (BD-RIS), unifying modes and architectures. With the proposed model, we develop efficient algorithms to jointly design transmit precoder and BDRIS matrix to maximize the sum-rate for RIS-aided systems. We also provide simulation results to compare the performance of BD-RISs with different modes/architectures. Simulation results show that under the same mode, fully- and group-connected RIS can effectively increase the sum-rate performance compared with single-connected RIS, and that hybrid RIS outperforms reflective/transmissive RIS with the same architecture.",2022-05-05T18:03:47Z,2022-08-05T10:31:26Z,http://arxiv.org/abs/2205.02866v2,http://arxiv.org/pdf/2205.02866v2,"cs.IT, eess.SP, math.IT"
"Full Duplex Massive MIMO Architectures: Recent Advances, Applications,   and Future Directions","George C. Alexandropoulos, Md Atiqul Islam, Besma Smida","The increasingly demanding objectives for next generation wireless communications have spurred recent research activities on multi-antenna transceiver hardware architectures and relevant intelligent communication schemes. Among them belong the Full Duplex (FD) Multiple-Input Multiple-Output (MIMO) architectures, which offer the potential for simultaneous uplink and downlink operations in the entire frequency band. However, as the number of antenna elements increases, the interference signal leaking from the transmitter of the FD radio to its receiver becomes more severe. In this article, we present a unified FD massive MIMO architecture comprising analog and digital transmit and receive BeamForming (BF), as well as analog and digital SI cancellation, which can be jointly optimized for various performance objectives and complexity requirements. Performance evaluation results for applications of the proposed architecture to fully digital and hybrid analog and digital BF operations using recent algorithmic designs, as well as simultaneous communication of data and control signals are presented. It is shown that the proposed architecture, for both small and large numbers of antennas, enables improved spectral efficiency FD communications with fewer analog cancellation elements compared to various benchmark schemes. The article is concluded with a list of open challenges and research directions for future FD massive MIMO communication systems and their promising applications.",2022-05-17T14:17:37Z,2022-05-17T14:17:37Z,http://arxiv.org/abs/2205.08393v1,http://arxiv.org/pdf/2205.08393v1,"cs.IT, eess.SP, math.IT"
Simple and Efficient Architectures for Semantic Segmentation,"Dushyant Mehta, Andrii Skliar, Haitam Ben Yahia, Shubhankar Borse, Fatih Porikli, Amirhossein Habibian, Tijmen Blankevoort","Though the state-of-the architectures for semantic segmentation, such as HRNet, demonstrate impressive accuracy, the complexity arising from their salient design choices hinders a range of model acceleration tools, and further they make use of operations that are inefficient on current hardware. This paper demonstrates that a simple encoder-decoder architecture with a ResNet-like backbone and a small multi-scale head, performs on-par or better than complex semantic segmentation architectures such as HRNet, FANet and DDRNets. Naively applying deep backbones designed for Image Classification to the task of Semantic Segmentation leads to sub-par results, owing to a much smaller effective receptive field of these backbones. Implicit among the various design choices put forth in works like HRNet, DDRNet, and FANet are networks with a large effective receptive field. It is natural to ask if a simple encoder-decoder architecture would compare favorably if comprised of backbones that have a larger effective receptive field, though without the use of inefficient operations like dilated convolutions. We show that with minor and inexpensive modifications to ResNets, enlarging the receptive field, very simple and competitive baselines can be created for Semantic Segmentation. We present a family of such simple architectures for desktop as well as mobile targets, which match or exceed the performance of complex models on the Cityscapes dataset. We hope that our work provides simple yet effective baselines for practitioners to develop efficient semantic segmentation models.",2022-06-16T15:08:34Z,2022-06-16T15:08:34Z,http://arxiv.org/abs/2206.08236v1,http://arxiv.org/pdf/2206.08236v1,"cs.CV, cs.LG, eess.IV"
Methodical Approach for Centralization Evaluation of Modern Automotive   E/E Architectures,"Lucas Mauser, Stefan Wagner, Peter Ziegler","Centralization is considered as a key enabler to master the CPU-intensive features of the modern car. The development and architecture change towards the next generation car is influenced by ADAS, connectivity, infotainment and the consequential need for cyber-security. There is already a high number of papers describing future centralized E/E architectures and technical instruments for centralization. What is missing is a methodical approach to analyze an existing system and find its potential for centralization on the function level. This paper introduces an approach, which serves a system designer or engineer to abstract functions and thus enables to shape a more centralized system architecture. The commonly known E/E architecture designs and the named instruments of current research are the basis for this abstraction. Based on the approach, new system architecture proposals can be set up to discuss and outweigh advantages and disadvantages of those. The approach is validated by applying it step by step to the inlet's derating function of a modern electric vehicle. A following discussion points out that many different factors affect the potential for centralization and centralization may not be the future of every function and system in general.",2022-09-28T14:11:58Z,2022-09-28T14:11:58Z,http://arxiv.org/abs/2209.14118v1,http://arxiv.org/pdf/2209.14118v1,"eess.SY, cs.SY"
CODEBench: A Neural Architecture and Hardware Accelerator Co-Design   Framework,"Shikhar Tuli, Chia-Hao Li, Ritvik Sharma, Niraj K. Jha","Recently, automated co-design of machine learning (ML) models and accelerator architectures has attracted significant attention from both the industry and academia. However, most co-design frameworks either explore a limited search space or employ suboptimal exploration techniques for simultaneous design decision investigations of the ML model and the accelerator. Furthermore, training the ML model and simulating the accelerator performance is computationally expensive. To address these limitations, this work proposes a novel neural architecture and hardware accelerator co-design framework, called CODEBench. It is composed of two new benchmarking sub-frameworks, CNNBench and AccelBench, which explore expanded design spaces of convolutional neural networks (CNNs) and CNN accelerators. CNNBench leverages an advanced search technique, BOSHNAS, to efficiently train a neural heteroscedastic surrogate model to converge to an optimal CNN architecture by employing second-order gradients. AccelBench performs cycle-accurate simulations for a diverse set of accelerator architectures in a vast design space. With the proposed co-design method, called BOSHCODE, our best CNN-accelerator pair achieves 1.4% higher accuracy on the CIFAR-10 dataset compared to the state-of-the-art pair, while enabling 59.1% lower latency and 60.8% lower energy consumption. On the ImageNet dataset, it achieves 3.7% higher Top1 accuracy at 43.8% lower latency and 11.2% lower energy consumption. CODEBench outperforms the state-of-the-art framework, i.e., Auto-NBA, by achieving 1.5% higher accuracy and 34.7x higher throughput, while enabling 11.0x lower energy-delay product (EDP) and 4.0x lower chip area on CIFAR-10.",2022-12-07T21:38:03Z,2022-12-07T21:38:03Z,http://arxiv.org/abs/2212.03965v1,http://arxiv.org/pdf/2212.03965v1,"cs.AR, cs.LG, eess.IV"
Single Cell Training on Architecture Search for Image Denoising,"Bokyeung Lee, Kyungdeuk Ko, Jonghwan Hong, Hanseok Ko","Neural Architecture Search (NAS) for automatically finding the optimal network architecture has shown some success with competitive performances in various computer vision tasks. However, NAS in general requires a tremendous amount of computations. Thus reducing computational cost has emerged as an important issue. Most of the attempts so far has been based on manual approaches, and often the architectures developed from such efforts dwell in the balance of the network optimality and the search cost. Additionally, recent NAS methods for image restoration generally do not consider dynamic operations that may transform dimensions of feature maps because of the dimensionality mismatch in tensor calculations. This can greatly limit NAS in its search for optimal network structure. To address these issues, we re-frame the optimal search problem by focusing at component block level. From previous work, it's been shown that an effective denoising block can be connected in series to further improve the network performance. By focusing at block level, the search space of reinforcement learning becomes significantly smaller and evaluation process can be conducted more rapidly. In addition, we integrate an innovative dimension matching modules for dealing with spatial and channel-wise mismatch that may occur in the optimal design search. This allows much flexibility in optimal network search within the cell block. With these modules, then we employ reinforcement learning in search of an optimal image denoising network at a module level. Computational efficiency of our proposed Denoising Prior Neural Architecture Search (DPNAS) was demonstrated by having it complete an optimal architecture search for an image restoration task by just one day with a single GPU.",2022-12-13T04:47:24Z,2022-12-13T04:47:24Z,http://arxiv.org/abs/2212.06368v1,http://arxiv.org/pdf/2212.06368v1,"cs.CV, eess.IV"
A High-Level Comparison of Recent Technologies for Massive MIMO   Architectures,"Hans Rosenberger, Bernhard Gäde, Ali Bereyhi, Doaa Ahmed, Vahid Jamali, Ralf R. Müller, Georg Fischer, Gaoning He, Mérouane Debbah","Since the introduction of massive MIMO (mMIMO), the design of a transceiver with feasible complexity has been a challenging problem. Initially, it was believed that the main issue in this respect is the overall RF-cost. However, as mMIMO is becoming more and more a key technology for future wireless networks, it is realized, that the RF-cost is only one of many implementational challenges and design trade-offs. In this paper, we present, analyze and compare various novel mMIMO architectures, considering recent emerging technologies such as intelligent surface-assisted and Rotman lens based architectures. These are compared to the conventional fully digital (FD) and hybrid analog-digital beamforming (HADB) approaches. To enable a fair comparison, we account for various hardware imperfections and losses and utilize a novel, universal algorithm for signal precoding. Based on our thorough investigations, we draw a generic efficiency to quality trade-off for various mMIMO architectures. We find that in a typical cellular communication setting the reflect/transmit array based architectures sketch the best overall trade-off. Further, we show that in a qualitative ranking the power efficiency of the considered architectures is independent of the frequency range.",2022-12-22T16:25:25Z,2022-12-22T16:25:25Z,http://arxiv.org/abs/2212.11842v1,http://arxiv.org/pdf/2212.11842v1,"cs.IT, eess.SP, math.IT"
D-TrAttUnet: Dual-Decoder Transformer-Based Attention Unet Architecture   for Binary and Multi-classes Covid-19 Infection Segmentation,"Fares Bougourzi, Cosimo Distante, Fadi Dornaika, Abdelmalik Taleb-Ahmed","In the last three years, the world has been facing a global crisis caused by Covid-19 pandemic. Medical imaging has been playing a crucial role in the fighting against this disease and saving the human lives. Indeed, CT-scans has proved their efficiency in diagnosing, detecting, and following-up the Covid-19 infection. In this paper, we propose a new Transformer-CNN based approach for Covid-19 infection segmentation from the CT slices. The proposed D-TrAttUnet architecture has an Encoder-Decoder structure, where compound Transformer-CNN encoder and Dual-Decoders are proposed. The Transformer-CNN encoder is built using Transformer layers, UpResBlocks, ResBlocks and max-pooling layers. The Dual-Decoder consists of two identical CNN decoders with attention gates. The two decoders are used to segment the infection and the lung regions simultaneously and the losses of the two tasks are joined. The proposed D-TrAttUnet architecture is evaluated for both Binary and Multi-classes Covid-19 infection segmentation. The experimental results prove the efficiency of the proposed approach to deal with the complexity of Covid-19 segmentation task from limited data. Furthermore, D-TrAttUnet architecture outperforms three baseline CNN segmentation architectures (Unet, AttUnet and Unet++) and three state-of-the-art architectures (AnamNet, SCOATNet and CopleNet), in both Binary and Mutli-classes segmentation tasks.",2023-03-27T20:05:09Z,2023-03-27T20:05:09Z,http://arxiv.org/abs/2303.15576v1,http://arxiv.org/pdf/2303.15576v1,"eess.IV, cs.CV"
Temporal Subsampling Diminishes Small Spatial Scales in Recurrent Neural   Network Emulators of Geophysical Turbulence,"Timothy A. Smith, Stephen G. Penny, Jason A. Platt, Tse-Chun Chen","The immense computational cost of traditional numerical weather and climate models has sparked the development of machine learning (ML) based emulators. Because ML methods benefit from long records of training data, it is common to use datasets that are temporally subsampled relative to the time steps required for the numerical integration of differential equations. Here, we investigate how this often overlooked processing step affects the quality of an emulator's predictions. We implement two ML architectures from a class of methods called reservoir computing: (1) a form of Nonlinear Vector Autoregression (NVAR), and (2) an Echo State Network (ESN). Despite their simplicity, it is well documented that these architectures excel at predicting low dimensional chaotic dynamics. We are therefore motivated to test these architectures in an idealized setting of predicting high dimensional geophysical turbulence as represented by Surface Quasi-Geostrophic dynamics. In all cases, subsampling the training data consistently leads to an increased bias at small spatial scales that resembles numerical diffusion. Interestingly, the NVAR architecture becomes unstable when the temporal resolution is increased, indicating that the polynomial based interactions are insufficient at capturing the detailed nonlinearities of the turbulent flow. The ESN architecture is found to be more robust, suggesting a benefit to the more expensive but more general structure. Spectral errors are reduced by including a penalty on the kinetic energy density spectrum during training, although the subsampling related errors persist. Future work is warranted to understand how the temporal resolution of training data affects other ML architectures.",2023-04-28T21:34:53Z,2023-09-21T19:03:12Z,http://arxiv.org/abs/2305.00100v2,http://arxiv.org/pdf/2305.00100v2,"cs.LG, physics.ao-ph, physics.flu-dyn"
"Beyond Diagonal Reconfigurable Intelligent Surfaces Utilizing Graph   Theory: Modeling, Architecture Design, and Optimization","Matteo Nerini, Shanpu Shen, Hongyu Li, Bruno Clerckx","Recently, beyond diagonal reconfigurable intelligent surface (BD-RIS) has been proposed to generalize conventional RIS. BD-RIS has a scattering matrix that is not restricted to being diagonal and thus brings a performance improvement over conventional RIS. While different BD-RIS architectures have been proposed, it still remains an open problem to develop a systematic approach to design BD-RIS architectures achieving the optimal trade-off between performance and circuit complexity. In this work, we propose novel modeling, architecture design, and optimization for BD-RIS based on graph theory. This graph theoretical modeling allows us to develop two new efficient BD-RIS architectures, denoted as tree-connected and forest-connected RIS. Tree-connected RIS, whose corresponding graph is a tree, is proven to be the least complex BD-RIS architecture able to achieve the performance upper bound in multiple-input single-output (MISO) systems. Besides, forest-connected RIS allows us to strike a balance between performance and complexity, further decreasing the complexity over tree-connected RIS. To optimize tree-connected RIS, we derive a closed-form global optimal solution, while forest-connected RIS is optimized through a low-complexity iterative algorithm. Numerical results confirm that tree-connected (resp. forest-connected) RIS achieves the same performance as fully-connected (resp. group-connected) RIS, while reducing the complexity by up to 16.4 times.",2023-05-08T19:33:33Z,2024-02-20T16:56:36Z,http://arxiv.org/abs/2305.05013v2,http://arxiv.org/pdf/2305.05013v2,"cs.IT, eess.SP, math.IT"
Comparative Analysis of Segment Anything Model and U-Net for Breast   Tumor Detection in Ultrasound and Mammography Images,"Mohsen Ahmadi, Masoumeh Farhadi Nia, Sara Asgarian, Kasra Danesh, Elyas Irankhah, Ahmad Gholizadeh Lonbar, Abbas Sharifi","In this study, the main objective is to develop an algorithm capable of identifying and delineating tumor regions in breast ultrasound (BUS) and mammographic images. The technique employs two advanced deep learning architectures, namely U-Net and pretrained SAM, for tumor segmentation. The U-Net model is specifically designed for medical image segmentation and leverages its deep convolutional neural network framework to extract meaningful features from input images. On the other hand, the pretrained SAM architecture incorporates a mechanism to capture spatial dependencies and generate segmentation results. Evaluation is conducted on a diverse dataset containing annotated tumor regions in BUS and mammographic images, covering both benign and malignant tumors. This dataset enables a comprehensive assessment of the algorithm's performance across different tumor types. Results demonstrate that the U-Net model outperforms the pretrained SAM architecture in accurately identifying and segmenting tumor regions in both BUS and mammographic images. The U-Net exhibits superior performance in challenging cases involving irregular shapes, indistinct boundaries, and high tumor heterogeneity. In contrast, the pretrained SAM architecture exhibits limitations in accurately identifying tumor areas, particularly for malignant tumors and objects with weak boundaries or complex shapes. These findings highlight the importance of selecting appropriate deep learning architectures tailored for medical image segmentation. The U-Net model showcases its potential as a robust and accurate tool for tumor detection, while the pretrained SAM architecture suggests the need for further improvements to enhance segmentation performance.",2023-06-21T18:49:21Z,2024-02-13T07:52:17Z,http://arxiv.org/abs/2306.12510v2,http://arxiv.org/pdf/2306.12510v2,"eess.IV, cs.CV, cs.LG"
"Neural Architectures Learning Fourier Transforms, Signal Processing and   Much More....",Prateek Verma,"This report will explore and answer fundamental questions about taking Fourier Transforms and tying it with recent advances in AI and neural architecture. One interpretation of the Fourier Transform is decomposing a signal into its constituent components by projecting them onto complex exponentials. Variants exist, such as discrete cosine transform that does not operate on the complex domain and projects an input signal to only cosine functions oscillating at different frequencies. However, this is a fundamental limitation, and it needs to be more suboptimal. The first one is that all kernels are sinusoidal: What if we could have some kernels adapted or learned according to the problem? What if we can use neural architectures for this? We show how one can learn these kernels from scratch for audio signal processing applications. We find that the neural architecture not only learns sinusoidal kernel shapes but discovers all kinds of incredible signal-processing properties. E.g., windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. Further, upon analysis of the filters, we find that the neural architecture has a comb filter-like structure on top of the learned kernels. Comb filters that allow harmonic frequencies to pass through are one of the core building blocks/types of filters similar to high-pass, low-pass, and band-pass filters of various traditional signal processing algorithms. Further, we can also use the convolution operation with a signal to be learned from scratch, and we will explore papers in the literature that uses this with that robust Transformer architectures. Further, we would also explore making the learned kernel's content adaptive, i.e., learning different kernels for different inputs.",2023-08-20T23:30:27Z,2023-08-20T23:30:27Z,http://arxiv.org/abs/2308.10388v1,http://arxiv.org/pdf/2308.10388v1,"cs.SD, cs.AI, cs.MM, eess.AS"
Nearest neighbor synthesis of CNOT circuits on general quantum   architectures,"Xinyu Chen, Mingqiang Zhu, Xueyun Cheng, Pengcheng Zhu, Zhijin Guan","In recent years, quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ). However, NISQ devices have inherent limitations in terms of connectivity and hardware noise, necessitating the transformation of quantum logic circuits for correct execution on NISQ chips. The synthesis of CNOT circuits considering physical constraints can transform quantum algorithms into low-level quantum circuits, which can be directly executed on physical chips. In the current trend, quantum chip architectures without Hamiltonian paths are gradually replacing architectures with Hamiltonian paths due to their scalability and low-noise characteristics. To this end, this paper addresses the nearest neighbor synthesis of CNOT circuits in the architecture with and without Hamiltonian paths, aiming to enhance the fidelity of the circuits after execution. Firstly, a key-qubit priority mapping model for the general architecture with and without Hamiltonian paths is proposed. Secondly, the initial mapping is further improved by using tabu search to reduce the number of CNOT gates after circuit synthesis and enhance its fidelity. Finally, the noise-aware CNOT circuit nearest neighbor synthesis algorithm for the general architecture is proposed based on the key-qubit priority mapping model. Experimental results show that the proposed method can enhance the fidelity of the CNOT circuit by about 64.7% on a real quantum computing device, achieving a significant optimization effect. Furthermore, the method can be extended to other circuits, thereby improving the overall performance of quantum computing on NISQ devices.",2023-10-01T06:30:58Z,2023-10-01T06:30:58Z,http://arxiv.org/abs/2310.00592v1,http://arxiv.org/pdf/2310.00592v1,"quant-ph, cs.ET"
Swordfish: A Framework for Evaluating Deep Neural Network-based   Basecalling using Computation-In-Memory with Non-Ideal Memristors,"Taha Shahroodi, Gagandeep Singh, Mahdi Zahedi, Haiyu Mao, Joel Lindegger, Can Firtina, Stephan Wong, Onur Mutlu, Said Hamdioui","Basecalling, an essential step in many genome analysis studies, relies on large Deep Neural Networks (DNNs) to achieve high accuracy. Unfortunately, these DNNs are computationally slow and inefficient, leading to considerable delays and resource constraints in the sequence analysis process. A Computation-In-Memory (CIM) architecture using memristors can significantly accelerate the performance of DNNs. However, inherent device non-idealities and architectural limitations of such designs can greatly degrade the basecalling accuracy, which is critical for accurate genome analysis. To facilitate the adoption of memristor-based CIM designs for basecalling, it is important to (1) conduct a comprehensive analysis of potential CIM architectures and (2) develop effective strategies for mitigating the possible adverse effects of inherent device non-idealities and architectural limitations.   This paper proposes Swordfish, a novel hardware/software co-design framework that can effectively address the two aforementioned issues. Swordfish incorporates seven circuit and device restrictions or non-idealities from characterized real memristor-based chips. Swordfish leverages various hardware/software co-design solutions to mitigate the basecalling accuracy loss due to such non-idealities. To demonstrate the effectiveness of Swordfish, we take Bonito, the state-of-the-art (i.e., accurate and fast), open-source basecaller as a case study. Our experimental results using Sword-fish show that a CIM architecture can realistically accelerate Bonito for a wide range of real datasets by an average of 25.7x, with an accuracy loss of 6.01%.",2023-10-06T16:37:03Z,2023-11-26T16:40:02Z,http://arxiv.org/abs/2310.04366v2,http://arxiv.org/pdf/2310.04366v2,"cs.AR, cs.ET, q-bio.GN"
The Safety Shell: an Architecture to Handle Functional Insufficiencies   in Automated Driving,"C. A. J. Hanselaar, E. Silvas, A. Terechko, W. P. M. H. Heemels","To enable highly automated vehicles where the driver is no longer a safety backup, the vehicle must deal with various Functional Insufficiencies (FIs). Thus-far, there is no widely accepted functional architecture that maximizes the availability of autonomy and ensures safety in complex vehicle operational design domains. In this paper, we present a survey of existing methods that strive to prevent or handle FIs. We observe that current design-time methods of preventing FIs lack completeness guarantees. Complementary solutions for on-line handling cannot suitably increase safety without seriously impacting availability of journey continuing autonomous functionality. To fill this gap, we propose the Safety Shell, a scalable multi-channel architecture and arbitration design, built upon preexisting functional safety redundant channel architectures. We compare this novel approach to existing architectures using numerical case studies. The results show that the Safety Shell architecture allows the automated vehicle to be as safe or safer compared to alternatives, while simultaneously improving availability of vehicle autonomy, thereby increasing the possible coverage of on-line functional insufficiency handling.",2023-10-20T14:00:42Z,2023-11-21T10:41:48Z,http://arxiv.org/abs/2311.08413v2,http://arxiv.org/pdf/2311.08413v2,"cs.RO, cs.SY, eess.SY"
ReconU-Net: a direct PET image reconstruction using U-Net architecture   with back projection-induced skip connection,"Fumio Hashimoto, Kibo Ote","[Objective] This study aims to introduce a novel back projection-induced U-Net-shaped architecture, called ReconU-Net, for deep learning-based direct positron emission tomography (PET) image reconstruction. Additionally, our objective is to analyze the behavior of direct PET image reconstruction and gain deeper insights by comparing the proposed ReconU-Net architecture with other encoder-decoder architectures without skip connections. [Approach] The proposed ReconU-Net architecture uniquely integrates the physical model of the back projection operation into the skip connection. This distinctive feature facilitates the effective transfer of intrinsic spatial information from the input sinogram to the reconstructed image via an embedded physical model. The proposed ReconU-Net was trained using Monte Carlo simulation data from the Brainweb phantom and tested on both simulated and real Hoffman brain phantom data. [Main results] The proposed ReconU-Net method generated a reconstructed image with a more accurate structure compared to other deep learning-based direct reconstruction methods. Further analysis showed that the proposed ReconU-Net architecture has the ability to transfer features of multiple resolutions, especially non-abstract high-resolution information, through skip connections. Despite limited training on simulated data, the proposed ReconU-Net successfully reconstructed the real Hoffman brain phantom, unlike other deep learning-based direct reconstruction methods, which failed to produce a reconstructed image. [Significance] The proposed ReconU-Net can improve the fidelity of direct PET image reconstruction, even when dealing with small training datasets, by leveraging the synergistic relationship between data-driven modeling and the physics model of the imaging process.",2023-12-05T04:51:42Z,2023-12-05T04:51:42Z,http://arxiv.org/abs/2312.02494v1,http://arxiv.org/pdf/2312.02494v1,"physics.med-ph, cs.CV, cs.LG"
"Mechatronic Design, Experimental Setup and Control Architecture Design   of a Novel 4 DoF Parallel Manipulator","Marina Valles, Pedro Araujo-Gomez, Vicente Mata, Angel Valera, Miguel Diaz-Rodriguez, Alvaro Page, Nidal M. Farhat","Although parallel manipulators (PMs) started with the introduction of architectures with 6 Degrees of Freedom (DoF), a vast number of applications require less than 6 DoF. Consequently, scholars have proposed architectures with 3 DoF and 4 DoF, but relatively few 4 DoF PMs have become prototypes, especially of the two rotation (2R) and two translation (2T) motion types. In this paper, we explain the mechatronics design, prototype and control architecture design of a 4 DoF PM with 2R2T motions. We chose to design a 4 DoF manipulator based on the motion needed to complete the tasks of lower limb rehabilitation.   To the author's best knowledge, PMs between 3 and 6 DoF for rehabilitation of lower limbs have not been proposed to date. The developed architecture enhances the three minimum DoF required by adding a 4 DoF which allows combinations of normal or tangential efforts in the joints, or torque acting on the knee. We put forward the inverse and forward displacement equations, describe the prototype, perform the experimental setup, and develop the hardware and control architecture. The tracking accuracy experiments from the proposed controller show that the manipulator can accomplish the required application.",2024-01-16T08:11:29Z,2024-01-16T08:11:29Z,http://arxiv.org/abs/2401.08192v1,http://arxiv.org/pdf/2401.08192v1,"cs.RO, cs.SY, eess.SY"
SPViz: A DSL-Driven Approach for Software Project Visualization Tooling,"Niklas Rentz, Reinhard von Hanxleden","For most service architectures, such as OSGi and Spring, architecture-specific tools allow software developers and architects to visualize otherwise obscure configurations hidden in the project files. Such visualization tools are often used for documentation purposes and help to better understand programs than with source code alone. However, such tools often do not address project-specific peculiarities or do not exist at all for less common architectures, requiring developers to use different visualization and analysis tools within the same architecture. Furthermore, many generic modeling tools and architecture visualization tools require their users to create and maintain models manually.   We here propose a DSL-driven approach that allows software architects to define and adapt their own project visualization tool. The approach, which we refer to as Software Project Visualization (SPViz), uses two DSLs, one to describe architectural elements and their relationships, and one to describe how these should be visualized. We demonstrate how SPViz can then automatically synthesize a customized, project-specific visualization tool that can adapt to changes in the underlying project automatically.   We implemented our approach in an open-source library, also termed SPViz and discuss and analyze four different tools that follow this concept, including open-source projects and projects from an industrial partner in the railway domain.",2024-01-30T14:41:40Z,2024-01-30T14:41:40Z,http://arxiv.org/abs/2401.17063v1,http://arxiv.org/pdf/2401.17063v1,"cs.SE, D.2.7; D.2.11"
Teacher-Student Learning based Low Complexity Relay Selection in   Wireless Powered Communications,"Aysun Gurur Onalan, Berkay Kopru, Sinem Coleri","Radio Frequency Energy Harvesting (RF-EH) networks are key enablers of massive Internet-of-things by providing controllable and long-distance energy transfer to energy-limited devices. Relays, helping either energy or information transfer, have been demonstrated to significantly improve the performance of these networks. This paper studies the joint relay selection, scheduling, and power control problem in multiple-source-multiple-relay RF-EH networks under nonlinear EH conditions. We first obtain the optimal solution to the scheduling and power control problem for the given relay selection. Then, the relay selection problem is formulated as a classification problem, for which two convolutional neural network (CNN) based architectures are proposed. While the first architecture employs conventional 2D convolution blocks and benefits from skip connections between layers; the second architecture replaces them with inception blocks, to decrease trainable parameter size without sacrificing accuracy for memory-constrained applications. To decrease the runtime complexity further, teacher-student learning is employed such that the teacher network is larger, and the student is a smaller size CNN-based architecture distilling the teacher's knowledge. A novel dichotomous search-based algorithm is employed to determine the best architecture for the student network. Our simulation results demonstrate that the proposed solutions provide lower complexity than the state-of-art iterative approaches without compromising optimality.",2024-02-03T20:22:41Z,2024-02-03T20:22:41Z,http://arxiv.org/abs/2402.02254v1,http://arxiv.org/pdf/2402.02254v1,"cs.LG, cs.NI, eess.SP"
Learning Wireless Data Knowledge Graph for Green Intelligent   Communications: Methodology and Experiments,"Yongming Huang, Xiaohu You, Hang Zhan, Shiwen He, Ningning Fu, Wei Xu","Intelligent communications have played a pivotal role in shaping the evolution of 6G networks. Native artificial intelligence (AI) within green communication systems must meet stringent real-time requirements. To achieve this, deploying lightweight and resource-efficient AI models is necessary. However, as wireless networks generate a multitude of data fields and indicators during operation, only a fraction of them imposes significant impact on the network AI models. Therefore, real-time intelligence of communication systems heavily relies on a small but critical set of the data that profoundly influences the performance of network AI models. These challenges underscore the need for innovative architectures and solutions. In this paper, we propose a solution, termed the pervasive multi-level (PML) native AI architecture, which integrates the concept of knowledge graph (KG) into the intelligent operational manipulations of mobile networks, resulting in the establishment of a wireless data KG. Leveraging the wireless data KG, we characterize the massive and complex data collected from wireless communication networks and analyze the relationships among various data fields. The obtained graph of data field relations enables the on-demand generation of minimal and effective datasets, referred to as feature datasets, tailored to specific application requirements. Consequently, this architecture not only enhances AI training, inference, and validation processes but also significantly reduces resource wastage and overhead for communication networks. To implement this architecture, we have developed a specific solution comprising a spatio-temporal heterogeneous graph attention neural network model (STREAM) as well as a feature dataset generation algorithm. Experiments are conducted to validate the effectiveness of the proposed architecture.",2024-04-16T07:55:34Z,2024-04-16T07:55:34Z,http://arxiv.org/abs/2404.10365v1,http://arxiv.org/pdf/2404.10365v1,"cs.NI, cs.LG, eess.SP"
einspace: Searching for Neural Architectures from Fundamental Operations,"Linus Ericsson, Miguel Espinosa, Chenhongyi Yang, Antreas Antoniou, Amos Storkey, Shay B. Cohen, Steven McDonagh, Elliot J. Crowley","Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren't diverse enough to include such transformations a priori. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.",2024-05-31T14:25:45Z,2024-10-30T12:35:56Z,http://arxiv.org/abs/2405.20838v2,http://arxiv.org/pdf/2405.20838v2,"cs.LG, cs.AI, cs.CV, stat.ML"
On the Expressive Power of Spectral Invariant Graph Neural Networks,"Bohang Zhang, Lingxiao Zhao, Haggai Maron","Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs.",2024-06-06T17:59:41Z,2024-06-06T17:59:41Z,http://arxiv.org/abs/2406.04336v1,http://arxiv.org/pdf/2406.04336v1,"cs.LG, cs.DM, cs.DS, math.CO, math.SP"
Dynamic Energy-Saving Design for Double-Faced Active RIS Assisted   Communications with Perfect/Imperfect CSI,"Yang Cao, Wenchi Cheng, Jingqing Wang, Wei Zhang","Although the emerging reconfigurable intelligent surface (RIS) paves a new way for next-generation wireless communications, it suffers from inherent flaws, i.e., double-fading attenuation effects and half-space coverage limitations. The state-of-the-art double-face active (DFA)-RIS architecture is proposed for significantly amplifying and transmitting incident signals in full-space. Despite the efficacy of DFA-RIS in mitigating the aforementioned flaws, its potential drawback is that the complex active hardware also incurs intolerable energy consumption. To overcome this drawback, in this paper we propose a novel dynamic energy-saving design for the DFA-RIS, called the sub-array based DFA-RIS architecture. This architecture divides the DFA-RIS into multiple sub-arrays, where the signal amplification function in each sub-array can be activated/deactivated dynamically and flexibly. Utilizing the above architecture, we develop the joint optimization scheme based on transmit beamforming, DFA-RIS configuration, and reflection amplifier (RA) operating pattern to maximize the energy efficiency (EE) of the DFA-RIS assisted multiuser MISO system considering the perfect/imperfect channel state information (CSI) case. Then, the penalty dual decomposition (PDD) based alternating optimization (AO) algorithm and the constrained stochastic majorization-minimization (CSMM) based AO algorithm address non-convex problems in the perfect/imperfect CSI case, respectively. Simulation results verified that our proposed sub-array based DFA-RIS architecture can benefit the EE of the system more than other RIS architectures.",2024-06-12T01:55:24Z,2024-06-12T01:55:24Z,http://arxiv.org/abs/2406.07807v1,http://arxiv.org/pdf/2406.07807v1,"cs.IT, eess.SP, math.IT"
Neural networks in non-metric spaces,Luca Galimberti,"Leveraging the infinite dimensional neural network architecture we proposed in arXiv:2109.13512v4 and which can process inputs from Fr\'echet spaces, and using the universal approximation property shown therein, we now largely extend the scope of this architecture by proving several universal approximation theorems for a vast class of input and output spaces. More precisely, the input space $\mathfrak X$ is allowed to be a general topological space satisfying only a mild condition (""quasi-Polish""), and the output space can be either another quasi-Polish space $\mathfrak Y$ or a topological vector space $E$. Similarly to arXiv:2109.13512v4, we show furthermore that our neural network architectures can be projected down to ""finite dimensional"" subspaces with any desirable accuracy, thus obtaining approximating networks that are easy to implement and allow for fast computation and fitting. The resulting neural network architecture is therefore applicable for prediction tasks based on functional data. To the best of our knowledge, this is the first result which deals with such a wide class of input/output spaces and simultaneously guarantees the numerical feasibility of the ensuing architectures. Finally, we prove an obstruction result which indicates that the category of quasi-Polish spaces is in a certain sense the correct category to work with if one aims at constructing approximating architectures on infinite-dimensional spaces $\mathfrak X$ which, at the same time, have sufficient expressive power to approximate continuous functions on $\mathfrak X$, are specified by a finite number of parameters only and are ""stable"" with respect to these parameters.",2024-06-13T16:44:58Z,2024-06-13T16:44:58Z,http://arxiv.org/abs/2406.09310v1,http://arxiv.org/pdf/2406.09310v1,"math.FA, cs.LG"
Unleashing OpenTitan's Potential: a Silicon-Ready Embedded Secure   Element for Root of Trust and Cryptographic Offloading,"Maicol Ciani, Emanuele Parisi, Alberto Musa, Francesco Barchi, Andrea Bartolini, Ari Kulmala, Rafail Psiakis, Angelo Garofalo, Andrea Acquaviva, Davide Rossi","The rapid advancement and exploration of open-hardware RISC-V platforms are driving significant changes in sectors like autonomous vehicles, smart-city infrastructure, and medical devices. OpenTitan stands out as a groundbreaking open-source RISC-V design with a comprehensive security toolkit as a standalone system-on-chip (SoC). OpenTitan includes Earl Grey, a fully implemented and silicon-proven SoC, and Darjeeling, announced but not yet fully implemented. Earl Grey targets standalone SoC implementations, while Darjeeling is for integrable implementations. The literature lacks a silicon-ready embedded implementation of an open-source Root of Trust, despite lowRISC's efforts on Darjeeling. We address the limitations of existing implementations by optimizing data transfer latency between memory and cryptographic accelerators to prevent under-utilization and ensure efficient task acceleration. Our contributions include a comprehensive methodology for integrating custom extensions and IPs into the Earl Grey architecture, architectural enhancements for system-level integration, support for varied boot modes, and improved data movement across the platform. These advancements facilitate deploying OpenTitan in broader SoCs, even without specific technology-dependent IPs, providing a deployment-ready research vehicle for the community. We integrated the extended Earl Grey architecture into a reference architecture in a 22nm FDX technology node, benchmarking the enhanced architecture's performance. The results show significant improvements in cryptographic processing speed, achieving up to 2.7x speedup for SHA-256/HMAC and 1.6x for AES accelerators compared to the baseline Earl Grey architecture.",2024-06-17T13:56:00Z,2024-06-17T13:56:00Z,http://arxiv.org/abs/2406.11558v1,http://arxiv.org/pdf/2406.11558v1,"eess.SY, cs.SY, eess.SP"
An Efficient General-Purpose Optical Accelerator for Neural Networks,"Sijie Fei, Amro Eldebiky, Grace Li Zhang, Bing Li, Ulf Schlichtmann","General-purpose optical accelerators (GOAs) have emerged as a promising platform to accelerate deep neural networks (DNNs) due to their low latency and energy consumption. Such an accelerator is usually composed of a given number of interleaving Mach-Zehnder- Interferometers (MZIs). This interleaving architecture, however, has a low efficiency when accelerating neural networks of various sizes due to the mismatch between weight matrices and the GOA architecture. In this work, a hybrid GOA architecture is proposed to enhance the mapping efficiency of neural networks onto the GOA. In this architecture, independent MZI modules are connected with microring resonators (MRRs), so that they can be combined to process large neural networks efficiently. Each of these modules implements a unitary matrix with inputs adjusted by tunable coefficients. The parameters of the proposed architecture are searched using genetic algorithm. To enhance the accuracy of neural networks, selected weight matrices are expanded to multiple unitary matrices applying singular value decomposition (SVD). The kernels in neural networks are also adjusted to use up the on-chip computational resources. Experimental results show that with a given number of MZIs, the mapping efficiency of neural networks on the proposed architecture can be enhanced by 21.87%, 21.20%, 24.69%, and 25.52% for VGG16 and Resnet18 on datasets Cifar10 and Cifar100, respectively. The energy consumption and computation latency can also be reduced by over 67% and 21%, respectively.",2024-09-02T13:04:08Z,2024-09-02T13:04:08Z,http://arxiv.org/abs/2409.12966v1,http://arxiv.org/pdf/2409.12966v1,"cs.NE, cs.SY, eess.SY"
Peeling Back the Layers: An In-Depth Evaluation of Encoder Architectures   in Neural News Recommenders,"Andreea Iana, Goran Glavaš, Heiko Paulheim","Encoder architectures play a pivotal role in neural news recommenders by embedding the semantic and contextual information of news and users. Thus, research has heavily focused on enhancing the representational capabilities of news and user encoders to improve recommender performance. Despite the significant impact of encoder architectures on the quality of news and user representations, existing analyses of encoder designs focus only on the overall downstream recommendation performance. This offers a one-sided assessment of the encoders' similarity, ignoring more nuanced differences in their behavior, and potentially resulting in sub-optimal model selection. In this work, we perform a comprehensive analysis of encoder architectures in neural news recommender systems. We systematically evaluate the most prominent news and user encoder architectures, focusing on their (i) representational similarity, measured with the Central Kernel Alignment, (ii) overlap of generated recommendation lists, quantified with the Jaccard similarity, and (iii) the overall recommendation performance. Our analysis reveals that the complexity of certain encoding techniques is often empirically unjustified, highlighting the potential for simpler, more efficient architectures. By isolating the effects of individual components, we provide valuable insights for researchers and practitioners to make better informed decisions about encoder selection and avoid unnecessary complexity in the design of news recommenders.",2024-10-02T12:21:31Z,2024-10-02T12:21:31Z,http://arxiv.org/abs/2410.01470v1,http://arxiv.org/pdf/2410.01470v1,"cs.IR, cs.AI, H.3.3; I.2.7"
Survey and Evaluation of Converging Architecture in LLMs based on   Footsteps of Operations,"Seongho Kim, Jihyun Moon, Juntaek Oh, Insu Choi, Joon-Sung Yang","The advent of the Attention mechanism and Transformer architecture enables contextually natural text generation and compresses the burden of processing entire source information into singular vectors. Based on these two main ideas, model sizes gradually increases to accommodate more precise and comprehensive information, leading to the current state-of-the-art LLMs being very large, with parameters around 70 billion. As the model sizes are growing, the demand for substantial storage and computational capacity increases. This leads to the development of high-bandwidth memory and accelerators, as well as a variety of model architectures designed to meet these requirements. We note that LLM architectures have increasingly converged. This paper analyzes how these converged architectures perform in terms of layer configurations, operational mechanisms, and model sizes, considering various hyperparameter settings. In this paper, we conduct a concise survey of the history of LLMs by tracing the evolution of their operational improvements. Furthermore, we summarize the performance trends of LLMs under various hyperparameter settings using the RTX 6000, which features the state-of-the-art Ada Lovelace architecture. We conclude that even the same model can exhibit different behaviors depending on the hyperparameters or whether it is deployed in server or edge environments.",2024-10-15T08:19:24Z,2024-10-15T08:19:24Z,http://arxiv.org/abs/2410.11381v1,http://arxiv.org/pdf/2410.11381v1,"cs.LG, cs.AI, cs.CL, 68T50, I.2.7"
Design Space Exploration of Embedded SoC Architectures for Real-Time   Optimal Control,"Kris Shengjun Dong, Dima Nikiforov, Widyadewi Soedarmadji, Minh Nguyen, Christopher Fletcher, Yakun Sophia Shao","Empowering resource-limited robots to execute computationally intensive tasks such as locomotion and manipulation is challenging. This project provides a comprehensive design space exploration to determine optimal hardware computation architectures suitable for model-based control algorithms. We profile and optimize representative architectural designs across general-purpose scalar, vector processors, and specialized accelerators. Specifically, we compare CPUs, vector machines, and domain-specialized accelerators with kernel-level benchmarks and end-to-end representative robotic workloads. Our exploration provides a quantitative performance, area, and utilization comparison and analyzes the trade-offs between these representative distinct architectural designs. We demonstrate that architectural modifications, software, and system optimization can alleviate bottlenecks and enhance utilization. Finally, we propose a code generation flow to simplify the engineering work for mapping robotic workloads to specialized architectures.",2024-10-16T01:04:10Z,2024-10-24T21:31:32Z,http://arxiv.org/abs/2410.12142v3,http://arxiv.org/pdf/2410.12142v3,"cs.RO, cs.SY, eess.SY"
Fruit Deformity Classification through Single-Input and Multi-Input   Architectures based on CNN Models using Real and Synthetic Images,"Tommy D. Beltran, Raul J. Villao, Luis E. Chuquimarca, Boris X. Vintimilla, Sergio A. Velastin","The present study focuses on detecting the degree of deformity in fruits such as apples, mangoes, and strawberries during the process of inspecting their external quality, employing Single-Input and Multi-Input architectures based on convolutional neural network (CNN) models using sets of real and synthetic images. The datasets are segmented using the Segment Anything Model (SAM), which provides the silhouette of the fruits. Regarding the single-input architecture, the evaluation of the CNN models is performed only with real images, but a methodology is proposed to improve these results using a pre-trained model with synthetic images. In the Multi-Input architecture, branches with RGB images and fruit silhouettes are implemented as inputs for evaluating CNN models such as VGG16, MobileNetV2, and CIDIS. However, the results revealed that the Multi-Input architecture with the MobileNetV2 model was the most effective in identifying deformities in the fruits, achieving accuracies of 90\%, 94\%, and 92\% for apples, mangoes, and strawberries, respectively. In conclusion, the Multi-Input architecture with the MobileNetV2 model is the most accurate for classifying levels of deformity in fruits.",2024-12-17T14:51:13Z,2024-12-17T14:51:13Z,http://arxiv.org/abs/2412.12966v1,http://arxiv.org/pdf/2412.12966v1,"cs.CV, 68T45, I.2; I.4; I.5"
Efficient Compilation for Shuttling Trapped-Ion Machines via the   Position Graph Architectural Abstraction,"Bao Bach, Ilya Safro, Ed Younis","With the growth of quantum platforms for gate-based quantum computation, compilation holds a crucial factor in deciding the success of the implementation. There has been rich research and development in compilation techniques for the superconducting-qubit regime. In contrast, the trapped-ion architectures, currently leading in robust quantum computations due to their reliable operations, do not have many competitive compilation strategies. This work presents a novel unifying abstraction, called the position graph, for different types of hardware architectures. Using this abstraction, we model trapped-ion Quantum Charge-Coupled Device (QCCD) architectures and enable high-quality, scalable superconducting compilation methods. In particular, we devise a scheduling algorithm called SHuttling-Aware PERmutative heuristic search algorithm (SHAPER) to tackle the complex constraints and dynamics of trapped-ion QCCD with the cooperation of state-of-the-art permutation-aware mapping. This approach generates native, executable circuits and ion instructions on the hardware that adheres to the physical constraints of shuttling-based quantum computers. Using the position graph abstraction, we evaluate our algorithm on theorized and actual architectures. Our algorithm can successfully compile programs for these architectures where other state-of-the-art algorithms fail. In the cases when other algorithms complete, our algorithm produces a schedule that is $14\%$ faster on average, up to $69\%$ in the best case.\\ {\bf Reproducibility:} source code and computational results are available at $[$will be added upon acceptance$]$",2025-01-21T19:39:03Z,2025-01-21T19:39:03Z,http://arxiv.org/abs/2501.12470v1,http://arxiv.org/pdf/2501.12470v1,"quant-ph, cs.ET"
Automated Microservice Pattern Instance Detection Using   Infrastructure-as-Code Artifacts and Large Language Models,Carlos Eduardo Duarte,"Documenting software architecture is essential to preserve architecture knowledge, even though it is frequently costly. Architecture pattern instances, including microservice pattern instances, provide important structural software information. Practitioners should document this information to prevent knowledge vaporization. However, architecture patterns may not be detectable by analyzing source code artifacts, requiring the analysis of other types of artifacts. Moreover, many existing pattern detection instance approaches are complex to extend. This article presents our ongoing PhD research, early experiments, and a prototype for a tool we call MicroPAD for automating the detection of microservice pattern instances. The prototype uses Large Language Models (LLMs) to analyze Infrastructure-as-Code (IaC) artifacts to aid detection, aiming to keep costs low and maximize the scope of detectable patterns. Early experiments ran the prototype thrice in 22 GitHub projects. We verified that 83\% of the patterns that the prototype identified were in the project. The costs of detecting the pattern instances were minimal. These results indicate that the approach is likely viable and, by lowering the entry barrier to automating pattern instance detection, could help democratize developer access to this category of architecture knowledge. Finally, we present our overall research methodology, planned future work, and an overview of MicroPAD's potential industrial impact.",2025-02-06T16:22:14Z,2025-02-06T16:22:14Z,http://arxiv.org/abs/2502.04188v1,http://arxiv.org/pdf/2502.04188v1,"cs.SE, D.2.11"
Components of an NSDL Architecture: Technical Scope and Functional Model,"David Fulker, Greg Janee","We describe work leading toward specification of a technical architecture for the National Science, Mathematics, Engineering, and Technology Education Digital Library (NSDL). This includes a technical scope and a functional model, with some elaboration on the particularly rich set of library services that NSDL is expected eventually to encompass.",2002-01-30T01:14:35Z,2002-01-30T01:14:35Z,http://arxiv.org/abs/cs/0201027v1,http://arxiv.org/pdf/cs/0201027v1,"cs.DL, H.3.7"
Stability Margins of $\mathcal{L}_1$ Adaptive Controller: Part II,"Chengyu Cao, Naira Hovakimyan","In Part I of this paper, we have developed a novel $\mathcal{L}_1$ adaptive control architecture that enables fast adaptation and leads to uniformly bounded transient and asymptotic tracking for system's both signals, input and output, simultaneously. In this paper, we derive the stability margins of $\mathcal{L}_1$ adaptive control architecture, including time-delay and gain margins in the presence of time-varying bounded disturbance.   Simulations verify the theoretical findings.",2006-08-15T15:22:53Z,2006-08-15T15:22:53Z,http://arxiv.org/abs/math/0608394v1,http://arxiv.org/pdf/math/0608394v1,"math.OC, 93C40"
Software architecture for an unattended remotely controlled telescope,"Robert Lucas, Ulrich Kolb",We report on the software architecture we developed for the Open University's remotely controlled telescope PIRATE. This facility is based in Mallorca and used in distance learning modules by undergraduate students and by postgraduate students for research projects.,2010-12-14T15:28:33Z,2010-12-14T15:28:33Z,http://arxiv.org/abs/1012.3058v1,http://arxiv.org/pdf/1012.3058v1,"astro-ph.IM, physics.ed-ph"
Distributed Denial of Service is a Scalability Problem,Yoo Chung,"Distributed denial of service attacks are often considered a security problem. While this may be the way to view the problem with today's Internet, new network architectures attempting to address the issue should view it as a scalability problem. In addition, they need to address the problem based on a rigorous foundation.",2011-04-01T01:15:51Z,2011-04-13T05:11:19Z,http://arxiv.org/abs/1104.0057v2,http://arxiv.org/pdf/1104.0057v2,"cs.NI, C.2.1; K.6.5"
Examining the Impact of Platform Properties on Quality Attributes,"Balwinder Sodhi, T. V. Prabhakar","We examine and bring out the architecturally significant characteristics of various virtualization and cloud oriented platforms. The impact of such characteristics on the ability of guest applications to achieve various quality attributes (QA) has also been determined by examining existing body of architecture knowledge. We observe from our findings that efficiency, resource elasticity and security are among the most impacted QAs, and virtualization platforms exhibit the maximum impact on various QAs.",2012-05-21T15:03:11Z,2012-05-21T15:03:11Z,http://arxiv.org/abs/1205.4626v1,http://arxiv.org/pdf/1205.4626v1,"cs.SE, D.2.11"
Hybrid Communication Architecture HCA,Kari Visala,"The beginning of the 21st century has seen many projects on distributed hash tables, both research and commercial. One of their aims has been to replace the first generation of file sharing software with scalable peer-to-peer architectures. On other fronts, the same techniques are applied, for example, to content delivery networks, streaming networks, cooperative caches, distributed file systems, and grid computing architectures for scientific use. This trend has emerged because with cooperative peers it is possible to asymptotically enhance the use of resouces in sharing of data compared to the basic client-server architecture.   The need for distribution of data is wide and one could argue that it is as fundamental a building block as the message passing of the Internet. As an answer to this need a new scalable architecture is introduced: Hybrid Communication Architecture (HCA), which provides both data sharing and message passing as communication primitives for applications. HCA can be regarded as an abstraction layer for communication which is further encapsulated by a higher-level middleware. HCA is aimed at general use, and it is not designed for any particular application. One key idea is to combine data sharing with streaming since together they enable many applications not easily implementable with only one of these features. For example, a game application could share the game world state between clients and modify it by using streaming. The other distinctive feature of the system is the use of knowledge of the physical network topology in the optimization of the communication. With a feasible business model, fault-tolerance, and security features, HCA is aimed eventually for real-life adoption.   This thesis presents the specification of the C++ client interface of HCA and the architecture and protocol of the distributed nodes forming the implementation.",2014-07-15T21:19:18Z,2014-07-15T21:19:18Z,http://arxiv.org/abs/1407.4149v1,http://arxiv.org/pdf/1407.4149v1,"cs.DC, cs.NI, 68M10"
Fast calculation of correlations in recognition systems,"Pavel Dourbal, Mikhail Pekker",Computationally efficient classification system architecture is proposed. It utilizes fast tensor-vector multiplication algorithm to apply linear operators upon input signals . The approach is applicable to wide variety of recognition system architectures ranging from single stage matched filter bank classifiers to complex neural networks with unlimited number of hidden layers.,2016-03-06T00:30:34Z,2016-03-06T00:30:34Z,http://arxiv.org/abs/1603.01772v1,http://arxiv.org/pdf/1603.01772v1,"cs.CV, 62H30, 65F05, 65F10, 65F30, 65F50, 68T05, 68T10, 94A11, 94A12,
  94A13, 94A15, F.2.1; G.1.0; G.1.3; G.4; H.4.2; I.1.2; I.2.2; I.5.2; I.5.4"
A proposed solution for analysis management in high energy physics,Mingrui Zhao,"This paper presents an architecture for the analysis management in high energy physics experiments. Some new concepts on data analysis are introduced. A protocol for organizing and operating an analysis is raised. A toolkit following this architecture is developed, which provides a solution of analysis management with both flexibility and reproducibility. A foreseen development of this toolkit is discussed.",2018-06-22T02:44:19Z,2018-06-22T02:44:19Z,http://arxiv.org/abs/1806.08787v1,http://arxiv.org/pdf/1806.08787v1,"physics.data-an, hep-ex"
Genetic Neural Architecture Search for automatic assessment of human   sperm images,"Erfan Miahi, Seyed Abolghasem Mirroshandel, Alexis Nasr","Male infertility is a disease which affects approximately 7% of men. Sperm morphology analysis (SMA) is one of the main diagnosis methods for this problem. Manual SMA is an inexact, subjective, non-reproducible, and hard to teach process. As a result, in this paper, we introduce a novel automatic SMA based on a neural architecture search algorithm termed Genetic Neural Architecture Search (GeNAS). For this purpose, we used a collection of images called MHSMA dataset contains 1,540 sperm images which have been collected from 235 patients with infertility problems. GeNAS is a genetic algorithm that acts as a meta-controller which explores the constrained search space of plain convolutional neural network architectures. Every individual of the genetic algorithm is a convolutional neural network trained to predict morphological deformities in different segments of human sperm (head, vacuole, and acrosome), and its fitness is calculated by a novel proposed method named GeNAS-WF especially designed for noisy, low resolution, and imbalanced datasets. Also, a hashing method is used to save each trained neural architecture fitness, so we could reuse them during fitness evaluation and speed up the algorithm. Besides, in terms of running time and computation power, our proposed architecture search method is far more efficient than most of the other existing neural architecture search algorithms. Additionally, other proposed methods have been evaluated on balanced datasets, whereas GeNAS is built specifically for noisy, low quality, and imbalanced datasets which are common in the field of medical imaging. In our experiments, the best neural architecture found by GeNAS has reached an accuracy of 91.66%, 77.33%, and 77.66% in the vacuole, head, and acrosome abnormality detection, respectively. In comparison to other proposed algorithms for MHSMA dataset, GeNAS achieved state-of-the-art results.",2019-09-20T11:25:05Z,2020-09-17T11:12:35Z,http://arxiv.org/abs/1909.09432v2,http://arxiv.org/pdf/1909.09432v2,"cs.LG, cs.CV, cs.NE, stat.ML, 68T01 (Primary) 68T45, 68T20 (Secondary)"
Architectures in parametric component-based systems: Qualitative and   quantitative modelling,"Maria Pittou, George Rahonis","One of the key aspects in component-based design is specifying the software architecture that characterizes the topology and the permissible interactions of the components of a system. To achieve well-founded design there is need to address both the qualitative and non-functional aspects of architectures. In this paper we study the qualitative and quantitative formal modelling of architectures applied on parametric component-based systems, that consist of an unknown number of instances of each component. Specifically, we introduce an extended propositional interaction logic and investigate its first-order level which serves as a formal language for the interactions of parametric systems. Our logics achieve to encode the execution order of interactions, which is a main feature in several important architectures, as well as to model recursive interactions. Moreover, we prove the decidability of equivalence, satisfiability, and validity of first-order extended interaction logic formulas, and provide several examples of formulas describing well-known architectures. We show the robustness of our theory by effectively extending our results for parametric weighted architectures. For this, we study the weighted counterparts of our logics over a commutative semiring, and we apply them for modelling the quantitative aspects of concrete architectures. Finally, we prove that the equivalence problem of weighted first-order extended interaction logic formulas is decidable in a large class of semirings, namely the class (of subsemirings) of skew fields.",2019-04-03T20:05:41Z,2021-12-29T12:42:54Z,http://arxiv.org/abs/1904.02222v16,http://arxiv.org/pdf/1904.02222v16,"cs.LO, 68Q45"
Improving the Long-Range Performance of Gated Graph Neural Networks,"Denis Lukovnikov, Jens Lehmann, Asja Fischer","Many popular variants of graph neural networks (GNNs) that are capable of handling multi-relational graphs may suffer from vanishing gradients. In this work, we propose a novel GNN architecture based on the Gated Graph Neural Network with an improved ability to handle long-range dependencies in multi-relational graphs. An experimental analysis on different synthetic tasks demonstrates that the proposed architecture outperforms several popular GNN models.",2020-07-19T13:25:38Z,2020-07-19T13:25:38Z,http://arxiv.org/abs/2007.09668v1,http://arxiv.org/pdf/2007.09668v1,"cs.LG, stat.ML"
A Single-Channel Architecture for Algebraic Integer Based 8$\times$8 2-D   DCT Computation,"A. Edirisuriya, A. Madanayake, R. J. Cintra, V. S. Dimitrov","An area efficient row-parallel architecture is proposed for the real-time implementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine transform (DCT) for image and video processing. The proposed architecture computes 8$\times$8 2-D DCT transform based on the Arai DCT algorithm. An improved fast algorithm for AI based 1-D DCT computation is proposed along with a single channel 2-D DCT architecture. The design improves on the 4-channel AI DCT architecture that was published recently by reducing the number of integer channels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The architecture offers exact computation of 8$\times$8 blocks of the 2-D DCT coefficients up to the FRS, which converts the coefficients from the AI representation to fixed-point format using the method of expansion factors. Prototype circuits corresponding to FRS blocks based on two expansion factors are realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6 XC6VLX240T device. Post place-and-route results show a 20% reduction in terms of area compared to the 2-D DCT architecture requiring five 1-D AI cores. The area-time and area-time${}^2$ complexity metrics are also reduced by 23% and 22% respectively for designs with 8-bit input word length. The digital realizations are simulated up to place and route for ASICs using 45 nm CMOS standard cells. The maximum estimated clock rate is 951 MHz for the CMOS realizations indicating 7.608$\cdot$10$^9$ pixels/seconds and a 8$\times$8 block rate of 118.875 MHz.",2017-10-27T03:32:48Z,2017-10-27T03:32:48Z,http://arxiv.org/abs/1710.09975v1,http://arxiv.org/pdf/1710.09975v1,"cs.AR, cs.MM, stat.ME"
A Cascade Architecture for Keyword Spotting on Mobile Devices,"Alexander Gruenstein, Raziel Alvarez, Chris Thornton, Mohammadali Ghodrat","We present a cascade architecture for keyword spotting with speaker verification on mobile devices. By pairing a small computational footprint with specialized digital signal processing (DSP) chips, we are able to achieve low power consumption while continuously listening for a keyword.",2017-12-10T22:47:29Z,2017-12-10T22:47:29Z,http://arxiv.org/abs/1712.03603v1,http://arxiv.org/pdf/1712.03603v1,"cs.SD, eess.AS"
Power Consumption Variation over Activation Functions,Leon Derczynski,"The power that machine learning models consume when making predictions can be affected by a model's architecture. This paper presents various estimates of power consumption for a range of different activation functions, a core factor in neural network model architecture design. Substantial differences in hardware performance exist between activation functions. This difference informs how power consumption in machine learning models can be reduced.",2020-06-12T14:40:46Z,2020-06-12T14:40:46Z,http://arxiv.org/abs/2006.07237v1,http://arxiv.org/pdf/2006.07237v1,"cs.LG, cs.NE, stat.ML"
CC-Light eQASM Architecture Specification,Xiang Fu,"This document is the specification of the CC-Light instantiation of executable QASM (eQASM), a quantum instruction set architecture (QISA) developed in QuTech targeting to control a seven-qubit superconducting quantum processor. This document can serve as a reference manual for low-level programmers, compiler backend developers, and microarchitecture implementers of eQASM. The design of CC-Light eQASM is under the Apache 2.0 License.",2020-05-30T07:55:07Z,2020-05-30T07:55:07Z,http://arxiv.org/abs/2006.09294v1,http://arxiv.org/pdf/2006.09294v1,"cs.PL, quant-ph"
Structural health monitoring with distributed wireless sensor networks,"Daniel-Ioan Curiac, Ovidiu Banias, Ioan Borza","Wireless Sensor Networks(WSN) are a today technology with great practicability in the real world. We focus on describing WSN architecture, regarding usefulness in constructions like structural health monitoring and importance, and advantages of using WSN in this domain",2018-07-10T08:24:47Z,2018-07-10T08:24:47Z,http://arxiv.org/abs/1807.09221v1,http://arxiv.org/pdf/1807.09221v1,"eess.SP, cs.NI"
Delay Monitor Circuit for Sensitive Nodes in SRAM-Based FPGA,"Mostafa Darvishi, Yves Audet, Yves Blaquiere",This paper presents a novel monitor circuit architecture and experiments performed for detection of extra combinational delays in a high frequency SRAM-Based FPGA on delay sensitive nodes due to transient ionizing radiation.,2018-07-30T12:23:10Z,2018-07-30T12:23:10Z,http://arxiv.org/abs/1807.11311v1,http://arxiv.org/pdf/1807.11311v1,"physics.space-ph, cs.AR, physics.ins-det"
Tensorized Optical Multimodal Fusion Network,"Yequan Zhao, Xian Xiao, Geza Kurczveil, Raymond G. Beausoleil, Zheng Zhang",We propose the first tensorized optical multimodal fusion network architecture with a self-attention mechanism and low-rank tensor fusion. Simulation results show $51.3 \times$ less hardware requirement and $3.7\times 10^{13}$ MAC/J energy efficiency.,2023-02-17T07:50:35Z,2023-02-17T07:50:35Z,http://arxiv.org/abs/2302.08744v1,http://arxiv.org/pdf/2302.08744v1,"eess.SP, cs.AR"
A Low-Dissipation and Scalable GEMM Accelerator with Silicon Nitride   Photonics,"Venkata Sai Praneeth Karempudi, Sairam Sri Vatsavai, Ishan Thakkar, Oluwaseun Adewunmi Alo, Jeffrey Todd Hastings, Justin Scott Woods","Over the past few years, several microring resonator (MRR)-based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs), which are found in abundance in deep learning workloads.These architectures have dramatically grown in popularity because they offer exceptional throughput and energy efficiency compared to their electronic counterparts. However, such architectures, due to their traditional realization based on the silicon-on-insulator (SOI) material platform, face two shortcomings. First, the high-index contrast of the SOI platform incurs high scattering losses, which mandates the provisioning of high optical input power.Second, SOI waveguides are susceptible to two-photon absorption, which can incur substantial optical signal losses at moderate-to-high signal fan-in. These shortcomings have severely detrimental effects on the achievable parallelism, throughput, and energy efficiency of SOI MRR-based GEMM accelerators. To address these shortcomings, we present a novel Silicon Nitride (SiN)-Based Photonic GEMM Accelerator called SiNPhAR. SiNPhAR architecture employs SiN-based active and passive devices to implement analog GEMM functions. Since the SiN material exhibits lower index contrast and no TPA, the optical signal losses in our SiNPhAR architecture are very low. This advantage significantly enhances the achievable processing parallelism, throughput, and energy efficiency of SiNPhAR architecture, compared to SOI-based photonic GEMM accelerators from prior work. We quantify and compare these benefits of SiNPhAR architecture via our cross-layer evaluation for a benchmark workload comprising four modern deep neural network models. From the system-level performance analysis, SiNPhAR demonstrates at least 1.7x better throughput FPS while consuming at least 2.8x better energy efficiency (FPS/W) than prior SOI-based GEMM accelerators.",2024-02-16T19:50:22Z,2024-02-16T19:50:22Z,http://arxiv.org/abs/2402.11047v1,http://arxiv.org/pdf/2402.11047v1,"cs.AR, cs.ET, cs.PF, physics.optics"
Coherent transceiver architecture enabling data transmission and optical   identification,"Stella Civelli, Marco Secondini, Pantea Nadimi Goki, Luca Potì",We propose a coherent transceiver architecture able to transmit information and enhance the security of the optical network by identifying other optical systems and subsystems. Simulations show that identification is obtained with sufficient reliability in standard operating conditions.,2024-04-08T08:11:07Z,2024-04-08T08:11:07Z,http://arxiv.org/abs/2404.05278v1,http://arxiv.org/pdf/2404.05278v1,"cs.IT, math.IT"
Enabling full-speed random access to the entire memory on the A100 GPU,Alden Walker,"We describe some features of the A100 memory architecture. In particular, we give a technique to reverse-engineer some hardware layout information. Using this information, we show how to avoid TLB issues to obtain full-speed random HBM access to the entire memory, as long as we constrain any particular thread to a reduced access window of less than 64GB.",2024-05-19T02:11:30Z,2024-05-19T02:11:30Z,http://arxiv.org/abs/2405.11425v1,http://arxiv.org/pdf/2405.11425v1,"cs.PF, cs.AR, C.4; B.3.3"
A Technical Note on the Architectural Effects on Maximum Dependency   Lengths of Recurrent Neural Networks,"Jonathan S. Kent, Michael M. Murray","This work proposes a methodology for determining the maximum dependency length of a recurrent neural network (RNN), and then studies the effects of architectural changes, including the number and neuron count of layers, on the maximum dependency lengths of traditional RNN, gated recurrent unit (GRU), and long-short term memory (LSTM) models.",2024-07-19T23:00:38Z,2024-07-19T23:00:38Z,http://arxiv.org/abs/2408.11946v1,http://arxiv.org/pdf/2408.11946v1,"cs.NE, I.2.6"
LSQCA: Resource-Efficient Load/Store Architecture for Limited-Scale   Fault-Tolerant Quantum Computing,"Takumi Kobori, Yasunari Suzuki, Yosuke Ueno, Teruo Tanimoto, Synge Todo, Yuuki Tokunaga","Current fault-tolerant quantum computer (FTQC) architectures utilize several encoding techniques to enable reliable logical operations with restricted qubit connectivity. However, such logical operations demand additional memory overhead to ensure fault tolerance. Since the main obstacle to practical quantum computing is the limited qubit count, our primary mission is to design floorplans that can reduce memory overhead without compromising computational capability. Despite extensive efforts to explore FTQC architectures, even the current state-of-the-art floorplan strategy devotes 50% of memory space to this overhead, not to data storage, to ensure unit-time random access to all logical qubits.   In this paper, we propose an FTQC architecture based on a novel floorplan strategy, Load/Store Quantum Computer Architecture (LSQCA), which can achieve almost 100% memory density. The idea behind our architecture is to separate all memory regions into small computational space called Computational Registers (CR) and space-efficient memory space called Scan-Access Memory (SAM). We define an instruction set for these abstract structures and provide concrete designs named point-SAM and line-SAM architectures. With this design, we can improve the memory density by allowing variable-latency memory access while concealing the latency with other bottlenecks. We also propose optimization techniques to exploit properties of quantum programs observed in our static analysis, such as access locality in memory reference timestamps. Our numerical results indicate that LSQCA successfully leverages this idea. In a resource-restricted situation, a specific benchmark shows that we can achieve about 90% memory density with 5% increase in the execution time compared to a conventional floorplan, which achieves at most 50% memory density for unit-time random access. Our design ensures broad quantum applicability.",2024-12-29T14:58:23Z,2024-12-29T14:58:23Z,http://arxiv.org/abs/2412.20486v1,http://arxiv.org/pdf/2412.20486v1,"quant-ph, cs.AR"
A Modular and Flexible Architecture for an Integrated Corpus Query   System,Oliver Christ,"The paper describes the architecture of an integrated and extensible corpus query system developed at the University of Stuttgart and gives examples of some of the modules realized within this architecture. The modules form the core of a corpus workbench. Within the proposed architecture, information required for the evaluation of queries may be derived from different knowledge sources (the corpus text, databases, on-line thesauri) and by different means: either through direct lookup in a database or by calling external tools which may infer the necessary information at the time of query evaluation. The information available and the method of information access can be stated declaratively and individually for each corpus, leading to a flexible, extensible and modular corpus workbench.",1994-08-02T13:41:45Z,1994-08-02T13:41:45Z,http://arxiv.org/abs/cmp-lg/9408005v1,http://arxiv.org/pdf/cmp-lg/9408005v1,"cmp-lg, cs.CL"
"Has a Consensus NL Generation Architecture Appeared, and is it   Psycholinguistically Plausible?",Ehud Reiter,"I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this `consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.",1994-11-30T18:44:10Z,1994-11-30T18:44:10Z,http://arxiv.org/abs/cmp-lg/9411032v1,http://arxiv.org/pdf/cmp-lg/9411032v1,"cmp-lg, cs.CL"
Data Flies Standby with ABR Service,R. Jain,Explanation of ABR service in plain language.,1998-09-24T05:31:55Z,1998-09-24T05:31:55Z,http://arxiv.org/abs/cs/9809100v1,http://arxiv.org/pdf/cs/9809100v1,"cs.NI, C.2.1"
Flysig: Dataflow Oriented Delay-Insensitive Processor for Rapid   Prototyping of Signal Processing,"Wolfram Hardt, Bernd Kleinjohann","As the one-chip integration of HW-modules designed by different companies becomes more and more popular reliability of a HW-design and evaluation of the timing behavior during the prototype stage are absolutely necessary. One way to guarantee reliability is the use of robust design styles, e.g., delay-insensitivity. For early timing evaluation two aspects must be considered: a) The timing needs to be proportional to technology variations and b) the implemented architecture should be identical for prototype and target. The first can be met also by delay-insensitive implementation. The latter one is the key point. A unified architecture is needed for prototyping as well as implementation. Our new approach to rapid prototyping of signal processing tasks is based on a configurable, delay-insensitive implemented processor called Flysig. In essence, the Flysig processor can be understood as a complex FPGA where the CLBs are substituted by bit-serial operators. In this paper the general concept is detailed and first experimental results are given for demonstration of the main advantages: delay-insensitive design style, direct correspondence between prototyping and target architecture, high performance and reasonable shortening of the design cycle.",1998-10-12T10:11:05Z,1998-10-12T10:11:05Z,http://arxiv.org/abs/cs/9810011v1,http://arxiv.org/pdf/cs/9810011v1,"cs.AR, C.1.3; C.3"
SNS Timing System,"B. oerter, R. Nelson, T. Shea, C. Sibley",This poster describes the timing system being designed for Spallation Neutron Source being built at Oak Ridge National lab.,2001-11-09T19:08:57Z,2001-11-09T19:08:57Z,http://arxiv.org/abs/cs/0111032v1,http://arxiv.org/pdf/cs/0111032v1,"cs.AR, B.m"
Developing Intellectual Network Management Facilities by Means of   Pattern Recognition Theory,Yuriy A. Chashkov,In this paper considered question of using pattern recognition methods in network equipment state identification.,2004-05-26T11:46:36Z,2004-05-26T11:46:36Z,http://arxiv.org/abs/cs/0405096v1,http://arxiv.org/pdf/cs/0405096v1,"cs.NI, C.2.3"
A Formal Architecture-Centric Model-Driven Approach for the Automatic   Generation of Grid Applications,"David Manset, Herve Verjus, Richard McClatchey, Flavio Oquendo","This paper discusses the concept of model-driven software engineering applied to the Grid application domain. As an extension to this concept, the approach described here, attempts to combine both formal architecture-centric and model-driven paradigms. It is a commonly recognized statement that Grid systems have seldom been designed using formal techniques although from past experience such techniques have shown advantages. This paper advocates a formal engineering approach to Grid system developments in an effort to contribute to the rigorous development of Grids software architectures. This approach addresses quality of service and cross-platform developments by applying the model-driven paradigm to a formal architecture-centric engineering method. This combination benefits from a formal semantic description power in addition to model-based transformations. The result of such a novel combined concept promotes the re-use of design models and facilitates developments in Grid computing.",2006-01-28T00:08:30Z,2006-01-28T00:08:30Z,http://arxiv.org/abs/cs/0601118v1,http://arxiv.org/pdf/cs/0601118v1,"cs.SE, D.2.11"
A SIMO Fiber Aided Wireless Network Architecture,"Siddharth Ray, Muriel Medard, Lizhong Zheng","The concept of a fiber aided wireless network architecture (FAWNA) is introduced in [Ray et al., Allerton Conference 2005], which allows high-speed mobile connectivity by leveraging the speed of optical networks. In this paper, we consider a single-input, multiple-output (SIMO) FAWNA, which consists of a SIMO wireless channel and an optical fiber channel, connected through wireless-optical interfaces. We propose a scheme where the received wireless signal at each interface is quantized and sent over the fiber. Though our architecture is similar to that of the classical CEO problem, our problem is different from it. We show that the capacity of our scheme approaches the capacity of the architecture, exponentially with fiber capacity. We also show that for a given fiber capacity, there is an optimal operating wireless bandwidth and an optimal number of wireless-optical interfaces. The wireless-optical interfaces of our scheme have low complexity and do not require knowledge of the transmitter code book. They are also extendable to FAWNAs with large number of transmitters and interfaces and, offer adaptability to variable rates, changing channel conditions and node positions.",2006-03-25T00:46:44Z,2006-03-25T00:46:44Z,http://arxiv.org/abs/cs/0603098v1,http://arxiv.org/pdf/cs/0603098v1,"cs.IT, math.IT"
The NorduGrid architecture and tools,"P. Eerola, T. Ekelof, M. Ellert, J. R. Hansen, A. Konstantinov, B. Konya, J. L. Nielsen, F. Ould-Saada, O. Smirnova, A. Waananen","The NorduGrid project designed a Grid architecture with the primary goal to meet the requirements of production tasks of the LHC experiments. While it is meant to be a rather generic Grid system, it puts emphasis on batch processing suitable for problems encountered in High Energy Physics. The NorduGrid architecture implementation uses the \globus{} as the foundation for various components, developed by the project. While introducing new services, the NorduGrid does not modify the Globus tools, such that the two can eventually co-exist. The NorduGrid topology is decentralized, avoiding a single point of failure. The NorduGrid architecture is thus a light-weight, non-invasive and dynamic one, while robust and scalable, capable of meeting most challenging tasks of High Energy Physics.",2003-05-31T04:05:19Z,2003-05-31T04:05:19Z,http://arxiv.org/abs/physics/0306002v1,http://arxiv.org/pdf/physics/0306002v1,"physics.comp-ph, cs.DC"
On the operating unit size of load/store architectures,"J. A. Bergstra, C. A. Middelburg","We introduce a strict version of the concept of a load/store instruction set architecture in the setting of Maurer machines. We take the view that transformations on the states of a Maurer machine are achieved by applying threads as considered in thread algebra to the Maurer machine. We study how the transformations on the states of the main memory of a strict load/store instruction set architecture that can be achieved by applying threads depend on the operating unit size, the cardinality of the instruction set, and the maximal number of states of the threads.",2007-11-06T11:16:34Z,2008-11-25T08:46:40Z,http://arxiv.org/abs/0711.0838v2,http://arxiv.org/pdf/0711.0838v2,"cs.AR, C.0; F.1.1; F.1.3"
The aDORe Federation Architecture,"Herbert Van de Sompel, Ryan Chute, Patrick Hochstenbach","The need to federate repositories emerges in two distinctive scenarios. In one scenario, scalability-related problems in the operation of a repository reach a point beyond which continued service requires parallelization and hence federation of the repository infrastructure. In the other scenario, multiple distributed repositories manage collections of interest to certain communities or applications, and federation is an approach to present a unified perspective across these repositories. The high-level, 3-Tier aDORe federation architecture can be used as a guideline to federate repositories in both cases. This paper describes the architecture, consisting of core interfaces for federated repositories in Tier-1, two shared infrastructure components in Tier-2, and a single-point of access to the federation in Tier-3. The paper also illustrates two large-scale deployments of the aDORe federation architecture: the aDORe Archive repository (over 100,000,000 digital objects) at the Los Alamos National Laboratory and the Ghent University Image Repository federation (multiple terabytes of image files).",2008-03-31T18:02:57Z,2008-03-31T18:02:57Z,http://arxiv.org/abs/0803.4511v1,http://arxiv.org/pdf/0803.4511v1,"cs.DL, H.3.7"
Archer: A Community Distributed Computing Infrastructure for Computer   Architecture Research and Education,"Renato Figueiredo, P. Oscar Boykin, Jose A. B. Fortes, Tao Li, Jie-Kwon Peir, David Wolinsky, Lizy John, David Kaeli, David Lilja, Sally McKee, Gokhan Memik, Alain Roy, Gary Tyson","This paper introduces Archer, a community-based computing resource for computer architecture research and education. The Archer infrastructure integrates virtualization and batch scheduling middleware to deliver high-throughput computing resources aggregated from resources distributed across wide-area networks and owned by different participating entities in a seamless manner. The paper discusses the motivations leading to the design of Archer, describes its core middleware components, and presents an analysis of the functionality and performance of a prototype wide-area deployment running a representative computer architecture simulation workload.",2008-07-11T02:47:55Z,2008-07-11T02:47:55Z,http://arxiv.org/abs/0807.1765v1,http://arxiv.org/pdf/0807.1765v1,"cs.AR, C.0; I.6.3; C.2.4"
On the Effect of Quantum Interaction Distance on Quantum Addition   Circuits,"Byung-Soo Choi, Rodney Van Meter","We investigate the theoretical limits of the effect of the quantum interaction distance on the speed of exact quantum addition circuits. For this study, we exploit graph embedding for quantum circuit analysis. We study a logical mapping of qubits and gates of any $\Omega(\log n)$-depth quantum adder circuit for two $n$-qubit registers onto a practical architecture, which limits interaction distance to the nearest neighbors only and supports only one- and two-qubit logical gates. Unfortunately, on the chosen $k$-dimensional practical architecture, we prove that the depth lower bound of any exact quantum addition circuits is no longer $\Omega(\log {n})$, but $\Omega(\sqrt[k]{n})$. This result, the first application of graph embedding to quantum circuits and devices, provides a new tool for compiler development, emphasizes the impact of quantum computer architecture on performance, and acts as a cautionary note when evaluating the time performance of quantum algorithms.",2008-09-25T03:59:24Z,2010-12-24T02:55:11Z,http://arxiv.org/abs/0809.4317v5,http://arxiv.org/pdf/0809.4317v5,"quant-ph, cs.AR, C.1.m; B.2.0; B.m"
CRT-Based High Speed Parallel Architecture for Long BCH Encoding,Hao Chen,"BCH (Bose-Chaudhuri-Hocquenghen) error correcting codes ([1]-[2]) are now widely used in communication systems and digital technology. Direct LFSR(linear feedback shifted register)-based encoding of a long BCH code suffers from serial-in and serial-out limitation and large fanout effect of some XOR gates. This makes the LFSR-based encoders of long BCH codes cannot keep up with the data transmission speed in some applications. Several parallel long parallel encoders for long cyclic codes have been proposed in [3]-[8]. The technique for eliminating the large fanout effect by J-unfolding method and some algebraic manipulation was presented in [7] and [8] . In this paper we propose a CRT(Chinese Remainder Theorem)-based parallel architecture for long BCH encoding. Our novel technique can be used to eliminate the fanout bottleneck. The only restriction on the speed of long BCH encoding of our CRT-based architecture is $log_2N$, where $N$ is the length of the BCH code.",2009-04-21T00:34:43Z,2009-04-21T00:34:43Z,http://arxiv.org/abs/0904.3148v1,http://arxiv.org/pdf/0904.3148v1,"cs.AR, cs.IT, math.IT"
Data Distribution Optimization using Offline Algorithms and a   Peer-to-Peer Small Diameter Tree Architecture with Bounded Node Degrees,"Mugurel Ionut Andreica, Eliana-Dina Tirsa, Nicolae Tapus","Multicast data transfers occur in many distributed systems and applications (e.g. IPTV, Grids, content delivery networks). Because of this, efficient multicast data distribution optimization techniques are required. In the first part of this paper we present a small diameter, bounded degree, collaborative peer-to-peer multicast tree architecture, which supports dynamic node arrivals and departures making local decisions only. The architecture is fault tolerant and, at low arrival and departure rates, converges towards a theoretically optimal structure. In the second part of the paper we consider several offline data distribution optimization problems, for which we present novel and time-efficient algorithmic solutions.",2009-06-01T21:07:18Z,2009-06-01T21:07:18Z,http://arxiv.org/abs/0906.0379v1,http://arxiv.org/pdf/0906.0379v1,"cs.DC, cs.DS, cs.NI, C.2.1; C.2.4; G.2.1; G.2.2"
Computing of Applied Digital Ecosystems,"G. Briscoe, P. De Wilde","A primary motivation for our research in digital ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex, dynamic problems. However, the computing technologies that contribute to these properties have not been made explicit in digital ecosystems research. Here, we discuss how different computing technologies can contribute to providing the necessary self-organising features, including Multi-Agent Systems, Service-Oriented Architectures, and distributed evolutionary computing. The potential for exploiting these properties in digital ecosystems is considered, suggesting how several key features of biological ecosystems can be exploited in Digital Ecosystems, and discussing how mimicking these features may assist in developing robust, scalable self-organising architectures. An example architecture, the Digital Ecosystem, is considered in detail. The Digital Ecosystem is then measured experimentally through simulations, considering the self-organised diversity of its evolving agent populations relative to the user request behaviour.",2009-10-05T04:29:29Z,2009-10-05T04:29:29Z,http://arxiv.org/abs/0910.0674v1,http://arxiv.org/pdf/0910.0674v1,"cs.NE, cs.MA, C.2.4; D.2.11; H.1.0"
Flare: Architecture for rapid and easy development of Internet-based   Applications,"Shashank Shekhar, Mohit Soni, NVSN Kalyan Chakravarthy","We propose an architecture, Flare, that is a structured and easy way to develop applications rapidly, in a multitude of languages, which make use of online storage of data and management of users. The architecture eliminates the need for server-side programming in most cases, creation and management of online database storage servers, re-creation of user management schemes and writing a lot of unnecessary code for accessing different web-based services using their APIs. A Web API provides a common API for various web-based services like Blogger [2], Wordpress, MSN Live, Facebook [3] etc. Access Libraries provided for major programming languages and platforms make it easy to develop applications using the Flare Web Service. We demonstrate a simple micro-blogging service developed using these APIs in two modes: a graphical browser-based mode, and a command-line mode in C++, which provide two different interfaces to the same account and data.",2009-11-17T22:25:49Z,2009-11-17T22:25:49Z,http://arxiv.org/abs/0911.3418v1,http://arxiv.org/pdf/0911.3418v1,"cs.NI, K.6.3; H.3.4; H.3.3; K.6.5"
Flow Splitting with Fate Sharing in a Next Generation Transport Services   Architecture,"Janardhan Iyengar, Bryan Ford","The challenges of optimizing end-to-end performance over diverse Internet paths has driven widespread adoption of in-path optimizers, which can destructively interfere with TCP's end-to-end semantics and with each other, and are incompatible with end-to-end IPsec. We identify the architectural cause of these conflicts and resolve them in Tng, an experimental next-generation transport services architecture, by factoring congestion control from end-to-end semantic functions. Through a technique we call ""queue sharing"", Tng enables in-path devices to interpose on, split, and optimize congestion controlled flows without affecting or seeing the end-to-end content riding these flows. Simulations show that Tng's decoupling cleanly addresses several common performance problems, such as communication over lossy wireless links and reduction of buffering-induced latency on residential links. A working prototype and several incremental deployment paths suggest Tng's practicality.",2009-12-04T19:36:43Z,2009-12-04T19:36:43Z,http://arxiv.org/abs/0912.0921v1,http://arxiv.org/pdf/0912.0921v1,"cs.NI, C.2.5"
Queue-Architecture and Stability Analysis in Cooperative Relay Networks,"Jubin Jose, Sriram Vishwanath","An abstraction of the physical layer coding using bit pipes that are coupled through data-rates is insufficient to capture notions such as node cooperation in cooperative relay networks. Consequently, network-stability analyses based on such abstractions are valid for non-cooperative schemes alone and meaningless for cooperative schemes. Motivated from this, this paper develops a framework that brings the information-theoretic coding scheme together with network-stability analysis. This framework does not constrain the system to any particular achievable scheme, i.e., the relays can use any cooperative coding strategy of its choice, be it amplify/compress/quantize or any alter-and-forward scheme. The paper focuses on the scenario when coherence duration is of the same order of the packet/codeword duration, the channel distribution is unknown and the fading state is only known causally. The main contributions of this paper are two-fold: first, it develops a low-complexity queue-architecture to enable stable operation of cooperative relay networks, and, second, it establishes the throughput optimality of a simple network algorithm that utilizes this queue-architecture.",2010-07-07T22:40:08Z,2010-07-07T22:40:08Z,http://arxiv.org/abs/1007.1255v1,http://arxiv.org/pdf/1007.1255v1,"cs.NI, cs.IT, math.IT"
An Optimal Controller Architecture for Poset-Causal Systems,"Parikshit Shah, Pablo Parrilo","We propose a novel and natural architecture for decentralized control that is applicable whenever the underlying system has the structure of a partially ordered set (poset). This controller architecture is based on the concept of Moebius inversion for posets, and enjoys simple and appealing separation properties, since the closed-loop dynamics can be analyzed in terms of decoupled subsystems. The controller structure provides rich and interesting connections between concepts from order theory such as Moebius inversion and control-theoretic concepts such as state prediction, correction, and separability. In addition, using our earlier results on H_2-optimal decentralized control for arbitrary posets, we prove that the H_2-optimal controller in fact possesses the proposed structure, thereby establishing the optimality of the new controller architecture.",2011-11-30T15:56:18Z,2011-11-30T15:56:18Z,http://arxiv.org/abs/1111.7221v1,http://arxiv.org/pdf/1111.7221v1,"math.OC, cs.SY"
A Secured Health Care Application Architecture for Cyber-Physical   Systems,"Jin Wang, Hassan Abid, Sungyoung Lee, Lei Shu, Feng Xia","Cyber-physical systems (CPS) can be viewed as a new generation of systems with integrated control, communication and computational capabilities. Like the internet transformed how humans interact with one another, cyber-physical systems will transform how people interact with the physical world. Currently, the study of CPS is still in its infancy and there exist many research issues and challenges ranging from electricity power, health care, transportation and smart building etc. In this paper, an introduction of CPeSC3 (cyber physical enhanced secured wireless sensor networks (WSNs) integrated cloud computing for u-life care) architecture and its application to the health care monitoring and decision support systems is given. The proposed CPeSC3 architecture is composed of three main components, namely 1) communication core, 2) computation core, and 3) resource scheduling and management core. Detailed analysis and explanation are given for relevant models such as cloud computing, real time scheduling and security models. Finally, a medical health care application scenario is presented based on our practical test-bed which has been built for 3 years.",2011-12-31T02:25:24Z,2011-12-31T02:25:24Z,http://arxiv.org/abs/1201.0213v1,http://arxiv.org/pdf/1201.0213v1,"cs.NI, 68M14, C.0"
Optimal Quantum Circuits for Nearest-Neighbor Architectures,David Rosenbaum,"We show that the depth of quantum circuits in the realistic architecture where a classical controller determines which local interactions to apply on the kD grid Z^k where k >= 2 is the same (up to a constant factor) as in the standard model where arbitrary interactions are allowed. This allows minimum-depth circuits (up to a constant factor) for the nearest-neighbor architecture to be obtained from minimum-depth circuits in the standard abstract model. Our work therefore justifies the standard assumption that interactions can be performed between arbitrary pairs of qubits. In particular, our results imply that Shor's algorithm, controlled operations and fanouts can be implemented in constant depth, polynomial size and polynomial width in this architecture.   We also present optimal non-adaptive quantum circuits for controlled operations and fanouts on a kD grid. These circuits have depth Theta(n^(1 / k)), size Theta(n) and width Theta(n). Our lower bound also applies to a more general class of operations.",2012-04-30T21:35:44Z,2013-05-08T01:50:41Z,http://arxiv.org/abs/1205.0036v3,http://arxiv.org/pdf/1205.0036v3,"quant-ph, cs.CC"
Tiled Algorithms for Matrix Computations on Multicore Architectures,Henricus Bouwmeester,"The current computer architecture has moved towards the multi/many-core structure. However, the algorithms in the current sequential dense numerical linear algebra libraries (e.g. LAPACK) do not parallelize well on multi/many-core architectures. A new family of algorithms, the tile algorithms, has recently been introduced to circumvent this problem. Previous research has shown that it is possible to write efficient and scalable tile algorithms for performing a Cholesky factorization, a (pseudo) LU factorization, and a QR factorization. The goal of this thesis is to study tiled algorithms in a multi/many-core setting and to provide new algorithms which exploit the current architecture to improve performance relative to current state-of-the-art libraries while maintaining the stability and robustness of these libraries.",2013-03-13T15:04:05Z,2013-03-13T15:04:05Z,http://arxiv.org/abs/1303.3182v1,http://arxiv.org/pdf/1303.3182v1,"cs.NA, cs.MS, math.NA"
Action-based Character AI in Video-games with CogBots Architecture: A   Preliminary Report,"Davide Aversa, Stavros Vassos","In this paper we propose an architecture for specifying the interaction of non-player characters (NPCs) in the game-world in a way that abstracts common tasks in four main conceptual components, namely perception, deliberation, control, action. We argue that this architecture, inspired by AI research on autonomous agents and robots, can offer a number of benefits in the form of abstraction, modularity, re-usability and higher degrees of personalization for the behavior of each NPC. We also show how this architecture can be used to tackle a simple scenario related to the navigation of NPCs under incomplete information about the obstacles that may obstruct the various way-points in the game, in a simple and effective way.",2013-07-11T17:38:24Z,2013-07-11T17:38:24Z,http://arxiv.org/abs/1307.3195v1,http://arxiv.org/pdf/1307.3195v1,"cs.AI, cs.SE, H.5.1; I.2.11"
ManyClaw: Slicing and dicing Riemann solvers for next generation highly   parallel architectures,"Andy R. Terrel, Kyle T. Mandli","Next generation computer architectures will include order of magnitude more intra-node parallelism; however, many application programmers have a difficult time keeping their codes current with the state-of-the-art machines. In this context, we analyze Hyperbolic PDE solvers, which are used in the solution of many important applications in science and engineering. We present ManyClaw, a project intended to explore the exploitation of intra-node parallelism in hyperbolic PDE solvers via the Clawpack software package for solving hyperbolic PDEs. Our goal is to separate the low level parallelism and the physical equations thus providing users the capability to leverage intra-node parallelism without explicitly writing code to take advantage of newer architectures.",2013-08-07T02:24:20Z,2013-08-07T02:24:20Z,http://arxiv.org/abs/1308.1464v1,http://arxiv.org/pdf/1308.1464v1,"cs.CE, cs.MS, C.1.4; D.1.3; G.1.8"
Partial Sums Computation In Polar Codes Decoding,"Guillaume Berhault, Camille Leroux, Christophe Jego, Dominique Dallet","Polar codes are the first error-correcting codes to provably achieve the channel capacity but with infinite codelengths. For finite codelengths the existing decoder architectures are limited in working frequency by the partial sums computation unit. We explain in this paper how the partial sums computation can be seen as a matrix multiplication. Then, an efficient hardware implementation of this product is investigated. It has reduced logic resources and interconnections. Formalized architectures, to compute partial sums and to generate the bits of the generator matrix k^n, are presented. The proposed architecture allows removing the multiplexing resources used to assigned to each processing elements the required partial sums.",2013-10-07T09:29:02Z,2015-01-09T14:11:44Z,http://arxiv.org/abs/1310.1712v2,http://arxiv.org/pdf/1310.1712v2,"cs.AR, cs.IT, math.IT"
Delay Learning Architectures for Memory and Classification,"Shaista Hussain, Arindam Basu, R. Wang, Tara Julia Hamilton","We present a neuromorphic spiking neural network, the DELTRON, that can remember and store patterns by changing the delays of every connection as opposed to modifying the weights. The advantage of this architecture over traditional weight based ones is simpler hardware implementation without multipliers or digital-analog converters (DACs) as well as being suited to time-based computing. The name is derived due to similarity in the learning rule with an earlier architecture called Tempotron. The DELTRON can remember more patterns than other delay-based networks by modifying a few delays to remember the most 'salient' or synchronous part of every spike pattern. We present simulations of memory capacity and classification ability of the DELTRON for different random spatio-temporal spike patterns. The memory capacity for noisy spike patterns and missing spikes are also shown. Finally, we present SPICE simulation results of the core circuits involved in a reconfigurable mixed signal implementation of this architecture.",2013-11-06T06:10:30Z,2014-02-27T07:26:37Z,http://arxiv.org/abs/1311.1294v2,http://arxiv.org/pdf/1311.1294v2,"cs.NE, q-bio.NC"
Towards Big Topic Modeling,"Jian-Feng Yan, Jia Zeng, Zhi-Qiang Liu, Yang Gao","To solve the big topic modeling problem, we need to reduce both time and space complexities of batch latent Dirichlet allocation (LDA) algorithms. Although parallel LDA algorithms on the multi-processor architecture have low time and space complexities, their communication costs among processors often scale linearly with the vocabulary size and the number of topics, leading to a serious scalability problem. To reduce the communication complexity among processors for a better scalability, we propose a novel communication-efficient parallel topic modeling architecture based on power law, which consumes orders of magnitude less communication time when the number of topics is large. We combine the proposed communication-efficient parallel architecture with the online belief propagation (OBP) algorithm referred to as POBP for big topic modeling tasks. Extensive empirical results confirm that POBP has the following advantages to solve the big topic modeling problem: 1) high accuracy, 2) communication-efficient, 3) fast speed, and 4) constant memory usage when compared with recent state-of-the-art parallel LDA algorithms on the multi-processor architecture.",2013-11-17T11:52:42Z,2013-11-17T11:52:42Z,http://arxiv.org/abs/1311.4150v1,http://arxiv.org/pdf/1311.4150v1,"cs.LG, cs.DC, cs.IR, stat.ML"
The complex architecture of primes and natural numbers,"Guillermo Garcia-Perez, M. Angeles Serrano, Marian Boguna","Natural numbers can be divided in two non-overlapping infinite sets, primes and composites, with composites factorizing into primes. Despite their apparent simplicity, the elucidation of the architecture of natural numbers with primes as building blocks remains elusive. Here, we propose a new approach to decoding the architecture of natural numbers based on complex networks and stochastic processes theory. We introduce a parameter-free non-Markovian dynamical model that naturally generates random primes and their relation with composite numbers with remarkable accuracy. Our model satisfies the prime number theorem as an emerging property and a refined version of Cram\'er's conjecture about the statistics of gaps between consecutive primes that seems closer to reality than the original Cram\'er's version. Regarding composites, the model helps us to derive the prime factors counting function, giving the probability of distinct prime factors for any integer. Probabilistic models like ours can help to get deeper insights about primes and the complex architecture of natural numbers.",2014-02-14T21:56:16Z,2014-07-24T11:05:51Z,http://arxiv.org/abs/1402.3612v2,http://arxiv.org/pdf/1402.3612v2,"math.NT, physics.soc-ph"
LIQUi|>: A Software Design Architecture and Domain-Specific Language for   Quantum Computing,"Dave Wecker, Krysta M. Svore","Languages, compilers, and computer-aided design tools will be essential for scalable quantum computing, which promises an exponential leap in our ability to execute complex tasks. LIQUi|> is a modular software architecture designed to control quantum hardware. It enables easy programming, compilation, and simulation of quantum algorithms and circuits, and is independent of a specific quantum architecture. LIQUi|> contains an embedded, domain-specific language designed for programming quantum algorithms, with F# as the host language. It also allows the extraction of a circuit data structure that can be used for optimization, rendering, or translation. The circuit can also be exported to external hardware and software environments. Two different simulation environments are available to the user which allow a trade-off between number of qubits and class of operations. LIQUi|> has been implemented on a wide range of runtimes as back-ends with a single user front-end. We describe the significant components of the design architecture and how to express any given quantum algorithm.",2014-02-18T20:46:01Z,2014-02-18T20:46:01Z,http://arxiv.org/abs/1402.4467v1,http://arxiv.org/pdf/1402.4467v1,"quant-ph, cs.ET, cs.PL"
High Throughput and Less Area AMP Architecture for Audio Signal   Restoration,"Swetha. R, Rukmani Devi. D","Audio restoration is effectively achieved by using low complexity algorithm called AMP. This algorithm has fast convergence and has lower computation intensity making it suitable for audio recovery problems. This paper focuses on restoring an audio signal by using VLSI architecture called AMP-M that implements AMP algorithm. This architecture employs MAC unit with fixed bit Wallace tree multiplier, FFT-MUX and various memory units (RAM) for audio restoration. VLSI and FPGA implementation results shows that reduced area, high throughput, low power is achieved making it suitable for real time audio recovery problems. Prominent examples are Magnetic Resonance Imaging (MRI), Radar and Wireless Communications.",2014-04-05T11:51:35Z,2014-04-05T11:51:35Z,http://arxiv.org/abs/1404.1468v1,http://arxiv.org/pdf/1404.1468v1,"cs.SD, cs.IT, math.IT"
Massive Antenna Arrays with Low Front-End Hardware Complexity: An   Enabling Technology for the Emerging Small Cell and Distributed Network   Architectures,"Vlasis I. Barousis, Mohammad A. Sedaghat, Ralf R. Müller, Constantinos B. Papadias","This paper presents the current state-of-the-art of massive antenna array architectures with significant front-end hardware savings, as an enabler for future small and powerful cell nodes that will be able to carry massive MIMO technology. Radio frequency (RF) hardware architectures with a single power amplifier are reviewed, compared, and found superior to conventional MIMO implementations in terms of cost, dissipated heat, and physical size. This progress on the RF-side allows to merge the two competing cellular concepts of virtual and massive MIMO into a hybrid approach of remote radio heads with massive MIMO arrays.",2014-07-29T14:19:19Z,2014-07-29T14:19:19Z,http://arxiv.org/abs/1407.7733v1,http://arxiv.org/pdf/1407.7733v1,"cs.NI, cs.IT, math.IT"
Technologies for Web and cloud service interaction: a survey,Harald Lampesberger,"The evolution of Web and service technologies has led to a wide landscape of standards and protocols for interaction between loosely coupled software components. Examples range from Web applications, mashups, apps, and mobile devices to enterprise-grade services. Cloud computing is the industrialization of service provision and delivery, where Web and enterprise services are converging on a technological level. The article discusses this technological landscape and, in particular, current trends with respect to cloud computing. The survey focuses on the communication aspect of interaction by reviewing languages, protocols, and architectures that drive today's standards and software implementations applicable in clouds. Technological advances will affect both client side and service side. There is a trend toward multiplexing, multihoming, and encryption in upcoming transport mechanisms, especially for architectures, where a client simultaneously sends a large number of requests to some service. Furthermore, there are emerging client-to-client communication capabilities in Web clients that could establish a foundation for upcoming Web-based messaging architectures.",2014-08-12T15:40:51Z,2015-03-09T12:19:32Z,http://arxiv.org/abs/1408.2751v3,http://arxiv.org/pdf/1408.2751v3,"cs.NI, cs.DC, A.1; C.2.2; C.2.4; H.3.4; H.3.5"
Old Wine in New Skins? Revisiting the Software Architecture for IP   Network Stacks on Constrained IoT Devices,"Hauke Petersen, Martine Lenders, Matthias Wählisch, Oliver Hahm, Emmanuel Baccelli","In this paper, we argue that existing concepts for the design and implementation of network stacks for constrained devices do not comply with the requirements of current and upcoming Internet of Things (IoT) use cases. The IoT requires not only a lightweight but also a modular network stack, based on standards. We discuss functional and non-functional requirements for the software architecture of the network stack on constrained IoT devices. Then, revisiting concepts from the early Internet as well as current implementations, we propose a future-proof alternative to existing IoT network stack architectures, and provide an initial evaluation of this proposal based on its implementation running on top of state-of-the-art IoT operating system and hardware.",2015-02-06T17:45:10Z,2015-02-06T17:45:10Z,http://arxiv.org/abs/1502.01968v1,http://arxiv.org/pdf/1502.01968v1,"cs.NI, C.2.1; C.2.2; C.3; D.4.4"
Overlapped-MIMO Radar Waveform Design for Coexistence With Communication   Systems,"Chowdhury Shahriar, Ahmed Abdelhadi, T. Charles Clancy","This paper explores an overlapped-multiple-input multiple-output (MIMO) antenna architecture and a spectrum sharing algorithm via null space projection (NSP) for radar-communications coexistence. In the overlapped-MIMO architecture, the transmit array of a collocated MIMO radar is partitioned into a number of subarrays that are allowed to overlap. Each of the antenna elements in these subarrays have signals orthogonal to each other and to the elements of the other subarrays. The proposed architecture not only improves sidelobe suppression to reduce interference to communications system, but also enjoys the advantages of MIMO radar without sacrificing the main desirable characteristics. The radar-centric spectrum sharing algorithm then projects the radar signal onto the null space of the communications system's interference channel, which helps to avoid interference from the radar. Numerical results are presented which show the performance of the proposed waveform design algorithm in terms of overall beampattern and sidelobe levels of the radar waveform and finally shows a comparison of the proposed system with existing collocated MIMO radar architectures.",2015-02-13T20:57:07Z,2015-02-13T20:57:07Z,http://arxiv.org/abs/1502.04117v1,http://arxiv.org/pdf/1502.04117v1,"cs.IT, math.IT"
Universal Memory Architectures for Autonomous Machines,"Dan P. Guralnik, Daniel E. Koditschek","We propose a self-organizing memory architecture for perceptual experience, capable of supporting autonomous learning and goal-directed problem solving in the absence of any prior information about the agent's environment. The architecture is simple enough to ensure (1) a quadratic bound (in the number of available sensors) on space requirements, and (2) a quadratic bound on the time-complexity of the update-execute cycle. At the same time, it is sufficiently complex to provide the agent with an internal representation which is (3) minimal among all representations of its class which account for every sensory equivalence class subject to the agent's belief state; (4) capable, in principle, of recovering the homotopy type of the system's state space; (5) learnable with arbitrary precision through a random application of the available actions. The provable properties of an effectively trained memory structure exploit a duality between weak poc sets -- a symbolic (discrete) representation of subset nesting relations -- and non-positively curved cubical complexes, whose rich convexity theory underlies the planning cycle of the proposed architecture.",2015-02-21T19:11:23Z,2015-02-21T19:11:23Z,http://arxiv.org/abs/1502.06132v1,http://arxiv.org/pdf/1502.06132v1,"cs.AI, cs.LG, cs.RO, math.MG"
Strategies for High-Throughput FPGA-based QC-LDPC Decoder Architecture,"Swapnil Mhaske, Hojin Kee, Tai Ly, Ahsan Aziz, Predrag Spasojevic","We propose without loss of generality strategies to achieve a high-throughput FPGA-based architecture for a QC-LDPC code based on a circulant-1 identity matrix construction. We present a novel representation of the parity-check matrix (PCM) providing a multi-fold throughput gain. Splitting of the node processing algorithm enables us to achieve pipelining of blocks and hence layers. By partitioning the PCM into not only layers but superlayers we derive an upper bound on the pipelining depth for the compact representation. To validate the architecture, a decoder for the IEEE 802.11n (2012) QC-LDPC is implemented on the Xilinx Kintex-7 FPGA with the help of the FPGA IP compiler [2] available in the NI LabVIEW Communication System Design Suite (CSDS) which offers an automated and systematic compilation flow where an optimized hardware implementation from the LDPC algorithm was generated in approximately 3 minutes, achieving an overall throughput of 608Mb/s (at 260MHz). As per our knowledge this is the fastest implementation of the IEEE 802.11n QC-LDPC decoder using an algorithmic compiler.",2015-03-10T16:54:39Z,2015-05-11T17:44:13Z,http://arxiv.org/abs/1503.02986v3,http://arxiv.org/pdf/1503.02986v3,"cs.AR, cs.IT, math.IT"
Highway Networks,"Rupesh Kumar Srivastava, Klaus Greff, Jürgen Schmidhuber","There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on ""information highways"". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.",2015-05-03T01:56:57Z,2015-11-03T18:15:15Z,http://arxiv.org/abs/1505.00387v2,http://arxiv.org/pdf/1505.00387v2,"cs.LG, cs.NE, 68T01, I.2.6; G.1.6"
Hyper Heterogeneous Cloud-based IMS Software Architecture: A   Proof-of-Concept and Empirical Analysis,"Pascal Potvin, Hanen Garcia Gamardo, Kim-Khoa Nguyen, Mohamed Cheriet","The IP Multimedia Subsystem (IMS) defined by the 3GPP has been mainly developed and deployed by telephony vendors on vendor-specific hardware. Recent advances in Network Function Virtualisation (NFV) technology paved the way for virtualized hardware and telephony function elasticity. As such, Telecom vendors have started to embrace the cloud as a deployment platform, usually selecting a privileged virtualization platform. Operators would like to deploy telecom functionality on their already existing IT cloud platforms. Achieving such flexibility would require the telecom vendors to adopt a software architecture allowing deployment on many cloud platforms or even heterogeneous cloud platforms. We propose a distributed software architecture enabling the deployment of a single software version on multiple cloud platforms thus allowing for a solution-based deployment. We also present a prototype we developed to study the characteristics of this architecture.",2015-07-14T22:27:16Z,2015-09-18T15:34:21Z,http://arxiv.org/abs/1507.04039v2,http://arxiv.org/pdf/1507.04039v2,"cs.DC, C.2.4; D.1.3; D.2.11"
A Mixed-ADC Receiver Architecture for Massive MIMO Systems,"Ning Liang, Wenyi Zhang","Motivated by the demand for energy-efficient communication solutions in the next generation cellular network, a mixed-ADC receiver architecture for massive multiple input multiple output (MIMO) systems is proposed, which differs from previous works in that herein one-bit analog-to-digital converters (ADCs) partially replace the conventionally assumed high-resolution ADCs. The information-theoretic tool of generalized mutual information (GMI) is exploited to analyze the achievable data rates of the proposed system architecture and an array of analytical results of engineering interest are obtained. For deterministic single input multiple output (SIMO) channels, a closed-form expression of the GMI is derived, based on which the linear combiner is optimized. Then, the asymptotic behaviors of the GMI in both low and high SNR regimes are explored, and the analytical results suggest a plausible ADC assignment scheme. Finally, the analytical framework is applied to the multi-user access scenario, and the corresponding numerical results demonstrate that the mixed system architecture with a relatively small number of high-resolution ADCs is able to achieve a large fraction of the channel capacity without output quantization.",2015-07-27T03:25:53Z,2015-07-27T03:25:53Z,http://arxiv.org/abs/1507.07290v1,http://arxiv.org/pdf/1507.07290v1,"cs.IT, math.IT"
Optimizing and Contrasting Recurrent Neural Network Architectures,Ben Krause,"Recurrent Neural Networks (RNNs) have long been recognized for their potential to model complex time series. However, it remains to be determined what optimization techniques and recurrent architectures can be used to best realize this potential. The experiments presented take a deep look into Hessian free optimization, a powerful second order optimization method that has shown promising results, but still does not enjoy widespread use. This algorithm was used to train to a number of RNN architectures including standard RNNs, long short-term memory, multiplicative RNNs, and stacked RNNs on the task of character prediction. The insights from these experiments led to the creation of a new multiplicative LSTM hybrid architecture that outperformed both LSTM and multiplicative RNNs. When tested on a larger scale, multiplicative LSTM achieved character level modelling results competitive with the state of the art for RNNs using very different methodology.",2015-10-16T17:16:14Z,2015-10-16T17:16:14Z,http://arxiv.org/abs/1510.04953v1,http://arxiv.org/pdf/1510.04953v1,"stat.ML, cs.LG, cs.NE"
"On the notion of ""von Neumann vicious circle"" coined by John Backus",Stanislaw Ambroszkiewicz,"""The von Neumann vicious circle"" means that non-von Neumann computer architectures cannot be developed because of the lack of widely available and effective non-von Neumann languages. New languages cannot be created because of lack of conceptual foundations for non-von Neumann architectures. The reason is that programming languages are high-level abstract isomorphic copies of von Neumann computer architectures. This constitutes the current paradigm in Computer Science. The paradigm is equivalent to the predominant view that computations on higher order objects (functionals) can be done only symbolically, i.e. by term rewriting. The paper is a short introduction to the papers arXiv:1501.03043 and arXiv:1510.02787 trying to break the paradigm by introducing a framework that may be seen as a higher order functional HDL (Hardware Description Language).",2016-02-03T19:36:23Z,2016-03-07T20:20:10Z,http://arxiv.org/abs/1602.02715v2,http://arxiv.org/pdf/1602.02715v2,"cs.PL, 03D, F.4.1"
TANGO: Transparent heterogeneous hardware Architecture deployment for   eNergy Gain in Operation,"Karim Djemame, Django Armstrong, Richard Kavanagh, Jean-Christophe Deprez, Ana Juan Ferrer, David Garcia Perez, Rosa Badia, Raul Sirvent, Jorge Ejarque, Yiannis Georgiou","The paper is concerned with the issue of how software systems actually use Heterogeneous Parallel Architectures (HPAs), with the goal of optimizing power consumption on these resources. It argues the need for novel methods and tools to support software developers aiming to optimise power consumption resulting from designing, developing, deploying and running software on HPAs, while maintaining other quality aspects of software to adequate and agreed levels. To do so, a reference architecture to support energy efficiency at application construction, deployment, and operation is discussed, as well as its implementation and evaluation plans.",2016-03-04T10:13:56Z,2016-03-04T10:13:56Z,http://arxiv.org/abs/1603.01407v1,http://arxiv.org/pdf/1603.01407v1,"cs.SE, cs.DC, C.1.4; C.2.4"
Architecture and Algorithms for Privacy Preserving Thermal Inertial Load   Management by A Load Serving Entity,"Abhishek Halder, Xinbo Geng, P. R. Kumar, Le Xie","Motivated by the growing importance of demand response in modern power system's operations, we propose an architecture and supporting algorithms for privacy preserving thermal inertial load management as a service provided by the load serving entity (LSE). We focus on an LSE managing a population of its customers' air conditioners, and propose a contractual model where the LSE guarantees quality of service to each customer in terms of keeping their indoor temperature trajectories within respective bands around the desired individual comfort temperatures. We show how the LSE can price the contracts differentiated by the flexibility embodied by the width of the specified bands. We address architectural questions of (i) how the LSE can strategize its energy procurement based on price and ambient temperature forecasts, (ii) how an LSE can close the real time control loop at the aggregate level while providing individual comfort guarantees to loads, without ever measuring the states of an air conditioner for privacy reasons. Control algorithms to enable our proposed architecture are given, and their efficacy is demonstrated on real data.",2016-06-30T16:39:44Z,2016-11-29T05:22:01Z,http://arxiv.org/abs/1606.09564v3,http://arxiv.org/pdf/1606.09564v3,"cs.SY, math.OC"
TTC: A Tensor Transposition Compiler for Multiple Architectures,"Paul Springer, Aravind Sankaran, Paolo Bientinesi","We consider the problem of transposing tensors of arbitrary dimension and describe TTC, an open source domain-specific parallel compiler. TTC generates optimized parallel C++/CUDA C code that achieves a significant fraction of the system's peak memory bandwidth. TTC exhibits high performance across multiple architectures, including modern AVX-based systems (e.g.,~Intel Haswell, AMD Steamroller), Intel's Knights Corner as well as different CUDA-based GPUs such as NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a meaningful baseline implementation generated by external C++ compilers; the results suggest that a domain-specific compiler can outperform its general purpose counterpart significantly: For instance, comparing with Intel's latest C++ compiler on the Haswell and Knights Corner architecture, TTC yields speedups of up to $8\times$ and $32\times$, respectively. We also showcase TTC's support for multiple leading dimensions, making it a suitable candidate for the generation of performance-critical packing functions that are at the core of the ubiquitous BLAS 3 routines.",2016-07-05T13:53:57Z,2016-07-05T13:53:57Z,http://arxiv.org/abs/1607.01249v1,http://arxiv.org/pdf/1607.01249v1,"cs.MS, cs.PF, G.4; D.3.4; I.1.2; I.1.3"
Parallelized Structures for MIMO FBMC under Strong Channel Frequency   Selectivity,"Xavier Mestre, David Gregoratti","A novel architecture for MIMO transmission and reception of filterbank multicarrier (FBMC) modulated signals under strong frequency selectivity is presented. The proposed system seeks to approximate an ideal frequency-selective precoder and linear receiver by Taylor expansion, exploiting the structure of the analysis and synthesis filterbanks. The resulting architecture is implemented by linearly combining conventional MIMO linear transceivers, which are applied to sequential derivatives of the original filterbank. The classical per-subcarrier precoding/linear receiver configuration is obtained as a special case of this architecture, when only one stage is fixed at both transmitter and receiver. An asymptotic expression for the resulting intersymbol/intercarrier (ISI/ICI) distortion is derived assuming that the number of subcarriers grows large. This expression can in practice be used in order to determine the number of parallel stages that need to be implemented in the proposed architecture. Performance evaluation studies confirm the substantial advantage of the proposed scheme in practical frequency-selective MIMO scenarios.",2016-07-05T14:37:15Z,2016-07-05T14:37:15Z,http://arxiv.org/abs/1607.01272v1,http://arxiv.org/pdf/1607.01272v1,"cs.IT, math.IT"
Recycle deep features for better object detection,"Wei Li, Matthias Breier, Dorit Merhof","Aiming at improving the performance of existing detection algorithms developed for different applications, we propose a region regression-based multi-stage class-agnostic detection pipeline, whereby the existing algorithms are employed for providing the initial detection proposals. Better detection is obtained by exploiting the power of deep learning in the region regress scheme while avoiding the requirement on a huge amount of reference data for training deep neural networks. Additionally, a novel network architecture with recycled deep features is proposed, which provides superior regression results compared to the commonly used architectures. As demonstrated on a data set with ~1200 samples of different classes, it is feasible to successfully train a deep neural network in our proposed architecture and use it to obtain the desired detection performance. Since only slight modifications are required to common network architectures and since the deep neural network is trained using the standard hyperparameters, the proposed detection is well accessible and can be easily adopted to a broad variety of detection tasks.",2016-07-18T13:42:56Z,2016-07-18T13:42:56Z,http://arxiv.org/abs/1607.05066v1,http://arxiv.org/pdf/1607.05066v1,"cs.CV, I.4; I.5"
Distributed Optimization for Client-Server Architecture with Negative   Gradient Weights,"Shripad Gade, Nitin H. Vaidya","Availability of both massive datasets and computing resources have made machine learning and predictive analytics extremely pervasive. In this work we present a synchronous algorithm and architecture for distributed optimization motivated by privacy requirements posed by applications in machine learning. We present an algorithm for the recently proposed multi-parameter-server architecture. We consider a group of parameter servers that learn a model based on randomized gradients received from clients. Clients are computational entities with private datasets (inducing a private objective function), that evaluate and upload randomized gradients to the parameter servers. The parameter servers perform model updates based on received gradients and share the model parameters with other servers. We prove that the proposed algorithm can optimize the overall objective function for a very general architecture involving $C$ clients connected to $S$ parameter servers in an arbitrary time varying topology and the parameter servers forming a connected network.",2016-08-12T18:34:06Z,2016-12-19T15:19:25Z,http://arxiv.org/abs/1608.03866v2,http://arxiv.org/pdf/1608.03866v2,"cs.DC, cs.LG, math.OC"
A new kind of parallelism and its programming in the Explicitly   Many-Processor Approach,János Végh,"The processor accelerators are effective because they are working not (completely) on principles of stored program computers. They use some kind of parallelism, and it is rather hard to program them effectively: a parallel architecture by means of (and thinking in) sequential programming. The recently introduced EMPA architecture uses a new kind of parallelism, which offers the potential of reaching higher degree of parallelism, and also provides extra possibilities and challenges. It not only provides synchronization and inherent parallelization, but also takes over some duties typically offered by the OS, and even opens the till now closed machine instructions for the end-user. A toolchain for EMPA architecture with Y86 cores has been prepared, including an assembler and a cycle-accurate simulator. The assembler is equipped with some meta-instructions, which allow to use all advanced possibilities of the EMPA architecture, and at the same time provide a (nearly) conventional-style programming. The cycle accurate simulator is able to execute the EMPA-aware object code, and is a good tool for developing algorithms for EMPA",2016-08-24T18:47:03Z,2016-08-24T18:47:03Z,http://arxiv.org/abs/1608.07155v1,http://arxiv.org/pdf/1608.07155v1,"cs.DC, 68N15, 68N19, C.1.3; D.2.11; F.1.2"
Latent Multi-task Architecture Learning,"Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders Søgaard","Multi-task learning (MTL) allows deep neural networks to learn from related tasks by sharing parameters with other networks. In practice, however, MTL involves searching an enormous space of possible parameter sharing architectures to find (a) the layers or subspaces that benefit from sharing, (b) the appropriate amount of sharing, and (c) the appropriate relative weights of the different task losses. Recent work has addressed each of the above problems in isolation. In this work we present an approach that learns a latent multi-task architecture that jointly addresses (a)--(c). We present experiments on synthetic data and data from OntoNotes 5.0, including four different tasks and seven different domains. Our extension consistently outperforms previous approaches to learning latent architectures for multi-task problems and achieves up to 15% average error reductions over common approaches to MTL.",2017-05-23T08:58:09Z,2018-11-19T10:30:52Z,http://arxiv.org/abs/1705.08142v3,http://arxiv.org/pdf/1705.08142v3,"stat.ML, cs.AI, cs.CL, cs.LG, cs.NE"
Intercell Interference-Aware Scheduling for Delay Sensitive Applications   in C-RAN,"Yi Li, M. Cenk Gursoy, Senem Velipasalar","Cloud radio access network (C-RAN) architecture is a new mobile network architecture that enables cooperative baseband processing and information sharing among multiple cells and achieves high adaptability to nonuniform traffic by centralizing the baseband processing resources in a virtualized baseband unit (BBU) pool. In this work, we formulate the utility of each user using a convex delay cost function, and design a two-step scheduling algorithm with good delay performance for the C-RAN architecture. In the first step, all users in multiple cells are grouped into small user groups, according to their interference levels and estimated utilities. In the second step, channels are matched to the user groups to maximize the system utility. The performance of our algorithm is further studied via simulations, and the advantages of C-RAN architecture is verified.",2017-08-02T17:58:39Z,2017-08-02T17:58:39Z,http://arxiv.org/abs/1708.00852v1,http://arxiv.org/pdf/1708.00852v1,"cs.IT, math.IT"
Deep vs. Diverse Architectures for Classification Problems,Colleen M. Farrelly,"This study compares various superlearner and deep learning architectures (machine-learning-based and neural-network-based) for classification problems across several simulated and industrial datasets to assess performance and computational efficiency, as both methods have nice theoretical convergence properties. Superlearner formulations outperform other methods at small to moderate sample sizes (500-2500) on nonlinear and mixed linear/nonlinear predictor relationship datasets, while deep neural networks perform well on linear predictor relationship datasets of all sizes. This suggests faster convergence of the superlearner compared to deep neural network architectures on many messy classification problems for real-world data.   Superlearners also yield interpretable models, allowing users to examine important signals in the data; in addition, they offer flexible formulation, where users can retain good performance with low-computational-cost base algorithms.   K-nearest-neighbor (KNN) regression demonstrates improvements using the superlearner framework, as well; KNN superlearners consistently outperform deep architectures and KNN regression, suggesting that superlearners may be better able to capture local and global geometric features through utilizing a variety of algorithms to probe the data space.",2017-08-21T15:31:44Z,2017-08-21T15:31:44Z,http://arxiv.org/abs/1708.06347v1,http://arxiv.org/pdf/1708.06347v1,"stat.ML, cs.LG"
A Neural Network Approach for Mixing Language Models,"Youssef Oualil, Dietrich Klakow","The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.",2017-08-23T13:27:16Z,2017-08-23T13:27:16Z,http://arxiv.org/abs/1708.06989v1,http://arxiv.org/pdf/1708.06989v1,"cs.CL, cs.AI, 97K50, I.2.7"
Multiplexing Analysis of Millimeter-Wave Massive MIMO Systems,"Dian-Wu Yue, Ha H. Nguyen, Shuai Xu","This paper is concerned with spatial multiplexing analysis for millimeter-wave (mmWave) massive MIMO systems. For a single-user mmWave system employing distributed antenna subarray architecture in which the transmitter and receiver consist of Kt and Kr subarrays, respectively, an asymptotic multiplexing gain formula is firstly derived when the numbers of antennas at subarrays go to infinity. Specifically, assuming that all subchannels have the same average number of propagation paths L, the formula implies that by employing such a distributed antenna-subarray architecture, an exact average maximum multiplexing gain of KrKtL can be achieved. This result means that compared to the co-located antenna architecture, using the distributed antenna-subarray architecture can scale up the maximum multiplexing gain proportionally to KrKt. In order to further reveal the relation between diversity gain and multiplexing gain, a simple characterization of the diversity-multiplexing tradeoff is also given. The multiplexing gain analysis is then extended to the multiuser scenario. Moreover, simulation results obtained with the hybrid analog/digital processing corroborate the analysis results.",2018-01-07T08:51:38Z,2018-07-29T06:06:47Z,http://arxiv.org/abs/1801.02987v3,http://arxiv.org/pdf/1801.02987v3,"cs.IT, math.IT"
Randomly weighted CNNs for (music) audio classification,"Jordi Pons, Xavier Serra","The computer vision literature shows that randomly weighted neural networks perform reasonably as feature extractors. Following this idea, we study how non-trained (randomly weighted) convolutional neural networks perform as feature extractors for (music) audio classification tasks. We use features extracted from the embeddings of deep architectures as input to a classifier - with the goal to compare classification accuracies when using different randomly weighted architectures. By following this methodology, we run a comprehensive evaluation of the current deep architectures for audio classification, and provide evidence that the architectures alone are an important piece for resolving (music) audio problems using deep neural networks.",2018-05-01T08:49:53Z,2019-02-14T19:14:52Z,http://arxiv.org/abs/1805.00237v3,http://arxiv.org/pdf/1805.00237v3,"cs.SD, eess.AS"
Auto-Keras: An Efficient Neural Architecture Search System,"Haifeng Jin, Qingquan Song, Xia Hu","Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.",2018-06-27T03:18:35Z,2019-03-26T04:38:37Z,http://arxiv.org/abs/1806.10282v3,http://arxiv.org/pdf/1806.10282v3,"cs.LG, cs.AI, stat.ML"
A Partition-Driven Integrated Security Architecture for Cyber-Physical   Systems,"Yahya Javed, Muhamad Felemban, Tawfeeq Shawly, Jason Kobes, Arif Ghafoor","Emerging cyber-physical systems incorporate systems of systems that have functional interdependencies. With the increase in complexity of the cyber-physical systems, the attack surface also expands, making cyber-physical systems more vulnerable to cyber-attacks. The functional interdependencies exacerbate the security risk as a cyber-attack that compromises one constituent system of a cyber-physical system can disseminate to others. This can result in a cascade effect that can impair the operability of the whole cyber-physical system. In this article, we present a novel security architecture that localizes the cyber-attack in a timely manner, and simultaneously recovers the affected cyber-physical system functionality. We have evaluated the performance of the architecture for advanced metering infrastructure-based pricing cyber-attacks scenario. The simulation results exhibit the effectiveness of the proposed architecture in containing the attack in terms of system availability and its impact on the electric load distribution in the power grid.",2019-01-10T05:07:38Z,2019-01-10T05:07:38Z,http://arxiv.org/abs/1901.03018v1,http://arxiv.org/pdf/1901.03018v1,"eess.SY, cs.SY"
CrossNet: Latent Cross-Consistency for Unpaired Image Translation,"Omry Sendik, Dani Lischinski, Daniel Cohen-Or","Recent GAN-based architectures have been able to deliver impressive performance on the general task of image-to-image translation. In particular, it was shown that a wide variety of image translation operators may be learned from two image sets, containing images from two different domains, without establishing an explicit pairing between the images. This was made possible by introducing clever regularizers to overcome the under-constrained nature of the unpaired translation problem. In this work, we introduce a novel architecture for unpaired image translation, and explore several new regularizers enabled by it. Specifically, our architecture comprises a pair of GANs, as well as a pair of translators between their respective latent spaces. These cross-translators enable us to impose several regularizing constraints on the learnt image translation operator, collectively referred to as latent cross-consistency. Our results show that our proposed architecture and latent cross-consistency constraints are able to outperform the existing state-of-the-art on a variety of image translation tasks.",2019-01-14T19:31:58Z,2019-05-26T19:47:34Z,http://arxiv.org/abs/1901.04530v2,http://arxiv.org/pdf/1901.04530v2,"cs.LG, cs.CV, stat.ML"
Spatial Broadcast Decoder: A Simple Architecture for Learning   Disentangled Representations in VAEs,"Nicholas Watters, Loic Matthey, Christopher P. Burgess, Alexander Lerchner","We present a simple neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-""coordinate"" channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent distribution of VAEs, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space. It provides a particularly dramatic benefit when applied to datasets with small objects. We also emphasize a method for visualizing learned latent spaces that helped us diagnose our models and may prove useful for others aiming to assess data representations. Finally, we show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance.",2019-01-21T18:08:49Z,2019-08-14T10:02:46Z,http://arxiv.org/abs/1901.07017v2,http://arxiv.org/pdf/1901.07017v2,"cs.LG, cs.CV, stat.ML"
DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine   Learning,"Fei Qi, Zhaohui Xia, Gaoyang Tang, Hang Yang, Yu Song, Guangrui Qian, Xiong An, Chunhuan Lin, Guangming Shi","As an emerging field, Automated Machine Learning (AutoML) aims to reduce or eliminate manual operations that require expertise in machine learning. In this paper, a graph-based architecture is employed to represent flexible combinations of ML models, which provides a large searching space compared to tree-based and stacking-based architectures. Based on this, an evolutionary algorithm is proposed to search for the best architecture, where the mutation and heredity operators are the key for architecture evolution. With Bayesian hyper-parameter optimization, the proposed approach can automate the workflow of machine learning. On the PMLB dataset, the proposed approach shows the state-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn. Some of the optimized models are with complex structures which are difficult to obtain in manual design.",2018-11-20T08:42:41Z,2018-11-20T08:42:41Z,http://arxiv.org/abs/1901.08013v1,http://arxiv.org/pdf/1901.08013v1,"cs.NE, cs.LG, stat.ML"
On Multiterminal Communication over MIMO Channels with One-bit ADCs at   the Receivers,"Abbas Khalili, Farhad Shirani, Elza Erkip, Yonina C. Eldar","The fundamental limits of communication over multiple-input multiple-output (MIMO) networks are considered when a limited number of one-bit analog to digital converters (ADC) are used at the receiver terminals. Prior works have mainly focused on point-to-point communications, where receiver architectures consisting of a concatenation of an analog processing module, a limited number of one-bit ADCs with non-adaptive thresholds, and a digital processing module are considered. In this work, a new receiver architecture is proposed which utilizes adaptive threshold one-bit ADCs - where the ADC thresholds at each channel-use are dependent on the channel outputs in the previous channel-uses - to mitigate the quantization rate-loss. Coding schemes are proposed for communication over the point-to-point and broadcast channels, and achievable rate regions are derived. In the high SNR regime, it is shown that using the proposed architectures and coding schemes leads to the largest achievable rate regions among all receiver architectures with the same number of one-bit ADCs.",2019-01-30T01:04:37Z,2019-01-31T23:26:23Z,http://arxiv.org/abs/1901.10628v2,http://arxiv.org/pdf/1901.10628v2,"cs.IT, math.IT"
STCN: Stochastic Temporal Convolutional Networks,"Emre Aksan, Otmar Hilliges","Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs), while providing computational and modeling advantages due to inherent parallelism. However, currently there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to the decoupling of the deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model is capable of predicting high-quality synthetic samples over a long-range temporal horizon in modeling of handwritten text.",2019-02-18T13:44:59Z,2019-02-18T13:44:59Z,http://arxiv.org/abs/1902.06568v1,http://arxiv.org/pdf/1902.06568v1,"cs.LG, stat.ML"
Core Access Hybridization in 5G,"Sudhir K. Routray, Habib H. Mohammed","The fifth generation (5G) cellular communication system will have several advanced features, majority of which will come into existence for the first time. According to the ITU recommendations, 5G will provide minimum 3 GB/s end to end data rates under the static conditions and in the mobile condition the minimum data rate is 100 Mb/s. The latency will be brought down to 1 ms and the device densities will increase by many folds. These features cannot be provided in the pure wireless domain. Certainly, the support of the optical networks is essential to provide these advanced features in 5G communication. These features need special architectures. In this article, we provide advanced optical wireless hybrid architectures for 5G networks. We explain the importance of this architecture and also show the nodal configuration of these hybrid networks.",2019-03-24T17:43:29Z,2019-03-24T17:43:29Z,http://arxiv.org/abs/1903.10027v1,http://arxiv.org/pdf/1903.10027v1,"eess.SP, cs.NI"
A practicable guide to the quantum computation architectures,"Hou Ian, Biao Chen, Wei Zhao","The primordial model of quantum computation was introduced over thirty years ago and the first quantum algorithms have appeared for over twenty years. Yet the exact architectures for quantum computer seem foreign to an undergraduate student major in computer science or engineering, even though the mass media has helped popularize the terminologies in the past decade. Despite being a cutting-edge technology from both the theoretical and the experimental perspectives, quantum computation is indeed imminent and it would be helpful to give the undergraduate students at least a skeleton understanding of what a quantum computer stands for. Since instruction-set architectures originated from classical computing models are familiar, we propose analogously a set of quantum instructions, which can be composed to implement renowned quantum algorithms. Albeit the similarity one can draw between classical and quantum computer architectures, current quantum instructions are fundamentally incommensurable from their classical counterparts because they lack the innate capability to implement logical deductions and recursions. We discuss this trait in length and illustrate why it is held responsible that current quantum computers not be considered general computers.",2019-03-26T08:55:27Z,2019-05-07T10:33:41Z,http://arxiv.org/abs/1903.10739v2,http://arxiv.org/pdf/1903.10739v2,"quant-ph, cs.ET"
Modular Universal Reparameterization: Deep Multi-task Learning Across   Diverse Domains,"Elliot Meyerson, Risto Miikkulainen","As deep learning applications continue to become more diverse, an interesting question arises: Can general problem solving arise from jointly learning several such diverse tasks? To approach this question, deep multi-task learning is extended in this paper to the setting where there is no obvious overlap between task architectures. The idea is that any set of (architecture,task) pairs can be decomposed into a set of potentially related subproblems, whose sharing is optimized by an efficient stochastic algorithm. The approach is first validated in a classic synthetic multi-task learning benchmark, and then applied to sharing across disparate architectures for vision, NLP, and genomics tasks. It discovers regularities across these domains, encodes them into sharable modules, and combines these modules systematically to improve performance in the individual tasks. The results confirm that sharing learned functionality across diverse domains and architectures is indeed beneficial, thus establishing a key ingredient for general problem solving in the future.",2019-05-31T22:00:43Z,2019-10-28T17:51:14Z,http://arxiv.org/abs/1906.00097v2,http://arxiv.org/pdf/1906.00097v2,"cs.LG, cs.NE, stat.ML"
Towards Task and Architecture-Independent Generalization Gap Predictors,"Scott Yak, Javier Gonzalvo, Hanna Mazzawi","Can we use deep learning to predict when deep learning works? Our results suggest the affirmative. We created a dataset by training 13,500 neural networks with different architectures, on different variations of spiral datasets, and using different optimization parameters. We used this dataset to train task-independent and architecture-independent generalization gap predictors for those neural networks. We extend Jiang et al. (2018) to also use DNNs and RNNs and show that they outperform the linear model, obtaining $R^2=0.965$. We also show results for architecture-independent, task-independent, and out-of-distribution generalization gap prediction tasks. Both DNNs and RNNs consistently and significantly outperform linear models, with RNNs obtaining $R^2=0.584$.",2019-06-04T16:10:15Z,2019-06-04T16:10:15Z,http://arxiv.org/abs/1906.01550v1,http://arxiv.org/pdf/1906.01550v1,"stat.ML, cs.LG"
One-Shot Neural Architecture Search via Compressive Sensing,"Minsu Cho, Mohammadreza Soltani, Chinmay Hegde","Neural Architecture Search remains a very challenging meta-learning problem. Several recent techniques based on parameter-sharing idea have focused on reducing the NAS running time by leveraging proxy models, leading to architectures with competitive performance compared to those with hand-crafted designs. In this paper, we propose an iterative technique for NAS, inspired by algorithms for learning low-degree sparse Boolean functions. We validate our approach on the DARTs search space (Liu et al., 2018b) and NAS-Bench-201 (Yang et al., 2020). In addition, we provide theoretical analysis via upper bounds on the number of validation error measurements needed for reliable learning, and include ablation studies to further in-depth understanding of our technique.",2019-06-07T02:35:52Z,2022-02-07T18:21:56Z,http://arxiv.org/abs/1906.02869v2,http://arxiv.org/pdf/1906.02869v2,"cs.LG, stat.ML"
A Hierarchical Architecture for Sequential Decision-Making in Autonomous   Driving using Deep Reinforcement Learning,"Majid Moghadam, Gabriel Hugh Elkaim","Tactical decision making is a critical feature for advanced driving systems, that incorporates several challenges such as complexity of the uncertain environment and reliability of the autonomous system. In this work, we develop a multi-modal architecture that includes the environmental modeling of ego surrounding and train a deep reinforcement learning (DRL) agent that yields consistent performance in stochastic highway driving scenarios. To this end, we feed the occupancy grid of the ego surrounding into the DRL agent and obtain the high-level sequential commands (i.e. lane change) to send them to lower-level controllers. We will show that dividing the autonomous driving problem into a multi-layer control architecture enables us to leverage the AI power to solve each layer separately and achieve an admissible reliability score. Comparing with end-to-end approaches, this architecture enables us to end up with a more reliable system which can be implemented in actual self-driving cars.",2019-06-20T07:05:20Z,2019-06-20T07:05:20Z,http://arxiv.org/abs/1906.08464v1,http://arxiv.org/pdf/1906.08464v1,"cs.RO, cs.AI, cs.LG, cs.SY, eess.SY"
Learn to Allocate Resources in Vehicular Networks,"Liang Wang, Hao Ye, Le Liang, Geoffrey Ye Li","Resource allocation has a direct and profound impact on the performance of vehicle-to-everything (V2X) networks. Considering the dynamic nature of vehicular environments, it is appealing to devise a decentralized strategy to perform effective resource sharing. In this paper, we exploit deep learning to promote coordination among multiple vehicles and propose a hybrid architecture consisting of centralized decision making and distributed resource sharing to maximize the long-term sum rate of all vehicles. To reduce the network signaling overhead, each vehicle uses a deep neural network to compress its own observed information that is thereafter fed back to the centralized decision-making unit, which employs a deep Q-network to allocate resources and then sends the decision results to all vehicles. We further adopt a quantization layer for each vehicle that learns to quantize the continuous feedback. Extensive simulation results demonstrate that the proposed hybrid architecture can achieve near-optimal performance. Meanwhile, there exists an optimal number of continuous feedback and binary feedback, respectively. Besides, this architecture is robust to different feedback intervals, input noise, and feedback noise.",2019-07-30T16:41:33Z,2019-07-30T16:41:33Z,http://arxiv.org/abs/1908.03447v1,http://arxiv.org/pdf/1908.03447v1,"cs.NI, cs.IT, cs.LG, eess.SP, math.IT"
AutoGAN: Neural Architecture Search for Generative Adversarial Networks,"Xinyu Gong, Shiyu Chang, Yifan Jiang, Zhangyang Wang","Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN",2019-08-11T00:52:30Z,2019-08-11T00:52:30Z,http://arxiv.org/abs/1908.03835v1,http://arxiv.org/pdf/1908.03835v1,"cs.CV, cs.LG, eess.IV"
"Projections of achievable performance for Weather & Climate Dwarfs, and   for entire NWP applications, on hybrid architectures","Michał Kulczewski, Marek Błażewicz, Sebastian Ciesielski","This document is one of the deliverable reports created for the ESCAPE project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather Prediction at Exascale. The project develops world-class, extreme-scale computing capabilities for European operational numerical weather prediction and future climate models. This is done by identifying Weather & Climate dwarfs which are key patterns in terms of computation and communication (in the spirit of the Berkeley dwarfs). These dwarfs are then optimised for different hardware architectures (single and multi-node) and alternative algorithms are explored. Performance portability is addressed through the use of domain specific languages.   This deliverable contains the description of the performance and energy models for the selected Weather & Climate dwarfs for different hardware architectures, multinode with GPU accelerators in particular. Presented performance models are extension to model provided in Deliverable 3.2. With some further enhancements, they are incorporated in the DCworms simulator. In particular, extended models allow to predict computational and energy performance on different architectures: single and multinodes, equipped with CPUs and GPUs accelerators. This allows to provide feasible performance projection at system scale.",2019-08-16T17:56:37Z,2019-08-16T17:56:37Z,http://arxiv.org/abs/1908.06098v1,http://arxiv.org/pdf/1908.06098v1,"cs.DC, D.2.8; G.1.8; G.4"
Deep Recurrent Architectures for Seismic Tomography,"Amir Adler, Mauricio Araya-Polo, Tomaso Poggio","This paper introduces novel deep recurrent neural network architectures for Velocity Model Building (VMB), which is beyond what Araya-Polo et al 2018 pioneered with the Machine Learning-based seismic tomography built with convolutional non-recurrent neural network. Our investigation includes the utilization of basic recurrent neural network (RNN) cells, as well as Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) cells. Performance evaluation reveals that salt bodies are consistently predicted more accurately by GRU and LSTM-based architectures, as compared to non-recurrent architectures. The results take us a step closer to the final goal of a reliable fully Machine Learning-based tomography from pre-stack data, which when achieved will reduce the VMB turnaround from weeks to days.",2019-08-12T07:10:19Z,2019-08-12T07:10:19Z,http://arxiv.org/abs/1908.07824v1,http://arxiv.org/pdf/1908.07824v1,"physics.geo-ph, eess.IV, eess.SP"
A scalable constructive algorithm for the optimization of neural network   architectures,"Massimiliano Lupo Pasini, Junqi Yin, Ying Wai Li, Markus Eisenbach","We propose a new scalable method to optimize the architecture of an artificial neural network. The proposed algorithm, called Greedy Search for Neural Network Architecture, aims to determine a neural network with minimal number of layers that is at least as performant as neural networks of the same structure identified by other hyperparameter search algorithms in terms of accuracy and computational cost. Numerical results performed on benchmark datasets show that, for these datasets, our method outperforms state-of-the-art hyperparameter optimization algorithms in terms of attainable predictive performance by the selected neural network architecture, and time-to-solution for the hyperparameter optimization to complete.",2019-09-07T17:22:28Z,2021-04-21T14:13:57Z,http://arxiv.org/abs/1909.03306v3,http://arxiv.org/pdf/1909.03306v3,"cs.LG, cs.NE, stat.ML, 68T01, 68Q32, 68T05, 68T10, 68W20"
Safety-Critical Adaptive Control with Nonlinear Reference Model Systems,"Ehsan Arabi, Kunal Garg, Dimitra Panagou","In this paper, a model reference adaptive control architecture is proposed for uncertain nonlinear systems to achieve prescribed performance guarantees. Specifically, a general nonlinear reference model system is considered that captures an ideal and safe system behavior. An adaptive control architecture is then proposed to suppress the effects of system uncertainties without any prior knowledge of their magnitude and rate upper bounds. More importantly, the proposed control architecture enforces the system state trajectories to evolve within a user-specified prescribed distance from the reference system trajectories, satisfying the safety constraints. This eliminates the ad-hoc tuning process for the adaptation rate that is conventionally required in model reference adaptive control to ensure safety. The efficacy of the proposed control architecture is also demonstrated through an illustrative numerical example.",2019-09-17T16:25:47Z,2020-04-13T03:31:36Z,http://arxiv.org/abs/1909.07916v3,http://arxiv.org/pdf/1909.07916v3,"eess.SY, cs.SY"
Understanding and Robustifying Differentiable Architecture Search,"Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter","Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.",2019-09-20T18:03:06Z,2020-01-28T14:14:05Z,http://arxiv.org/abs/1909.09656v2,http://arxiv.org/pdf/1909.09656v2,"cs.LG, cs.AI, cs.CV, stat.ML"
Video Surveillance of Highway Traffic Events by Deep Learning   Architectures,"Matteo Tiezzi, Stefano Melacci, Marco Maggini, Angelo Frosini","In this paper we describe a video surveillance system able to detect traffic events in videos acquired by fixed videocameras on highways. The events of interest consist in a specific sequence of situations that occur in the video, as for instance a vehicle stopping on the emergency lane. Hence, the detection of these events requires to analyze a temporal sequence in the video stream. We compare different approaches that exploit architectures based on Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). A first approach extracts vectors of features, mostly related to motion, from each video frame and exploits a RNN fed with the resulting sequence of vectors. The other approaches are based directly on the sequence of frames, that are eventually enriched with pixel-wise motion information. The obtained stream is processed by an architecture that stacks a CNN and a RNN, and we also investigate a transfer-learning-based model. The results are very promising and the best architecture will be tested online in real operative conditions.",2019-09-06T15:36:02Z,2019-09-06T15:36:02Z,http://arxiv.org/abs/1909.12235v1,http://arxiv.org/pdf/1909.12235v1,"cs.CV, cs.LG, stat.ML"
Deep Neural Architecture Search with Deep Graph Bayesian Optimization,"Lizheng Ma, Jiaxu Cui, Bo Yang","Bayesian optimization (BO) is an effective method of finding the global optima of black-box functions. Recently BO has been applied to neural architecture search and shows better performance than pure evolutionary strategies. All these methods adopt Gaussian processes (GPs) as surrogate function, with the handcraft similarity metrics as input. In this work, we propose a Bayesian graph neural network as a new surrogate, which can automatically extract features from deep neural architectures, and use such learned features to fit and characterize black-box objectives and their uncertainty. Based on the new surrogate, we then develop a graph Bayesian optimization framework to address the challenging task of deep neural architecture search. Experiment results show our method significantly outperforms the comparative methods on benchmark tasks.",2019-05-14T17:13:04Z,2019-05-14T17:13:04Z,http://arxiv.org/abs/1905.06159v1,http://arxiv.org/pdf/1905.06159v1,"cs.LG, stat.ML"
Adaptive Stochastic Natural Gradient Method for One-Shot Neural   Architecture Search,"Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari, Kento Uchida, Shota Saito, Kouhei Nishida","High sensitivity of neural architecture search (NAS) methods against their input such as step-size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-the-art performances with low computational budgets both on image classification and inpainting tasks.",2019-05-21T10:39:04Z,2019-05-21T10:39:04Z,http://arxiv.org/abs/1905.08537v1,http://arxiv.org/pdf/1905.08537v1,"cs.LG, cs.NE, stat.ML"
Simple Muscle Architecture Analysis (SMA): an ImageJ macro tool to   automate measurements in B-mode ultrasound scans,"Olivier R. Seynnes, Neil J. Cronin","In vivo measurements of muscle architecture (i.e. the spatial arrangement of muscle fascicles) are routinely included in research and clinical settings to monitor muscle structure, function and plasticity. However, in most cases such measurements are performed manually, and more reliable and time-efficient automated methods are either lacking completely, or are inaccessible to those without expertise in image analysis. In this work, we propose an ImageJ script to automate the entire analysis process of muscle architecture in ultrasound images: Simple Muscle Architecture Analysis (SMA). Images are filtered in the spatial and frequency domains with built-in commands and external plugins to highlight aponeuroses and fascicles. Fascicle dominant orientation is then computed in regions of interest using the OrientationJ plugin. Bland-Altman plots of analyses performed manually or with SMA indicates that the automated analysis does not induce any systematic bias and that both methods agree equally through the range of measurements. Our test results illustrate the suitability of SMA to analyse images from superficial muscles acquired with a broad range of ultrasound settings.",2019-05-23T06:24:07Z,2019-05-31T12:34:21Z,http://arxiv.org/abs/1905.09490v2,http://arxiv.org/pdf/1905.09490v2,"eess.IV, physics.med-ph, q-bio.TO"
Loss Surface Modality of Feed-Forward Neural Network Architectures,"Anna Sergeevna Bosman, Andries Engelbrecht, Mardé Helbig","It has been argued in the past that high-dimensional neural networks do not exhibit local minima capable of trapping an optimisation algorithm. However, the relationship between loss surface modality and the neural architecture parameters, such as the number of hidden neurons per layer and the number of hidden layers, remains poorly understood. This study employs fitness landscape analysis to study the modality of neural network loss surfaces under various feed-forward architecture settings. An increase in the problem dimensionality is shown to yield a more searchable and more exploitable loss surface. An increase in the hidden layer width is shown to effectively reduce the number of local minima, and simplify the shape of the global attractor. An increase in the architecture depth is shown to sharpen the global attractor, thus making it more exploitable.",2019-05-24T14:59:48Z,2020-01-30T14:02:16Z,http://arxiv.org/abs/1905.10268v2,http://arxiv.org/pdf/1905.10268v2,"cs.LG, cs.NE, stat.ML"
Attentional Policies for Cross-Context Multi-Agent Reinforcement   Learning,"Matthew A. Wright, Roberto Horowitz","Many potential applications of reinforcement learning in the real world involve interacting with other agents whose numbers vary over time. We propose new neural policy architectures for these multi-agent problems. In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we follow the spirit of recent work on the power of relational inductive biases in deep networks by learning multi-agent relationships at the policy level via an attentional architecture. In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents' state information when selecting their next action. The structure of our architectures allow them to be applied on environments with varying numbers of agents. We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and significantly outperforming it when scaling to large numbers of agents.",2019-05-31T06:02:52Z,2019-05-31T06:02:52Z,http://arxiv.org/abs/1905.13428v1,http://arxiv.org/pdf/1905.13428v1,"cs.LG, cs.MA, cs.SY, stat.ML"
LightSpeech: Lightweight and Fast Text to Speech with Neural   Architecture Search,"Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Jinzhu Li, Sheng Zhao, Enhong Chen, Tie-Yan Liu","Text to speech (TTS) has been broadly used to synthesize natural and intelligible speech in different scenarios. Deploying TTS in various end devices such as mobile phones or embedded devices requires extremely small memory usage and inference latency. While non-autoregressive TTS models such as FastSpeech have achieved significantly faster inference speed than autoregressive models, their model size and inference latency are still large for the deployment in resource constrained devices. In this paper, we propose LightSpeech, which leverages neural architecture search~(NAS) to automatically design more lightweight and efficient models based on FastSpeech. We first profile the components of current FastSpeech model and carefully design a novel search space containing various lightweight and potentially effective architectures. Then NAS is utilized to automatically discover well performing architectures within the search space. Experiments show that the model discovered by our method achieves 15x model compression ratio and 6.5x inference speedup on CPU with on par voice quality. Audio demos are provided at https://speechresearch.github.io/lightspeech.",2021-02-08T07:45:06Z,2021-02-08T07:45:06Z,http://arxiv.org/abs/2102.04040v1,http://arxiv.org/pdf/2102.04040v1,"cs.SD, cs.AI, cs.LG, eess.AS"
Contrastive Embeddings for Neural Architectures,"Daniel Hesslow, Iacopo Poli","The performance of algorithms for neural architecture search strongly depends on the parametrization of the search space. We use contrastive learning to identify networks across different initializations based on their data Jacobians, and automatically produce the first architecture embeddings independent from the parametrization of the search space. Using our contrastive embeddings, we show that traditional black-box optimization algorithms, without modification, can reach state-of-the-art performance in Neural Architecture Search. As our method provides a unified embedding space, we perform for the first time transfer learning between search spaces. Finally, we show the evolution of embeddings during training, motivating future studies into using embeddings at different training stages to gain a deeper understanding of the networks in a search space.",2021-02-08T14:06:35Z,2021-05-07T13:24:07Z,http://arxiv.org/abs/2102.04208v2,http://arxiv.org/pdf/2102.04208v2,"cs.LG, stat.ML"
Power Minimization in Vehicular Cloud Architecture,"Fatemah S. Behbehani, Taisir Elgorashi, Jaafar M. H. Elmirghani","Modern vehicles equipped with on-board units (OBU) are playing an essential role in the smart city revolution. The vehicular processing resources, however, are not used to their fullest potential. The concept of vehicular clouds is proposed to exploit the underutilized vehicular resources to supplement cloud computing services to relieve the burden on cloud data centers and improve quality of service. In this paper we introduce a vehicular cloud architecture supported by fixed edge computing nodes and the central cloud. A mixed integer linear programming (MLP) model is developed to optimize the allocation of the computing demands in the distributed architecture while minimizing power consumption. The results show power savings as high as 84% over processing in the conventional cloud. A heuristic with performance approaching that of the MILP model is developed to allocate computing demands in real time.",2021-02-17T20:13:28Z,2021-02-17T20:13:28Z,http://arxiv.org/abs/2102.09011v1,http://arxiv.org/pdf/2102.09011v1,"cs.NI, eess.SP"
CheckSoft : A Scalable Event-Driven Software Architecture for Keeping   Track of People and Things in People-Centric Spaces,"Rohan Sarkar, Avinash C. Kak","We present CheckSoft, a scalable event-driven software architecture for keeping track of people-object interactions in people-centric applications such as airport checkpoint security areas, automated retail stores, smart libraries, and so on. The architecture works off the video data generated in real time by a network of surveillance cameras. Although there are many different aspects to automating these applications, the most difficult part of the overall problem is keeping track of the interactions between the people and the objects. CheckSoft uses finite-state-machine (FSM) based logic for keeping track of such interactions which allows the system to quickly reject any false detections of the interactions by the video cameras. CheckSoft is easily scalable since the architecture is based on multi-processing in which a separate process is assigned to each human and to each ""storage container"" for the objects. A storage container may be a shelf on which the objects are displayed or a bin in which the objects are stored, depending on the specific application in which CheckSoft is deployed.",2021-02-21T05:22:55Z,2021-02-21T05:22:55Z,http://arxiv.org/abs/2102.10513v1,http://arxiv.org/pdf/2102.10513v1,"cs.SE, cs.AI, cs.DC, eess.IV"
The Black-Box Simplex Architecture for Runtime Assurance of Autonomous   CPS,"Usama Mehmood, Sanaz Sheikhi, Stanley Bak, Scott A. Smolka, Scott D. Stoller","The Simplex Architecture is a runtime assurance framework where control authority may switch from an unverified and potentially unsafe advanced controller to a backup baseline controller in order to maintain the safety of an autonomous cyber-physical system. In this work, we show that runtime checks can replace the requirement to statically verify safety of the baseline controller. This is important as there are many powerful control techniques, such as model-predictive control and neural network controllers, that work well in practice but are difficult to statically verify. Since the method does not use internal information about the advanced or baseline controller, we call the approach the Black-Box Simplex Architecture. We prove the architecture is safe and present two case studies where (i) model-predictive control provides safe multi-robot coordination, and (ii) neural networks provably prevent collisions in groups of F-16 aircraft, despite the controllers occasionally outputting unsafe commands.",2021-02-24T09:18:32Z,2022-05-31T08:46:54Z,http://arxiv.org/abs/2102.12981v3,http://arxiv.org/pdf/2102.12981v3,"cs.SE, cs.SY, eess.SY"
Security and Privacy in Future Internet Architectures - Benefits and   Challenges of Content Centric Networks,Roman Lutz,"As the shortcomings of our current Internet become more and more obvious, researchers have started creating alternative approaches for the Internet of the future. Their design goals are mainly content-orientation, security, support for mobility and cloud computing. The probably most popular architecture is called Content Centric Networking. Every communication is treated as a distribution of content and caches are used within the network to improve the effectiveness. While the performance gain of Content Centric Networks is undoubted, there are questions about security and especially privacy since it is not one of its main design principle. In this work, we compare the Content Centric Networking approach with the current Internet with respect to security and privacy. We analyze improvements that have been made and new problems that have yet to be resolved. The Internet of the future could be content-oriented, so it is essential to identify potential security and privacy issues that are inherent to the architecture early on.",2016-01-06T19:06:36Z,2016-01-13T15:56:23Z,http://arxiv.org/abs/1601.01278v2,http://arxiv.org/pdf/1601.01278v2,"cs.CR, cs.NI, C.2.1"
Massive MIMO Combining with Switches,"Ahmed Alkhateeb, Young-Han Nam, Jianzhong Zhang, Robert W. Heath Jr","Massive multiple-input multiple-output (MIMO) is expected to play a central role in future wireless systems. The deployment of large antenna arrays at the base station and the mobile users offers multiplexing and beamforming gains that boost system spectral efficiency. Unfortunately, the high cost and power consumption of components like analog-to-digital converters makes assigning an RF chain per antenna and applying typical fully digital precoding/combining solutions difficult. In this paper, a novel architecture for massive MIMO receivers, consisting of arrays of switches and constant (non-tunable) phase shifters, is proposed. This architecture applies a quasi-coherent combining in the RF domain to reduce the number of required RF chains. An algorithm that designs the RF combining for this architecture is developed and analyzed. Results show that the proposed massive MIMO combining model can achieve a comparable performance to the fully-digital receiver architecture in single-user and multi-user massive MIMO setups.",2016-01-27T17:51:44Z,2016-01-27T17:51:44Z,http://arxiv.org/abs/1601.07468v1,http://arxiv.org/pdf/1601.07468v1,"cs.IT, math.IT"
Reconfigurable Antennas in mmWave MIMO Systems,"Mojtaba Ahmadi Almasi, Hani Mehrpouyan, Vida Vakilian, Nader Behdad, Hamid Jafarkhani","The key obstacle to achieving the full potential of the millimeter wave (mmWave) band has been the poor propagation characteristics of wireless signals in this band. One approach to overcome this issue is to use antennas that can support higher gains while providing beam adaptability and diversity, i.e., reconfigurable antennas. In this article, we present a new architecture for mmWave multiple-input multiple-output (MIMO) communications that uses a new class of reconfigurable antennas. More specifically, the proposed lens-based antennas can support multiple radiation patterns while using a single radio frequency chain. Moreover, by using a beam selection network, each antenna beam can be steered in the desired direction. Further, using the proposed reconfigurable antenna in a MIMO architecture, we propose a new signal processing algorithm that uses the additional degrees of freedom provided by the antennas to overcome propagation issues at mmWave frequencies. Our simulation results show that the proposed reconfigurable antenna MIMO architecture significantly enhances the performance of mmWave communication systems.",2017-10-14T00:39:11Z,2017-10-24T16:06:16Z,http://arxiv.org/abs/1710.05111v2,http://arxiv.org/pdf/1710.05111v2,"eess.SP, cs.IT, cs.NI, math.IT"
Cell-Free and User-Centric Massive MIMO at Millimeter Wave Frequencies,"Mario Alonzo, Stefano Buzzi","In a cell-free (CF) massive MIMO architecture a very large number of distributed access points (APs) simultaneously and jointly serves a much smaller number of mobile stations (MSs); a variant of the cell-free technique is the user-centric (UC) approach, wherein each AP just decodes a reduced set of MSs, practically the ones that are received best. This paper introduces and analyzes the CF and UC architectures at millimeter wave (mmWave) frequencies. First of all, a multiuser clustered channel model is introduced in order to account for the correlation among the channels of nearby users; then, an uplink multiuser channel estimation scheme is described along with low-complexity hybrid analog/digital beamforming architectures. Interestingly, in the proposed scheme no channel estimation is needed at the MSs, and the beamforming schemes used at the MSs are channel-independent and have a very simple structure. Numerical results show that the considered architectures provide good performance, especially in lightly loaded systems, with the UC approach outperforming the CF one.",2017-10-21T15:54:46Z,2017-11-05T12:38:41Z,http://arxiv.org/abs/1710.07817v2,http://arxiv.org/pdf/1710.07817v2,"cs.IT, math.IT"
Deep Neural Network Approximation using Tensor Sketching,"Shiva Prasad Kasiviswanathan, Nina Narodytska, Hongxia Jin","Deep neural networks are powerful learning models that achieve state-of-the-art performance on many computer vision, speech, and language processing tasks. In this paper, we study a fundamental question that arises when designing deep network architectures: Given a target network architecture can we design a smaller network architecture that approximates the operation of the target network? The question is, in part, motivated by the challenge of parameter reduction (compression) in modern deep neural networks, as the ever increasing storage and memory requirements of these networks pose a problem in resource constrained environments.   In this work, we focus on deep convolutional neural network architectures, and propose a novel randomized tensor sketching technique that we utilize to develop a unified framework for approximating the operation of both the convolutional and fully connected layers. By applying the sketching technique along different tensor dimensions, we design changes to the convolutional and fully connected layers that substantially reduce the number of effective parameters in a network. We show that the resulting smaller network can be trained directly, and has a classification accuracy that is comparable to the original network.",2017-10-21T20:14:00Z,2017-10-21T20:14:00Z,http://arxiv.org/abs/1710.07850v1,http://arxiv.org/pdf/1710.07850v1,"stat.ML, cs.AI, cs.LG"
On Path Memory in List Successive Cancellation Decoder of Polar Codes,"ChenYang Xia, YouZhe Fan, Ji Chen, Chi-Ying Tsui","Polar code is a breakthrough in coding theory. Using list successive cancellation decoding with large list size L, polar codes can achieve excellent error correction performance. The L partial decoded vectors are stored in the path memory and updated according to the results of list management. In the state-of-the-art designs, the memories are implemented with registers and a large crossbar is used for copying the partial decoded vectors from one block of memory to another during the update. The architectures are quite area-costly when the code length and list size are large. To solve this problem, we propose two optimization schemes for the path memory in this work. First, a folded path memory architecture is presented to reduce the area cost. Second, we show a scheme that the path memory can be totally removed from the architecture. Experimental results show that these schemes effectively reduce the area of path memory.",2017-12-06T06:18:30Z,2017-12-09T13:00:37Z,http://arxiv.org/abs/1712.02053v2,http://arxiv.org/pdf/1712.02053v2,"cs.IT, cs.AR, math.IT"
Hyperparameters Optimization in Deep Convolutional Neural Network /   Bayesian Approach with Gaussian Process Prior,Pushparaja Murugan,"Convolutional Neural Network is known as ConvNet have been extensively used in many complex machine learning tasks. However, hyperparameters optimization is one of a crucial step in developing ConvNet architectures, since the accuracy and performance are reliant on the hyperparameters. This multilayered architecture parameterized by a set of hyperparameters such as the number of convolutional layers, number of fully connected dense layers & neurons, the probability of dropout implementation, learning rate. Hence the searching the hyperparameter over the hyperparameter space are highly difficult to build such complex hierarchical architecture. Many methods have been proposed over the decade to explore the hyperparameter space and find the optimum set of hyperparameter values. Reportedly, Gird search and Random search are said to be inefficient and extremely expensive, due to a large number of hyperparameters of the architecture. Hence, Sequential model-based Bayesian Optimization is a promising alternative technique to address the extreme of the unknown cost function. The recent study on Bayesian Optimization by Snoek in nine convolutional network parameters is achieved the lowerest error report in the CIFAR-10 benchmark. This article is intended to provide the overview of the mathematical concept behind the Bayesian Optimization over a Gaussian prior.",2017-12-19T21:48:56Z,2017-12-19T21:48:56Z,http://arxiv.org/abs/1712.07233v1,http://arxiv.org/pdf/1712.07233v1,"cs.CV, cs.LG, stat.ML"
A New Reconfigurable Antenna MIMO Architecture for mmWave Communication,"Mojtaba Ahmadi Almasi, Hani Mehrpouyan, Vida Vakilian, Nader Behdad, Hamid Jafarkhani","The large spectrum available in the millimeter-Wave (mmWave) band has emerged as a promising solution for meeting the huge capacity requirements of the 5th generation (5G) wireless networks. However, to fully harness the potential of mmWave communications, obstacles such as severe path loss, channel sparsity and hardware complexity should be overcome. In this paper, we introduce a generalized reconfigurable antenna multiple-input multiple-output (MIMO) architecture that takes advantage of lens-based reconfigurable antennas. The considered antennas can support multiple radiation patterns simultaneously by using a single RF chain. The degrees of freedom provided by the reconfigurable antennas are used to, first, combat channel sparsity in MIMO mmWave systems. Further, to suppress high path loss and shadowing at mmWave frequencies, we use a rate-one space-time block code. Our analysis and simulations show that the proposed reconfigurable MIMO architecture achieves full-diversity gain by using linear receivers and without requiring channel state information at the transmitter. Moreover, simulations show that the proposed architecture outperforms traditional MIMO transmission schemes in mmWave channel settings.",2018-03-25T19:18:53Z,2018-03-25T19:18:53Z,http://arxiv.org/abs/1803.09314v1,http://arxiv.org/pdf/1803.09314v1,"cs.IT, math.IT"
Multi-Task Siamese Neural Network for Improving Replay Attack Detection,"Patrick von Platen, Fei Tao, Gokhan Tur","Automatic speaker verification systems are vulnerable to audio replay attacks which bypass security by replaying recordings of authorized speakers. Replay attack detection (RA) detection systems built upon Residual Neural Networks (ResNet)s have yielded astonishing results on the public benchmark ASVspoof 2019 Physical Access challenge. With most teams using fine-tuned feature extraction pipelines and model architectures, the generalizability of such systems remains questionable though. In this work, we analyse the effect of discriminative feature learning in a multi-task learning (MTL) setting can have on the generalizability and discriminability of RA detection systems. We use a popular ResNet architecture optimized by the cross-entropy criterion as our baseline and compare it to the same architecture optimized by MTL using Siamese Neural Networks (SNN). It can be shown that SNN outperform the baseline by relative 26.8 % Equal Error Rate (EER). We further enhance the model's architecture and demonstrate that SNN with additional reconstruction loss yield another significant improvement of relative 13.8 % EER.",2020-02-16T00:21:16Z,2020-02-16T00:21:16Z,http://arxiv.org/abs/2002.07629v1,http://arxiv.org/pdf/2002.07629v1,"eess.AS, cs.LG, cs.SD, stat.ML"
An Energy-Efficient Accelerator Architecture with Serial Accumulation   Dataflow for Deep CNNs,"Mehdi Ahmadi, Shervin Vakili, J. M. Pierre Langlois","Convolutional Neural Networks (CNNs) have shown outstanding accuracy for many vision tasks during recent years. When deploying CNNs on portable devices and embedded systems, however, the large number of parameters and computations result in long processing time and low battery life. An important factor in designing CNN hardware accelerators is to efficiently map the convolution computation onto hardware resources. In addition, to save battery life and reduce energy consumption, it is essential to reduce the number of DRAM accesses since DRAM consumes orders of magnitude more energy compared to other operations in hardware. In this paper, we propose an energy-efficient architecture which maximally utilizes its computational units for convolution operations while requiring a low number of DRAM accesses. The implementation results show that the proposed architecture performs one image recognition task using the VGGNet model with a latency of 393 ms and only 251.5 MB of DRAM accesses.",2020-02-15T02:40:20Z,2020-02-15T02:40:20Z,http://arxiv.org/abs/2002.07711v1,http://arxiv.org/pdf/2002.07711v1,"eess.SP, cs.AR"
Local Propagation in Constraint-based Neural Network,"Giuseppe Marra, Matteo Tiezzi, Stefano Melacci, Alessandro Betti, Marco Maggini, Marco Gori","In this paper we study a constraint-based representation of neural network architectures. We cast the learning problem in the Lagrangian framework and we investigate a simple optimization procedure that is well suited to fulfil the so-called architectural constraints, learning from the available supervisions. The computational structure of the proposed Local Propagation (LP) algorithm is based on the search for saddle points in the adjoint space composed of weights, neural outputs, and Lagrange multipliers. All the updates of the model variables are locally performed, so that LP is fully parallelizable over the neural units, circumventing the classic problem of gradient vanishing in deep networks. The implementation of popular neural models is described in the context of LP, together with those conditions that trace a natural connection with Backpropagation. We also investigate the setting in which we tolerate bounded violations of the architectural constraints, and we provide experimental evidence that LP is a feasible approach to train shallow and deep networks, opening the road to further investigations on more complex architectures, easily describable by constraints.",2020-02-18T16:47:38Z,2020-04-17T10:20:48Z,http://arxiv.org/abs/2002.07720v2,http://arxiv.org/pdf/2002.07720v2,"cs.LG, stat.ML"
Neural Architecture Search For Fault Diagnosis,"Xudong Li, Yang Hu, Jianhua Zheng, Mingtao Li","Data-driven methods have made great progress in fault diagnosis, especially deep learning method. Deep learning is suitable for processing big data, and has a strong feature extraction ability to realize end-to-end fault diagnosis systems. However, designing neural network architecture requires rich professional knowledge and debugging experience, and a lot of experiments are needed to screen models and hyperparameters, increasing the difficulty of developing deep learning models. Frortunately, neural architecture search (NAS) is developing rapidly, and is becoming one of the next directions for deep learning. In this paper, we proposed a NAS method for fault diagnosis using reinforcement learning. A recurrent neural network is used as an agent to generate network architecture. The accuracy of the generated network on the validation dataset is fed back to the agent as a reward, and the parameters of the agent are updated through the strategy gradient algorithm. We use PHM 2009 Data Challenge gearbox dataset to prove the effectiveness of proposed method, and obtain state-of-the-art results compared with other artificial designed network structures. To author's best knowledge, it's the first time that NAS has been applied in fault diagnosis.",2020-02-19T04:03:51Z,2020-02-19T04:03:51Z,http://arxiv.org/abs/2002.07997v1,http://arxiv.org/pdf/2002.07997v1,"cs.LG, eess.SP"
A Hybrid Systems-based Hierarchical Control Architecture for   Heterogeneous Field Robot Teams,"Chanyoung Ju, Hyoung Il Son","Field robot systems have recently been applied to a wide range of research fields. Making such systems more automated, advanced, and activated requires cooperation among heterogeneous robots. Classic control theory is inefficient in managing large-scale complex dynamic systems. Therefore, the supervisory control theory based on discrete event system needs to be introduced to overcome this limitation. In this study, we propose a hybrid systems-based hierarchical control architecture through a supervisory control-based high-level controller and a traditional control-based low-level controller. The hybrid systems and its dynamics are modeled through a formal method called hybrid automata, and the behavior specifications expressing the control objectives for cooperation are designed. Additionally, a modular supervisor that is more scalable and maintainable than a centralized supervisory controller was synthesized. The proposed hybrid systems and hierarchical control architecture were implemented, validated, and then evaluated for performance through the physics-based simulator. Experimental results confirmed that the heterogeneous field robot team satisfied the given specifications and presented systematic results, validating the efficiency of the proposed control architecture.",2020-02-20T07:39:59Z,2020-02-20T07:39:59Z,http://arxiv.org/abs/2002.08602v1,http://arxiv.org/pdf/2002.08602v1,"cs.RO, cs.SY, eess.SY"
On some neural network architectures that can represent viscosity   solutions of certain high dimensional Hamilton--Jacobi partial differential   equations,"Jérôme Darbon, Tingwei Meng","We propose novel connections between several neural network architectures and viscosity solutions of some Hamilton--Jacobi (HJ) partial differential equations (PDEs) whose Hamiltonian is convex and only depends on the spatial gradient of the solution. To be specific, we prove that under certain assumptions, the two neural network architectures we proposed represent viscosity solutions to two sets of HJ PDEs with zero error. We also implement our proposed neural network architectures using Tensorflow and provide several examples and illustrations. Note that these neural network representations can avoid curve of dimensionality for certain HJ PDEs, since they do not involve neither grids nor discretization. Our results suggest that efficient dedicated hardware implementation for neural networks can be leveraged to evaluate viscosity solutions of certain HJ PDEs.",2020-02-22T18:50:03Z,2020-11-03T19:11:27Z,http://arxiv.org/abs/2002.09750v3,http://arxiv.org/pdf/2002.09750v3,"math.NA, cs.NA"
An Optimization and Generalization Analysis for Max-Pooling Networks,"Alon Brutzkus, Amir Globerson","Max-Pooling operations are a core component of deep learning architectures. In particular, they are part of most convolutional architectures used in machine vision, since pooling is a natural approach to pattern detection problems. However, these architectures are not well understood from a theoretical perspective. For example, we do not understand when they can be globally optimized, and what is the effect of over-parameterization on generalization. Here we perform a theoretical analysis of a convolutional max-pooling architecture, proving that it can be globally optimized, and can generalize well even for highly over-parameterized models. Our analysis focuses on a data generating distribution inspired by pattern detection problem, where a ""discriminative"" pattern needs to be detected among ""spurious"" patterns. We empirically validate that CNNs significantly outperform fully connected networks in our setting, as predicted by our theoretical results.",2020-02-22T22:26:26Z,2021-03-04T11:45:12Z,http://arxiv.org/abs/2002.09781v4,http://arxiv.org/pdf/2002.09781v4,"cs.LG, stat.ML"
"Is my Neural Network Neuromorphic? Taxonomy, Recent Trends and Future   Directions in Neuromorphic Engineering","Sumon Kumar Bose, Jyotibdha Acharya, Arindam Basu","In this paper, we review recent work published over the last 3 years under the umbrella of Neuromorphic engineering to analyze what are the common features among such systems. We see that there is no clear consensus but each system has one or more of the following features:(1) Analog computing (2) Non vonNeumann Architecture and low-precision digital processing (3) Spiking Neural Networks (SNN) with components closely related to biology. We compare recent machine learning accelerator chips to show that indeed analog processing and reduced bit precision architectures have best throughput, energy and area efficiencies. However, pure digital architectures can also achieve quite high efficiencies by just adopting a non von-Neumann architecture. Given the design automation tools for digital hardware design, it raises a question on the likelihood of adoption of analog processing in the near future for industrial designs. Next, we argue about the importance of defining standards and choosing proper benchmarks for the progress of neuromorphic system designs and propose some desired characteristics of such benchmarks. Finally, we show brain-machine interfaces as a potential task that fulfils all the criteria of such benchmarks.",2020-02-27T07:10:23Z,2020-02-27T07:10:23Z,http://arxiv.org/abs/2002.11945v1,http://arxiv.org/pdf/2002.11945v1,"cs.ET, cs.LG, cs.NE, stat.ML"
L1-Adaptive MPPI Architecture for Robust and Agile Control of   Multirotors,"Jintasit Pravitra, Kasey A. Ackerman, Chengyu Cao, Naira Hovakimyan, Evangelos A. Theodorou","This paper presents a multirotor control architecture, where Model Predictive Path Integral Control (MPPI) and L1 adaptive control are combined to achieve both fast model predictive trajectory planning and robust trajectory tracking. MPPI provides a framework to solve nonlinear MPC with complex cost functions in real-time. However, it often lacks robustness, especially when the simulated dynamics are different from the true dynamics. We show that the L1 adaptive controller robustifies the architecture, allowing the overall system to behave similar to the nominal system simulated with MPPI. The architecture is validated in a simulated multirotor racing environment.",2020-03-31T22:53:15Z,2020-03-31T22:53:15Z,http://arxiv.org/abs/2004.00152v1,http://arxiv.org/pdf/2004.00152v1,"eess.SY, cs.SY"
Temporal Shift GAN for Large Scale Video Generation,"Andres Munoz, Mohammadreza Zolfaghari, Max Argus, Thomas Brox","Video generation models have become increasingly popular in the last few years, however the standard 2D architectures used today lack natural spatio-temporal modelling capabilities. In this paper, we present a network architecture for video generation that models spatio-temporal consistency without resorting to costly 3D architectures. The architecture facilitates information exchange between neighboring time points, which improves the temporal consistency of both the high level structure as well as the low-level details of the generated frames. The approach achieves state-of-the-art quantitative performance, as measured by the inception score on the UCF-101 dataset as well as better qualitative results. We also introduce a new quantitative measure (S3) that uses downstream tasks for evaluation. Moreover, we present a new multi-label dataset MaisToy, which enables us to evaluate the generalization of the model.",2020-04-04T00:40:52Z,2020-11-10T19:46:08Z,http://arxiv.org/abs/2004.01823v2,http://arxiv.org/pdf/2004.01823v2,"cs.CV, cs.LG, eess.IV, I.2.10"
Utterance-level Sequential Modeling For Deep Gaussian Process Based   Speech Synthesis Using Simple Recurrent Unit,"Tomoki Koriyama, Hiroshi Saruwatari","This paper presents a deep Gaussian process (DGP) model with a recurrent architecture for speech sequence modeling. DGP is a Bayesian deep model that can be trained effectively with the consideration of model complexity and is a kernel regression model that can have high expressibility. In the previous studies, it was shown that the DGP-based speech synthesis outperformed neural network-based one, in which both models used a feed-forward architecture. To improve the naturalness of synthetic speech, in this paper, we show that DGP can be applied to utterance-level modeling using recurrent architecture models. We adopt a simple recurrent unit (SRU) for the proposed model to achieve a recurrent architecture, in which we can execute fast speech parameter generation by using the high parallelization nature of SRU. The objective and subjective evaluation results show that the proposed SRU-DGP-based speech synthesis outperforms not only feed-forward DGP but also automatically tuned SRU- and long short-term memory (LSTM)-based neural networks.",2020-04-22T19:51:36Z,2020-04-22T19:51:36Z,http://arxiv.org/abs/2004.10823v1,http://arxiv.org/pdf/2004.10823v1,"eess.AS, cs.LG, cs.SD, stat.ML"
VM placement over WDM-TDM AWGR PON Based Data Centre Architecture,"Azza E. A. Eltraify, Sanaa Hamid Mohamed, Jaafar M. H. Elmirghani","Passive optical networks (PON) can play a vital role in data centres and access fog solutions by providing scalable, cost and energy efficient architectures. This paper proposes a Mixed Integer Linear Programming (MILP) model to optimize the placement of virtual machines (VMs) over an energy efficient WDM-TDM AWGR PON based data centre architecture. In this optimization, the use of VMs and their requirements affect the optimum number of servers utilized in the data centre when minimizing the power consumption and enabling more efficient utilization of servers is considered. Two power consumption minimization objectives were examined for up to 20 VMs with different computing and networking requirements. The results indicate that considering the minimization of the processing and networking power consumption in the allocation of VMs in the WDM-TDM AWGR PON can reduce the networking power consumption by up to 70% compared to the minimization of the processing power consumption.",2020-05-07T16:17:12Z,2020-05-07T16:17:12Z,http://arxiv.org/abs/2005.03590v1,http://arxiv.org/pdf/2005.03590v1,"cs.NI, eess.SP"
Segmentation of Macular Edema Datasets with Small Residual 3D U-Net   Architectures,"Jonathan Frawley, Chris G. Willcocks, Maged Habib, Caspar Geenen, David H. Steel, Boguslaw Obara","This paper investigates the application of deep convolutional neural networks with prohibitively small datasets to the problem of macular edema segmentation. In particular, we investigate several different heavily regularized architectures. We find that, contrary to popular belief, neural architectures within this application setting are able to achieve close to human-level performance on unseen test images without requiring large numbers of training examples. Annotating these 3D datasets is difficult, with multiple criteria required. It takes an experienced clinician two days to annotate a single 3D image, whereas our trained model achieves similar performance in less than a second. We found that an approach which uses targeted dataset augmentation, alongside architectural simplification with an emphasis on residual design, has acceptable generalization performance - despite relying on fewer than 15 training examples.",2020-05-10T15:34:46Z,2020-05-10T15:34:46Z,http://arxiv.org/abs/2005.04697v1,http://arxiv.org/pdf/2005.04697v1,"eess.IV, cs.CV, cs.LG"
"Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,   and Cross-corpus Setting for Speech Emotion Recognition","Siddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, Björn W. Schuller","Speech emotion recognition systems (SER) can achieve high accuracy when the training and test data are identically distributed, but this assumption is frequently violated in practice and the performance of SER systems plummet against unforeseen data shifts. The design of robust models for accurate SER is challenging, which limits its use in practical applications. In this paper we propose a deeper neural network architecture wherein we fuse DenseNet, LSTM and Highway Network to learn powerful discriminative features which are robust to noise. We also propose data augmentation with our network architecture to further improve the robustness. We comprehensively evaluate the architecture coupled with data augmentation against (1) noise, (2) adversarial attacks and (3) cross-corpus settings. Our evaluations on the widely used IEMOCAP and MSP-IMPROV datasets show promising results when compared with existing studies and state-of-the-art models.",2020-05-18T04:21:02Z,2020-07-26T02:44:44Z,http://arxiv.org/abs/2005.08453v3,http://arxiv.org/pdf/2005.08453v3,"cs.SD, eess.AS"
Brain-inspired Distributed Cognitive Architecture,"Leendert A Remmelzwaal, Amit K Mishra, George F R Ellis","In this paper we present a brain-inspired cognitive architecture that incorporates sensory processing, classification, contextual prediction, and emotional tagging. The cognitive architecture is implemented as three modular web-servers, meaning that it can be deployed centrally or across a network for servers. The experiments reveal two distinct operations of behaviour, namely high- and low-salience modes of operations, which closely model attention in the brain. In addition to modelling the cortex, we have demonstrated that a bio-inspired architecture introduced processing efficiencies. The software has been published as an open source platform, and can be easily extended by future research teams. This research lays the foundations for bio-realistic attention direction and sensory selection, and we believe that it is a key step towards achieving a bio-realistic artificial intelligent system.",2020-05-18T11:38:32Z,2020-05-18T11:38:32Z,http://arxiv.org/abs/2005.08603v1,http://arxiv.org/pdf/2005.08603v1,"q-bio.NC, cs.NE"
"Exploring Recurrent, Memory and Attention Based Architectures for   Scoring Interactional Aspects of Human-Machine Text Dialog","Vikram Ramanarayanan, Matthew Mulholland, Debanjan Ghosh","An important step towards enabling English language learners to improve their conversational speaking proficiency involves automated scoring of multiple aspects of interactional competence and subsequent targeted feedback. This paper builds on previous work in this direction to investigate multiple neural architectures -- recurrent, attention and memory based -- along with feature-engineered models for the automated scoring of interactional and topic development aspects of text dialog data. We conducted experiments on a conversational database of text dialogs from human learners interacting with a cloud-based dialog system, which were triple-scored along multiple dimensions of conversational proficiency. We find that fusion of multiple architectures performs competently on our automated scoring task relative to expert inter-rater agreements, with (i) hand-engineered features passed to a support vector learner and (ii) transformer-based architectures contributing most prominently to the fusion.",2020-05-20T03:23:00Z,2020-05-20T03:23:00Z,http://arxiv.org/abs/2005.09834v1,http://arxiv.org/pdf/2005.09834v1,"cs.HC, cs.LG, cs.SD, eess.AS"
Classical and Quantum Data Interaction in Programming Languages: A   Runtime Architecture,"Evandro Chagas Ribeiro da Rosa, Rafael de Santiago","We propose a runtime architecture that can be used in the development of a quantum programming language and its programming environment. The proposed runtime architecture enables dynamic interaction between classical and quantum data following the restriction that a quantum computer is available in the cloud as a batch computer, with no interaction with the classical computer during its execution. It is done by leaving the quantum code generation for the runtime and introducing the concept of futures for quantum measurements. When implemented in a quantum programming language, those strategies aim to facilitate the development of quantum applications, especially for beginning programmers and students. Being suitable for the current Noisy Intermediate-Scale Quantum (NISQ) Computers, the runtime architecture is also appropriate for simulation and future Fault-Tolerance Quantum Computers.",2020-05-29T23:51:24Z,2020-05-29T23:51:24Z,http://arxiv.org/abs/2006.00131v1,http://arxiv.org/pdf/2006.00131v1,"quant-ph, cs.PL"
A provably stable neural network Turing Machine,"John Stogin, Ankur Mali, C Lee Giles","We introduce a neural stack architecture, including a differentiable parametrized stack operator that approximates stack push and pop operations for suitable choices of parameters that explicitly represents a stack. We prove the stability of this stack architecture: after arbitrarily many stack operations, the state of the neural stack still closely resembles the state of the discrete stack. Using the neural stack with a recurrent neural network, we introduce a neural network Pushdown Automaton (nnPDA) and prove that nnPDA with finite/bounded neurons and time can simulate any PDA. Furthermore, we extend our construction and propose new architecture neural state Turing Machine (nnTM). We prove that differentiable nnTM with bounded neurons can simulate Turing Machine (TM) in real-time. Just like the neural stack, these architectures are also stable. Finally, we extend our construction to show that differentiable nnTM is equivalent to Universal Turing Machine (UTM) and can simulate any TM with only \textbf{seven finite/bounded precision} neurons. This work provides a new theoretical bound for the computational capability of bounded precision RNNs augmented with memory.",2020-06-05T19:45:49Z,2022-09-18T16:20:48Z,http://arxiv.org/abs/2006.03651v4,http://arxiv.org/pdf/2006.03651v4,"cs.LG, cs.FL, stat.ML"
Speedy Performance Estimation for Neural Architecture Search,"Binxin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, Yarin Gal","Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopped validation accuracy may correlate poorly with fully trained performance, and model-based estimators require large training sets. We instead propose to estimate the final test performance based on a simple measure of training speed. Our estimator is theoretically motivated by the connection between generalisation and training speed, and is also inspired by the reformulation of a PAC-Bayes bound under the Bayesian setting. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate on various NAS search spaces that our estimator consistently outperforms other alternatives in achieving better correlation with the true test performance rankings. We further show that our estimator can be easily incorporated into both query-based and one-shot NAS methods to improve the speed or quality of the search.",2020-06-08T11:48:09Z,2021-06-08T02:41:51Z,http://arxiv.org/abs/2006.04492v2,http://arxiv.org/pdf/2006.04492v2,"stat.ML, cs.LG"
Learning to Stop While Learning to Predict,"Xinshi Chen, Hanjun Dai, Yu Li, Xin Gao, Le Song","There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stopping criteria for outputting results at different iterations, many algorithm-inspired deep models are restricted to a ``fixed-depth'' for all inputs. Similar to algorithms, the optimal depth of a deep architecture may be different for different input instances, either to avoid ``over-thinking'', or because we want to compute less for operations converged already. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stopping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes perspective and design a novel and effective training procedure which decomposes the task into an oracle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, including learning sparse recovery, few-shot meta learning, and computer vision tasks.",2020-06-09T07:22:01Z,2020-06-09T07:22:01Z,http://arxiv.org/abs/2006.05082v1,http://arxiv.org/pdf/2006.05082v1,"cs.LG, stat.ML"
Wide and Deep Graph Neural Networks with Distributed Online Learning,"Zhan Gao, Fernando Gama, Alejandro Ribeiro","Graph neural networks (GNNs) learn representations from network data with naturally distributed architectures, rendering them well-suited candidates for decentralized learning. Oftentimes, this decentralized graph support changes with time due to link failures or topology variations. These changes create a mismatch between the graphs on which GNNs were trained and the ones on which they are tested. Online learning can be used to retrain GNNs at testing time, overcoming this issue. However, most online algorithms are centralized and work on convex problems (which GNNs rarely lead to). This paper proposes the Wide and Deep GNN (WD-GNN), a novel architecture that can be easily updated with distributed online learning mechanisms. The WD-GNN comprises two components: the wide part is a bank of linear graph filters and the deep part is a GNN. At training time, the joint architecture learns a nonlinear representation from data. At testing time, the deep part (nonlinear) is left unchanged, while the wide part is retrained online, leading to a convex problem. We derive convergence guarantees for this online retraining procedure and further propose a decentralized alternative. Experiments on the robot swarm control for flocking corroborate theory and show potential of the proposed architecture for distributed online learning.",2020-06-11T12:48:03Z,2020-10-24T04:14:55Z,http://arxiv.org/abs/2006.06376v2,http://arxiv.org/pdf/2006.06376v2,"cs.LG, stat.ML"
DNF-Net: A Neural Architecture for Tabular Data,"Ami Abutbul, Gal Elidan, Liran Katzir, Ran El-Yaniv","A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present DNF-Net a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. In addition, DNF-Net promotes localized decisions that are taken over small subsets of the features. We present an extensive empirical study showing that DNF-Nets significantly and consistently outperform FCNs over tabular data. With relatively few hyperparameters, DNF-Nets open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of DNF-Net including the three inductive bias elements, namely, Boolean formulation, locality, and feature selection.",2020-06-11T14:21:45Z,2020-06-11T14:21:45Z,http://arxiv.org/abs/2006.06465v1,http://arxiv.org/pdf/2006.06465v1,"cs.LG, stat.ML"
FrostNet: Towards Quantization-Aware Network Architecture Search,"Taehoon Kim, YoungJoon Yoo, Jihoon Yang","INT8 quantization has become one of the standard techniques for deploying convolutional neural networks (CNNs) on edge devices to reduce the memory and computational resource usages. By analyzing quantized performances of existing mobile-target network architectures, we can raise an issue regarding the importance of network architecture for optimal INT8 quantization. In this paper, we present a new network architecture search (NAS) procedure to find a network that guarantees both full-precision (FLOAT32) and quantized (INT8) performances. We first propose critical but straightforward optimization method which enables quantization-aware training (QAT) : floating-point statistic assisting (StatAssist) and stochastic gradient boosting (GradBoost). By integrating the gradient-based NAS with StatAssist and GradBoost, we discovered a quantization-efficient network building block, Frost bottleneck. Furthermore, we used Frost bottleneck as the building block for hardware-aware NAS to obtain quantization-efficient networks, FrostNets, which show improved quantization performances compared to other mobile-target networks while maintaining competitive FLOAT32 performance. Our FrostNets achieve higher recognition accuracy than existing CNNs with comparable latency when quantized, due to higher latency reduction rate (average 65%).",2020-06-17T06:40:43Z,2020-11-30T10:09:33Z,http://arxiv.org/abs/2006.09679v4,http://arxiv.org/pdf/2006.09679v4,"cs.LG, cs.CV, stat.ML"
Neural Anisotropy Directions,"Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard","In this work, we analyze the role of the network architecture in shaping the inductive bias of deep classifiers. To that end, we start by focusing on a very simple problem, i.e., classifying a class of linearly separable distributions, and show that, depending on the direction of the discriminative feature of the distribution, many state-of-the-art deep convolutional neural networks (CNNs) have a surprisingly hard time solving this simple task. We then define as neural anisotropy directions (NADs) the vectors that encapsulate the directional inductive bias of an architecture. These vectors, which are specific for each architecture and hence act as a signature, encode the preference of a network to separate the input data based on some particular features. We provide an efficient method to identify NADs for several CNN architectures and thus reveal their directional inductive biases. Furthermore, we show that, for the CIFAR-10 dataset, NADs characterize the features used by CNNs to discriminate between different classes.",2020-06-17T08:36:28Z,2020-10-14T10:21:58Z,http://arxiv.org/abs/2006.09717v2,http://arxiv.org/pdf/2006.09717v2,"cs.LG, cs.CV, stat.ML"
DrNAS: Dirichlet Neural Architecture Search,"Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, Cho-Jui Hsieh","This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting. On NAS-Bench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.",2020-06-18T08:23:02Z,2021-03-16T02:32:55Z,http://arxiv.org/abs/2006.10355v4,http://arxiv.org/pdf/2006.10355v4,"cs.LG, cs.CV, stat.ML"
On the Empirical Neural Tangent Kernel of Standard Finite-Width   Convolutional Neural Network Architectures,"Maxim Samarin, Volker Roth, David Belius","The Neural Tangent Kernel (NTK) is an important milestone in the ongoing effort to build a theory for deep learning. Its prediction that sufficiently wide neural networks behave as kernel methods, or equivalently as random feature models, has been confirmed empirically for certain wide architectures. It remains an open question how well NTK theory models standard neural network architectures of widths common in practice, trained on complex datasets such as ImageNet. We study this question empirically for two well-known convolutional neural network architectures, namely AlexNet and LeNet, and find that their behavior deviates significantly from their finite-width NTK counterparts. For wider versions of these networks, where the number of channels and widths of fully-connected layers are increased, the deviation decreases.",2020-06-24T11:40:36Z,2020-06-24T11:40:36Z,http://arxiv.org/abs/2006.13645v1,http://arxiv.org/pdf/2006.13645v1,"cs.LG, cs.CV, stat.ML"
Incremental Training of a Recurrent Neural Network Exploiting a   Multi-Scale Dynamic Memory,"Antonio Carta, Alessandro Sperduti, Davide Bacciu","The effectiveness of recurrent neural networks can be largely influenced by their ability to store into their dynamical memory information extracted from input sequences at different frequencies and timescales. Such a feature can be introduced into a neural architecture by an appropriate modularization of the dynamic memory. In this paper we propose a novel incrementally trained recurrent architecture targeting explicitly multi-scale learning. First, we show how to extend the architecture of a simple RNN by separating its hidden state into different modules, each subsampling the network hidden activations at different frequencies. Then, we discuss a training algorithm where new modules are iteratively added to the model to learn progressively longer dependencies. Each new module works at a slower frequency than the previous ones and it is initialized to encode the subsampled sequence of hidden activations. Experimental results on synthetic and real-world datasets on speech recognition and handwritten characters show that the modular architecture and the incremental training algorithm improve the ability of recurrent neural networks to capture long-term dependencies.",2020-06-29T08:35:49Z,2020-06-29T08:35:49Z,http://arxiv.org/abs/2006.16800v1,http://arxiv.org/pdf/2006.16800v1,"cs.LG, stat.ML"
Amortized Neural Networks for Low-Latency Speech Recognition,"Jonathan Macoskey, Grant P. Strimel, Jinru Su, Ariya Rastrow","We introduce Amortized Neural Networks (AmNets), a compute cost- and latency-aware network architecture particularly well-suited for sequence modeling tasks. We apply AmNets to the Recurrent Neural Network Transducer (RNN-T) to reduce compute cost and latency for an automatic speech recognition (ASR) task. The AmNets RNN-T architecture enables the network to dynamically switch between encoder branches on a frame-by-frame basis. Branches are constructed with variable levels of compute cost and model capacity. Here, we achieve variable compute for two well-known candidate techniques: one using sparse pruning and the other using matrix factorization. Frame-by-frame switching is determined by an arbitrator network that requires negligible compute overhead. We present results using both architectures on LibriSpeech data and show that our proposed architecture can reduce inference cost by up to 45\% and latency to nearly real-time without incurring a loss in accuracy.",2021-08-03T15:05:13Z,2021-08-03T15:05:13Z,http://arxiv.org/abs/2108.01553v1,http://arxiv.org/pdf/2108.01553v1,"eess.AS, cs.SD"
Bifocal Neural ASR: Exploiting Keyword Spotting for Inference   Optimization,"Jonathan Macoskey, Grant P. Strimel, Ariya Rastrow","We present Bifocal RNN-T, a new variant of the Recurrent Neural Network Transducer (RNN-T) architecture designed for improved inference time latency on speech recognition tasks. The architecture enables a dynamic pivot for its runtime compute pathway, namely taking advantage of keyword spotting to select which component of the network to execute for a given audio frame. To accomplish this, we leverage a recurrent cell we call the Bifocal LSTM (BFLSTM), which we detail in the paper. The architecture is compatible with other optimization strategies such as quantization, sparsification, and applying time-reduction layers, making it especially applicable for deployed, real-time speech recognition settings. We present the architecture and report comparative experimental results on voice-assistant speech recognition tasks. Specifically, we show our proposed Bifocal RNN-T can improve inference cost by 29.1% with matching word error rates and only a minor increase in memory size.",2021-08-03T18:58:39Z,2021-08-03T18:58:39Z,http://arxiv.org/abs/2108.01704v1,http://arxiv.org/pdf/2108.01704v1,"eess.AS, cs.SD"
An Image-based Generator Architecture for Synthetic Image Refinement,Alex Nasser,"Proposed are alternative generator architectures for Boundary Equilibrium Generative Adversarial Networks, motivated by Learning from Simulated and Unsupervised Images through Adversarial Training. It disentangles the need for a noise-based latent space. The generator will operate mainly as a refiner network to gain a photo-realistic presentation of the given synthetic images. It also attempts to resolve the latent space's poorly understood properties by eliminating the need for noise injection and replacing it with an image-based concept. The new flexible and simple generator architecture will also give the power to control the trade-off between restrictive refinement and expressiveness ability. Contrary to other available methods, this architecture will not require a paired or unpaired dataset of real and synthetic images for the training phase. Only a relatively small set of real images would suffice.",2021-08-10T22:58:07Z,2021-08-10T22:58:07Z,http://arxiv.org/abs/2108.04957v1,http://arxiv.org/pdf/2108.04957v1,"cs.CV, cs.LG, eess.IV"
NeighCNN: A CNN based SAR Speckle Reduction using Feature preserving   Loss Function,"Praveen Ravirathinam, Darshan Agrawal, J. Jennifer Ranjani","Coherent imaging systems like synthetic aperture radar are susceptible to multiplicative noise that makes applications like automatic target recognition challenging. In this paper, NeighCNN, a deep learning-based speckle reduction algorithm that handles multiplicative noise with relatively simple convolutional neural network architecture, is proposed. We have designed a loss function which is an unique combination of weighted sum of Euclidean, neighbourhood, and perceptual loss for training the deep network. Euclidean and neighbourhood losses take pixel-level information into account, whereas perceptual loss considers high-level semantic features between two images. Various synthetic, as well as real SAR images, are used for testing the NeighCNN architecture, and the results verify the noise removal and edge preservation abilities of the proposed architecture. Performance metrics like peak-signal-to-noise ratio, structural similarity index, and universal image quality index are used for evaluating the efficiency of the proposed architecture on synthetic images.",2021-08-26T04:20:07Z,2021-08-26T04:20:07Z,http://arxiv.org/abs/2108.11573v1,http://arxiv.org/pdf/2108.11573v1,"eess.IV, cs.CV"
Enterprise Architecture Model Transformation Engine,"Erik Heiland, Peter Hillmann, Andreas Karcher","With increasing linkage within value chains, the IT systems of different companies are also being connected with each other. This enables the integration of services within the movement of Industry 4.0 in order to improve the quality and performance of the processes. Enterprise architecture models form the basis for this with a better buisness IT-alignment. However, the heterogeneity of the modeling frameworks and description languages makes a concatenation considerably difficult, especially differences in syntax, semantic and relations. Therefore, this paper presents a transformation engine to convert enterprise architecture models between several languages. We developed the first generic translation approach that is free of specific meta-modeling, which is flexible adaptable to arbitrary modeling languages. The transformation process is defined by various pattern matching techniques using a rule-based description language. It uses set theory and first-order logic for an intuitive description as a basis. The concept is practical evaluated using an example in the area of a large German IT-service provider. Anyhow, the approach is applicable between a wide range of enterprise architecture frameworks.",2021-08-15T11:10:42Z,2021-08-15T11:10:42Z,http://arxiv.org/abs/2108.13169v1,http://arxiv.org/pdf/2108.13169v1,"cs.DC, cs.AI, cs.CV, cs.PL, cs.SE, cs.SY, eess.SY"
Deep Learning Transformer Architecture for Named Entity Recognition on   Low Resourced Languages: State of the art results,Ridewaan Hanslo,"This paper reports on the evaluation of Deep Learning (DL) transformer architecture models for Named-Entity Recognition (NER) on ten low-resourced South African (SA) languages. In addition, these DL transformer models were compared to other Neural Network and Machine Learning (ML) NER models. The findings show that transformer models substantially improve performance when applying discrete fine-tuning parameters per language. Furthermore, fine-tuned transformer models outperform other neural network and machine learning models on NER with the low-resourced SA languages. For example, the transformer models obtained the highest F-scores for six of the ten SA languages and the highest average F-score surpassing the Conditional Random Fields ML model. Practical implications include developing high-performance NER capability with less effort and resource costs, potentially improving downstream NLP tasks such as Machine Translation (MT). Therefore, the application of DL transformer architecture models for NLP NER sequence tagging tasks on low-resourced SA languages is viable. Additional research could evaluate the more recent transformer architecture models on other Natural Language Processing tasks and applications, such as Phrase chunking, MT, and Part-of-Speech tagging.",2021-11-01T11:02:01Z,2022-10-01T11:38:57Z,http://arxiv.org/abs/2111.00830v2,http://arxiv.org/pdf/2111.00830v2,"cs.CL, cs.AI, I.2.7"
How Neural Architectures Affect Deep Learning for Communication   Networks?,"Yifei Shen, Jun Zhang, Khaled B. Letaief","In recent years, there has been a surge in applying deep learning to various challenging design problems in communication networks. The early attempts adopt neural architectures inherited from applications such as computer vision, which suffer from poor generalization, scalability, and lack of interpretability. To tackle these issues, domain knowledge has been integrated into the neural architecture design, which achieves near-optimal performance in large-scale networks and generalizes well under different system settings. This paper endeavors to theoretically validate the importance and effects of neural architectures when applying deep learning to design communication networks. We prove that by exploiting permutation invariance, a common property in communication networks, graph neural networks (GNNs) converge faster and generalize better than fully connected multi-layer perceptrons (MLPs), especially when the number of nodes (e.g., users, base stations, or antennas) is large. Specifically, we prove that under common assumptions, for a communication network with $n$ nodes, GNNs converge $O(n \log n)$ times faster and their generalization error is $O(n)$ times lower, compared with MLPs.",2021-11-03T13:34:43Z,2022-03-21T08:02:27Z,http://arxiv.org/abs/2111.02215v2,http://arxiv.org/pdf/2111.02215v2,"cs.IT, math.IT"
NAS-Bench-x11 and the Power of Learning Curves,"Shen Yan, Colin White, Yash Savani, Frank Hutter","While early research in neural architecture search (NAS) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-fidelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the final validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-fidelity algorithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11.",2021-11-05T16:41:06Z,2021-11-05T16:41:06Z,http://arxiv.org/abs/2111.03602v1,http://arxiv.org/pdf/2111.03602v1,"cs.LG, cs.AI, cs.NE, stat.ML"
Six Critical Challenges for 6G Wireless Systems,"Harsh Tataria, Mansoor Shafi, Mischa Dohler, Shu Sun","A large number of papers are now appearing on sixth-generation (6G) wireless systems, covering different aspects, ranging from vision, architecture, applications, and technology breakthroughs. With cellular systems in mind, this paper presents six critical, yet fundamental challenges that must be overcome before development and deployment of 6G systems. These include: Opening the sub-terahertz (sub-THz) spectrum for increased bandwidths and the ability to utilize these bandwidths, pushing the limits of semiconductor technologies for operation within the sub-THz bands, transceiver design and architectures to realize the high peak data rates, and realizations of sub-millisecond latencies at the network-level to achieve the 6G key performance indicators. Additionally, since 6G systems will not be introduced in a green fields environment, backwards compatibility with existing systems is discussed. Where possible, we present practical solutions to realize the identified challenges.",2021-11-17T02:46:49Z,2021-11-17T02:46:49Z,http://arxiv.org/abs/2111.08871v1,http://arxiv.org/pdf/2111.08871v1,"cs.IT, cs.AR, cs.NI, math.IT"
Trajectory Prediction & Path Planning for an Object Intercepting UAV   with a Mounted Depth Camera,"Jasper Tan, Arijit Dasgupta, Arjun Agrawal, Sutthiphong Srigrarom","A novel control & software architecture using ROS C++ is introduced for object interception by a UAV with a mounted depth camera and no external aid. Existing work in trajectory prediction focused on the use of off-board tools like motion capture rooms to intercept thrown objects. The present study designs the UAV architecture to be completely on-board capable of object interception with the use of a depth camera and point cloud processing. The architecture uses an iterative trajectory prediction algorithm for non-propelled objects like a ping-pong ball. A variety of path planning approaches to object interception and their corresponding scenarios are discussed, evaluated & simulated in Gazebo. The successful simulations exemplify the potential of using the proposed architecture for the on-board autonomy of UAVs intercepting objects.",2021-11-17T13:04:02Z,2021-11-17T13:04:02Z,http://arxiv.org/abs/2111.09083v1,http://arxiv.org/pdf/2111.09083v1,"cs.RO, cs.SY, eess.SY"
A Modular 1D-CNN Architecture for Real-time Digital Pre-distortion,"Udara De Silva, Toshiaki Koike-Akino, Rui Ma, Ao Yamashita, Hideyuki Nakamizo",This study reports a novel hardware-friendly modular architecture for implementing one dimensional convolutional neural network (1D-CNN) digital predistortion (DPD) technique to linearize RF power amplifier (PA) real-time.The modular nature of our design enables DPD system adaptation for variable resource and timing constraints.Our work also presents a co-simulation architecture to verify the DPD performance with an actual power amplifier hardware-in-the-loop.The experimental results with 100 MHz signals show that the proposed 1D-CNN obtains superior performance compared with other neural network architectures for real-time DPD application.,2021-11-18T11:30:23Z,2021-11-18T11:30:23Z,http://arxiv.org/abs/2111.09637v1,http://arxiv.org/pdf/2111.09637v1,"eess.SP, cs.LG"
Audio representations for deep learning in sound synthesis: A review,"Anastasia Natsiou, Sean O'Leary","The rise of deep learning algorithms has led many researchers to withdraw from using classic signal processing methods for sound generation. Deep learning models have achieved expressive voice synthesis, realistic sound textures, and musical notes from virtual instruments. However, the most suitable deep learning architecture is still under investigation. The choice of architecture is tightly coupled to the audio representations. A sound's original waveform can be too dense and rich for deep learning models to deal with efficiently - and complexity increases training time and computational cost. Also, it does not represent sound in the manner in which it is perceived. Therefore, in many cases, the raw audio has been transformed into a compressed and more meaningful form using upsampling, feature-extraction, or even by adopting a higher level illustration of the waveform. Furthermore, conditional on the form chosen, additional conditioning representations, different model architectures, and numerous metrics for evaluating the reconstructed sound have been investigated. This paper provides an overview of audio representations applied to sound synthesis using deep learning. Additionally, it presents the most significant methods for developing and evaluating a sound synthesis architecture using deep learning models, always depending on the audio representation.",2022-01-07T15:08:47Z,2022-01-07T15:08:47Z,http://arxiv.org/abs/2201.02490v1,http://arxiv.org/pdf/2201.02490v1,"cs.SD, cs.LG, eess.AS"
Modeling Short-Range Microwave Networks to Scale Superconducting Quantum   Computation,"Nicholas LaRacuente, Kaitlin N. Smith, Poolad Imany, Kevin L. Silverman, Frederic T. Chong","A core challenge for superconducting quantum computers is to scale up the number of qubits in each processor without increasing noise or cross-talk. Distributed quantum computing across small qubit arrays, known as chiplets, can address these challenges in a scalable manner. We propose a chiplet architecture over microwave links with potential to exceed monolithic performance on near-term hardware. Our methods of modeling and evaluating the chiplet architecture bridge the physical and network layers in these processors. We find evidence that distributing computation across chiplets may reduce the overall error rates associated with moving data across the device, despite higher error figures for transfers across links. Preliminary analyses suggest that latency is not substantially impacted, and that at least some applications and architectures may avoid bottlenecks around chiplet boundaries. In the long-term, short-range networks may underlie quantum computers just as local area networks underlie classical datacenters and supercomputers today.",2022-01-21T18:20:59Z,2024-12-18T20:40:29Z,http://arxiv.org/abs/2201.08825v4,http://arxiv.org/pdf/2201.08825v4,"quant-ph, cs.AR, cs.DC"
A Grid Fault Tolerant Doubly Fed Induction Generator Wind Turbine via   Series Connected Grid Side Converter,"Patrick S Flannery, Giri Venkataramanan","With steadily increasing wind turbine penetration, regulatory standards for grid interconnection are evolving to require that wind generation systems ride-through disturbances such as faults and support the grid during such events. Conventional modifications to the doubly fed induction generation (DFIG) architecture for providing ride-through result in limited control of the turbine shaft and grid current during fault events. A DFIG architecture in which the grid side converter is connected in series as opposed to parallel with the grid connection has shown improved low voltage ride through but poor power processing capabilities. In this paper a unified DFIG wind turbine architecture which employs a parallel grid side rectifier and series grid side converter is presented. The combination of these two converters enables unencumbered power processing and improved voltage disturbance ride through. A dynamic model and control structure for this unified architecture is developed. The operation of the system is illustrated using computer simulations.",2022-01-21T19:55:45Z,2022-01-21T19:55:45Z,http://arxiv.org/abs/2201.08879v1,http://arxiv.org/pdf/2201.08879v1,"eess.SY, cs.SY"
Prophet: A Speculative Multi-threading Execution Model with   Architectural Support Based on CMP,"Dong Zhaoyu, Gao Bing, Zhao Yinliang, Song Shaolong, Du Yanning","Speculative multi-threading (SpMT) has been proposed as a perspective method to exploit Chip Multiprocessors (CMP) hardware potential. It is a thread level speculation (TLS) model mainly depending on software and hardware co-design. This paper researches speculative thread-level parallelism of general purpose programs and a speculative multi-threading execution model called Prophet is presented. The architectural support for Prophet execution model is designed based on CMP. In Prophet the inter-thread data dependency are predicted by pre-computation slice (p-slice) to reduce RAW violation. Prophet multi-versioning Cache system along with thread state control mechanism in architectural support are utilized for buffering the speculative data, and a snooping bus based cache coherence protocol is used to detect data dependence violation. The simulation-based evaluation shows that the Prophet system could achieve significant speedup for general-purpose programs.",2014-12-10T08:43:27Z,2014-12-10T08:43:27Z,http://arxiv.org/abs/1412.3224v1,http://arxiv.org/pdf/1412.3224v1,"cs.AR, C.1.4"
A Game of Attribute Decomposition for Software Architecture Design,"Jiamou Liu, Ziheng Wei","Attribute-driven software architecture design aims to provide decision support by taking into account the quality attributes of softwares. A central question in this process is: What architecture design best fulfills the desirable software requirements? To answer this question, a system designer needs to make tradeoffs among several potentially conflicting quality attributes. Such decisions are normally ad-hoc and rely heavily on experiences. We propose a mathematical approach to tackle this problem. Game theory naturally provides the basic language: Players represent requirements, and strategies involve setting up coalitions among the players. In this way we propose a novel model, called decomposition game, for attribute-driven design. We present its solution concept based on the notion of cohesion and expansion-freedom and prove that a solution always exists. We then investigate the computational complexity of obtaining a solution. The game model and the algorithms may serve as a general framework for providing useful guidance for software architecture design. We present our results through running examples and a case study on a real-life software project.",2015-08-12T05:07:04Z,2015-08-12T05:07:04Z,http://arxiv.org/abs/1508.02812v1,http://arxiv.org/pdf/1508.02812v1,"cs.GT, cs.SE, D.2.11; F.2.0"
A General Framework for Low-Resolution Receivers for MIMO Channels,"Stefano Rini, Luca Barletta, Yonina C. Eldar, Elza Erkip","The capacity of a discrete-time multi-input multi-output (MIMO) Gaussian channel with output quantization is investigated for different receiver architectures. A general formulation of this problem is proposed in which the antenna outputs are processed by analog combiners while sign quantizers are used for analog-to-digital conversion. To exemplify this approach, four analog receiver architectures of varying generality and complexity are considered: (a) multiple antenna selection and sign quantization of the antenna outputs, (b) single antenna selection and multilevel quantization, (c) multiple antenna selection and multilevel quantization, and (d) linear combining of the antenna outputs and multilevel quantization. Achievable rates are studied as a function of the number of available sign quantizers and compared among different architectures. In particular, it is shown that architecture (a) is sufficient to attain the optimal high signal-to-noise ratio performance for a MIMO receiver in which the number of antennas is larger than the number of sign quantizers. Numerical evaluations of the average performance are presented for the case in which the channel gains are i.i.d. Gaussian.",2017-02-27T03:31:43Z,2017-07-07T06:31:34Z,http://arxiv.org/abs/1702.08133v2,http://arxiv.org/pdf/1702.08133v2,"cs.IT, math.IT"
Deep Recurrent NMF for Speech Separation by Unfolding Iterative   Thresholding,"Scott Wisdom, Thomas Powers, James Pitton, Les Atlas","In this paper, we propose a novel recurrent neural network architecture for speech separation. This architecture is constructed by unfolding the iterations of a sequential iterative soft-thresholding algorithm (ISTA) that solves the optimization problem for sparse nonnegative matrix factorization (NMF) of spectrograms. We name this network architecture deep recurrent NMF (DR-NMF). The proposed DR-NMF network has three distinct advantages. First, DR-NMF provides better interpretability than other deep architectures, since the weights correspond to NMF model parameters, even after training. This interpretability also provides principled initializations that enable faster training and convergence to better solutions compared to conventional random initialization. Second, like many deep networks, DR-NMF is an order of magnitude faster at test time than NMF, since computation of the network output only requires evaluating a few layers at each time step. Third, when a limited amount of training data is available, DR-NMF exhibits stronger generalization and separation performance compared to sparse NMF and state-of-the-art long-short term memory (LSTM) networks. When a large amount of training data is available, DR-NMF achieves lower yet competitive separation performance compared to LSTM networks.",2017-09-21T01:46:19Z,2017-09-21T01:46:19Z,http://arxiv.org/abs/1709.07124v1,http://arxiv.org/pdf/1709.07124v1,"cs.SD, cs.LG, stat.ML"
On Characterizing the Capacity of Neural Networks using Algebraic   Topology,"William H. Guss, Ruslan Salakhutdinov","The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for fully-connected neural networks.",2018-02-13T02:32:10Z,2018-02-13T02:32:10Z,http://arxiv.org/abs/1802.04443v1,http://arxiv.org/pdf/1802.04443v1,"cs.LG, cs.CG, cs.NE, math.AT, stat.ML"
Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for   Optimizing Protein Functions,"Anvita Gupta, James Zou","Generative Adversarial Networks (GANs) represent an attractive and novel approach to generate realistic data, such as genes, proteins, or drugs, in synthetic biology. Here, we apply GANs to generate synthetic DNA sequences encoding for proteins of variable length. We propose a novel feedback-loop architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene sequences for desired properties using an external function analyzer. The proposed architecture also has the advantage that the analyzer need not be differentiable. We apply the feedback-loop mechanism to two examples: 1) generating synthetic genes coding for antimicrobial peptides, and 2) optimizing synthetic genes for the secondary structure of their resulting peptides. A suite of metrics demonstrate that the GAN generated proteins have desirable biophysical properties. The FBGAN architecture can also be used to optimize GAN-generated datapoints for useful properties in domains beyond genomics.",2018-04-05T07:17:42Z,2018-04-05T07:17:42Z,http://arxiv.org/abs/1804.01694v1,http://arxiv.org/pdf/1804.01694v1,"q-bio.GN, cs.LG, cs.NE"
Guided Proceduralization: Optimizing Geometry Processing and Grammar   Extraction for Architectural Models,"Ilke Demir, Daniel G. Aliaga","We describe a guided proceduralization framework that optimizes geometry processing on architectural input models to extract target grammars. We aim to provide efficient artistic workflows by creating procedural representations from existing 3D models, where the procedural expressiveness is controlled by the user. Architectural reconstruction and modeling tasks have been handled as either time consuming manual processes or procedural generation with difficult control and artistic influence. We bridge the gap between creation and generation by converting existing manually modeled architecture to procedurally editable parametrized models, and carrying the guidance to procedural domain by letting the user define the target procedural representation. Additionally, we propose various applications of such procedural representations, including guided completion of point cloud models, controllable 3D city modeling, and other benefits of procedural modeling.",2018-07-06T22:22:53Z,2018-07-06T22:22:53Z,http://arxiv.org/abs/1807.02578v1,http://arxiv.org/pdf/1807.02578v1,"cs.GR, cs.CV, 68U05, 65D18"
Adaptive Neural Trees,"Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya Nori","Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.",2018-07-17T23:01:35Z,2019-06-09T19:32:34Z,http://arxiv.org/abs/1807.06699v5,http://arxiv.org/pdf/1807.06699v5,"cs.NE, cs.CV, cs.LG, stat.ML"
Towards Automated Deep Learning: Efficient Joint Neural Architecture and   Hyperparameter Search,"Arber Zela, Aaron Klein, Stefan Falkner, Frank Hutter","While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.",2018-07-18T13:11:08Z,2018-07-18T13:11:08Z,http://arxiv.org/abs/1807.06906v1,http://arxiv.org/pdf/1807.06906v1,"cs.LG, cs.AI, cs.CV, stat.ML"
Effects of Degradations on Deep Neural Network Architectures,"Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal","Deep convolutional neural networks (CNN) have massively influenced recent advances in large-scale image classification. More recently, a dynamic routing algorithm with capsules (groups of neurons) has shown state-of-the-art recognition performance. However, the behavior of such networks in the presence of a degrading signal (noise) is mostly unexplored. An analytical study on different network architectures toward noise robustness is essential for selecting the appropriate model in a specific application scenario. This paper presents an extensive performance analysis of six deep architectures for image classification on six most common image degradation models. In this study, we have compared VGG-16, VGG-19, ResNet-50, Inception-v3, MobileNet and CapsuleNet architectures on Gaussian white, Gaussian color, salt-and-pepper, Gaussian blur, motion blur and JPEG compression noise models.",2018-07-26T13:20:57Z,2025-02-18T16:40:45Z,http://arxiv.org/abs/1807.10108v6,http://arxiv.org/pdf/1807.10108v6,"cs.CV, eess.IV"
UNet++: A Nested U-Net Architecture for Medical Image Segmentation,"Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming Liang","In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.",2018-07-18T04:08:21Z,2018-07-18T04:08:21Z,http://arxiv.org/abs/1807.10165v1,http://arxiv.org/pdf/1807.10165v1,"cs.CV, cs.LG, eess.IV, stat.ML"
MCRM: Mother Compact Recurrent Memory,"Abduallah A. Mohamed, Christian Claudel","LSTMs and GRUs are the most common recurrent neural network architectures used to solve temporal sequence problems. The two architectures have differing data flows dealing with a common component called the cell state (also referred to as the memory). We attempt to enhance the memory by presenting a modification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are a type of a nested LSTM-GRU architecture where the cell state is the GRU hidden state. The concatenation of the forget gate and input gate interactions from the LSTM are considered an input to the GRU cell. Because MCRMs has this type of nesting, MCRMs have a compact memory pattern consisting of neurons that acts explicitly in both long-term and short-term fashions. For some specific tasks, empirical results show that MCRMs outperform previously used architectures.",2018-08-04T15:48:39Z,2019-08-06T21:37:25Z,http://arxiv.org/abs/1808.02016v3,http://arxiv.org/pdf/1808.02016v3,"cs.NE, cs.LG, stat.ML"
Using Regular Languages to Explore the Representational Capacity of   Recurrent Neural Architectures,"Abhijit Mahalunkar, John D. Kelleher","The presence of Long Distance Dependencies (LDDs) in sequential data poses significant challenges for computational models. Various recurrent neural architectures have been designed to mitigate this issue. In order to test these state-of-the-art architectures, there is growing need for rich benchmarking datasets. However, one of the drawbacks of existing datasets is the lack of experimental control with regards to the presence and/or degree of LDDs. This lack of control limits the analysis of model performance in relation to the specific challenge posed by LDDs. One way to address this is to use synthetic data having the properties of subregular languages. The degree of LDDs within the generated data can be controlled through the k parameter, length of the generated strings, and by choosing appropriate forbidden strings. In this paper, we explore the capacity of different RNN extensions to model LDDs, by evaluating these models on a sequence of SPk synthesized datasets, where each subsequent dataset exhibits a longer degree of LDD. Even though SPk are simple languages, the presence of LDDs does have significant impact on the performance of recurrent neural architectures, thus making them prime candidate in benchmarking tasks.",2018-08-15T15:20:49Z,2018-08-15T15:20:49Z,http://arxiv.org/abs/1808.05128v1,http://arxiv.org/pdf/1808.05128v1,"cs.LG, stat.ML"
Neural Architecture Search: A Survey,"Thomas Elsken, Jan Hendrik Metzen, Frank Hutter","Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.",2018-08-16T08:45:01Z,2019-04-26T09:50:47Z,http://arxiv.org/abs/1808.05377v3,http://arxiv.org/pdf/1808.05377v3,"stat.ML, cs.LG, cs.NE"
Optimizing B-spline surfaces for developability and paneling   architectural freeform surfaces,"Konstantinos Gavriil, Alexander Schiftner, Helmut Pottmann","Motivated by applications in architecture and design, we present a novel method for increasing the developability of a B-spline surface. We use the property that the Gauss image of a developable surface is 1-dimensional and can be locally well approximated by circles. This is cast into an algorithm for thinning the Gauss image by increasing the planarity of the Gauss images of appropriate neighborhoods. A variation of the main method allows us to tackle the problem of paneling a freeform architectural surface with developable panels, in particular enforcing rotational cylindrical, rotational conical and planar panels, which are the main preferred types of developable panels in architecture due to the reduced cost of manufacturing.",2018-08-22T20:53:08Z,2019-02-26T09:04:34Z,http://arxiv.org/abs/1808.07560v3,http://arxiv.org/pdf/1808.07560v3,"cs.GR, math.DG"
Acoustic Features Fusion using Attentive Multi-channel Deep Architecture,"Gaurav Bhatt, Akshita Gupta, Aditya Arora, Balasubramanian Raman","In this paper, we present a novel deep fusion architecture for audio classification tasks. The multi-channel model presented is formed using deep convolution layers where different acoustic features are passed through each channel. To enable dissemination of information across the channels, we introduce attention feature maps that aid in the alignment of frames. The output of each channel is merged using interaction parameters that non-linearly aggregate the representative features. Finally, we evaluate the performance of the proposed architecture on three benchmark datasets:- DCASE-2016 and LITIS Rouen (acoustic scene recognition), and CHiME-Home (tagging). Our experimental results suggest that the architecture presented outperforms the standard baselines and achieves outstanding performance on the task of acoustic scene recognition and audio tagging.",2018-11-02T15:31:22Z,2018-11-02T15:31:22Z,http://arxiv.org/abs/1811.00936v1,http://arxiv.org/pdf/1811.00936v1,"cs.SD, eess.AS"
Deep Genetic Network,"Siddhartha Dhar Choudhury, Shashank Pandey, Kunal Mehrotra","Optimizing a neural network's performance is a tedious and time taking process, this iterative process does not have any defined solution which can work for all the problems. Optimization can be roughly categorized into - Architecture and Hyperparameter optimization. Many algorithms have been devised to address this problem. In this paper we introduce a neural network architecture (Deep Genetic Network) which will optimize its parameters during training based on its fitness. Deep Genetic Net uses genetic algorithms along with deep neural networks to address the hyperparameter optimization problem, this approach uses ideas like mating and mutation which are key to genetic algorithms which help the neural net architecture to learn to optimize its hyperparameters by itself rather than depending on a person to explicitly set the values. Using genetic algorithms for this problem proved to work exceptionally well when given enough time to train the network. The proposed architecture is found to work well in optimizing hyperparameters in affine, convolutional and recurrent layers proving to be a good choice for conventional supervised learning tasks.",2018-11-05T17:04:02Z,2019-03-19T17:02:45Z,http://arxiv.org/abs/1811.01845v2,http://arxiv.org/pdf/1811.01845v2,"cs.LG, cs.NE, stat.ML"
MIMO Channel Information Feedback Using Deep Recurrent Network,"Chao Lu, Wei Xu, Hong Shen, Jun Zhu, Kezhi Wang","In a multiple-input multiple-output (MIMO) system, the availability of channel state information (CSI) at the transmitter is essential for performance improvement. Recent convolutional neural network (NN) based techniques show competitive ability in realizing CSI compression and feedback. By introducing a new NN architecture, we enhance the accuracy of quantized CSI feedback in MIMO communications. The proposed NN architecture invokes a module named long short-term memory (LSTM) which admits the NN to benefit from exploiting temporal and frequency correlations of wireless channels. Compromising performance with complexity, we further modify the NN architecture with a significantly reduced number of parameters to be trained. Finally, experiments show that the proposed NN architectures achieve better performance in terms of both CSI compression and recovery accuracy.",2018-11-19T07:47:37Z,2018-11-19T07:47:37Z,http://arxiv.org/abs/1811.07535v1,http://arxiv.org/pdf/1811.07535v1,"cs.IT, math.IT"
Networks for Nonlinear Diffusion Problems in Imaging,"Simon Arridge, Andreas Hauptmann","A multitude of imaging and vision tasks have seen recently a major transformation by deep learning methods and in particular by the application of convolutional neural networks. These methods achieve impressive results, even for applications where it is not apparent that convolutions are suited to capture the underlying physics.   In this work we develop a network architecture based on nonlinear diffusion processes, named DiffNet. By design, we obtain a nonlinear network architecture that is well suited for diffusion related problems in imaging. Furthermore, the performed updates are explicit, by which we obtain better interpretability and generalisability compared to classical convolutional neural network architectures. The performance of DiffNet tested on the inverse problem of nonlinear diffusion with the Perona-Malik filter on the STL-10 image dataset. We obtain competitive results to the established U-Net architecture, with a fraction of parameters and necessary training data.",2018-11-29T11:54:54Z,2018-11-29T11:54:54Z,http://arxiv.org/abs/1811.12084v1,http://arxiv.org/pdf/1811.12084v1,"cs.CV, cs.LG, math.AP, math.NA, 58J65, 94A08, 35R30"
k-hop Graph Neural Networks,"Giannis Nikolentzos, George Dasoulas, Michalis Vazirgiannis","Graph neural networks (GNNs) have emerged recently as a powerful architecture for learning node and graph representations. Standard GNNs have the same expressive power as the Weisfeiler-Leman test of graph isomorphism in terms of distinguishing non-isomorphic graphs. However, it was recently shown that this test cannot identify fundamental graph properties such as connectivity and triangle freeness. We show that GNNs also suffer from the same limitation. To address this limitation, we propose a more expressive architecture, k-hop GNNs, which updates a node's representation by aggregating information not only from its direct neighbors, but from its k-hop neighborhood. We show that the proposed architecture can identify fundamental graph properties. We evaluate the proposed architecture on standard node classification and graph classification datasets. Our experimental evaluation confirms our theoretical findings since the proposed model achieves performance better or comparable to standard GNNs and to state-of-the-art algorithms.",2019-07-13T11:31:57Z,2020-08-09T21:50:43Z,http://arxiv.org/abs/1907.06051v2,http://arxiv.org/pdf/1907.06051v2,"stat.ML, cs.LG"
Comparison of Neural Network Architectures for Spectrum Sensing,"Ziyu Ye, Andrew Gilman, Qihang Peng, Kelly Levick, Pamela Cosman, Larry Milstein","Different neural network (NN) architectures have different advantages. Convolutional neural networks (CNNs) achieved enormous success in computer vision, while recurrent neural networks (RNNs) gained popularity in speech recognition. It is not known which type of NN architecture is the best fit for classification of communication signals. In this work, we compare the behavior of fully-connected NN (FC), CNN, RNN, and bi-directional RNN (BiRNN) in a spectrum sensing task. The four NN architectures are compared on their detection performance, requirement of training data, computational complexity, and memory requirement. Given abundant training data and computational and memory resources, CNN, RNN, and BiRNN are shown to achieve similar performance. The performance of FC is worse than that of the other three types, except in the case where computational complexity is stringently limited.",2019-07-15T21:24:31Z,2019-07-15T21:24:31Z,http://arxiv.org/abs/1907.07321v1,http://arxiv.org/pdf/1907.07321v1,"eess.SP, cs.LG, stat.ML"
Deep Neural Network Approach to Forward-Inverse Problems,"Hyeontae Jo, Hwijae Son, Hyung Ju Hwang, Eunheui Kim","In this paper, we construct approximated solutions of Differential Equations (DEs) using the Deep Neural Network (DNN). Furthermore, we present an architecture that includes the process of finding model parameters through experimental data, the inverse problem. That is, we provide a unified framework of DNN architecture that approximates an analytic solution and its model parameters simultaneously. The architecture consists of a feed forward DNN with non-linear activation functions depending on DEs, automatic differentiation, reduction of order, and gradient based optimization method. We also prove theoretically that the proposed DNN solution converges to an analytic solution in a suitable function space for fundamental DEs. Finally, we perform numerical experiments to validate the robustness of our simplistic DNN architecture for 1D transport equation, 2D heat equation, 2D wave equation, and the Lotka-Volterra system.",2019-07-27T04:52:14Z,2019-07-27T04:52:14Z,http://arxiv.org/abs/1907.12925v1,http://arxiv.org/pdf/1907.12925v1,"math.NA, cs.LG, cs.NA, math.AP"
Deep Learning Models for Global Coordinate Transformations that   Linearize PDEs,"Craig Gin, Bethany Lusch, Steven L. Brunton, J. Nathan Kutz","We develop a deep autoencoder architecture that can be used to find a coordinate transformation which turns a nonlinear PDE into a linear PDE. Our architecture is motivated by the linearizing transformations provided by the Cole-Hopf transform for Burgers equation and the inverse scattering transform for completely integrable PDEs. By leveraging a residual network architecture, a near-identity transformation can be exploited to encode intrinsic coordinates in which the dynamics are linear. The resulting dynamics are given by a Koopman operator matrix $\mathbf{K}$. The decoder allows us to transform back to the original coordinates as well. Multiple time step prediction can be performed by repeated multiplication by the matrix $\mathbf{K}$ in the intrinsic coordinates. We demonstrate our method on a number of examples, including the heat equation and Burgers equation, as well as the substantially more challenging Kuramoto-Sivashinsky equation, showing that our method provides a robust architecture for discovering interpretable, linearizing transforms for nonlinear PDEs.",2019-11-07T01:46:33Z,2019-11-07T01:46:33Z,http://arxiv.org/abs/1911.02710v1,http://arxiv.org/pdf/1911.02710v1,"math.DS, cs.LG, physics.comp-ph, stat.ML, 35A22, 35A35, 37M99, 65P99, 68T99"
ResUNet++: An Advanced Architecture for Medical Image Segmentation,"Debesh Jha, Pia H. Smedsrud, Michael A. Riegler, Dag Johansen, Thomas de Lange, Pal Halvorsen, Havard D. Johansen","Accurate computer-aided polyp detection and segmentation during colonoscopy examinations can help endoscopists resect abnormal tissue and thereby decrease chances of polyps growing into cancer. Towards developing a fully automated model for pixel-wise polyp segmentation, we propose ResUNet++, which is an improved ResUNet architecture for colonoscopic image segmentation. Our experimental evaluations show that the suggested architecture produces good segmentation results on publicly available datasets. Furthermore, ResUNet++ significantly outperforms U-Net and ResUNet, two key state-of-the-art deep learning architectures, by achieving high evaluation scores with a dice coefficient of 81.33%, and a mean Intersection over Union (mIoU) of 79.27% for the Kvasir-SEG dataset and a dice coefficient of 79.55%, and a mIoU of 79.62% with CVC-612 dataset.",2019-11-16T18:04:17Z,2019-11-16T18:04:17Z,http://arxiv.org/abs/1911.07067v1,http://arxiv.org/pdf/1911.07067v1,"eess.IV, cs.CV"
Inspect Transfer Learning Architecture with Dilated Convolution,"Syeda Noor Jaha Azim, Md. Aminur Rab Ratul","There are many award-winning pre-trained Convolutional Neural Network (CNN), which have a common phenomenon of increasing depth in convolutional layers. However, I inspect on VGG network, which is one of the famous model submitted to ILSVRC-2014, to show that slight modification in the basic architecture can enhance the accuracy result of the image classification task. In this paper, We present two improve architectures of pre-trained VGG-16 and VGG-19 networks that apply transfer learning when trained on a different dataset. I report a series of experimental result on various modification of the primary VGG networks and achieved significant out-performance on image classification task by: (1) freezing the first two blocks of the convolutional layers to prevent over-fitting and (2) applying different combination of dilation rate in the last three blocks of convolutional layer to reduce image resolution for feature extraction. Both the proposed architecture achieves a competitive result on CIFAR-10 and CIFAR-100 dataset.",2019-11-20T08:45:56Z,2019-11-20T08:45:56Z,http://arxiv.org/abs/1911.08769v1,http://arxiv.org/pdf/1911.08769v1,"cs.LG, cs.CV, stat.ML"
Parallel Implementations for Computing the Minimum Distance of a Random   Linear Code on Multicomputers,"Gregorio Quintana-Ortí, Fernando Hernando, Francisco D. Igual","The minimum distance of a linear code is a key concept in information theory. Therefore, the time required by its computation is very important to many problems in this area. In this paper, we introduce a family of implementations of the Brouwer-Zimmermann algorithm for distributed-memory architectures for computing the minimum distance of a random linear code over F2. Both current commercial and public-domain software only work on either unicore architectures or shared-memory architectures, which are limited in the number of cores/processors employed in the computation. Our implementations focus on distributed-memory architectures, thus being able to employ hundreds or even thousands of cores in the computation of the minimum distance. Our experimental results show that our implementations are much faster, even up to several orders of magnitude, than current implementations widely used nowadays.",2019-11-20T15:24:23Z,2019-11-20T15:24:23Z,http://arxiv.org/abs/1911.08963v1,http://arxiv.org/pdf/1911.08963v1,"cs.DC, cs.IT, math.IT"
Verifiability and Predictability: Interpreting Utilities of Network   Architectures for Point Cloud Processing,"Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang","In this paper, we diagnose deep neural networks for 3D point cloud processing to explore utilities of different intermediate-layer network architectures. We propose a number of hypotheses on the effects of specific intermediate-layer network architectures on the representation capacity of DNNs. In order to prove the hypotheses, we design five metrics to diagnose various types of DNNs from the following perspectives, information discarding, information concentration, rotation robustness, adversarial robustness, and neighborhood inconsistency. We conduct comparative studies based on such metrics to verify the hypotheses. We further use the verified hypotheses to revise intermediate-layer architectures of existing DNNs and improve their utilities. Experiments demonstrate the effectiveness of our method.",2019-11-20T17:33:19Z,2021-04-01T03:54:03Z,http://arxiv.org/abs/1911.09053v3,http://arxiv.org/pdf/1911.09053v3,"cs.CV, cs.LG, stat.ML"
"""You might also like this model"": Data Driven Approach for Recommending   Deep Learning Models for Unknown Image Datasets","Ameya Prabhu, Riddhiman Dasgupta, Anush Sankaran, Srikanth Tamilselvam, Senthil Mani","For an unknown (new) classification dataset, choosing an appropriate deep learning architecture is often a recursive, time-taking, and laborious process. In this research, we propose a novel technique to recommend a suitable architecture from a repository of known models. Further, we predict the performance accuracy of the recommended architecture on the given unknown dataset, without the need for training the model. We propose a model encoder approach to learn a fixed length representation of deep learning architectures along with its hyperparameters, in an unsupervised fashion. We manually curate a repository of image datasets with corresponding known deep learning models and show that the predicted accuracy is a good estimator of the actual accuracy. We discuss the implications of the proposed approach for three benchmark images datasets and also the challenges in using the approach for text modality. To further increase the reproducibility of the proposed approach, the entire implementation is made publicly available along with the trained models.",2019-11-26T10:01:35Z,2020-05-20T20:45:57Z,http://arxiv.org/abs/1911.11433v2,http://arxiv.org/pdf/1911.11433v2,"cs.LG, cs.CV, cs.IR, eess.IV, stat.ML"
Towards Efficient Superconducting Quantum Processor Architecture Design,"Gushu Li, Yufei Ding, Yuan Xie","More computational resources (i.e., more physical qubits and qubit connections) on a superconducting quantum processor not only improve the performance but also result in more complex chip architecture with lower yield rate. Optimizing both of them simultaneously is a difficult problem due to their intrinsic trade-off. Inspired by the application-specific design principle, this paper proposes an automatic design flow to generate simplified superconducting quantum processor architecture with negligible performance loss for different quantum programs. Our architecture-design-oriented profiling method identifies program components and patterns critical to both the performance and the yield rate. A follow-up hardware design flow decomposes the complicated design procedure into three subroutines, each of which focuses on different hardware components and cooperates with corresponding profiling results and physical constraints. Experimental results show that our design methodology could outperform IBM's general-purpose design schemes with better Pareto-optimal results.",2019-11-28T22:15:18Z,2019-11-28T22:15:18Z,http://arxiv.org/abs/1911.12879v1,http://arxiv.org/pdf/1911.12879v1,"quant-ph, cs.ET"
Deep Networks with Adaptive Nyström Approximation,"Luc Giffon, Stéphane Ayache, Thierry Artières, Hachem Kadri","Recent work has focused on combining kernel methods and deep learning to exploit the best of the two approaches. Here, we introduce a new architecture of neural networks in which we replace the top dense layers of standard convolutional architectures with an approximation of a kernel function by relying on the Nystr{\""o}m approximation. Our approach is easy and highly flexible. It is compatible with any kernel function and it allows exploiting multiple kernels. We show that our architecture has the same performance than standard architecture on datasets like SVHN and CIFAR100. One benefit of the method lies in its limited number of learnable parameters which makes it particularly suited for small training set sizes, e.g. from 5 to 20 samples per class.",2019-11-29T10:26:59Z,2019-11-29T10:26:59Z,http://arxiv.org/abs/1911.13036v1,http://arxiv.org/pdf/1911.13036v1,"cs.LG, stat.ML"
Streaming automatic speech recognition with the transformer model,"Niko Moritz, Takaaki Hori, Jonathan Le Roux","Encoder-decoder based sequence-to-sequence models have demonstrated state-of-the-art results in end-to-end automatic speech recognition (ASR). Recently, the transformer architecture, which uses self-attention to model temporal context information, has been shown to achieve significantly lower word error rates (WERs) compared to recurrent neural network (RNN) based system architectures. Despite its success, the practical usage is limited to offline ASR tasks, since encoder-decoder architectures typically require an entire speech utterance as input. In this work, we propose a transformer based end-to-end ASR system for streaming ASR, where an output must be generated shortly after each spoken word. To achieve this, we apply time-restricted self-attention for the encoder and triggered attention for the encoder-decoder attention mechanism. Our proposed streaming transformer architecture achieves 2.8% and 7.2% WER for the ""clean"" and ""other"" test data of LibriSpeech, which to our knowledge is the best published streaming end-to-end ASR result for this task.",2020-01-08T18:58:02Z,2020-06-30T18:29:07Z,http://arxiv.org/abs/2001.02674v5,http://arxiv.org/pdf/2001.02674v5,"cs.SD, cs.CL, cs.LG, eess.AS, stat.ML"
MixPath: A Unified Approach for One-shot Neural Architecture Search,"Xiangxiang Chu, Shun Lu, Xudong Li, Bo Zhang","Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state-of-the-art results on ImageNet.",2020-01-16T15:24:26Z,2023-07-19T12:58:18Z,http://arxiv.org/abs/2001.05887v4,http://arxiv.org/pdf/2001.05887v4,"cs.LG, cs.CV, stat.ML"
Multi-objective Neural Architecture Search via Non-stationary Policy   Gradient,"Zewei Chen, Fengwei Zhou, George Trimponias, Zhenguo Li","Multi-objective Neural Architecture Search (NAS) aims to discover novel architectures in the presence of multiple conflicting objectives. Despite recent progress, the problem of approximating the full Pareto front accurately and efficiently remains challenging. In this work, we explore the novel reinforcement learning (RL) based paradigm of non-stationary policy gradient (NPG). NPG utilizes a non-stationary reward function, and encourages a continuous adaptation of the policy to capture the entire Pareto front efficiently. We introduce two novel reward functions with elements from the dominant paradigms of scalarization and evolution. To handle non-stationarity, we propose a new exploration scheme using cosine temperature decay with warm restarts. For fast and accurate architecture evaluation, we introduce a novel pre-trained shared model that we continuously fine-tune throughout training. Our extensive experimental study with various datasets shows that our framework can approximate the full Pareto front well at fast speeds. Moreover, our discovered cells can achieve supreme predictive performance compared to other multi-objective NAS methods, and other single-objective NAS methods at similar network sizes. Our work demonstrates the potential of NPG as a simple, efficient, and effective paradigm for multi-objective NAS.",2020-01-23T10:37:11Z,2020-01-31T03:42:43Z,http://arxiv.org/abs/2001.08437v2,http://arxiv.org/pdf/2001.08437v2,"cs.LG, stat.ML"
On the computational power and complexity of Spiking Neural Networks,"Johan Kwisthout, Nils Donselaar","The last decade has seen the rise of neuromorphic architectures based on artificial spiking neural networks, such as the SpiNNaker, TrueNorth, and Loihi systems. The massive parallelism and co-locating of computation and memory in these architectures potentially allows for an energy usage that is orders of magnitude lower compared to traditional Von Neumann architectures. However, to date a comparison with more traditional computational architectures (particularly with respect to energy usage) is hampered by the lack of a formal machine model and a computational complexity theory for neuromorphic computation. In this paper we take the first steps towards such a theory. We introduce spiking neural networks as a machine model where---in contrast to the familiar Turing machine---information and the manipulation thereof are co-located in the machine. We introduce canonical problems, define hierarchies of complexity classes and provide some first completeness results.",2020-01-23T10:40:16Z,2020-01-23T10:40:16Z,http://arxiv.org/abs/2001.08439v1,http://arxiv.org/pdf/2001.08439v1,"cs.CC, cs.NE, 68Q05, F.1.1; F.1.3"
Real-time Federated Evolutionary Neural Architecture Search,"Hangyu Zhu, Yaochu Jin","Federated learning is a distributed machine learning approach to privacy preservation and two major technical challenges prevent a wider application of federated learning. One is that federated learning raises high demands on communication, since a large number of model parameters must be transmitted between the server and the clients. The other challenge is that training large machine learning models such as deep neural networks in federated learning requires a large amount of computational resources, which may be unrealistic for edge devices such as mobile phones. The problem becomes worse when deep neural architecture search is to be carried out in federated learning. To address the above challenges, we propose an evolutionary approach to real-time federated neural architecture search that not only optimize the model performance but also reduces the local payload. During the search, a double-sampling technique is introduced, in which for each individual, a randomly sampled sub-model of a master model is transmitted to a number of randomly sampled clients for training without reinitialization. This way, we effectively reduce computational and communication costs required for evolutionary optimization and avoid big performance fluctuations of the local models, making the proposed framework well suited for real-time federated neural architecture search.",2020-03-04T17:03:28Z,2020-03-04T17:03:28Z,http://arxiv.org/abs/2003.02793v1,http://arxiv.org/pdf/2003.02793v1,"cs.LG, cs.DC, stat.ML"
TRANS-BLSTM: Transformer with Bidirectional LSTM for Language   Understanding,"Zhiheng Huang, Peng Xu, Davis Liang, Ajay Mishra, Bing Xiang","Bidirectional Encoder Representations from Transformers (BERT) has recently achieved state-of-the-art performance on a broad range of NLP tasks including sentence classification, machine translation, and question answering. The BERT model architecture is derived primarily from the transformer. Prior to the transformer era, bidirectional Long Short-Term Memory (BLSTM) has been the dominant modeling architecture for neural machine translation and question answering. In this paper, we investigate how these two modeling techniques can be combined to create a more powerful model architecture. We propose a new architecture denoted as Transformer with BLSTM (TRANS-BLSTM) which has a BLSTM layer integrated to each transformer block, leading to a joint modeling framework for transformer and BLSTM. We show that TRANS-BLSTM models consistently lead to improvements in accuracy compared to BERT baselines in GLUE and SQuAD 1.1 experiments. Our TRANS-BLSTM model obtains an F1 score of 94.01% on the SQuAD 1.1 development dataset, which is comparable to the state-of-the-art result.",2020-03-16T03:38:51Z,2020-03-16T03:38:51Z,http://arxiv.org/abs/2003.07000v1,http://arxiv.org/pdf/2003.07000v1,"cs.CL, cs.LG, cs.SD, eess.AS"
A Single-RF Architecture for Multiuser Massive MIMO via Reflecting   Surfaces,"Ali Bereyhi, Vahid Jamali, Ralf R. Müller, Antonia M. Tulino, Georg Fischer, Robert Schober","In this work, we propose a new single-RF MIMO architecture which enjoys high scalability and energy-efficiency. The transmitter in this proposal consists of a single RF illuminator radiating towards a reflecting surface. Each element on the reflecting surface re-transmits its received signal after applying a phase-shift, such that a desired beamforming pattern is obtained. For this architecture, the problem of beamforming is interpreted as linear regression and a solution is derived via the method of least-squares. Using this formulation, a fast iterative algorithm for tuning of the reflecting surface is developed. Numerical results demonstrate that the proposed architecture is fully compatible with current designs of reflecting surfaces.",2020-03-17T18:48:18Z,2020-03-17T18:48:18Z,http://arxiv.org/abs/2003.07885v1,http://arxiv.org/pdf/2003.07885v1,"cs.IT, eess.SP, math.IT"
Architectural Resilience to Foreground-and-Background Adversarial Noise,"Carl Cheng, Evan Hu","Adversarial attacks in the form of imperceptible perturbations of normal images have been extensively studied, and for every new defense methodology created, multiple adversarial attacks are found to counteract it. In particular, a popular style of attack, exemplified in recent years by DeepFool and Carlini-Wagner, relies solely on white-box scenarios in which full access to the predictive model and its weights are required. In this work, we instead propose distinct model-agnostic benchmark perturbations of images in order to investigate the resilience and robustness of different network architectures. Results empirically determine that increasing depth within most types of Convolutional Neural Networks typically improves model resilience towards general attacks, with improvement steadily decreasing as the model becomes deeper. Additionally, we find that a notable difference in adversarial robustness exists between residual architectures with skip connections and non-residual architectures of similar complexity. Our findings provide direction for future understanding of residual connections and depth on network robustness.",2020-03-23T01:38:20Z,2020-06-07T05:28:09Z,http://arxiv.org/abs/2003.10045v2,http://arxiv.org/pdf/2003.10045v2,"cs.CV, cs.LG, eess.IV"
MiLeNAS: Efficient Neural Architecture Search via Mixed-Level   Reformulation,"Chaoyang He, Haishan Ye, Li Shen, Tong Zhang","Many recently proposed methods for Neural Architecture Search (NAS) can be formulated as bilevel optimization. For efficient implementation, its solution requires approximations of second-order methods. In this paper, we demonstrate that gradient errors caused by such approximations lead to suboptimality, in the sense that the optimization procedure fails to converge to a (locally) optimal solution. To remedy this, this paper proposes \mldas, a mixed-level reformulation for NAS that can be optimized efficiently and reliably. It is shown that even when using a simple first-order method on the mixed-level formulation, \mldas\ can achieve a lower validation error for NAS problems. Consequently, architectures obtained by our method achieve consistently higher accuracies than those obtained from bilevel optimization. Moreover, \mldas\ proposes a framework beyond DARTS. It is upgraded via model size-based search and early stopping strategies to complete the search process in around 5 hours. Extensive experiments within the convolutional architecture search space validate the effectiveness of our approach.",2020-03-27T05:06:54Z,2020-03-27T05:06:54Z,http://arxiv.org/abs/2003.12238v1,http://arxiv.org/pdf/2003.12238v1,"cs.LG, cs.CV, stat.ML"
MIP An AI Distributed Architectural Model to Introduce Cognitive   computing capabilities in Cyber Physical Systems (CPS),"Pasquale Giampa, Massimiliano Dibitonto","This paper introduces the MIP Platform architecture model, a novel AI-based cognitive computing platform architecture. The goal of the proposed application of MIP is to reduce the implementation burden for the usage of AI algorithms applied to cognitive computing and fluent HMI interactions within the manufacturing process in a cyber-physical production system. The cognitive inferencing engine of MIP is a deterministic cognitive module that processes declarative goals, identifies Intents and Entities, selects suitable actions and associated algorithms, and invokes for the execution a processing logic (Function) configured in the internal Function-as-aService or Connectivity Engine. Constant observation and evaluation against performance criteria assess the performance of Lambda(s) for many and varying scenarios. The modular design with well-defined interfaces enables the reusability and extensibility of FaaS components. An integrated BigData platform implements this modular design supported by technologies such as Docker, Kubernetes for virtualization and orchestration of the individual components and their communication. The implementation of the architecture is evaluated using a real-world use case later discussed in this paper.",2020-03-30T00:59:31Z,2020-03-30T00:59:31Z,http://arxiv.org/abs/2003.13174v1,http://arxiv.org/pdf/2003.13174v1,"cs.DC, D.2.10"
A Privacy-Preserving Distributed Architecture for   Deep-Learning-as-a-Service,"Simone Disabato, Alessandro Falcetta, Alessio Mongelluzzo, Manuel Roveri","Deep-learning-as-a-service is a novel and promising computing paradigm aiming at providing machine/deep learning solutions and mechanisms through Cloud-based computing infrastructures. Thanks to its ability to remotely execute and train deep learning models (that typically require high computational loads and memory occupation), such an approach guarantees high performance, scalability, and availability. Unfortunately, such an approach requires to send information to be processed (e.g., signals, images, positions, sounds, videos) to the Cloud, hence having potentially catastrophic-impacts on the privacy of users. This paper introduces a novel distributed architecture for deep-learning-as-a-service that is able to preserve the user sensitive data while providing Cloud-based machine and deep learning services. The proposed architecture, which relies on Homomorphic Encryption that is able to perform operations on encrypted data, has been tailored for Convolutional Neural Networks (CNNs) in the domain of image analysis and implemented through a client-server REST-based approach. Experimental results show the effectiveness of the proposed architecture.",2020-03-30T15:12:03Z,2020-03-30T15:12:03Z,http://arxiv.org/abs/2003.13541v1,http://arxiv.org/pdf/2003.13541v1,"cs.LG, stat.ML"
Dataless Model Selection with the Deep Frame Potential,"Calvin Murdock, Simon Lucey","Choosing a deep neural network architecture is a fundamental problem in applications that require balancing performance and parameter efficiency. Standard approaches rely on ad-hoc engineering or computationally expensive validation on a specific dataset. We instead attempt to quantify networks by their intrinsic capacity for unique and robust representations, enabling efficient architecture comparisons without requiring any data. Building upon theoretical connections between deep learning and sparse approximation, we propose the deep frame potential: a measure of coherence that is approximately related to representation stability but has minimizers that depend only on network structure. This provides a framework for jointly quantifying the contributions of architectural hyper-parameters such as depth, width, and skip connections. We validate its use as a criterion for model selection and demonstrate correlation with generalization error on a variety of common residual and densely connected network architectures.",2020-03-30T23:27:25Z,2020-03-30T23:27:25Z,http://arxiv.org/abs/2003.13866v1,http://arxiv.org/pdf/2003.13866v1,"cs.LG, cs.CV, stat.ML"
Can weight sharing outperform random architecture search? An   investigation with TuNAS,"Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, Quoc Le","Efficient Neural Architecture Search methods based on weight sharing have shown good promise in democratizing Neural Architecture Search for computer vision models. There is, however, an ongoing debate whether these efficient methods are significantly better than random search. Here we perform a thorough comparison between efficient and random search methods on a family of progressively larger and more challenging search spaces for image classification and detection on ImageNet and COCO. While the efficacies of both methods are problem-dependent, our experiments demonstrate that there are large, realistic tasks where efficient search methods can provide substantial gains over random search. In addition, we propose and evaluate techniques which improve the quality of searched architectures and reduce the need for manual hyper-parameter tuning.   Source code and experiment data are available at https://github.com/google-research/google-research/tree/master/tunas",2020-08-13T21:32:40Z,2020-08-13T21:32:40Z,http://arxiv.org/abs/2008.06120v1,http://arxiv.org/pdf/2008.06120v1,"cs.LG, cs.CV, stat.ML, I.2.10"
Orthogonalized SGD and Nested Architectures for Anytime Neural Networks,"Chengcheng Wan, Henry Hoffmann, Shan Lu, Michael Maire","We propose a novel variant of SGD customized for training network architectures that support anytime behavior: such networks produce a series of increasingly accurate outputs over time. Efficient architectural designs for these networks focus on re-using internal state; subnetworks must produce representations relevant for both immediate prediction as well as refinement by subsequent network stages. We consider traditional branched networks as well as a new class of recursively nested networks. Our new optimizer, Orthogonalized SGD, dynamically re-balances task-specific gradients when training a multitask network. In the context of anytime architectures, this optimizer projects gradients from later outputs onto a parameter subspace that does not interfere with those from earlier outputs. Experiments demonstrate that training with Orthogonalized SGD significantly improves generalization accuracy of anytime networks.",2020-08-15T03:06:34Z,2020-08-15T03:06:34Z,http://arxiv.org/abs/2008.06635v1,http://arxiv.org/pdf/2008.06635v1,"cs.LG, stat.ML"
Finding Fast Transformers: One-Shot Neural Architecture Search by   Component Composition,"Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won Chung, Jason Riesa","Transformer-based models have achieved stateof-the-art results in many tasks in natural language processing. However, such models are usually slow at inference time, making deployment difficult. In this paper, we develop an efficient algorithm to search for fast models while maintaining model quality. We describe a novel approach to decompose the Transformer architecture into smaller components, and propose a sampling-based one-shot architecture search method to find an optimal model for inference. The model search process is more efficient than alternatives, adding only a small overhead to training time. By applying our methods to BERT-base architectures, we achieve 10% to 30% speedup for pre-trained BERT and 70% speedup on top of a previous state-of-the-art distilled BERT model on Cloud TPU-v2 with a generally acceptable drop in performance.",2020-08-15T23:12:25Z,2020-08-15T23:12:25Z,http://arxiv.org/abs/2008.06808v1,http://arxiv.org/pdf/2008.06808v1,"cs.LG, stat.ML"
Black Box to White Box: Discover Model Characteristics Based on   Strategic Probing,"Josh Kalin, Matthew Ciolino, David Noever, Gerry Dozier","In Machine Learning, White Box Adversarial Attacks rely on knowing underlying knowledge about the model attributes. This works focuses on discovering to distrinct pieces of model information: the underlying architecture and primary training dataset. With the process in this paper, a structured set of input probes and the output of the model become the training data for a deep classifier. Two subdomains in Machine Learning are explored: image based classifiers and text transformers with GPT-2. With image classification, the focus is on exploring commonly deployed architectures and datasets available in popular public libraries. Using a single transformer architecture with multiple levels of parameters, text generation is explored by fine tuning off different datasets. Each dataset explored in image and text are distinguishable from one another. Diversity in text transformer outputs implies further research is needed to successfully classify architecture attribution in text domain.",2020-09-07T14:44:28Z,2020-09-07T14:44:28Z,http://arxiv.org/abs/2009.03136v1,http://arxiv.org/pdf/2009.03136v1,"cs.LG, stat.ML"
A Diagnostic Study of Explainability Techniques for Text Classification,"Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein","Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",2020-09-25T12:01:53Z,2020-09-25T12:01:53Z,http://arxiv.org/abs/2009.13295v1,http://arxiv.org/pdf/2009.13295v1,"cs.CL, cs.LG, cs.CL, cs.AI, I.2.7"
MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search,"Cristian Cioflan, Radu Timofte","Neural Architecture Search (NAS) has proved effective in offering outperforming alternatives to handcrafted neural networks. In this paper we analyse the benefits of NAS for image classification tasks under strict computational constraints. Our aim is to automate the design of highly efficient deep neural networks, capable of offering fast and accurate predictions and that could be deployed on a low-memory, low-power system-on-chip. The task thus becomes a three-party trade-off between accuracy, computational complexity, and memory requirements. To address this concern, we propose Multi-Scale Resource-Aware Neural Architecture Search (MS-RANAS). We employ a one-shot architecture search approach in order to obtain a reduced search cost and we focus on an anytime prediction setting. Through the usage of multiple-scaled features and early classifiers, we achieved state-of-the-art results in terms of accuracy-speed trade-off.",2020-09-29T11:56:01Z,2020-09-29T11:56:01Z,http://arxiv.org/abs/2009.13940v1,http://arxiv.org/pdf/2009.13940v1,"cs.CV, cs.LG, I.4.9; I.5.4"
"Transportation Internet: Concepts, Models, and Architectures",Hui Li,"Disruptive changes in vehicles and transportation have been triggered by automated, connected, electrified and shared mobility. Autonomous vehicles, like Internet data packets, are transported from one address to another through the road network. The Internet has become a general network transmission paradigm, and the Energy Internet is a successful application of this paradigm to the field of energy. By introducing the Internet paradigm to the field of transportation, this paper is the first to propose the Transportation Internet. Based on the concept of the Transportation Internet, fundamental models, such as the switching, routing, and hierarchical models, are established to form basic theories; new architectures, such as transportation routers and software defined transportation, are proposed to make transportation interconnected and open; system verifications, such as prototype and simulation, are also carried out to prove feasibility and advancement. The Transportation Internet, which is of far-reaching significance in science and industry, has brought systematic breakthroughs in theory, architecture, and technology, explored innovative research directions, and provided an Internet-like solution for the new generation of transportation.",2020-10-14T08:43:35Z,2021-09-29T06:42:50Z,http://arxiv.org/abs/2010.06880v2,http://arxiv.org/pdf/2010.06880v2,"eess.SY, cs.NI, cs.SI, cs.SY"
Performance Analysis of a Quantum Monte Carlo Application on Multiple   Hardware Architectures Using the HPX Runtime,"Weile Wei, Arghya Chatterjee, Kevin Huck, Oscar Hernandez, Hartmut Kaiser","This paper describes how we successfully used the HPX programming model to port the DCA++ application on multiple architectures that include POWER9, x86, ARM v8, and NVIDIA GPUs. We describe the lessons we can learn from this experience as well as the benefits of enabling the HPX in the application to improve the CPU threading part of the code, which led to an overall 21% improvement across architectures. We also describe how we used HPX-APEX to raise the level of abstraction to understand performance issues and to identify tasking optimization opportunities in the code, and how these relate to CPU/GPU utilization counters, device memory allocation over time, and CPU kernel-level context switches on a given architecture.",2020-10-14T13:54:57Z,2020-10-19T18:41:08Z,http://arxiv.org/abs/2010.07098v3,http://arxiv.org/pdf/2010.07098v3,"cs.DC, cond-mat.mtrl-sci, cond-mat.str-el, cond-mat.supr-con"
Joint Optimization for Coordinated Charging Control of Commercial   Electric Vehicles Under Distributed Hydrogen Energy Supply,"Teng Long, Qing-Shan Jia","The transition to the zero-carbon power system is underway accelerating recently. Hydrogen energy and electric vehicles (EVs) are promising solutions on the supply and demand sides. This paper presents a novel architecture that includes hydrogen production stations (HPSs), fast charging stations (FCSs), and commercial EVs. The proposed architecture jointly optimizes the distributed hydrogen energy dispatch and the EV charging location selection, and is formulated by a time-varying bi-level bipartite graph (T-BBG) model for real-time operation. We develop a bi-level iteration optimization method combining linear programming (LP) and Kuhn-Munkres (KM) algorithm to solve the joint problem whose optimality is proved theoretically. The effectiveness of the proposed architecture on reducing the operating cost is verified via case studies in Shanghai. The proposed method outperforms other strategies and improves the performance by at least 13% which shows the potential economic benefits of the joint architecture. The convergence and impact of the pile number, battery capacity, EV speed and penalty factor are assessed.",2020-10-16T02:50:06Z,2021-03-23T13:00:13Z,http://arxiv.org/abs/2010.08121v3,http://arxiv.org/pdf/2010.08121v3,"eess.SY, cs.SY"
One Model to Reconstruct Them All: A Novel Way to Use the Stochastic   Noise in StyleGAN,"Christian Bartz, Joseph Bethge, Haojin Yang, Christoph Meinel","Generative Adversarial Networks (GANs) have achieved state-of-the-art performance for several image generation and manipulation tasks. Different works have improved the limited understanding of the latent space of GANs by embedding images into specific GAN architectures to reconstruct the original images. We present a novel StyleGAN-based autoencoder architecture, which can reconstruct images with very high quality across several data domains. We demonstrate a previously unknown grade of generalizablility by training the encoder and decoder independently and on different datasets. Furthermore, we provide new insights about the significance and capabilities of noise inputs of the well-known StyleGAN architecture. Our proposed architecture can handle up to 40 images per second on a single GPU, which is approximately 28x faster than previous approaches. Finally, our model also shows promising results, when compared to the state-of-the-art on the image denoising task, although it was not explicitly designed for this task.",2020-10-21T16:24:07Z,2020-10-21T16:24:07Z,http://arxiv.org/abs/2010.11113v1,http://arxiv.org/pdf/2010.11113v1,"cs.CV, cs.LG, eess.IV"
A P4 Data Plane for the Quantum Internet,"Wojciech Kozlowski, Fernando Kuipers, Stephanie Wehner","The quantum technology revolution brings with it the promise of a quantum internet. A new -- quantum -- network stack will be needed to account for the fundamentally new properties of quantum entanglement. The first realisations of quantum networks are imminent and research interest in quantum network protocols has started growing. In the non-quantum world, programmable data planes have broken the pattern of ossification of the protocol stack and enabled a new -- software-defined -- network software architecture. Similarly, a programmable quantum data plane could pave the way for a software-defined quantum network architecture. In this paper, we demonstrate how we use P4$_{16}$ to explore abstractions and device architectures for quantum networks.",2020-10-21T19:37:23Z,2020-10-21T19:37:23Z,http://arxiv.org/abs/2010.11263v1,http://arxiv.org/pdf/2010.11263v1,"cs.NI, quant-ph"
Scene-Agnostic Multi-Microphone Speech Dereverberation,"Yochai Yemini, Ethan Fetaya, Haggai Maron, Sharon Gannot","Neural networks (NNs) have been widely applied in speech processing tasks, and, in particular, those employing microphone arrays. Nevertheless, most existing NN architectures can only deal with fixed and position-specific microphone arrays. In this paper, we present an NN architecture that can cope with microphone arrays whose number and positions of the microphones are unknown, and demonstrate its applicability in the speech dereverberation task. To this end, our approach harnesses recent advances in deep learning on set-structured data to design an architecture that enhances the reverberant log-spectrum. We use noisy and noiseless versions of a simulated reverberant dataset to test the proposed architecture. Our experiments on the noisy data show that the proposed scene-agnostic setup outperforms a powerful scene-aware framework, sometimes even with fewer microphones. With the noiseless dataset we show that, in most cases, our method outperforms the position-aware network as well as the state-of-the-art weighted linear prediction error (WPE) algorithm.",2020-10-22T17:13:12Z,2021-06-10T18:17:26Z,http://arxiv.org/abs/2010.11875v2,http://arxiv.org/pdf/2010.11875v2,"eess.AS, cs.LG, cs.SD"
Dual-decoder Transformer for Joint Automatic Speech Recognition and   Multilingual Speech Translation,"Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent Besacier","We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.",2020-11-02T04:59:50Z,2020-11-02T04:59:50Z,http://arxiv.org/abs/2011.00747v1,http://arxiv.org/pdf/2011.00747v1,"cs.CL, cs.SD, eess.AS"
Deep coastal sea elements forecasting using U-Net based models,"Jesús García Fernández, Ismail Alaoui Abdellaoui, Siamak Mehrkanoon","The supply and demand of energy is influenced by meteorological conditions. The relevance of accurate weather forecasts increases as the demand for renewable energy sources increases. The energy providers and policy makers require weather information to make informed choices and establish optimal plans according to the operational objectives. Due to the recent development of deep learning techniques applied to satellite imagery, weather forecasting that uses remote sensing data has also been the subject of major progress. The present paper investigates multiple steps ahead frame prediction for coastal sea elements in the Netherlands using U-Net based architectures. Hourly data from the Copernicus observation programme spanned over a period of 2 years has been used to train the models and make the forecasting, including seasonal predictions. We propose a variation of the U-Net architecture and further extend this novel model using residual connections, parallel convolutions and asymmetric convolutions in order to introduce three additional architectures. In particular, we show that the architecture equipped with parallel and asymmetric convolutions as well as skip connections outperforms the other three discussed models.",2020-11-06T12:02:31Z,2021-11-08T21:25:15Z,http://arxiv.org/abs/2011.03303v2,http://arxiv.org/pdf/2011.03303v2,"cs.LG, cs.CV, eess.IV, I.2; I.5"
FDNAS: Improving Data Privacy and Model Diversity in AutoML,"Chunhui Zhang, Yongyuan Liang, Xiaoming Yuan, Lei Cheng","To prevent the leakage of private information while enabling automated machine intelligence, there is an emerging trend to integrate federated learning and Neural Architecture Search (NAS). Although promising as it may seem, the coupling of difficulties from both two tenets makes the algorithm development quite challenging. In particular, how to efficiently search the optimal neural architecture directly from massive non-iid data of clients in a federated manner remains to be a hard nut to crack. To tackle this challenge, in this paper, by leveraging the advances in proxy-less NAS, we propose a Federated Direct Neural Architecture Search (FDNAS) framework that allows hardware-aware NAS from decentralized non-iid data of clients. To further adapt for various data distributions of clients, inspired by meta-learning, a cluster Federated Direct Neural Architecture Search (CFDNAS) framework is proposed to achieve client-aware NAS, in the sense that each client can learn a tailored deep learning model for its particular data distribution. Extensive experiments on real-world non-iid datasets show state-of-the-art accuracy-efficiency trade-offs for various hardware and data distributions of clients. Our codes will be released publicly upon paper acceptance.",2020-11-06T14:13:42Z,2020-11-06T14:13:42Z,http://arxiv.org/abs/2011.03372v1,http://arxiv.org/pdf/2011.03372v1,"cs.LG, cs.AI, eess.SP"
Wide-band butterfly network: stable and efficient inversion via   multi-frequency neural networks,"Matthew Li, Laurent Demanet, Leonardo Zepeda-Núñez","We introduce an end-to-end deep learning architecture called the wide-band butterfly network (WideBNet) for approximating the inverse scattering map from wide-band scattering data. This architecture incorporates tools from computational harmonic analysis, such as the butterfly factorization, and traditional multi-scale methods, such as the Cooley-Tukey FFT algorithm, to drastically reduce the number of trainable parameters to match the inherent complexity of the problem. As a result WideBNet is efficient: it requires fewer training points than off-the-shelf architectures, and has stable training dynamics, thus it can rely on standard weight initialization strategies. The architecture automatically adapts to the dimensions of the data with only a few hyper-parameters that the user must specify. WideBNet is able to produce images that are competitive with optimization-based approaches, but at a fraction of the cost, and we also demonstrate numerically that it learns to super-resolve scatterers in the full aperture scattering setup.",2020-11-24T21:48:43Z,2021-10-28T22:07:09Z,http://arxiv.org/abs/2011.12413v2,http://arxiv.org/pdf/2011.12413v2,"cs.LG, cs.NA, cs.NE, math.NA, stat.ML"
Hardware Implementation of Iterative Projection-Aggregation Decoding of   Reed-Muller Codes,"Marzieh Hashemipour-Nazari, Kees Goossens, Alexios Balatsoukas-Stimming","In this work, we present a simplification and a corresponding hardware architecture for hard-decision recursive projection-aggregation (RPA) decoding of Reed-Muller (RM) codes. In particular, we transform the recursive structure of RPA decoding into a simpler and iterative structure with minimal error-correction degradation. Our simulation results for RM(7,3) show that the proposed simplification has a small error-correcting performance degradation (0.005 in terms of channel crossover probability) while reducing the average number of computations by up to 40%. In addition, we describe the first fully parallel hardware architecture for simplified RPA decoding. We present FPGA implementation results for an RM(6,3) code on a Xilinx Virtex-7 FPGA showing that our proposed architecture achieves a throughput of 171 Mbps at a frequency of 80 MHz.",2020-12-01T15:38:06Z,2020-12-01T15:38:06Z,http://arxiv.org/abs/2012.00581v1,http://arxiv.org/pdf/2012.00581v1,"cs.IT, cs.AR, eess.SP, math.IT"
Aerial Imagery Pixel-level Segmentation,"Michael R. Heffels, Joaquin Vanschoren","Aerial imagery can be used for important work on a global scale. Nevertheless, the analysis of this data using neural network architectures lags behind the current state-of-the-art on popular datasets such as PASCAL VOC, CityScapes and Camvid. In this paper we bridge the performance-gap between these popular datasets and aerial imagery data. Little work is done on aerial imagery with state-of-the-art neural network architectures in a multi-class setting. Our experiments concerning data augmentation, normalisation, image size and loss functions give insight into a high performance setup for aerial imagery segmentation datasets. Our work, using the state-of-the-art DeepLabv3+ Xception65 architecture, achieves a mean IOU of 70% on the DroneDeploy validation set. With this result, we clearly outperform the current publicly available state-of-the-art validation set mIOU (65%) performance with 5%. Furthermore, to our knowledge, there is no mIOU benchmark for the test set. Hence, we also propose a new benchmark on the DroneDeploy test set using the best performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%.",2020-12-03T16:09:09Z,2020-12-03T16:09:09Z,http://arxiv.org/abs/2012.02024v1,http://arxiv.org/pdf/2012.02024v1,"cs.CV, I.4.6"
Towards speech enhancement using a variational U-Net architecture,"Eike J. Nustede, Jörn Anemüller","We investigate the viability of a variational U-Net architecture for denoising of single-channel audio data. Deep network speech enhancement systems commonly aim to estimate filter masks, or opt to work on the waveform signal, potentially neglecting relationships across higher dimensional spectro-temporal features. We study the adoption of a probabilistic bottleneck into the classic U-Net architecture for direct spectral reconstruction. Evaluation of several ablation network variants is carried out using signal-to-distortion ratio and perceptual measures, on audio data that includes known and unknown noise types as well as reverberation. Our experiments show that the residual (skip) connections in the proposed system are a prerequisite for successful spectral reconstruction, i.e., without filter mask estimation. Results show, on average, an advantage of the proposed variational U-Net architecture over its classic, non-variational version in signal enhancement performance under reverberant conditions of 0.31 and 6.98 in PESQ and STOI scores, respectively. Anecdotal evidence points to improved suppression of impulsive noise sources with the variational U-Net compared to the recurrent mask estimation network baseline.",2020-12-07T11:30:35Z,2021-03-03T09:56:32Z,http://arxiv.org/abs/2012.03594v2,http://arxiv.org/pdf/2012.03594v2,"eess.AS, cs.SD"
Compiler Design for Distributed Quantum Computing,"Davide Ferrari, Angela Sara Cacciapuoti, Michele Amoretti, Marcello Caleffi","In distributed quantum computing architectures, with the network and communications functionalities provided by the Quantum Internet, remote quantum processing units (QPUs) can communicate and cooperate for executing computational tasks that single NISQ devices cannot handle by themselves. To this aim, distributed quantum computing requires a new generation of quantum compilers, for mapping any quantum algorithm to any distributed quantum computing architecture. With this perspective, in this paper, we first discuss the main challenges arising with compiler design for distributed quantum computing. Then, we analytically derive an upper bound of the overhead induced by quantum compilation for distributed quantum computing. The derived bound accounts for the overhead induced by the underlying computing architecture as well as the additional overhead induced by the sub-optimal quantum compiler -- expressly designed through the paper to achieve three key features, namely, general-purpose, efficient and effective. Finally, we validate the analytical results and we confirm the validity of the compiler design through an extensive performance analysis.",2020-12-17T15:48:32Z,2020-12-17T15:48:32Z,http://arxiv.org/abs/2012.09680v1,http://arxiv.org/pdf/2012.09680v1,"quant-ph, cs.DC, cs.NI"
Gender Bias in Multilingual Neural Machine Translation: The Architecture   Matters,"Marta R. Costa-jussà, Carlos Escolano, Christine Basta, Javier Ferrando, Roser Batlle, Ksenia Kharitonova","Multilingual Neural Machine Translation architectures mainly differ in the amount of sharing modules and parameters among languages. In this paper, and from an algorithmic perspective, we explore if the chosen architecture, when trained with the same data, influences the gender bias accuracy. Experiments in four language pairs show that Language-Specific encoders-decoders exhibit less bias than the Shared encoder-decoder architecture. Further interpretability analysis of source embeddings and the attention shows that, in the Language-Specific case, the embeddings encode more gender information, and its attention is more diverted. Both behaviors help in mitigating gender bias.",2020-12-24T09:27:52Z,2020-12-24T09:27:52Z,http://arxiv.org/abs/2012.13176v1,http://arxiv.org/pdf/2012.13176v1,"cs.CL, I.2.7"
Neural Architecture Search via Combinatorial Multi-Armed Bandit,"Hanxun Huang, Xingjun Ma, Sarah M. Erfani, James Bailey","Neural Architecture Search (NAS) has gained significant popularity as an effective tool for designing high performance deep neural networks (DNNs). NAS can be performed via policy gradient, evolutionary algorithms, differentiable architecture search or tree-search methods. While significant progress has been made for both policy gradient and differentiable architecture search, tree-search methods have so far failed to achieve comparable accuracy or search efficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed Bandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large search space into smaller blocks where tree-search methods can be applied more effectively and efficiently. We further leverage a tree-based method called Nested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our approach discovers a cell structure that achieves a low error rate that is comparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times faster than current tree-search methods. Moreover, the discovered structure transfers well to large-scale datasets such as ImageNet.",2021-01-01T23:29:33Z,2021-04-24T14:13:15Z,http://arxiv.org/abs/2101.00336v2,http://arxiv.org/pdf/2101.00336v2,"cs.LG, cs.CV, stat.ML"
A Reconfigurable Convolution-in-Pixel CMOS Image Sensor Architecture,"Ruibing Song, Kejie Huang, Zongsheng Wang, Haibin Shen","The separation of the data capture and analysis in modern vision systems has led to a massive amount of data transfer between the end devices and cloud computers, resulting in long latency, slow response, and high power consumption. Efficient hardware architectures are under focused development to enable Artificial Intelligence (AI) at the resource-limited end sensing devices. One of the most promising solutions is to enable Processing-in-Pixel (PIP) scheme. However, the conventional schemes suffer from the low fill-factor issue. This paper proposes a PIP based CMOS sensor architecture, which allows convolution operation before the column readout circuit to significantly improve the image reading speed with much lower power consumption. The simulation results show that the proposed architecture could support the computing efficiency up to 11.65 TOPS/W at the 8-bit weight configuration, which is three times as high as the conventional schemes. The transistors required for each pixel are only 2.5T, significantly improving the fill-factor.",2021-01-09T07:10:03Z,2021-10-13T08:31:14Z,http://arxiv.org/abs/2101.03308v2,http://arxiv.org/pdf/2101.03308v2,"eess.IV, cs.LG"
Partition of unity networks: deep hp-approximation,"Kookjin Lee, Nathaniel A. Trask, Ravi G. Patel, Mamikon A. Gulian, Eric C. Cyr","Approximation theorists have established best-in-class optimal approximation rates of deep neural networks by utilizing their ability to simultaneously emulate partitions of unity and monomials. Motivated by this, we propose partition of unity networks (POUnets) which incorporate these elements directly into the architecture. Classification architectures of the type used to learn probability measures are used to build a meshfree partition of space, while polynomial spaces with learnable coefficients are associated to each partition. The resulting hp-element-like approximation allows use of a fast least-squares optimizer, and the resulting architecture size need not scale exponentially with spatial dimension, breaking the curse of dimensionality. An abstract approximation result establishes desirable properties to guide network design. Numerical results for two choices of architecture demonstrate that POUnets yield hp-convergence for smooth functions and consistently outperform MLPs for piecewise polynomial functions with large numbers of discontinuities.",2021-01-27T08:26:11Z,2021-01-27T08:26:11Z,http://arxiv.org/abs/2101.11256v1,http://arxiv.org/pdf/2101.11256v1,"cs.LG, cs.NA, math.NA, stat.ML"
Keyword Transformer: A Self-Attention Model for Keyword Spotting,"Axel Berg, Mark O'Connor, Miguel Tairum Cruz","The Transformer architecture has been successful across many domains, including natural language processing, computer vision and speech recognition. In keyword spotting, self-attention has primarily been used on top of convolutional or recurrent encoders. We investigate a range of ways to adapt the Transformer architecture to keyword spotting and introduce the Keyword Transformer (KWT), a fully self-attentional architecture that exceeds state-of-the-art performance across multiple tasks without any pre-training or additional data. Surprisingly, this simple architecture outperforms more complex models that mix convolutional, recurrent and attentive layers. KWT can be used as a drop-in replacement for these models, setting two new benchmark records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on the 12 and 35-command tasks respectively.",2021-04-01T21:15:30Z,2021-06-15T13:06:01Z,http://arxiv.org/abs/2104.00769v3,http://arxiv.org/pdf/2104.00769v3,"eess.AS, cs.CL, cs.LG, cs.SD"
Rethinking and Improving the Robustness of Image Style Transfer,"Pei Wang, Yijun Li, Nuno Vasconcelos","Extensive research in neural style transfer methods has shown that the correlation between features extracted by a pre-trained VGG network has a remarkable ability to capture the visual style of an image. Surprisingly, however, this stylization quality is not robust and often degrades significantly when applied to features from more advanced and lightweight networks, such as those in the ResNet family. By performing extensive experiments with different network architectures, we find that residual connections, which represent the main architectural difference between VGG and ResNet, produce feature maps of small entropy, which are not suitable for style transfer. To improve the robustness of the ResNet architecture, we then propose a simple yet effective solution based on a softmax transformation of the feature activations that enhances their entropy. Experimental results demonstrate that this small magic can greatly improve the quality of stylization results, even for networks with random weights. This suggests that the architecture used for feature extraction is more important than the use of learned weights for the task of style transfer.",2021-04-08T03:24:45Z,2021-04-08T03:24:45Z,http://arxiv.org/abs/2104.05623v1,http://arxiv.org/pdf/2104.05623v1,"cs.CV, eess.IV"
On the Design of Deep Priors for Unsupervised Audio Restoration,"Vivek Sivaraman Narayanaswamy, Jayaraman J. Thiagarajan, Andreas Spanias","Unsupervised deep learning methods for solving audio restoration problems extensively rely on carefully tailored neural architectures that carry strong inductive biases for defining priors in the time or spectral domain. In this context, lot of recent success has been achieved with sophisticated convolutional network constructions that recover audio signals in the spectral domain. However, in practice, audio priors require careful engineering of the convolutional kernels to be effective at solving ill-posed restoration tasks, while also being easy to train. To this end, in this paper, we propose a new U-Net based prior that does not impact either the network complexity or convergence behavior of existing convolutional architectures, yet leads to significantly improved restoration. In particular, we advocate the use of carefully designed dilation schedules and dense connections in the U-Net architecture to obtain powerful audio priors. Using empirical studies on standard benchmarks and a variety of ill-posed restoration tasks, such as audio denoising, in-painting and source separation, we demonstrate that our proposed approach consistently outperforms widely adopted audio prior architectures.",2021-04-14T23:16:25Z,2021-04-14T23:16:25Z,http://arxiv.org/abs/2104.07161v1,http://arxiv.org/pdf/2104.07161v1,"cs.SD, cs.LG, eess.AS"
Quantum Architecture Search via Deep Reinforcement Learning,"En-Jui Kuo, Yao-Lung L. Fang, Samuel Yen-Chi Chen","Recent advances in quantum computing have drawn considerable attention to building realistic application for and using quantum computers. However, designing a suitable quantum circuit architecture requires expert knowledge. For example, it is non-trivial to design a quantum gate sequence for generating a particular quantum state with as fewer gates as possible. We propose a quantum architecture search framework with the power of deep reinforcement learning (DRL) to address this challenge. In the proposed framework, the DRL agent can only access the Pauli-$X$, $Y$, $Z$ expectation values and a predefined set of quantum operations for learning the target quantum state, and is optimized by the advantage actor-critic (A2C) and proximal policy optimization (PPO) algorithms. We demonstrate a successful generation of quantum gate sequences for multi-qubit GHZ states without encoding any knowledge of quantum physics in the agent. The design of our framework is rather general and can be employed with other DRL architectures or optimization methods to study gate synthesis and compilation for many quantum states.",2021-04-15T18:53:26Z,2021-04-15T18:53:26Z,http://arxiv.org/abs/2104.07715v1,http://arxiv.org/pdf/2104.07715v1,"quant-ph, cs.AI, cs.LG, cs.NE"
NeuroRAN: Rethinking Virtualization for AI-native Radio Access Networks   in 6G,"Paris Carbone, Gyoergy Dan, James Gross, Bo Goeransson, Marina Petrova","Network softwarization has revolutionized the architecture of cellular wireless networks. State-of-the-art container based virtual radio access networks (vRAN) provide enormous flexibility and reduced life cycle management costs, but they also come with prohibitive energy consumption. We argue that for future AI-native wireless networks to be flexible and energy efficient, there is a need for a new abstraction in network softwarization that caters for neural network type of workloads and allows a large degree of service composability. In this paper we present the NeuroRAN architecture, which leverages stateful function as a user facing execution model, and is complemented with virtualized resources and decentralized resource management. We show that neural network based implementations of common transceiver functional blocks fit the proposed architecture, and we discuss key research challenges related to compilation and code generation, resource management, reliability and security.",2021-04-16T13:38:57Z,2021-04-16T13:38:57Z,http://arxiv.org/abs/2104.08111v1,http://arxiv.org/pdf/2104.08111v1,"cs.NI, eess.SP"
Bag of Baselines for Multi-objective Joint Neural Architecture Search   and Hyperparameter Optimization,"Julia Guerrero-Viu, Sven Hauns, Sergio Izquierdo, Guilherme Miotto, Simon Schrodi, Andre Biedenkapp, Thomas Elsken, Difan Deng, Marius Lindauer, Frank Hutter","Neural architecture search (NAS) and hyperparameter optimization (HPO) make deep learning accessible to non-experts by automatically finding the architecture of the deep neural network to use and tuning the hyperparameters of the used training pipeline. While both NAS and HPO have been studied extensively in recent years, NAS methods typically assume fixed hyperparameters and vice versa - there exists little work on joint NAS + HPO. Furthermore, NAS has recently often been framed as a multi-objective optimization problem, in order to take, e.g., resource requirements into account. In this paper, we propose a set of methods that extend current approaches to jointly optimize neural architectures and hyperparameters with respect to multiple objectives. We hope that these methods will serve as simple baselines for future research on multi-objective joint NAS + HPO. To facilitate this, all our code is available at https://github.com/automl/multi-obj-baselines.",2021-05-03T17:04:56Z,2021-05-03T17:04:56Z,http://arxiv.org/abs/2105.01015v1,http://arxiv.org/pdf/2105.01015v1,"cs.LG, cs.AI, stat.ML"
1D CNN Architectures for Music Genre Classification,"Safaa Allamy, Alessandro Lameiras Koerich","This paper proposes a 1D residual convolutional neural network (CNN) architecture for music genre classification and compares it with other recent 1D CNN architectures. The 1D CNNs learn a representation and a discriminant directly from the raw audio signal. Several convolutional layers capture the time-frequency characteristics of the audio signal and learn various filters relevant to the music genre recognition task. The proposed approach splits the audio signal into overlapped segments using a sliding window to comply with the fixed-length input constraint of the 1D CNNs. As a result, music genre classification can be carried out on a single audio segment or on the aggregation of the predictions on several audio segments, which improves the final accuracy. The performance of the proposed 1D residual CNN is assessed on a public dataset of 1,000 audio clips. The experimental results have shown that it achieves 80.93% of mean accuracy in classifying music genres and outperforms other 1D CNN architectures.",2021-05-15T22:33:48Z,2021-05-15T22:33:48Z,http://arxiv.org/abs/2105.07302v1,http://arxiv.org/pdf/2105.07302v1,"cs.SD, eess.AS"
Neural Network Based Sleep Phases Classification for Resource Constraint   Environments,"Berkay Köprü, Murat Aslan, Alisher Kholmatov","Sleep is restoration process of the body. The efficiency of this restoration process is directly correlated to the amount of time spent at each sleep phase. Hence, automatic tracking of sleep via wearable devices has attracted both the researchers and industry. Current state-of-the-art sleep tracking solutions are memory and processing greedy and they require cloud or mobile phone connectivity. We propose a memory efficient sleep tracking architecture which can work in the embedded environment without needing any cloud or mobile phone connection. In this study, a novel architecture is proposed that consists of a feature extraction and Artificial Neural Networks based stacking classifier. Besides, we discussed how to tackle with sequential nature of the sleep staging for the memory constraint environments through the proposed framework. To verify the system, a dataset is collected from 24 different subjects for 31 nights with a wrist worn device having 3-axis accelerometer (ACC) and photoplethysmogram (PPG) sensors. Over the collected dataset, the proposed classification architecture achieves 20\% and 14\% better F1 scores than its competitors. Apart from the superior performance, proposed architecture is a promising solution for resource constraint embedded systems by allocating only 4.2 kilobytes of memory (RAM).",2021-05-25T11:06:07Z,2021-05-25T11:06:07Z,http://arxiv.org/abs/2105.11452v1,http://arxiv.org/pdf/2105.11452v1,"eess.SP, cs.LG"
Cine-MRI detection of abdominal adhesions with spatio-temporal deep   learning,"Bram de Wilde, Richard P. G. ten Broek, Henkjan Huisman","Adhesions are an important cause of chronic pain following abdominal surgery. Recent developments in abdominal cine-MRI have enabled the non-invasive diagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of sliding motion during movement. Diagnosis and mapping of adhesions improves the management of patients with pain. Detection of abdominal adhesions on cine-MRI is challenging from both a radiological and deep learning perspective. We focus on classifying presence or absence of adhesions in sagittal abdominal cine-MRI series. We experimented with spatio-temporal deep learning architectures centered around a ConvGRU architecture. A hybrid architecture comprising a ResNet followed by a ConvGRU model allows to classify a whole time-series. Compared to a stand-alone ResNet with a two time-point (inspiration/expiration) input, we show an increase in classification performance (AUROC) from 0.74 to 0.83 ($p<0.05$). Our full temporal classification approach adds only a small amount (5%) of parameters to the entire architecture, which may be useful for other medical imaging problems with a temporal dimension.",2021-06-15T12:33:54Z,2021-06-15T12:33:54Z,http://arxiv.org/abs/2106.08094v1,http://arxiv.org/pdf/2106.08094v1,"eess.IV, cs.CV"
Drum-Aware Ensemble Architecture for Improved Joint Musical Beat and   Downbeat Tracking,"Ching-Yu Chiu, Alvin Wen-Yu Su, Yi-Hsuan Yang","This paper presents a novel system architecture that integrates blind source separation with joint beat and downbeat tracking in musical audio signals. The source separation module segregates the percussive and non-percussive components of the input signal, over which beat and downbeat tracking are performed separately and then the results are aggregated with a learnable fusion mechanism. This way, the system can adaptively determine how much the tracking result for an input signal should depend on the input's percussive or non-percussive components. Evaluation on four testing sets that feature different levels of presence of drum sounds shows that the new architecture consistently outperforms the widely-adopted baseline architecture that does not employ source separation.",2021-06-16T10:47:44Z,2021-06-16T10:47:44Z,http://arxiv.org/abs/2106.08685v1,http://arxiv.org/pdf/2106.08685v1,"cs.SD, cs.LG, eess.AS"
BiX-NAS: Searching Efficient Bi-directional Architecture for Medical   Image Segmentation,"Xinyi Wang, Tiange Xiang, Chaoyi Zhang, Yang Song, Dongnan Liu, Heng Huang, Weidong Cai","The recurrent mechanism has recently been introduced into U-Net in various medical image segmentation tasks. Existing studies have focused on promoting network recursion via reusing building blocks. Although network parameters could be greatly saved, computational costs still increase inevitably in accordance with the pre-set iteration time. In this work, we study a multi-scale upgrade of a bi-directional skip connected network and then automatically discover an efficient architecture by a novel two-phase Neural Architecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method reduces the network computational cost by sifting out ineffective multi-scale features at different levels and iterations. We evaluate BiX-NAS on two segmentation tasks using three different medical image datasets, and the experimental results show that our BiX-NAS searched architecture achieves the state-of-the-art performance with significantly lower computational cost.",2021-06-26T14:33:04Z,2021-07-01T09:03:36Z,http://arxiv.org/abs/2106.14033v3,http://arxiv.org/pdf/2106.14033v3,"eess.IV, cs.CV"
Flare: Flexible In-Network Allreduce,"Daniele De Sensi, Salvatore Di Girolamo, Saleh Ashkboos, Shigang Li, Torsten Hoefler","The allreduce operation is one of the most commonly used communication routines in distributed applications. To improve its bandwidth and to reduce network traffic, this operation can be accelerated by offloading it to network switches, that aggregate the data received from the hosts, and send them back the aggregated result. However, existing solutions provide limited customization opportunities and might provide suboptimal performance when dealing with custom operators and data types, with sparse data, or when reproducibility of the aggregation is a concern. To deal with these problems, in this work we design a flexible programmable switch by using as a building block PsPIN, a RISC-V architecture implementing the sPIN programming model. We then design, model, and analyze different algorithms for executing the aggregation on this architecture, showing performance improvements compared to state-of-the-art approaches.",2021-06-29T16:58:32Z,2021-06-29T16:58:32Z,http://arxiv.org/abs/2106.15565v1,http://arxiv.org/pdf/2106.15565v1,"cs.DC, cs.AR, cs.NI, C.2.4; C.2.1; B.4.3"
Design and Implementation of MIMO Transmission Based on Dual-Polarized   Reconfigurable Intelligent Surface,"Xiangyu Chen, Jun Chen Ke, Wankai Tang, Ming Zheng Chen, Jun Yan Dai, Ertugrul Basar, Shi Jin, Qiang Cheng, Tie Jun Cui","Multiple-input multiple-output (MIMO) signaling is one of the key technologies of current mobile communication systems. However, the complex and expensive radio frequency (RF) chains have always limited the increase of MIMO scale. In this paper, we propose a MIMO transmission architecture based on a dual-polarized reconfigurable intelligent surface (RIS), which can directly achieve modulation and transmission of multichannel signals without the need for conventional RF chains. Compared with previous works, the proposed architecture can improve the integration of RIS-based transmission systems. A prototype of the dual-polarized RIS-based MIMO transmission system is built and the experimental results confirm the feasibility of the proposed architecture. The dual-polarized RIS-based MIMO transmission architecture provides a promising solution for realizing low-cost ultra-massive MIMO towards future networks.",2021-07-02T12:48:55Z,2021-07-02T12:48:55Z,http://arxiv.org/abs/2107.01041v1,http://arxiv.org/pdf/2107.01041v1,"cs.IT, eess.SP, math.IT"
Cascaded Complementary Filter Architecture for Sensor Fusion in Attitude   Estimation,"Parag Narkhede, Shashi Poddar, Rahee Walambe, George Ghinea, Ketan Kotecha","Attitude estimation is the process of computing the orientation angles of an object with respect to a fixed frame of reference. Gyroscope, accelerometer, and magnetometer are some of the fundamental sensors used in attitude estimation. The orientation angles computed from these sensors are combined using the sensor fusion methodologies to obtain accurate estimates. The complementary filter is one of the widely adopted techniques whose performance is highly dependent on the appropriate selection of its gain parameters. This paper presents a novel cascaded architecture of the complementary filter that employs a nonlinear and linear version of the complementary filter within one framework. The nonlinear version is used to correct the gyroscope bias, while the linear version estimates the attitude angle. The significant advantage of the proposed architecture is its independence of the filter parameters, thereby avoiding tuning the filters gain parameters. The proposed architecture does not require any mathematical modeling of the system and is computationally inexpensive. The proposed methodology is applied to the real-world datasets, and the estimation results were found to be promising compared to the other state-of-the-art algorithms.",2021-07-08T16:51:30Z,2021-07-08T16:51:30Z,http://arxiv.org/abs/2107.03970v1,http://arxiv.org/pdf/2107.03970v1,"eess.SY, cs.SY"
Approximation Theory of Convolutional Architectures for Time Series   Modelling,"Haotian Jiang, Zhong Li, Qianxiao Li","We study the approximation properties of convolutional architectures applied to time series modelling, which can be formulated mathematically as a functional approximation problem. In the recurrent setting, recent results reveal an intricate connection between approximation efficiency and memory structures in the data generation process. In this paper, we derive parallel results for convolutional architectures, with WaveNet being a prime example. Our results reveal that in this new setting, approximation efficiency is not only characterised by memory, but also additional fine structures in the target relationship. This leads to a novel definition of spectrum-based regularity that measures the complexity of temporal relationships under the convolutional approximation scheme. These analyses provide a foundation to understand the differences between architectural choices for time series modelling and can give theoretically grounded guidance for practical applications.",2021-07-20T09:19:26Z,2021-07-20T09:19:26Z,http://arxiv.org/abs/2107.09355v1,http://arxiv.org/pdf/2107.09355v1,"cs.LG, stat.ML, 68W25, 68T07, 37M10, I.2.6"
An FPGA cached sparse matrix vector product (SpMV) for unstructured   computational fluid dynamics simulations,"Guillermo Oyarzun, Daniel Peyrolon, Carlos Alvarez, Xavier Martorell","Field Programmable Gate Arrays generate algorithmic specific architectures that improve the code's FLOP per watt ratio. Such devices are re-gaining interest due to the rise of new tools that facilitate their programming, such as OmpSs. The computational fluid dynamics community is always investigating new architectures that can improve its algorithm's performance. Commonly, those algorithms have a low arithmetic intensity and only reach a small percentage of the peak performance. The sparse matrix-vector multiplication is one of the most time-consuming operations on unstructured simulations. The matrix's sparsity pattern determines the indirect memory accesses of the multiplying vector. This data path is hard to predict, making traditional implementations fail. In this work, we present an FPGA architecture that maximizes the vector's re-usability by introducing a cache-like architecture. The cache is implemented as a circular list that maintains the BRAM vector components while needed. Following this strategy, up to 16 times of acceleration is obtained compared to a naive implementation of the algorithm.",2021-07-24T06:09:37Z,2021-07-24T06:09:37Z,http://arxiv.org/abs/2107.12371v1,http://arxiv.org/pdf/2107.12371v1,"physics.comp-ph, cs.DC"
Exponentiation Using Laplace Expansion,Bhavesh Lakhotia,This article derives an equation for exponentiation that can be used for calculating exponents using a parallel computing architecture.,2021-06-20T08:37:39Z,2021-06-20T08:37:39Z,http://arxiv.org/abs/2107.13520v1,http://arxiv.org/pdf/2107.13520v1,"math.NA, cs.DC, cs.NA"
Horizontal and Vertical Collaboration for VR Delivery in MEC-Enabled   Small-Cell Networks,"Zhuojia Gu, Hancheng Lu, Chenkai Zou","Due to the large bandwidth, low latency and computationally intensive features of virtual reality (VR) video applications, the current resource-constrained wireless and edge networks cannot meet the requirements of on-demand VR delivery. In this letter, we propose a joint horizontal and vertical collaboration architecture in mobile edge computing (MEC)-enabled small-cell networks for VR delivery. In the proposed architecture, multiple MEC servers can jointly provide VR head-mounted devices (HMDs) with edge caching and viewpoint computation services, while the computation tasks can also be performed at HMDs or on the cloud. Power allocation at base stations (BSs) is considered in coordination with horizontal collaboration (HC) and vertical collaboration (VC) of MEC servers to obtain lower end-to-end latency of VR delivery. A joint caching, power allocation and task offloading problem is then formulated, and a discrete branch-reduce-and-bound (DBRB) algorithm inspired by monotone optimization is proposed to effectively solve the problem. Simulation results demonstrate the advantage of the proposed architecture and algorithm in terms of existing ones.",2021-09-05T02:36:47Z,2021-09-05T02:36:47Z,http://arxiv.org/abs/2109.01971v1,http://arxiv.org/pdf/2109.01971v1,"cs.NI, eess.SP"
Untrained Graph Neural Networks for Denoising,"Samuel Rey, Santiago Segarra, Reinhard Heckel, Antonio G. Marques","A fundamental problem in signal processing is to denoise a signal. While there are many well-performing methods for denoising signals defined on regular supports, such as images defined on two-dimensional grids of pixels, many important classes of signals are defined over irregular domains such as graphs. This paper introduces two untrained graph neural network architectures for graph signal denoising, provides theoretical guarantees for their denoising capabilities in a simple setup, and numerically validates the theoretical results in more general scenarios. The two architectures differ on how they incorporate the information encoded in the graph, with one relying on graph convolutions and the other employing graph upsampling operators based on hierarchical clustering. Each architecture implements a different prior over the targeted signals. To numerically illustrate the validity of the theoretical results and to compare the performance of the proposed architectures with other denoising alternatives, we present several experimental results with real and synthetic datasets.",2021-09-24T00:57:14Z,2023-02-16T21:49:12Z,http://arxiv.org/abs/2109.11700v2,http://arxiv.org/pdf/2109.11700v2,"eess.SP, cs.LG"
A Comparison of Neural Network Architectures for Data-Driven   Reduced-Order Modeling,"Anthony Gruber, Max Gunzburger, Lili Ju, Zhu Wang","The popularity of deep convolutional autoencoders (CAEs) has engendered new and effective reduced-order models (ROMs) for the simulation of large-scale dynamical systems. Despite this, it is still unknown whether deep CAEs provide superior performance over established linear techniques or other network-based methods in all modeling scenarios. To elucidate this, the effect of autoencoder architecture on its associated ROM is studied through the comparison of deep CAEs against two alternatives: a simple fully connected autoencoder, and a novel graph convolutional autoencoder. Through benchmark experiments, it is shown that the superior autoencoder architecture for a given ROM application is highly dependent on the size of the latent space and the structure of the snapshot data, with the proposed architecture demonstrating benefits on data with irregular connectivity when the latent space is sufficiently large.",2021-10-05T23:42:09Z,2022-02-08T22:31:36Z,http://arxiv.org/abs/2110.03442v3,http://arxiv.org/pdf/2110.03442v3,"cs.LG, cs.NA, math.NA, 65M22, 65M60"
Ensemble Neural Representation Networks,"Milad Soltany Kadarvish, Hesam Mojtahedi, Hossein Entezari Zarch, Amirhossein Kazerouni, Alireza Morsali, Azra Abtahi, Farokh Marvasti","Implicit Neural Representation (INR) has recently attracted considerable attention for storing various types of signals in continuous forms. The existing INR networks require lengthy training processes and high-performance computational resources. In this paper, we propose a novel sub-optimal ensemble architecture for INR that resolves the aforementioned problems. In this architecture, the representation task is divided into several sub-tasks done by independent sub-networks. We show that the performance of the proposed ensemble INR architecture may decrease if the dimensions of sub-networks increase. Hence, it is vital to suggest an optimization algorithm to find the sub-optimal structure of the ensemble network, which is done in this paper. According to the simulation results, the proposed architecture not only has significantly fewer floating-point operations (FLOPs) and less training time, but it also has better performance in terms of Peak Signal to Noise Ratio (PSNR) compared to those of its counterparts.",2021-10-07T12:49:21Z,2022-03-15T14:36:56Z,http://arxiv.org/abs/2110.04124v2,http://arxiv.org/pdf/2110.04124v2,"cs.LG, eess.IV, eess.SP"
Music Source Separation with Deep Equilibrium Models,"Yuichiro Koyama, Naoki Murata, Stefan Uhlich, Giorgio Fabbro, Shusuke Takahashi, Yuki Mitsufuji","While deep neural network-based music source separation (MSS) is very effective and achieves high performance, its model size is often a problem for practical deployment. Deep implicit architectures such as deep equilibrium models (DEQ) were recently proposed, which can achieve higher performance than their explicit counterparts with limited depth while keeping the number of parameters small. This makes DEQ also attractive for MSS, especially as it was originally applied to sequential modeling tasks in natural language processing and thus should in principle be also suited for MSS. However, an investigation of a good architecture and training scheme for MSS with DEQ is needed as the characteristics of acoustic signals are different from those of natural language data. Hence, in this paper we propose an architecture and training scheme for MSS with DEQ. Starting with the architecture of Open-Unmix (UMX), we replace its sequence model with DEQ. We refer to our proposed method as DEQ-based UMX (DEQ-UMX). Experimental results show that DEQ-UMX performs better than the original UMX while reducing its number of parameters by 30%.",2021-10-13T04:34:46Z,2022-04-28T06:42:41Z,http://arxiv.org/abs/2110.06494v2,http://arxiv.org/pdf/2110.06494v2,"cs.SD, eess.AS"
Comparison of SVD and factorized TDNN approaches for speech to text,"Jeffrey Josanne Michael, Nagendra Kumar Goel, Navneeth K, Jonas Robertson, Shravan Mishra","This work concentrates on reducing the RTF and word error rate of a hybrid HMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We find this architecture particularly useful for lightly reverberated environments. However, these models tend to demand more computation than is desirable. In this work, we explore alternate architectures employing singular value decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as well as to the affine transforms of every LSTM cell. We compare this approach with specifying bottleneck layers similar to those introduced by SVD before training. Additionally, we reduced the search space of the decoding graph to make it a better fit to operate in real-time applications. We report -61.57% relative reduction in RTF and almost 1% relative decrease in WER for our architecture trained on Fisher data along with reverberated versions of this dataset in order to match one of our target test distributions.",2021-10-13T20:54:37Z,2021-10-13T20:54:37Z,http://arxiv.org/abs/2110.07027v1,http://arxiv.org/pdf/2110.07027v1,"cs.SD, cs.CL, eess.AS"
How Does Momentum Benefit Deep Neural Networks Architecture Design? A   Few Case Studies,"Bao Wang, Hedi Xia, Tan Nguyen, Stanley Osher","We present and review an algorithmic and theoretical framework for improving neural network architecture design via momentum. As case studies, we consider how momentum can improve the architecture design for recurrent neural networks (RNNs), neural ordinary differential equations (ODEs), and transformers. We show that integrating momentum into neural network architectures has several remarkable theoretical and empirical benefits, including 1) integrating momentum into RNNs and neural ODEs can overcome the vanishing gradient issues in training RNNs and neural ODEs, resulting in effective learning long-term dependencies. 2) momentum in neural ODEs can reduce the stiffness of the ODE dynamics, which significantly enhances the computational efficiency in training and testing. 3) momentum can improve the efficiency and accuracy of transformers.",2021-10-13T21:11:04Z,2021-10-19T02:55:21Z,http://arxiv.org/abs/2110.07034v2,http://arxiv.org/pdf/2110.07034v2,"cs.LG, cs.NA, math.DS, math.NA, 68Txx"
NeuralArTS: Structuring Neural Architecture Search with Type Theory,"Robert Wu, Nayan Saxena, Rohan Jain","Neural Architecture Search (NAS) algorithms automate the task of finding optimal deep learning architectures given an initial search space of possible operations. Developing these search spaces is usually a manual affair with pre-optimized search spaces being more efficient, rather than searching from scratch. In this paper we present a new framework called Neural Architecture Type System (NeuralArTS) that categorizes the infinite set of network operations in a structured type system. We further demonstrate how NeuralArTS can be applied to convolutional layers and propose several future directions.",2021-10-17T03:28:27Z,2021-11-05T04:59:29Z,http://arxiv.org/abs/2110.08710v3,http://arxiv.org/pdf/2110.08710v3,"cs.LG, cs.LO, cs.PL, stat.ML"
FLOWER: A comprehensive dataflow compiler for high-level synthesis,"Puya Amiri, Arsène Pérard-Gayot, Richard Membarth, Philipp Slusallek, Roland Leißa, Sebastian Hack","FPGAs have found their way into data centers as accelerator cards, making reconfigurable computing more accessible for high-performance applications. At the same time, new high-level synthesis compilers like Xilinx Vitis and runtime libraries such as XRT attract software programmers into the reconfigurable domain. While software programmers are familiar with task-level and data-parallel programming, FPGAs often require different types of parallelism. For example, data-driven parallelism is mandatory to obtain satisfactory hardware designs for pipelined dataflow architectures. However, software programmers are often not acquainted with dataflow architectures - resulting in poor hardware designs.   In this work we present FLOWER, a comprehensive compiler infrastructure that provides automatic canonical transformations for high-level synthesis from a domain-specific library. This allows programmers to focus on algorithm implementations rather than low-level optimizations for dataflow architectures. We show that FLOWER allows to synthesize efficient implementations for high-performance streaming applications targeting System-on-Chip and FPGA accelerator cards, in the context of image processing and computer vision.",2021-12-14T23:28:57Z,2021-12-14T23:28:57Z,http://arxiv.org/abs/2112.07789v1,http://arxiv.org/pdf/2112.07789v1,"cs.AR, cs.DC, cs.PL, eess.IV"
EEG-Transformer: Self-attention from Transformer Architecture for   Decoding EEG of Imagined Speech,"Young-Eun Lee, Seo-Hyun Lee","Transformers are groundbreaking architectures that have changed a flow of deep learning, and many high-performance models are developing based on transformer architectures. Transformers implemented only with attention with encoder-decoder structure following seq2seq without using RNN, but had better performance than RNN. Herein, we investigate the decoding technique for electroencephalography (EEG) composed of self-attention module from transformer architecture during imagined speech and overt speech. We performed classification of nine subjects using convolutional neural network based on EEGNet that captures temporal-spectral-spatial features from EEG of imagined speech and overt speech. Furthermore, we applied the self-attention module to decoding EEG to improve the performance and lower the number of parameters. Our results demonstrate the possibility of decoding brain activities of imagined speech and overt speech using attention modules. Also, only single channel EEG or ear-EEG can be used to decode the imagined speech for practical BCIs.",2021-12-15T15:16:40Z,2021-12-15T15:16:40Z,http://arxiv.org/abs/2112.09239v1,http://arxiv.org/pdf/2112.09239v1,"cs.HC, cs.SD, eess.AS"
A Model-Based Architecture for Automatic Anesthesia Infusion,Dayang Liu,"This paper presents a model-based control architecture. Based on the Medical Cyber-physical Systems (MCPS) concept, we construct a safe and reliable automatic anesthesia control closed-loop system. The control architecture uses the patient's propofol pharmacokinetic/pharmacodynamic model, using the calculated results as a feedback signal to correct the model's uncertainties. During anesthesia, the bispectral index (BIS) level is used as a control variable to control the depth of anesthesia. The proposed control architecture can automatically adjust the infusion rate of anesthesia drugs without concussion or overshoot. The system adaptively adjusts the drug infusion rate to maintain the BIS level at a stable target value. We utilized MATLAB-Simulink to verify the effectiveness of this method, and the robustness test was carried out by adding noise blocks. At last, we analyze the simulation result of the average patient. In general, the proposed method achieves a stable and fast induction period during the maintenance phase of anesthesia with a good disturbance rejection.",2021-12-28T13:48:14Z,2021-12-28T13:48:14Z,http://arxiv.org/abs/2112.14144v1,http://arxiv.org/pdf/2112.14144v1,"eess.SY, cs.SY"
MPVNN: Mutated Pathway Visible Neural Network Architecture for   Interpretable Prediction of Cancer-specific Survival Risk,"Gourab Ghosh Roy, Nicholas Geard, Karin Verspoor, Shan He","Survival risk prediction using gene expression data is important in making treatment decisions in cancer. Standard neural network (NN) survival analysis models are black boxes with lack of interpretability. More interpretable visible neural network (VNN) architectures are designed using biological pathway knowledge. But they do not model how pathway structures can change for particular cancer types. We propose a novel Mutated Pathway VNN or MPVNN architecture, designed using prior signaling pathway knowledge and gene mutation data-based edge randomization simulating signal flow disruption. As a case study, we use the PI3K-Akt pathway and demonstrate overall improved cancer-specific survival risk prediction results of MPVNN over standard non-NN and other similar sized NN survival analysis methods. We show that trained MPVNN architecture interpretation, which points to smaller sets of genes connected by signal flow within the PI3K-Akt pathway that are important in risk prediction for particular cancer types, is reliable.",2022-02-02T05:22:24Z,2022-02-02T05:22:24Z,http://arxiv.org/abs/2202.00882v1,http://arxiv.org/pdf/2202.00882v1,"q-bio.QM, cs.LG"
Differentiable Economics for Randomized Affine Maximizer Auctions,"Michael Curry, Tuomas Sandholm, John Dickerson","A recent approach to automated mechanism design, differentiable economics, represents auctions by rich function approximators and optimizes their performance by gradient descent. The ideal auction architecture for differentiable economics would be perfectly strategyproof, support multiple bidders and items, and be rich enough to represent the optimal (i.e. revenue-maximizing) mechanism. So far, such an architecture does not exist. There are single-bidder approaches (MenuNet, RochetNet) which are always strategyproof and can represent optimal mechanisms. RegretNet is multi-bidder and can approximate any mechanism, but is only approximately strategyproof. We present an architecture that supports multiple bidders and is perfectly strategyproof, but cannot necessarily represent the optimal mechanism. This architecture is the classic affine maximizer auction (AMA), modified to offer lotteries. By using the gradient-based optimization tools of differentiable economics, we can now train lottery AMAs, competing with or outperforming prior approaches in revenue.",2022-02-06T22:01:21Z,2022-02-06T22:01:21Z,http://arxiv.org/abs/2202.02872v1,http://arxiv.org/pdf/2202.02872v1,"cs.GT, cs.LG, econ.GN, q-fin.EC"
Approaches to Artificial General Intelligence: An Analysis,Soumil Rathi,"This paper is an analysis of the different methods proposed to achieve AGI, including Human Brain Emulation, AIXI and Integrated Cognitive Architecture. First, the definition of AGI as used in this paper has been defined, and its requirements have been stated. For each proposed method mentioned, the method in question was summarized and its key processes were detailed, showcasing how it functioned. Then, each method listed was analyzed, taking various factors into consideration, such as technological requirements, computational ability, and adequacy to the requirements. It was concluded that while there are various methods to achieve AGI that could work, such as Human Brain Emulation and Integrated Cognitive Architectures, the most promising method to achieve AGI is Integrated Cognitive Architectures. This is because Human Brain Emulation was found to require scanning technologies that will most likely not be available until the 2030s, making it unlikely to be created before then. Moreover, Integrated Cognitive Architectures has reduced computational requirements and a suitable functionality for General Intelligence, making it the most likely way to achieve AGI.",2022-01-29T05:21:09Z,2022-01-29T05:21:09Z,http://arxiv.org/abs/2202.03153v1,http://arxiv.org/pdf/2202.03153v1,"cs.AI, I.2.0; I.2.6"
Monitoring Time Series With Missing Values: a Deep Probabilistic   Approach,"Oshri Barazani, David Tolpin","Systems are commonly monitored for health and security through collection and streaming of multivariate time series. Advances in time series forecasting due to adoption of multilayer recurrent neural network architectures make it possible to forecast in high-dimensional time series, and identify and classify novelties early, based on subtle changes in the trends. However, mainstream approaches to multi-variate time series predictions do not handle well cases when the ongoing forecast must include uncertainty, nor they are robust to missing data. We introduce a new architecture for time series monitoring based on combination of state-of-the-art methods of forecasting in high-dimensional time series with full probabilistic handling of uncertainty. We demonstrate advantage of the architecture for time series forecasting and novelty detection, in particular with partially missing data, and empirically evaluate and compare the architecture to state-of-the-art approaches on a real-world data set.",2022-03-09T17:53:47Z,2022-03-09T17:53:47Z,http://arxiv.org/abs/2203.04916v1,http://arxiv.org/pdf/2203.04916v1,"stat.ML, cs.LG"
Multilingual Simultaneous Speech Translation,"Shashank Subramanya, Jan Niehues","Applications designed for simultaneous speech translation during events such as conferences or meetings need to balance quality and lag while displaying translated text to deliver a good user experience. One common approach to building online spoken language translation systems is by leveraging models built for offline speech translation. Based on a technique to adapt end-to-end monolingual models, we investigate multilingual models and different architectures (end-to-end and cascade) on the ability to perform online speech translation. On the multilingual TEDx corpus, we show that the approach generalizes to different architectures. We see similar gains in latency reduction (40% relative) across languages and architectures. However, the end-to-end architecture leads to smaller translation quality losses after adapting to the online model. Furthermore, the approach even scales to zero-shot directions.",2022-03-28T15:17:11Z,2022-03-29T07:55:11Z,http://arxiv.org/abs/2203.14835v2,http://arxiv.org/pdf/2203.14835v2,"cs.CL, cs.SD, eess.AS"
"Two-Leg Deep Space Relay Architectures: Performance, Challenges, and   Perspectives","Dario Modenini, Alfredo Locarini, Lorenzo Valentini, Alberto Faedi, Paolo Tortora, Davide Rovelli, Nicolò Mazzali, Marco Chiani, Enrico Paolini","In this paper, architectures for interplanetary communications that feature the use of a data relay are investigated. In the considered ""two-leg"" architecture, a spacecraft orbiting the Earth, or in orbit at a Lagrange point, receives data from a deep space probe (leg-1) and relays them towards ground (leg-2). Different wireless technologies for the interplanetary link, namely, radio frequencies above the Ka band and optical frequencies, are considered. Moreover, the cases of transparent and regenerative relaying as well as different different orbital configurations are addressed, offering a thorough analysis of such systems from different viewpoints. Results show that, under certain constraints in terms of pointing accuracy and onboard antenna size, the adoption of a two-leg architecture can achieve the data rates supported by direct space-to-Earth link configurations with remarkably smaller ground station antennas.",2022-05-27T21:00:55Z,2022-05-27T21:00:55Z,http://arxiv.org/abs/2205.14234v1,http://arxiv.org/pdf/2205.14234v1,"eess.SP, cs.SY, eess.SY"
On an Information and Control Architecture for Future Electric Energy   Systems,"Le Xie, Tong Huang, P. R. Kumar, Anupam A. Thatte, Sanjoy K. Mitter","This paper presents considerations towards an information and control architecture for future electric energy systems driven by massive changes resulting from the societal goals of decarbonization and electrification. This paper describes the new requirements and challenges of an extended information and control architecture that need to be addressed for continued reliable delivery of electricity. It identifies several new actionable information and control loops, along with their spatial and temporal scales of operation, which can together meet the needs of future grids and enable deep decarbonization of the electricity sector. The present architecture of electric power grids designed in a different era is thereby extensible to allow the incorporation of increased renewables and other emerging electric loads.",2022-06-01T00:25:09Z,2022-11-05T01:39:04Z,http://arxiv.org/abs/2206.00160v3,http://arxiv.org/pdf/2206.00160v3,"eess.SY, cs.SY"
Mathematical Operations and Equation Solving with Reconfigurable   Metadevices,"Dimitrios Tzarouchis, Mario Junior Mencagli, Brian Edwards, Nader Engheta","Performing analog computations with metastructures is an emerging wave-based paradigm for solving mathematical problems. For such devices, one major challenge is their reconfigurability, especially without the need for a priori mathematical computations or computationally-intensive optimization. Their equation-solving capabilities are applied only to matrices with special spectral (eigenvalue) distribution. Here we report the theory and design of wave-based metastructures using tunable elements capable of solving integral/differential equations in a fully-reconfigurable fashion. We consider two architectures: the Miller architecture, which requires the singular-value decomposition, and an alternative intuitive direct-complex-matrix (DCM) architecture introduced here, which does not require a priori mathematical decomposition. As examples, we demonstrate, using system-level simulation tools, the solutions of integral and differential equations. We then expand the matrix inverting capabilities of both architectures toward evaluating the generalized Moore-Penrose matrix inversion. Therefore, we provide evidence that metadevices can implement generalized matrix inversions and act as the basis for the gradient descent method for solutions to a wide variety of problems. Finally, a general upper bound of the solution convergence time reveals the rich potential that such metadevices can offer for stationary iterative schemes.",2022-05-31T14:39:44Z,2022-05-31T14:39:44Z,http://arxiv.org/abs/2206.02549v1,http://arxiv.org/pdf/2206.02549v1,"physics.app-ph, cs.NA, math.NA, physics.optics"
Scale up your In-Memory Accelerator: Leveraging Wireless-on-Chip   Communication for AIMC-based CNN Inference,"Nazareno Bruschi, Giuseppe Tagliavini, Francesco Conti, Sergi Abadal, Alberto Cabellos-Aparicio, Eduard Alarcón, Geethan Karunaratne, Irem Boybat, Luca Benini, Davide Rossi","Analog In-Memory Computing (AIMC) is emerging as a disruptive paradigm for heterogeneous computing, potentially delivering orders of magnitude better peak performance and efficiency over traditional digital signal processing architectures on Matrix-Vector multiplication. However, to sustain this throughput in real-world applications, AIMC tiles must be supplied with data at very high bandwidth and low latency; this poses an unprecedented pressure on the on-chip communication infrastructure, which becomes the system's performance and efficiency bottleneck. In this context, the performance and plasticity of emerging on-chip wireless communication paradigms provide the required breakthrough to up-scale on-chip communication in large AIMC devices. This work presents a many-tile AIMC architecture with inter-tile wireless communication that integrates multiple heterogeneous computing clusters, embedding a mix of parallel RISC-V cores and AIMC tiles. We perform an extensive design space exploration of the proposed architecture and discuss the benefits of exploiting emerging on-chip communication technologies such as wireless transceivers in the millimeter-wave and terahertz bands.",2022-06-03T13:13:54Z,2022-06-03T13:13:54Z,http://arxiv.org/abs/2206.04796v1,http://arxiv.org/pdf/2206.04796v1,"cs.AR, cs.SY, eess.SY"
On the Prediction Network Architecture in RNN-T for ASR,"Dario Albesano, Jesús Andrés-Ferrer, Nicola Ferri, Puming Zhan","RNN-T models have gained popularity in the literature and in commercial systems because of their competitiveness and capability of operating in online streaming mode. In this work, we conduct an extensive study comparing several prediction network architectures for both monotonic and original RNN-T models. We compare 4 types of prediction networks based on a common state-of-the-art Conformer encoder and report results obtained on Librispeech and an internal medical conversation data set. Our study covers both offline batch-mode and online streaming scenarios. In contrast to some previous works, our results show that Transformer does not always outperform LSTM when used as prediction network along with Conformer encoder. Inspired by our scoreboard, we propose a new simple prediction network architecture, N-Concat, that outperforms the others in our on-line streaming benchmark. Transformer and n-gram reduced architectures perform very similarly yet with some important distinct behaviour in terms of previous context. Overall we obtained up to 4.1 % relative WER improvement compared to our LSTM baseline, while reducing prediction network parameters by nearly an order of magnitude (8.4 times).",2022-06-29T13:11:46Z,2022-06-29T13:11:46Z,http://arxiv.org/abs/2206.14618v1,http://arxiv.org/pdf/2206.14618v1,"eess.AS, cs.AI"
Deep Learning and Symbolic Regression for Discovering Parametric   Equations,"Michael Zhang, Samuel Kim, Peter Y. Lu, Marin Soljačić","Symbolic regression is a machine learning technique that can learn the governing formulas of data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity and dimensionality of the systems that it can analyze. Deep learning on the other hand has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. We propose a neural network architecture to extend symbolic regression to parametric systems where some coefficient may vary but the structure of the underlying governing equation remains constant. We demonstrate our method on various analytic expressions, ODEs, and PDEs with varying coefficients and show that it extrapolates well outside of the training domain. The neural network-based architecture can also integrate with other deep learning architectures so that it can analyze high-dimensional data while being trained end-to-end. To this end we integrate our architecture with convolutional neural networks to analyze 1D images of varying spring systems.",2022-07-01T16:25:59Z,2023-05-28T15:10:44Z,http://arxiv.org/abs/2207.00529v2,http://arxiv.org/pdf/2207.00529v2,"cs.LG, cs.SC, physics.comp-ph, physics.data-an"
Hybrid Precoding for Mixture Use of Phase Shifters and Switches in   mmWave Massive MIMO,"Chenhao Qi, Qiang Liu, Xianghao Yu, Geoffrey Ye Li","A variable-phase-shifter (VPS) architecture with hybrid precoding for mixture use of phase shifters and switches, is proposed for millimeter wave massive multiple-input multiple-output communications. For the VPS architecture, a hybrid precoding design (HPD) scheme, called VPS-HPD, is proposed to optimize the phases according to the channel state information by alternately optimizing the analog precoder and digital precoder. To reduce the computational complexity of the VPS-HPD scheme, a low-complexity HPD scheme for the VPS architecture (VPS-LC-HPD) including alternating optimization in three stages is then proposed, where each stage has a closed-form solution and can be efficiently implemented. To reduce the hardware complexity introduced by the large number of switches, we consider a group-connected VPS architecture and propose a HPD scheme, where the HPD problem is divided into multiple independent subproblems with each subproblem flexibly solved by the VPS-HPD or VPS-LC-HPD scheme. Simulation results verify the effectiveness of the propose schemes and show that the proposed schemes can achieve satisfactory spectral efficiency performance with reduced computational complexity or hardware complexity.",2022-08-01T09:54:44Z,2022-08-01T09:54:44Z,http://arxiv.org/abs/2208.00714v1,http://arxiv.org/pdf/2208.00714v1,"cs.IT, eess.SP, math.IT"
Computer vision-based analysis of buildings and built environments: A   systematic review of current approaches,"Małgorzata B. Starzyńska, Robin Roussel, Sam Jacoby, Ali Asadipour","Analysing 88 sources published from 2011 to 2021, this paper presents a first systematic review of the computer vision-based analysis of buildings and the built environments to assess its value to architectural and urban design studies. Following a multi-stage selection process, the types of algorithms and data sources used are discussed in respect to architectural applications such as a building classification, detail classification, qualitative environmental analysis, building condition survey, and building value estimation. This reveals current research gaps and trends, and highlights two main categories of research aims. First, to use or optimise computer vision methods for architectural image data, which can then help automate time-consuming, labour-intensive, or complex tasks of visual analysis. Second, to explore the methodological benefits of machine learning approaches to investigate new questions about the built environment by finding patterns and relationships between visual, statistical, and qualitative data, which can overcome limitations of conventional manual analysis. The growing body of research offers new methods to architectural and design studies, with the paper identifying future challenges and directions of research.",2022-08-01T14:17:51Z,2022-08-01T14:17:51Z,http://arxiv.org/abs/2208.00881v1,http://arxiv.org/pdf/2208.00881v1,"cs.CV, I.4.8; J.5"
Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End   Speech Recognition,"Andrei Andrusenko, Rauf Nasretdinov, Aleksei Romanenko","Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.",2022-08-16T10:40:15Z,2023-03-11T10:00:17Z,http://arxiv.org/abs/2208.07657v3,http://arxiv.org/pdf/2208.07657v3,"eess.AS, cs.LG, cs.SD"
Improved Zero-Shot Audio Tagging & Classification with Patchout   Spectrogram Transformers,"Paul Primus, Gerhard Widmer","Standard machine learning models for tagging and classifying acoustic signals cannot handle classes that were not seen during training. Zero-Shot (ZS) learning overcomes this restriction by predicting classes based on adaptable class descriptions. This study sets out to investigate the effectiveness of self-attention-based audio embedding architectures for ZS learning. To this end, we compare the very recent patchout spectrogram transformer with two classic convolutional architectures. We evaluate these three architectures on three tasks and on three different benchmark datasets: general-purpose tagging on AudioSet, environmental sound classification on ESC-50, and instrument tagging on OpenMIC. Our results show that the self-attention-based embedding methods outperform both compared convolutional architectures in all of these settings. By designing training and test data accordingly, we observe that prediction performance suffers significantly when the `semantic distance' between training and new test classes is large, an effect that will deserve more detailed investigations.",2022-08-24T09:48:22Z,2022-08-24T09:48:22Z,http://arxiv.org/abs/2208.11402v1,http://arxiv.org/pdf/2208.11402v1,"cs.SD, cs.LG, eess.AS, eess.SP"
PaRTAA: A Real-time Multiprocessor for Mixed-Criticality Airborne   Systems,"Shibarchi Majumder, Jens F D Nielsen, Thomas Bak","Mixed-criticality systems, where multiple systems with varying criticality-levels share a single hardware platform, require isolation between tasks with different criticality-levels. Isolation can be achieved with software-based solutions or can be enforced by a hardware level partitioning. An asymmetric multiprocessor architecture offers hardware-based isolation at the cost of underutilized hardware resources, and the inter-core communication mechanism is often a single point of failure in such architectures. In contrast, a partitioned uniprocessor offers efficient resource utilization at the cost of limited scalability.   We propose a partitioned real-time asymmetric architecture (PaRTAA) specifically designed for mixed-criticality airborne systems, featuring robust partitioning within processing elements for establishing isolation between tasks with varying criticality. The granularity in the processing element offers efficient resource utilization where inter-dependent tasks share the same processing element for sequential execution while preserving isolation, and independent tasks simultaneously execute on different processing elements as per system requirements.",2022-08-31T05:35:37Z,2022-08-31T05:35:37Z,http://arxiv.org/abs/2208.14645v1,http://arxiv.org/pdf/2208.14645v1,"cs.AR, cs.DC, cs.SY, eess.SY"
TB or not TB? Acoustic cough analysis for tuberculosis classification,"Geoffrey Frost, Grant Theron, Thomas Niesler","In this work, we explore recurrent neural network architectures for tuberculosis (TB) cough classification. In contrast to previous unsuccessful attempts to implement deep architectures in this domain, we show that a basic bidirectional long short-term memory network (BiLSTM) can achieve improved performance. In addition, we show that by performing greedy feature selection in conjunction with a newly-proposed attention-based architecture that learns patient invariant features, substantially better generalisation can be achieved compared to a baseline and other considered architectures. Furthermore, this attention mechanism allows an inspection of the temporal regions of the audio signal considered to be important for classification to be performed. Finally, we develop a neural style transfer technique to infer idealised inputs which can subsequently be analysed. We find distinct differences between the idealised power spectra of TB and non-TB coughs, which provide clues about the origin of the features in the audio signal.",2022-09-02T10:17:07Z,2022-09-02T10:17:07Z,http://arxiv.org/abs/2209.00934v1,http://arxiv.org/pdf/2209.00934v1,"eess.AS, cs.LG, cs.SD"
LeVoice ASR Systems for the ISCSLP 2022 Intelligent Cockpit Speech   Recognition Challenge,"Yan Jia, Mi Hong, Jingyu Hou, Kailong Ren, Sifan Ma, Jin Wang, Fangzhen Peng, Yinglin Ji, Lin Yang, Junjie Wang","This paper describes LeVoice automatic speech recognition systems to track2 of intelligent cockpit speech recognition challenge 2022. Track2 is a speech recognition task without limits on the scope of model size. Our main points include deep learning based speech enhancement, text-to-speech based speech generation, training data augmentation via various techniques and speech recognition model fusion. We compared and fused the hybrid architecture and two kinds of end-to-end architecture. For end-to-end modeling, we used models based on connectionist temporal classification/attention-based encoder-decoder architecture and recurrent neural network transducer/attention-based encoder-decoder architecture. The performance of these models is evaluated with an additional language model to improve word error rates. As a result, our system achieved 10.2\% character error rate on the challenge test set data and ranked third place among the submitted systems in the challenge.",2022-10-14T12:35:25Z,2022-10-17T03:28:26Z,http://arxiv.org/abs/2210.07749v2,http://arxiv.org/pdf/2210.07749v2,"eess.AS, cs.SD"
Bayesian Convolutional Deep Sets with Task-Dependent Stationary Prior,"Yohan Jung, Jinkyoo Park","Convolutional deep sets are the architecture of a deep neural network (DNN) that can model stationary stochastic process. This architecture uses the kernel smoother and the DNN to construct the translation equivariant functional representations, and thus reflects the inductive bias of the stationarity into DNN. However, since this architecture employs the kernel smoother known as the non-parametric model, it may produce ambiguous representations when the number of data points is not given sufficiently. To remedy this issue, we introduce Bayesian convolutional deep sets that construct the random translation equivariant functional representations with stationary prior. Furthermore, we present how to impose the task-dependent prior for each dataset because a wrongly imposed prior forms an even worse representation than that of the kernel smoother. We validate the proposed architecture and its training on various experiments with time-series and image datasets.",2022-10-22T06:26:02Z,2022-10-22T06:26:02Z,http://arxiv.org/abs/2210.12363v1,http://arxiv.org/pdf/2210.12363v1,"stat.ML, cs.LG, stat.ME"
ConnectedUNets++: Mass Segmentation from Whole Mammographic Images,"Prithul Sarker, Sushmita Sarker, George Bebis, Alireza Tavakkoli","Deep learning has made a breakthrough in medical image segmentation in recent years due to its ability to extract high-level features without the need for prior knowledge. In this context, U-Net is one of the most advanced medical image segmentation models, with promising results in mammography. Despite its excellent overall performance in segmenting multimodal medical images, the traditional U-Net structure appears to be inadequate in various ways. There are certain U-Net design modifications, such as MultiResUNet, Connected-UNets, and AU-Net, that have improved overall performance in areas where the conventional U-Net architecture appears to be deficient. Following the success of UNet and its variants, we have presented two enhanced versions of the Connected-UNets architecture: ConnectedUNets+ and ConnectedUNets++. In ConnectedUNets+, we have replaced the simple skip connections of Connected-UNets architecture with residual skip connections, while in ConnectedUNets++, we have modified the encoder-decoder structure along with employing residual skip connections. We have evaluated our proposed architectures on two publicly available datasets, the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) and INbreast.",2022-10-25T00:04:29Z,2022-11-04T07:08:51Z,http://arxiv.org/abs/2210.13668v3,http://arxiv.org/pdf/2210.13668v3,"eess.IV, cs.CV, cs.LG"
Stardust: Compiling Sparse Tensor Algebra to a Reconfigurable Dataflow   Architecture,"Olivia Hsu, Alexander Rucker, Tian Zhao, Kunle Olukotun, Fredrik Kjolstad","We introduce Stardust, a compiler that compiles sparse tensor algebra to reconfigurable dataflow architectures (RDAs). Stardust introduces new user-provided data representation and scheduling language constructs for mapping to resource-constrained accelerated architectures. Stardust uses the information provided by these constructs to determine on-chip memory placement and to lower to the Capstan RDA through a parallel-patterns rewrite system that targets the Spatial programming model. The Stardust compiler is implemented as a new compilation path inside the TACO open-source system. Using cycle-accurate simulation, we demonstrate that Stardust can generate more Capstan tensor operations than its authors had implemented and that it results in 138$\times$ better performance than generated CPU kernels and 41$\times$ better performance than generated GPU kernels.",2022-11-07T01:01:43Z,2022-11-07T01:01:43Z,http://arxiv.org/abs/2211.03251v1,http://arxiv.org/pdf/2211.03251v1,"cs.PL, cs.AR, D.3"
Detecting train driveshaft damages using accelerometer signals and   Differential Convolutional Neural Networks,"Antía López Galdo, Alejandro Guerrero-López, Pablo M. Olmos, María Jesús Gómez García","Railway axle maintenance is critical to avoid catastrophic failures. Nowadays, condition monitoring techniques are becoming more prominent in the industry to prevent enormous costs and damage to human lives. This paper proposes the development of a railway axle condition monitoring system based on advanced 2D-Convolutional Neural Network (CNN) architectures applied to time-frequency representations of vibration signals. For this purpose, several preprocessing steps and different types of Deep Learning (DL) and Machine Learning (ML) architectures are discussed to design an accurate classification system. The resultant system converts the railway axle vibration signals into time-frequency domain representations, i.e., spectrograms, and, thus, trains a two-dimensional CNN to classify them depending on their cracks. The results showed that the proposed approach outperforms several alternative methods tested. The CNN architecture has been tested in 3 different wheelset assemblies, achieving AUC scores of 0.93, 0.86, and 0.75 outperforming any other architecture and showing a high level of reliability when classifying 4 different levels of defects.",2022-11-15T15:04:06Z,2022-11-15T15:04:06Z,http://arxiv.org/abs/2211.09011v1,http://arxiv.org/pdf/2211.09011v1,"cs.LG, eess.SP"
"Architecture, Protocols, and Algorithms for Location-Aware Services in   Beyond 5G Networks","Peter Hammarberg, Julia Vinogradova, Gábor Fodor, Ritesh Shreevastav, Satyam Dwivedi, Fredrik Gunnarsson","The automotive and railway industries are rapidly transforming with a strong drive towards automation and digitalization, with the goal of increased convenience, safety, efficiency, and sustainability. Since assisted and fully automated automotive and train transport services increasingly rely on vehicle-to-everything communications, and high-accuracy real-time positioning, it is necessary to continuously maintain high-accuracy localization, even in occlusion scenes such as tunnels, urban canyons, or areas covered by dense foliage. In this paper, we review the 5G positioning framework of the 3rd Generation Partnership Project in terms of methods and architecture and propose enhancements to meet the stringent requirements imposed by the transport industry. In particular, we highlight the benefit of fusing cellular and sensor measurements and discuss required architecture and protocol support for achieving this at the network side. We also propose a positioning framework to fuse cellular network measurements with measurements by onboard sensors. We illustrate the viability of the proposed fusion-based positioning approach using a numerical example.",2022-11-27T10:24:33Z,2022-11-27T10:24:33Z,http://arxiv.org/abs/2211.14781v1,http://arxiv.org/pdf/2211.14781v1,"cs.NI, cs.IT, math.IT"
Synthetic Low-Field MRI Super-Resolution Via Nested U-Net Architecture,"Aryan Kalluvila, Neha Koonjoo, Danyal Bhutto, Marcio Rockenbach, Matthew S. Rosen","Low-field (LF) MRI scanners have the power to revolutionize medical imaging by providing a portable and cheaper alternative to high-field MRI scanners. However, such scanners are usually significantly noisier and lower quality than their high-field counterparts. The aim of this paper is to improve the SNR and overall image quality of low-field MRI scans to improve diagnostic capability. To address this issue, we propose a Nested U-Net neural network architecture super-resolution algorithm that outperforms previously suggested deep learning methods with an average PSNR of 78.83 and SSIM of 0.9551. We tested our network on artificial noisy downsampled synthetic data from a major T1 weighted MRI image dataset called the T1-mix dataset. One board-certified radiologist scored 25 images on the Likert scale (1-5) assessing overall image quality, anatomical structure, and diagnostic confidence across our architecture and other published works (SR DenseNet, Generator Block, SRCNN, etc.). We also introduce a new type of loss function called natural log mean squared error (NLMSE). In conclusion, we present a more accurate deep learning method for single image super-resolution applied to synthetic low-field MRI via a Nested U-Net architecture.",2022-11-28T04:09:21Z,2022-11-28T04:09:21Z,http://arxiv.org/abs/2211.15047v1,http://arxiv.org/pdf/2211.15047v1,"eess.IV, cs.CV"
Correlation recurrent units: A novel neural architecture for improving   the predictive performance of time-series data,"Sunghyun Sim, Dohee Kim, Hyerim Bae","The time-series forecasting (TSF) problem is a traditional problem in the field of artificial intelligence. Models such as Recurrent Neural Network (RNN), Long Short Term Memory (LSTM), and GRU (Gate Recurrent Units) have contributed to improving the predictive accuracy of TSF. Furthermore, model structures have been proposed to combine time-series decomposition methods, such as seasonal-trend decomposition using Loess (STL) to ensure improved predictive accuracy. However, because this approach is learned in an independent model for each component, it cannot learn the relationships between time-series components. In this study, we propose a new neural architecture called a correlation recurrent unit (CRU) that can perform time series decomposition within a neural cell and learn correlations (autocorrelation and correlation) between each decomposition component. The proposed neural architecture was evaluated through comparative experiments with previous studies using five univariate time-series datasets and four multivariate time-series data. The results showed that long- and short-term predictive performance was improved by more than 10%. The experimental results show that the proposed CRU is an excellent method for TSF problems compared to other neural architectures.",2022-11-30T00:47:03Z,2024-08-28T15:17:44Z,http://arxiv.org/abs/2211.16653v3,http://arxiv.org/pdf/2211.16653v3,"cs.LG, cs.AI, eess.SP"
"Deep Learning Architectures for FSCV, a Comparison","Thomas Twomey, Leonardo Barbosa, Terry Lohrenz, P. Read Montague","We examined multiple deep neural network (DNN) architectures for suitability in predicting neurotransmitter concentrations from labeled in vitro fast scan cyclic voltammetry (FSCV) data collected on carbon fiber electrodes. Suitability is determined by the predictive performance in the ""out-of-probe"" case, the response to artificially induced electrical noise, and the ability to predict when the model will be errant for a given probe. This work extends prior comparisons of time series classification models by focusing on this specific task. It extends previous applications of machine learning to FSCV task by using a much larger data set and by incorporating recent advancements in deep neural networks. The InceptionTime architecture, a deep convolutional neural network, has the best absolute predictive performance of the models tested but was more susceptible to noise. A naive multilayer perceptron architecture had the second lowest prediction error and was less affected by the artificial noise, suggesting that convolutions may not be as important for this task as one might suspect.",2022-12-05T00:20:10Z,2022-12-05T00:20:10Z,http://arxiv.org/abs/2212.01960v1,http://arxiv.org/pdf/2212.01960v1,"physics.med-ph, cs.LG"
Parameter Efficient Transfer Learning for Various Speech Processing   Tasks,"Shinta Otake, Rei Kawakami, Nakamasa Inoue","Fine-tuning of self-supervised models is a powerful transfer learning method in a variety of fields, including speech processing, since it can utilize generic feature representations obtained from large amounts of unlabeled data. Fine-tuning, however, requires a new parameter set for each downstream task, which is parameter inefficient. Adapter architecture is proposed to partially solve this issue by inserting lightweight learnable modules into a frozen pre-trained model. However, existing adapter architectures fail to adaptively leverage low- to high-level features stored in different layers, which is necessary for solving various kinds of speech processing tasks. Thus, we propose a new adapter architecture to acquire feature representations more flexibly for various speech tasks. In experiments, we applied this adapter to WavLM on four speech tasks. It performed on par or better than naive fine-tuning, with only 11% of learnable parameters. It also outperformed an existing adapter architecture.",2022-12-06T06:33:20Z,2022-12-06T06:33:20Z,http://arxiv.org/abs/2212.02780v1,http://arxiv.org/pdf/2212.02780v1,"cs.MM, cs.SD, eess.AS"
Comparing Point Cloud Strategies for Collider Event Classification,"Peter Onyisi, Delon Shen, Jesse Thaler","In this paper, we compare several event classification architectures defined on the point cloud representation of collider events. These approaches, which are based on the frameworks of deep sets and edge convolutions, circumvent many of the difficulties associated with traditional feature engineering. To benchmark our architectures against more traditional event classification strategies, we perform a case study involving Higgs boson decays to tau leptons. We find a 2.5 times increase in performance compared to a baseline ATLAS analysis with engineered features. Our point cloud architectures can be viewed as simplified versions of graph neural networks, where each particle in the event corresponds to a graph node. In our case study, we find the best balance of performance and computational cost for simple pairwise architectures, which are based on learned edge features.",2022-12-20T21:22:09Z,2023-07-12T07:43:39Z,http://arxiv.org/abs/2212.10659v2,http://arxiv.org/pdf/2212.10659v2,"hep-ph, hep-ex, physics.data-an"
Environment for the Design and Automation of New CDPR Architectures,"Josue Rivera, Julio Garrido, Enrique Riveiro, Diego Silva","This paper presents a design and automation environment to study the control trajectory for new CDPR architectures, for instance CDPRs with an unusual number of cables or different motor location in the robot frame. In order to test the environment capabilities, an architecture of a planar under-constrained CDPR was designed, simulated, and implemented using standard industrial hardware. Both the simulated model and industrial prototype were running the same trajectories to determine the time delay and the error position between them. The tests have demonstrated that the simulated model of the CDPR reproduces the trajectories of the equivalent industrial prototype with a maximum deviation of 0.35% under loading and different speed conditions, despite the time delays produced by the data transmission and the non-deterministic communication protocols used to connect the industrial automation controller with the simulated model. The results have shown that the environment is suitable for trajectory control and workspace analysis of new CDPR architectures under different dynamic conditions.",2023-01-23T12:32:42Z,2023-01-23T12:32:42Z,http://arxiv.org/abs/2301.09396v1,http://arxiv.org/pdf/2301.09396v1,"cs.RO, cs.SY, eess.SY"
A predictive physics-aware hybrid reduced order model for reacting flows,"Adrián Corrochano, Rodolfo S. M. Freitas, Alessandro Parente, Soledad Le Clainche","In this work, a new hybrid predictive Reduced Order Model (ROM) is proposed to solve reacting flow problems. This algorithm is based on a dimensionality reduction using Proper Orthogonal Decomposition (POD) combined with deep learning architectures. The number of degrees of freedom is reduced from thousands of temporal points to a few POD modes with their corresponding temporal coefficients. Two different deep learning architectures have been tested to predict the temporal coefficients, based on recursive (RNN) and convolutional (CNN) neural networks. From each architecture, different models have been created to understand the behavior of each parameter of the neural network. Results show that these architectures are able to predict the temporal coefficients of the POD modes, as well as the whole snapshots. The RNN shows lower prediction error for all the variables analyzed. The model was also found capable of predicting more complex simulations showing transfer learning capabilities.",2023-01-24T08:39:20Z,2023-01-24T08:39:20Z,http://arxiv.org/abs/2301.09860v1,http://arxiv.org/pdf/2301.09860v1,"cs.LG, physics.data-an"
Cell-free mMIMO Support in the O-RAN Architecture: A PHY Layer   Perspective for 5G and Beyond Networks,"Vida Ranjbar, Adam Girycki, Md Arifur Rahman, Sofie Pollin, Marc Moonen, Evgenii Vinogradov","To keep supporting next-generation requirements, the radio access infrastructure will increasingly densify. Cell-free (CF) network architectures are emerging, combining dense deployments with extreme flexibility in allocating resources to users. In parallel, the Open Radio Access Networks (O-RAN) paradigm is transforming RAN towards an open, intelligent, virtualized, and fully interoperable architecture. This paradigm brings the needed flexibility and intelligent control opportunities for CF networking. In this paper, we document the current O-RAN terminology and contrast it with some common CF processing approaches. We then discuss the main O-RAN innovations and research challenges that remain to be solved.",2023-01-25T06:49:34Z,2023-01-25T06:49:34Z,http://arxiv.org/abs/2301.10429v1,http://arxiv.org/pdf/2301.10429v1,"cs.NI, cs.SY, eess.SP, eess.SY"
Comparison of two photonic sampling mixer architectures based on SOA-MZI   for all-optical frequency up-conversion,"Dimitrios Kastritsis, Thierry Rampone, Kyriakos Zoiros, Ammar Sharaiha","An experimental comparison of the conversion gain and harmonic distortion performance between Switching and Modulation architectures of an all-optical photonic sampler mixer up-converter using a Semiconductor Optical Amplifier-based Mach-Zehnder Interferometer (SOA-MZI) is presented. The process of frequency up-conversion from 1 GHz to 9 GHz is evaluated. Because of their different principle of operation, the Switching architecture demonstrates higher positive conversion gain by approximately 6 dB and 8 dB for standard and differential configuration, respectively, while the Modulation architecture achieves lower harmonic distortion up to 8 dB, depending on the modulation index of the 1 GHz signal.",2022-12-15T11:14:08Z,2022-12-15T11:14:08Z,http://arxiv.org/abs/2302.01286v1,http://arxiv.org/pdf/2302.01286v1,"eess.SP, physics.app-ph, physics.optics"
Numerical Methods For PDEs Over Manifolds Using Spectral Physics   Informed Neural Networks,"Yuval Zelig, Shai Dekel","We introduce an approach for solving PDEs over manifolds using physics informed neural networks whose architecture aligns with spectral methods. The networks are trained to take in as input samples of an initial condition, a time stamp and point(s) on the manifold and then output the solution's value at the given time and point(s). We provide proofs of our method for the heat equation on the interval and examples of unique network architectures that are adapted to nonlinear equations on the sphere and the torus. We also show that our spectral-inspired neural network architectures outperform the standard physics informed architectures. Our extensive experimental results include generalization studies where the testing dataset of initial conditions is randomly sampled from a significantly larger space than the training set.",2023-02-10T15:33:32Z,2023-09-03T12:54:36Z,http://arxiv.org/abs/2302.05322v3,http://arxiv.org/pdf/2302.05322v3,"cs.LG, cs.NA, math.NA"
Low Latency Video Denoising for Online Conferencing Using CNN   Architectures,"Altanai Bisht, Ana Carolina de Souza Mendes, Justin David Thoreson II, Shadrokh Samavi","In this paper, we propose a pipeline for real-time video denoising with low runtime cost and high perceptual quality. The vast majority of denoising studies focus on image denoising. However, a minority of research works focusing on video denoising do so with higher performance costs to obtain higher quality while maintaining temporal coherence. The approach we introduce in this paper leverages the advantages of both image and video-denoising architectures. Our pipeline first denoises the keyframes or one-fifth of the frames using HI-GAN blind image denoising architecture. Then, the remaining four-fifths of the noisy frames and the denoised keyframe data are fed into the FastDVDnet video denoising model. The final output is rendered in the user's display in real-time. The combination of these low-latency neural network architectures produces real-time denoising with high perceptual quality with applications in video conferencing and other real-time media streaming systems. A custom noise detector analyzer provides real-time feedback to adapt the weights and improve the models' output.",2023-02-17T00:55:54Z,2023-02-17T00:55:54Z,http://arxiv.org/abs/2302.08638v1,http://arxiv.org/pdf/2302.08638v1,"eess.IV, cs.CV"
On Neural Architectures for Deep Learning-based Source Separation of   Co-Channel OFDM Signals,"Gary C. F. Lee, Amir Weiss, Alejandro Lancho, Yury Polyanskiy, Gregory W. Wornell","We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can confer about 30 dB improvement in performance.",2023-03-11T16:29:13Z,2023-03-15T21:01:47Z,http://arxiv.org/abs/2303.06438v2,http://arxiv.org/pdf/2303.06438v2,"eess.SP, cs.LG"
Bayesian Quadrature for Neural Ensemble Search,"Saad Hamid, Xingchen Wan, Martin Jørgensen, Binxin Ru, Michael Osborne","Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -- tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -- that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.",2023-03-15T18:37:41Z,2023-03-17T16:59:46Z,http://arxiv.org/abs/2303.08874v2,http://arxiv.org/pdf/2303.08874v2,"stat.ML, cs.LG"
Thrust vector control and state estimation architecture for low-cost   small-scale launchers,"Pedro dos Santos, Paulo Oliveira","This paper proposes an integrated architecture for Thrust Vector Control (TVC) and state estimation for low-cost small-scale launchers, naturally unstable, and propelled by a solid motor. The architecture is based on a non-linear, six-degrees-of-freedom model for the generic thrust-vector-controlled launcher dynamics and kinematics, deduced and implemented in a realistic simulation environment. For estimation and control design purposes, a linearized version of the model is proposed. Single-nozzle TVC actuation is adopted, allowing for pitch and yaw control, with the control law being derived from the Linear Quadratic Regulator (LQR) with additional integral action (LQI). The control system is implemented through gain scheduling. Full state estimation is performed resorting to complementary kinematic filters, closely related to linear Kalman fitering theory. The architecture, composed by the navigation and control systems, is tested in simulation environment, demonstrating satisfactory attitude tracking performance and robustness to both external disturbances and model uncertainties.",2023-03-29T19:25:26Z,2023-04-04T09:49:50Z,http://arxiv.org/abs/2303.16983v2,http://arxiv.org/pdf/2303.16983v2,"eess.SY, cs.SY"
Ultra Sharp : Study of Single Image Super Resolution using Residual   Dense Network,Karthick Prasad Gunasekaran,"For years, Single Image Super Resolution (SISR) has been an interesting and ill-posed problem in computer vision. The traditional super-resolution (SR) imaging approaches involve interpolation, reconstruction, and learning-based methods. Interpolation methods are fast and uncomplicated to compute, but they are not so accurate and reliable. Reconstruction-based methods are better compared with interpolation methods, but they are time-consuming and the quality degrades as the scaling increases. Even though learning-based methods like Markov random chains are far better than all the previous ones, they are unable to match the performance of deep learning models for SISR. This study examines the Residual Dense Networks architecture proposed by Yhang et al. [17] and analyzes the importance of its components. By leveraging hierarchical features from original low-resolution (LR) images, this architecture achieves superior performance, with a network structure comprising four main blocks, including the residual dense block (RDB) as the core. Through investigations of each block and analyses using various loss metrics, the study evaluates the effectiveness of the architecture and compares it to other state-of-the-art models that differ in both architecture and components.",2023-04-21T10:32:24Z,2023-04-24T00:39:21Z,http://arxiv.org/abs/2304.10870v2,http://arxiv.org/pdf/2304.10870v2,"cs.CV, eess.IV"
Accelerating Genome Analysis via Algorithm-Architecture Co-Design,"Onur Mutlu, Can Firtina","High-throughput sequencing (HTS) technologies have revolutionized the field of genomics, enabling rapid and cost-effective genome analysis for various applications. However, the increasing volume of genomic data generated by HTS technologies presents significant challenges for computational techniques to effectively analyze genomes. To address these challenges, several algorithm-architecture co-design works have been proposed, targeting different steps of the genome analysis pipeline. These works explore emerging technologies to provide fast, accurate, and low-power genome analysis.   This paper provides a brief review of the recent advancements in accelerating genome analysis, covering the opportunities and challenges associated with the acceleration of the key steps of the genome analysis pipeline. Our analysis highlights the importance of integrating multiple steps of genome analysis using suitable architectures to unlock significant performance improvements and reduce data movement and energy consumption. We conclude by emphasizing the need for novel strategies and techniques to address the growing demands of genomic data generation and analysis.",2023-04-30T14:25:53Z,2023-05-31T13:48:33Z,http://arxiv.org/abs/2305.00492v4,http://arxiv.org/pdf/2305.00492v4,"cs.AR, q-bio.GN"
Unrolled Architectures for High-Throughput Encoding of Multi-Kernel   Polar Codes,"Hossein Rezaei, Elham Abbasi, Nandana Rajatheva, Matti Latva-aho","Over the past decade, polar codes have received significant traction and have been selected as the coding method for the control channel in fifth-generation (5G) wireless communication systems. However, conventional polar codes are reliant solely on binary (2x2) kernels, which restricts their block length to being only powers of 2. In response, multi-kernel (MK) polar codes have been proposed as a viable solution to attain greater code length flexibility. This paper proposes an unrolled architecture for encoding both systematic and non-systematic MK polar codes, capable of high-throughput encoding of codes constructed with binary, ternary (3x3), or binary-ternary mixed kernels. The proposed scheme exhibits an unprecedented level of flexibility by supporting 83 different codes and offering various architectures that provide trade-offs between throughput and resource consumption. The FPGA implementation results demonstrate that a partially-pipelined polar encoder of size N=4096 operating at a frequency of 270 MHz gives a throughput of 1080 Gbps. Additionally, a new compiler implemented in Python is given to automatically generate HDL modules for the desired encoders. By inserting the desired parameters, a designer can simply obtain all the necessary VHDL files for FPGA implementation.",2023-05-07T12:13:16Z,2023-05-07T12:13:16Z,http://arxiv.org/abs/2305.04257v1,http://arxiv.org/pdf/2305.04257v1,"cs.IT, cs.AR, math.IT"
Squeeze Excitation Embedded Attention UNet for Brain Tumor Segmentation,"Gaurav Prasanna, John Rohit Ernest, Lalitha G, Sathiya Narayanan","Deep Learning based techniques have gained significance over the past few years in the field of medicine. They are used in various applications such as classifying medical images, segmentation and identification. The existing architectures such as UNet, Attention UNet and Attention Residual UNet are already currently existing methods for the same application of brain tumor segmentation, but none of them address the issue of how to extract the features in channel level. In this paper, we propose a new architecture called Squeeze Excitation Embedded Attention UNet (SEEA-UNet), this architecture has both Attention UNet and Squeeze Excitation Network for better results and predictions, this is used mainly because to get information at both Spatial and channel levels. The proposed model was compared with the existing architectures based on the comparison it was found out that for lesser number of epochs trained, the proposed model performed better. Binary focal loss and Jaccard Coefficient were used to monitor the model's performance.",2023-05-13T06:46:07Z,2023-05-13T06:46:07Z,http://arxiv.org/abs/2305.07850v1,http://arxiv.org/pdf/2305.07850v1,"eess.IV, cs.CV"
U-DiT TTS: U-Diffusion Vision Transformer for Text-to-Speech,"Xin Jing, Yi Chang, Zijiang Yang, Jiangjian Xie, Andreas Triantafyllopoulos, Bjoern W. Schuller","Deep learning has led to considerable advances in text-to-speech synthesis. Most recently, the adoption of Score-based Generative Models (SGMs), also known as Diffusion Probabilistic Models (DPMs), has gained traction due to their ability to produce high-quality synthesized neural speech in neural speech synthesis systems. In SGMs, the U-Net architecture and its variants have long dominated as the backbone since its first successful adoption. In this research, we mainly focus on the neural network in diffusion-model-based Text-to-Speech (TTS) systems and propose the U-DiT architecture, exploring the potential of vision transformer architecture as the core component of the diffusion models in a TTS system. The modular design of the U-DiT architecture, inherited from the best parts of U-Net and ViT, allows for great scalability and versatility across different data scales. The proposed U-DiT TTS system is a mel spectrogram-based acoustic model and utilizes a pretrained HiFi-GAN as the vocoder. The objective (ie Frechet distance) and MOS results show that our DiT-TTS system achieves state-of-art performance on the single speaker dataset LJSpeech. Our demos are publicly available at: https://eihw.github.io/u-dit-tts/",2023-05-22T16:25:19Z,2023-05-22T16:25:19Z,http://arxiv.org/abs/2305.13195v1,http://arxiv.org/pdf/2305.13195v1,"cs.SD, eess.AS"
Benchmarking and modeling of analog and digital SRAM in-memory computing   architectures,"Pouya Houshmand, Jiacong Sun, Marian Verhelst","In-memory-computing is emerging as an efficient hardware paradigm for deep neural network accelerators at the edge, enabling to break the memory wall and exploit massive computational parallelism. Two design models have surged: analog in-memory-computing (AIMC) and digital in-memory-computing (DIMC), offering a different design space in terms of accuracy, efficiency and dataflow flexibility. This paper targets the fair comparison and benchmarking of both approaches to guide future designs, through a.) an overview of published architectures; b.) an analytical cost model for energy and throughput; c.) scheduling of workloads on a variety of modeled IMC architectures for end-to-end network efficiency analysis, offering valuable workload-hardware co-design insights.",2023-05-25T10:54:09Z,2023-05-25T10:54:09Z,http://arxiv.org/abs/2305.18335v1,http://arxiv.org/pdf/2305.18335v1,"cs.AR, eess.IV, eess.SP"
Low Latency Edge Classification GNN for Particle Trajectory Tracking on   FPGAs,"Shi-Yu Huang, Yun-Chen Yang, Yu-Ru Su, Bo-Cheng Lai, Javier Duarte, Scott Hauck, Shih-Chieh Hsu, Jin-Xuan Hu, Mark S. Neubauer","In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.",2023-06-20T06:57:24Z,2023-06-27T16:21:32Z,http://arxiv.org/abs/2306.11330v2,http://arxiv.org/pdf/2306.11330v2,"cs.AR, cs.LG, hep-ex"
Exploiting FPGA Capabilities for Accelerated Biomedical Computing,"Kayode Inadagbo, Baran Arig, Nisanur Alici, Murat Isik","This study presents advanced neural network architectures including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for enhanced ECG signal analysis using Field Programmable Gate Arrays (FPGAs). We utilize the MIT-BIH Arrhythmia Database for training and validation, introducing Gaussian noise to improve algorithm robustness. The implemented models feature various layers for distinct processing and classification tasks and techniques like EarlyStopping callback and Dropout layer are used to mitigate overfitting. Our work also explores the development of a custom Tensor Compute Unit (TCU) accelerator for the PYNQ Z1 board, offering comprehensive steps for FPGA-based machine learning, including setting up the Tensil toolchain in Docker, selecting architecture, configuring PS-PL, and compiling and executing models. Performance metrics such as latency and throughput are calculated for practical insights, demonstrating the potential of FPGAs in high-performance biomedical computing. The study ultimately offers a guide for optimizing neural network performance on FPGAs for various applications.",2023-07-16T01:20:17Z,2023-07-16T01:20:17Z,http://arxiv.org/abs/2307.07914v1,http://arxiv.org/pdf/2307.07914v1,"cs.AR, cs.LG, eess.SP"
Space-Air-Ground Integrated Network (SAGIN): A Survey,"Jiming Chen, Han Zhang, Zhe Xie","Since existing mobile communication networks may not be able to meet the low latency and high-efficiency requirements of emerging technologies and applications, novel network architectures need to be investigated to support these new requirements. As a new network architecture that integrates satellite systems, air networks and ground communication, Space-Air-Ground Integrated Network (SAGIN) has attracted extensive attention in recent years. This paper summarizes the recent research work on SAGIN from several aspects, with the basic information of SAGIN first introduced, followed by the physical characteristics. Then the drive and prospects of the current SAGIN architecture in supporting new requirements are deeply analyzed. On this basis, the requirements and challenges are analyzed. Finally, it summarizes the existing solutions and prospects the future research directions.",2023-07-27T08:37:52Z,2023-07-27T08:37:52Z,http://arxiv.org/abs/2307.14697v1,http://arxiv.org/pdf/2307.14697v1,"cs.NI, eess.SP"
Ensemble Learning with Residual Transformer for Brain Tumor Segmentation,"Lanhong Yao, Zheyuan Zhang, Ulas Bagci","Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize brain tumor segmentation.",2023-07-31T19:47:33Z,2023-07-31T19:47:33Z,http://arxiv.org/abs/2308.00128v1,http://arxiv.org/pdf/2308.00128v1,"eess.IV, cs.CV, cs.LG"
Complexity Scaling for Speech Denoising,"Hangting Chen, Jianwei Yu, Chao Weng","Computational complexity is critical when deploying deep learning-based speech denoising models for on-device applications. Most prior research focused on optimizing model architectures to meet specific computational cost constraints, often creating distinct neural network architectures for different complexity limitations. This study conducts complexity scaling for speech denoising tasks, aiming to consolidate models with various complexities into a unified architecture. We present a Multi-Path Transform-based (MPT) architecture to handle both low- and high-complexity scenarios. A series of MPT networks present high performance covering a wide range of computational complexities on the DNS challenge dataset. Moreover, inspired by the scaling experiments in natural language processing, we explore the empirical relationship between model performance and computational cost on the denoising task. As the complexity number of multiply-accumulate operations (MACs) is scaled from 50M/s to 15G/s on MPT networks, we observe a linear increase in the values of PESQ-WB and SI-SNR, proportional to the logarithm of MACs, which might contribute to the understanding and application of complexity scaling in speech denoising tasks.",2023-09-14T14:45:17Z,2023-09-14T14:45:17Z,http://arxiv.org/abs/2309.07757v1,http://arxiv.org/pdf/2309.07757v1,"eess.AS, cs.SD"
Diverse Neural Audio Embeddings -- Bringing Features back !,Prateek Verma,"With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training end-to-end models.",2023-09-15T20:27:47Z,2024-09-13T12:06:13Z,http://arxiv.org/abs/2309.08751v2,http://arxiv.org/pdf/2309.08751v2,"cs.SD, cs.AI, cs.LG, cs.MM, eess.AS"
Double-Layer Power Control for Mobile Cell-Free XL-MIMO with Multi-Agent   Reinforcement Learning,"Ziheng Liu, Jiayi Zhang, Zhilong Liu, Huahua Xiao, Bo Ai","Cell-free (CF) extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as a promising technology for enabling future wireless communication systems. Significant attention has been generated by its considerable advantages in augmenting degrees of freedom. In this paper, we first investigate a CF XL-MIMO system with base stations equipped with XL-MIMO panels under a dynamic environment. Then, we propose an innovative multi-agent reinforcement learning (MARL)-based power control algorithm that incorporates predictive management and distributed optimization architecture, which provides a dynamic strategy for addressing high-dimension signal processing problems. Specifically, we compare various MARL-based algorithms, which shows that the proposed MARL-based algorithm effectively strikes a balance between spectral efficiency (SE) performance and convergence time. Moreover, we consider a double-layer power control architecture based on the large-scale fading coefficients between antennas to suppress interference within dynamic systems. Compared to the single-layer architecture, the results obtained unveil that the proposed double-layer architecture has a nearly24% SE performance improvement, especially with massive antennas and smaller antenna spacing.",2023-09-29T09:19:01Z,2023-09-29T09:19:01Z,http://arxiv.org/abs/2309.17079v1,http://arxiv.org/pdf/2309.17079v1,"cs.IT, eess.SP, math.IT"
Extensions to the SENSEI In situ Framework for Heterogeneous   Architectures,"Burlen Loring, E. Wes Bethel, Gunther H. Weber, Michael W. Mahoney","The proliferation of GPUs and accelerators in recent supercomputing systems, so called heterogeneous architectures, has led to increased complexity in execution environments and programming models as well as to deeper memory hierarchies on these systems. In this work, we discuss challenges that arise in in situ code coupling on these heterogeneous architectures. In particular, we present data and execution model extensions to the SENSEI in situ framework that are targeted at the effective use of systems with heterogeneous architectures. We then use these new data and execution model extensions to investigate several in situ placement and execution configurations and to analyze the impact these choices have on overall performance.",2023-10-04T16:05:48Z,2023-10-04T16:05:48Z,http://arxiv.org/abs/2310.02926v1,http://arxiv.org/pdf/2310.02926v1,"cs.DC, I.6.6; E.1"
Streamlining Brain Tumor Classification with Custom Transfer Learning in   MRI Images,"Javed Hossain, Md. Touhidul Islam, Md. Taufiqul Haque Khan Tusar","Brain tumors are increasingly prevalent, characterized by the uncontrolled spread of aberrant tissues in the brain, with almost 700,000 new cases diagnosed globally each year. Magnetic Resonance Imaging (MRI) is commonly used for the diagnosis of brain tumors and accurate classification is a critical clinical procedure. In this study, we propose an efficient solution for classifying brain tumors from MRI images using custom transfer learning networks. While several researchers have employed various pre-trained architectures such as RESNET-50, ALEXNET, VGG-16, and VGG-19, these methods often suffer from high computational complexity. To address this issue, we present a custom and lightweight model using a Convolutional Neural Network-based pre-trained architecture with reduced complexity. Specifically, we employ the VGG-19 architecture with additional hidden layers, which reduces the complexity of the base architecture but improves computational efficiency. The objective is to achieve high classification accuracy using a novel approach. Finally, the result demonstrates a classification accuracy of 96.42%.",2023-10-19T19:13:04Z,2023-10-19T19:13:04Z,http://arxiv.org/abs/2310.13108v1,http://arxiv.org/pdf/2310.13108v1,"eess.IV, cs.CV, cs.LG"
Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Snow   Layers in Radar Echograms,"Debvrat Varshney, Masoud Yari, Oluwanisola Ibikunle, Jilu Li, John Paden, Aryya Gangopadhyay, Maryam Rahnemoonfar","Airborne radar sensors capture the profile of snow layers present on top of an ice sheet. Accurate tracking of these layers is essential to calculate their thicknesses, which are required to investigate the contribution of polar ice cap melt to sea-level rise. However, automatically processing the radar echograms to detect the underlying snow layers is a challenging problem. In our work, we develop wavelet-based multi-scale deep learning architectures for these radar echograms to improve snow layer detection. These architectures estimate the layer depths with a mean absolute error of 3.31 pixels and 94.3% average precision, achieving higher generalizability as compared to state-of-the-art snow layer detection networks. These depth estimates also agree well with physically drilled stake measurements. Such robust architectures can be used on echograms from future missions to efficiently trace snow layers, estimate their individual thicknesses and thus support sea-level rise projection models.",2023-10-30T14:30:27Z,2025-01-24T03:23:47Z,http://arxiv.org/abs/2310.19574v2,http://arxiv.org/pdf/2310.19574v2,"cs.CV, eess.IV"
Approximating Langevin Monte Carlo with ResNet-like Neural Network   architectures,"Charles Miranda, Janina Schütte, David Sommer, Martin Eigel","We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.",2023-11-06T16:31:09Z,2024-12-10T10:55:44Z,http://arxiv.org/abs/2311.03242v3,http://arxiv.org/pdf/2311.03242v3,"cs.LG, math.PR, math.ST, stat.ML, stat.TH"
The Impact of Changes to Daylight Illumination level on Architectural   experience in Offices Based on VR and EEG,"Pegah Payedar-Ardakani, Yousef Gorji-Mahlabani, Abdolhamid Ghanbaran, Reza Ebrahimpour","This study investigates the influence of varying illumination levels on architectural experiences by employing a comprehensive approach that combines self-reported assessments and neurophysiological measurements. Thirty participants were exposed to nine distinct illumination conditions in a controlled virtual reality environment. Subjective assessments, collected through questionnaires in which participants were asked to rate how pleasant, interesting, exciting, calming, complex, bright and spacious they found the space. Objective measurements of brain activity were collected by electroencephalogram (EEG). Data analysis demonstrated that illumination levels significantly influenced cognitive engagement and different architectural experience indicators. This alignment between subjective assessment and EEG data underscores the relationship between illuminance and architectural experiences. The study bridges the gap between quantitative and qualitative assessments, providing a deeper understanding of the intricate connection between lighting conditions and human responses. These findings contribute to the enhancement of environmental design based on neuroscientific insights, emphasizing the critical role of well-considered daylighting design in positively influencing occupants' cognitive and emotional states within built environments.",2023-11-08T21:23:37Z,2024-01-27T07:48:36Z,http://arxiv.org/abs/2311.05028v2,http://arxiv.org/pdf/2311.05028v2,"cs.HC, q-bio.NC"
Trust your BMS: Designing a Lightweight Authentication Architecture for   Industrial Networks,"Fikret Basic, Christian Steger, Christian Seifert, Robert Kofler","With the advent of clean energy awareness and systems that rely on extensive battery usage, the community has seen an increased interest in the development of more complex and secure Battery Management Systems (BMS). In particular, the inclusion of BMS in modern complex systems like electric vehicles and power grids has presented a new set of security-related challenges. A concern is shown when BMS are intended to extend their communication with external system networks, as their interaction can leave many backdoors open that potential attackers could exploit. Hence, it is highly desirable to find a general design that can be used for BMS and its system inclusion. In this work, a security architecture solution is proposed intended for the communication between BMS and other system devices. The aim of the proposed architecture is to be easily applicable in different industrial settings and systems, while at the same time keeping the design lightweight in nature.",2023-11-09T16:37:34Z,2023-11-09T16:37:34Z,http://arxiv.org/abs/2311.05498v1,http://arxiv.org/pdf/2311.05498v1,"cs.CR, cs.NI, cs.SE, cs.SY, eess.SY"
OISA: Architecting an Optical In-Sensor Accelerator for Efficient Visual   Computing,"Mehrdad Morsali, Sepehr Tabrizchi, Deniz Najafi, Mohsen Imani, Mahdi Nikdast, Arman Roohi, Shaahin Angizi","Targeting vision applications at the edge, in this work, we systematically explore and propose a high-performance and energy-efficient Optical In-Sensor Accelerator architecture called OISA for the first time. Taking advantage of the promising efficiency of photonic devices, the OISA intrinsically implements a coarse-grained convolution operation on the input frames in an innovative minimum-conversion fashion in low-bit-width neural networks. Such a design remarkably reduces the power consumption of data conversion, transmission, and processing in the conventional cloud-centric architecture as well as recently-presented edge accelerators. Our device-to-architecture simulation results on various image data-sets demonstrate acceptable accuracy while OISA achieves 6.68 TOp/s/W efficiency. OISA reduces power consumption by a factor of 7.9 and 18.4 on average compared with existing electronic in-/near-sensor and ASIC accelerators.",2023-11-30T16:04:39Z,2023-11-30T16:04:39Z,http://arxiv.org/abs/2311.18655v1,http://arxiv.org/pdf/2311.18655v1,"cs.AR, eess.SP"
On the Trade-Off between Stability and Representational Capacity in   Graph Neural Networks,"Zhan Gao, Amanda Prorok, Elvin Isufi","Analyzing the stability of graph neural networks (GNNs) under topological perturbations is key to understanding their transferability and the role of each architecture component. However, stability has been investigated only for particular architectures, questioning whether it holds for a broader spectrum of GNNs or only for a few instances. To answer this question, we study the stability of EdgeNet: a general GNN framework that unifies more than twenty solutions including the convolutional and attention-based classes, as well as graph isomorphism networks and hybrid architectures. We prove that all GNNs within the EdgeNet framework are stable to topological perturbations. By studying the effect of different EdgeNet categories on the stability, we show that GNNs with fewer degrees of freedom in their parameter space, linked to a lower representational capacity, are more stable. The key factor yielding this trade-off is the eigenvector misalignment between the EdgeNet parameter matrices and the graph shift operator. For example, graph convolutional neural networks that assign a single scalar per signal shift (hence, with a perfect alignment) are more stable than the more involved node or edge-varying counterparts. Extensive numerical results corroborate our theoretical findings and highlight the role of different architecture components in the trade-off.",2023-12-04T22:07:17Z,2023-12-04T22:07:17Z,http://arxiv.org/abs/2312.02372v1,http://arxiv.org/pdf/2312.02372v1,"eess.SP, cs.LG"
EGAIN: Extended GAn INversion,"Wassim Kabbani, Marcel Grimmer, Christoph Busch","Generative Adversarial Networks (GANs) have witnessed significant advances in recent years, generating increasingly higher quality images, which are non-distinguishable from real ones. Recent GANs have proven to encode features in a disentangled latent space, enabling precise control over various semantic attributes of the generated facial images such as pose, illumination, or gender. GAN inversion, which is projecting images into the latent space of a GAN, opens the door for the manipulation of facial semantics of real face images. This is useful for numerous applications such as evaluating the performance of face recognition systems. In this work, EGAIN, an architecture for constructing GAN inversion models, is presented. This architecture explicitly addresses some of the shortcomings in previous GAN inversion models. A specific model with the same name, egain, based on this architecture is also proposed, demonstrating superior reconstruction quality over state-of-the-art models, and illustrating the validity of the EGAIN architecture.",2023-12-22T23:25:17Z,2023-12-22T23:25:17Z,http://arxiv.org/abs/2312.15116v1,http://arxiv.org/pdf/2312.15116v1,"cs.CV, cs.AI, I.4.m; I.4.5"
Faster ISNet for Background Bias Mitigation on Deep Neural Networks,"Pedro R. A. S. Bassi, Sergio Decherchi, Andrea Cavalli","Bias or spurious correlations in image backgrounds can impact neural networks, causing shortcut learning (Clever Hans Effect) and hampering generalization to real-world data. ISNet, a recently introduced architecture, proposed the optimization of Layer-Wise Relevance Propagation (LRP, an explanation technique) heatmaps, to mitigate the influence of backgrounds on deep classifiers. However, ISNet's training time scales linearly with the number of classes in an application. Here, we propose reformulated architectures whose training time becomes independent from this number. Additionally, we introduce a concise and model-agnostic LRP implementation. We challenge the proposed architectures using synthetic background bias, and COVID-19 detection in chest X-rays, an application that commonly presents background bias. The networks hindered background attention and shortcut learning, surpassing multiple state-of-the-art models on out-of-distribution test datasets. Representing a potentially massive training speed improvement over ISNet, the proposed architectures introduce LRP optimization into a gamut of applications that the original model cannot feasibly handle.",2024-01-16T14:49:26Z,2024-03-31T19:01:07Z,http://arxiv.org/abs/2401.08409v2,http://arxiv.org/pdf/2401.08409v2,"eess.IV, cs.CV, cs.CY, cs.LG"
Functional SDE approximation inspired by a deep operator network   architecture,"Martin Eigel, Charles Miranda","A novel approach to approximate solutions of Stochastic Differential Equations (SDEs) by Deep Neural Networks is derived and analysed. The architecture is inspired by the notion of Deep Operator Networks (DeepONets), which is based on operator learning in function spaces in terms of a reduced basis also represented in the network. In our setting, we make use of a polynomial chaos expansion (PCE) of stochastic processes and call the corresponding architecture SDEONet. The PCE has been used extensively in the area of uncertainty quantification (UQ) with parametric partial differential equations. This however is not the case with SDE, where classical sampling methods dominate and functional approaches are seen rarely. A main challenge with truncated PCEs occurs due to the drastic growth of the number of components with respect to the maximum polynomial degree and the number of basis elements. The proposed SDEONet architecture aims to alleviate the issue of exponential complexity by learning an optimal sparse truncation of the Wiener chaos expansion. A complete convergence and complexity analysis is presented, making use of recent Neural Network approximation results. Numerical experiments illustrate the promising performance of the suggested approach in 1D and higher dimensions.",2024-02-05T14:12:35Z,2024-02-05T14:12:35Z,http://arxiv.org/abs/2402.03028v1,http://arxiv.org/pdf/2402.03028v1,"math.NA, cs.LG, cs.NA, 65C30, 60H10, 91G60, 60H35, 68T07"
Decoder-Only Image Registration,"Xi Jia, Wenqi Lu, Xinxing Cheng, Jinming Duan","In unsupervised medical image registration, the predominant approaches involve the utilization of a encoder-decoder network architecture, allowing for precise prediction of dense, full-resolution displacement fields from given paired images. Despite its widespread use in the literature, we argue for the necessity of making both the encoder and decoder learnable in such an architecture. For this, we propose a novel network architecture, termed LessNet in this paper, which contains only a learnable decoder, while entirely omitting the utilization of a learnable encoder. LessNet substitutes the learnable encoder with simple, handcrafted features, eliminating the need to learn (optimize) network parameters in the encoder altogether. Consequently, this leads to a compact, efficient, and decoder-only architecture for 3D medical image registration. Evaluated on two publicly available brain MRI datasets, we demonstrate that our decoder-only LessNet can effectively and efficiently learn both dense displacement and diffeomorphic deformation fields in 3D. Furthermore, our decoder-only LessNet can achieve comparable registration performance to state-of-the-art methods such as VoxelMorph and TransMorph, while requiring significantly fewer computational resources. Our code and pre-trained models are available at https://github.com/xi-jia/LessNet.",2024-02-05T23:30:37Z,2024-02-05T23:30:37Z,http://arxiv.org/abs/2402.03585v1,http://arxiv.org/pdf/2402.03585v1,"cs.CV, eess.IV"
SpeechCLIP+: Self-supervised multi-task representation learning for   speech via CLIP and speech-image data,"Hsuan-Fu Wang, Yi-Jen Shih, Heng-Jui Chang, Layne Berry, Puyuan Peng, Hung-yi Lee, Hsin-Min Wang, David Harwath","The recently proposed visually grounded speech model SpeechCLIP is an innovative framework that bridges speech and text through images via CLIP without relying on text transcription. On this basis, this paper introduces two extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire (CIF) module to replace a fixed number of CLS tokens in the cascaded architecture. Second, we propose a new hybrid architecture that merges the cascaded and parallel architectures of SpeechCLIP into a multi-task learning framework. Our experimental evaluation is performed on the Flickr8k and SpokenCOCO datasets. The results show that in the speech keyword extraction task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our hybrid architecture, cascaded task learning boosts the performance of the parallel branch in image-speech retrieval tasks.",2024-02-10T14:26:42Z,2024-02-10T14:26:42Z,http://arxiv.org/abs/2402.06959v1,http://arxiv.org/pdf/2402.06959v1,"cs.CL, cs.SD, eess.AS"
Weisfeiler-Leman at the margin: When more expressivity matters,"Billy J. Franks, Christopher Morris, Ameya Velingker, Floris Geerts","The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the MPNN's weights toward the maximum margin solution. Further, we introduce variations of expressive $1$-WL-based kernel and MPNN architectures with provable generalization properties. Our empirical study confirms the validity of our theoretical findings.",2024-02-12T11:03:52Z,2024-05-28T15:52:02Z,http://arxiv.org/abs/2402.07568v2,http://arxiv.org/pdf/2402.07568v2,"cs.LG, cs.DM, cs.NE, stat.ML"
A Novel Approach to WaveNet Architecture for RF Signal Separation with   Learnable Dilation and Data Augmentation,"Yu Tian, Ahmed Alhammadi, Abdullah Quran, Abubakar Sani Ali","In this paper, we address the intricate issue of RF signal separation by presenting a novel adaptation of the WaveNet architecture that introduces learnable dilation parameters, significantly enhancing signal separation in dense RF spectrums. Our focused architectural refinements and innovative data augmentation strategies have markedly improved the model's ability to discern complex signal sources. This paper details our comprehensive methodology, including the refined model architecture, data preparation techniques, and the strategic training strategy that have been pivotal to our success. The efficacy of our approach is evidenced by the substantial improvements recorded: a 58.82\% increase in SINR at a BER of $10^{-3}$ for OFDM-QPSK with EMI Signal 1, surpassing traditional benchmarks. Notably, our model achieved first place in the challenge \cite{datadrivenrf2024}, demonstrating its superior performance and establishing a new standard for machine learning applications within the RF communications domain.",2024-02-08T06:36:29Z,2024-02-08T06:36:29Z,http://arxiv.org/abs/2402.09461v1,http://arxiv.org/pdf/2402.09461v1,"eess.SP, cs.LG"
Opening the black box of language acquisition,"Jérôme Michaud, Anna Jon-and","Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory as a key component of the language learning process. Since other animals do not seem to have a faithful sequence memory, this may explain why only humans have developed complex languages.",2024-02-18T19:11:58Z,2024-02-18T19:11:58Z,http://arxiv.org/abs/2402.11681v1,http://arxiv.org/pdf/2402.11681v1,"cs.CL, cs.NA, math.NA"
Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based   Computing,Piotr Dudek,"This paper proposes the design and implementation strategy of a novel computing architecture, the Factor Machine. The work is a step towards a general-purpose parallel system operating in a non-sequential manner, exploiting processing/memory co-integration and replacing the traditional Turing/von Neumann model of a computer system with a framework based on ""factorised computation"". This architecture is inspired by neural information processing principles and aims to progress the development of brain-like machine intelligence systems, through providing a computing substrate designed from the ground up to enable efficient implementations of algorithms based on relational networks. The paper provides a rationale for such machine, in the context of the history of computing, and more recent developments in neuromorphic hardware, reviews its general features, and proposes a mixed-signal hardware implementation, based on using analogue circuits to carry out computation and localised and sparse communication between the compute units.",2024-02-19T13:26:42Z,2024-02-20T03:53:35Z,http://arxiv.org/abs/2402.12130v2,http://arxiv.org/pdf/2402.12130v2,"cs.AR, C.1.4"
Sparse Autoregressive Neural Networks for Classical Spin Systems,"Indaco Biazzo, Dian Wu, Giuseppe Carleo","Efficient sampling and approximation of Boltzmann distributions involving large sets of binary variables, or spins, are pivotal in diverse scientific fields even beyond physics. Recent advances in generative neural networks have significantly impacted this domain. However, these neural networks are often treated as black boxes, with architectures primarily influenced by data-driven problems in computational science. Addressing this gap, we introduce a novel autoregressive neural network architecture named TwoBo, specifically designed for sparse two-body interacting spin systems. We directly incorporate the Boltzmann distribution into its architecture and parameters, resulting in enhanced convergence speed, superior free energy accuracy, and reduced trainable parameters. We perform numerical experiments on disordered, frustrated systems with more than 1000 spins on grids and random graphs, and demonstrate its advantages compared to previous autoregressive and recurrent architectures. Our findings validate a physically informed approach and suggest potential extensions to multivalued variables and many-body interaction systems, paving the way for broader applications in scientific research.",2024-02-26T14:02:14Z,2024-06-21T17:54:15Z,http://arxiv.org/abs/2402.16579v2,http://arxiv.org/pdf/2402.16579v2,"cond-mat.stat-mech, cond-mat.dis-nn, math.CO, physics.comp-ph"
a-DCF: an architecture agnostic metric with application to   spoofing-robust speaker verification,"Hye-jin Shim, Jee-weon Jung, Tomi Kinnunen, Nicholas Evans, Jean-Francois Bonastre, Itshak Lapidot","Spoofing detection is today a mainstream research topic. Standard metrics can be applied to evaluate the performance of isolated spoofing detection solutions and others have been proposed to support their evaluation when they are combined with speaker detection. These either have well-known deficiencies or restrict the architectural approach to combine speaker and spoof detectors. In this paper, we propose an architecture-agnostic detection cost function (a-DCF). A generalisation of the original DCF used widely for the assessment of automatic speaker verification (ASV), the a-DCF is designed for the evaluation of spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisions in a Bayes risk sense, with explicitly defined class priors and detection cost model. We demonstrate the merit of the a-DCF through the benchmarking evaluation of architecturally-heterogeneous spoofing-robust ASV solutions.",2024-03-03T00:58:27Z,2024-03-03T00:58:27Z,http://arxiv.org/abs/2403.01355v1,http://arxiv.org/pdf/2403.01355v1,"eess.AS, cs.LG"
Assessing the Performance of Deep Learning for Automated Gleason Grading   in Prostate Cancer,"Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler","Prostate cancer is a dominant health concern calling for advanced diagnostic tools. Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures. A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles. The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance. Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades. The ConvNeXt model was capable of learning a balance between complexity and generalizability. Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially improving diagnostic efficiency for prostate cancer.",2024-03-25T12:26:32Z,2024-03-25T12:26:32Z,http://arxiv.org/abs/2403.16695v1,http://arxiv.org/pdf/2403.16695v1,"eess.IV, cs.CV, cs.LG, q-bio.TO"
The Topos of Transformer Networks,"Mattia Jacopo Villani, Peter McBurney","The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.",2024-03-27T10:06:33Z,2024-05-05T21:07:34Z,http://arxiv.org/abs/2403.18415v3,http://arxiv.org/pdf/2403.18415v3,"cs.LG, math.CT"
Symbiotic Control of Uncertain Dynamical Systems: Harnessing Synergy   Between Fixed-Gain Control and Adaptive Learning Architectures,"Tansel Yucelen, Selahattin Burak Sarsilmaz, Emre Yildirim","Both fixed-gain control and adaptive learning architectures aim to mitigate the effects of uncertainties. In particular, fixed-gain control offers more predictable closed-loop system behavior but requires the knowledge of uncertainty bounds. In contrast, while adaptive learning does not necessarily require such knowledge, it often results in less predictable closed-loop system behavior compared to fixed-gain control. To this end, this paper presents a novel symbiotic control framework that offers the strengths of fixed-gain control and adaptive learning architectures. Specifically, this framework synergistically integrates these architectures to mitigate the effects of uncertainties in a more predictable manner as compared to adaptive learning alone and it does not require any knowledge on such uncertainties. Both parametric and nonparametric uncertainties are considered, where we utilize neural networks to approximate the unknown uncertainty basis for the latter case. Counterintuitively, the proposed framework has the ability to achieve a desired level of closed-loop system behavior even with an insufficient number of neurons (e.g., when the neural network approximation error is large) or in the face of injudiciously selected adaptive learning parameters (e.g., high leakage term parameters).",2024-03-28T04:20:22Z,2024-03-28T04:20:22Z,http://arxiv.org/abs/2403.19139v1,http://arxiv.org/pdf/2403.19139v1,"eess.SY, cs.SY"
Gradient Networks,"Shreyas Chaudhari, Srinivasa Pranav, José M. F. Moura","Directly parameterizing and learning gradients of functions has widespread significance, with specific applications in inverse problems, generative modeling, and optimal transport. This paper introduces gradient networks (GradNets): novel neural network architectures that parameterize gradients of various function classes. GradNets exhibit specialized architectural constraints that ensure correspondence to gradient functions. We provide a comprehensive GradNet design framework that includes methods for transforming GradNets into monotone gradient networks (mGradNets), which are guaranteed to represent gradients of convex functions. Our results establish that our proposed GradNet (and mGradNet) universally approximate the gradients of (convex) functions. Furthermore, these networks can be customized to correspond to specific spaces of potential functions, including transformed sums of (convex) ridge functions. Our analysis leads to two distinct GradNet architectures, GradNet-C and GradNet-M, and we describe the corresponding monotone versions, mGradNet-C and mGradNet-M. Our empirical results demonstrate that these architectures provide efficient parameterizations and outperform existing methods by up to 15 dB in gradient field tasks and by up to 11 dB in Hamiltonian dynamics learning tasks.",2024-04-10T21:36:59Z,2025-01-25T02:05:28Z,http://arxiv.org/abs/2404.07361v3,http://arxiv.org/pdf/2404.07361v3,"cs.LG, cs.NE, eess.SP, math.OC"
Deep Dependency Networks and Advanced Inference Schemes for Multi-Label   Classification,"Shivvrat Arya, Yu Xiang, Vibhav Gogate","We present a unified framework called deep dependency networks (DDNs) that combines dependency networks and deep learning architectures for multi-label classification, with a particular emphasis on image and video data. The primary advantage of dependency networks is their ease of training, in contrast to other probabilistic graphical models like Markov networks. In particular, when combined with deep learning architectures, they provide an intuitive, easy-to-use loss function for multi-label classification. A drawback of DDNs compared to Markov networks is their lack of advanced inference schemes, necessitating the use of Gibbs sampling. To address this challenge, we propose novel inference schemes based on local search and integer linear programming for computing the most likely assignment to the labels given observations. We evaluate our novel methods on three video datasets (Charades, TACoS, Wetlab) and three image datasets (MS-COCO, PASCAL VOC, NUS-WIDE), comparing their performance with (a) basic neural architectures and (b) neural architectures combined with Markov networks equipped with advanced inference and learning techniques. Our results demonstrate the superiority of our new DDN methods over the two competing approaches.",2024-04-17T18:04:37Z,2024-04-17T18:04:37Z,http://arxiv.org/abs/2404.11667v1,http://arxiv.org/pdf/2404.11667v1,"cs.LG, cs.AI, cs.CV, stat.ML"
"U-Nets as Belief Propagation: Efficient Classification, Denoising, and   Diffusion in Generative Hierarchical Models",Song Mei,"U-Nets are among the most widely used architectures in computer vision, renowned for their exceptional performance in applications such as image segmentation, denoising, and diffusion modeling. However, a theoretical explanation of the U-Net architecture design has not yet been fully established.   This paper introduces a novel interpretation of the U-Net architecture by studying certain generative hierarchical models, which are tree-structured graphical models extensively utilized in both language and image domains. With their encoder-decoder structure, long skip connections, and pooling and up-sampling layers, we demonstrate how U-Nets can naturally implement the belief propagation denoising algorithm in such generative hierarchical models, thereby efficiently approximating the denoising functions. This leads to an efficient sample complexity bound for learning the denoising function using U-Nets within these models. Additionally, we discuss the broader implications of these findings for diffusion models in generative hierarchical models. We also demonstrate that the conventional architecture of convolutional neural networks (ConvNets) is ideally suited for classification tasks within these models. This offers a unified view of the roles of ConvNets and U-Nets, highlighting the versatility of generative hierarchical models in modeling complex data distributions across language and image domains.",2024-04-29T05:57:03Z,2024-05-01T16:49:57Z,http://arxiv.org/abs/2404.18444v2,http://arxiv.org/pdf/2404.18444v2,"cs.LG, cs.AI, math.ST, stat.ML, stat.TH"
Lipschitz constant estimation for general neural network architectures   using control tools,"Patricia Pauli, Dennis Gramlich, Frank Allgöwer","This paper is devoted to the estimation of the Lipschitz constant of general neural network architectures using semidefinite programming. For this purpose, we interpret neural networks as time-varying dynamical systems, where the $k$-th layer corresponds to the dynamics at time $k$. A key novelty with respect to prior work is that we use this interpretation to exploit the series interconnection structure of feedforward neural networks with a dynamic programming recursion. Nonlinearities, such as activation functions and nonlinear pooling layers, are handled with integral quadratic constraints. If the neural network contains signal processing layers (convolutional or state space model layers), we realize them as 1-D/2-D/N-D systems and exploit this structure as well. We distinguish ourselves from related work on Lipschitz constant estimation by more extensive structure exploitation (scalability) and a generalization to a large class of common neural network architectures. To show the versatility and computational advantages of our method, we apply it to different neural network architectures trained on MNIST and CIFAR-10.",2024-05-02T09:38:16Z,2024-11-25T14:35:07Z,http://arxiv.org/abs/2405.01125v2,http://arxiv.org/pdf/2405.01125v2,"cs.LG, cs.SY, eess.IV, eess.SY"
PETRA: Parallel End-to-end Training with Reversible Architectures,"Stéphane Rivaud, Louis Fournier, Thomas Pumir, Eugene Belilovsky, Michael Eickenberg, Edouard Oyallon","Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model training. We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations. PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other. By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed. We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on CIFAR-10, ImageNet32, and ImageNet, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models.",2024-06-04T07:35:23Z,2024-06-04T07:35:23Z,http://arxiv.org/abs/2406.02052v1,http://arxiv.org/pdf/2406.02052v1,"cs.LG, stat.ML"
Beyond Performance Plateaus: A Comprehensive Study on Scalability in   Speech Enhancement,"Wangyou Zhang, Kohei Saijo, Jee-weon Jung, Chenda Li, Shinji Watanabe, Yanmin Qian","Deep learning-based speech enhancement (SE) models have achieved impressive performance in the past decade. Numerous advanced architectures have been designed to deliver state-of-the-art performance; however, their scalability potential remains unrevealed. Meanwhile, the majority of research focuses on small-sized datasets with restricted diversity, leading to a plateau in performance improvement. In this paper, we aim to provide new insights for addressing the above issues by exploring the scalability of SE models in terms of architectures, model sizes, compute budgets, and dataset sizes. Our investigation involves several popular SE architectures and speech data from different domains. Experiments reveal both similarities and distinctions between the scaling effects in SE and other tasks such as speech recognition. These findings further provide insights into the under-explored SE directions, e.g., larger-scale multi-domain corpora and efficiently scalable architectures.",2024-06-06T17:20:21Z,2024-06-06T17:20:21Z,http://arxiv.org/abs/2406.04269v1,http://arxiv.org/pdf/2406.04269v1,"eess.AS, cs.SD"
Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal   LLMs by Teaching the Flow of Time,"Frank Seide, Morrie Doulaty, Yangyang Shi, Yashesh Gaur, Junteng Jia, Chunyang Wu","We introduce Speech ReaLLM, a new ASR architecture that marries ""decoder-only"" ASR with the RNN-T to make multimodal LLM architectures capable of real-time streaming. This is the first ""decoder-only"" ASR architecture designed to handle continuous audio without explicit end-pointing. Speech ReaLLM is a special case of the more general ReaLLM (""real-time LLM"") approach, also introduced here for the first time. The idea is inspired by RNN-T: Instead of generating a response only at the end of a user prompt, generate after every input token received in real time (it is often empty). On Librispeech ""test"", an 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an external LM or auxiliary loss). This is only slightly above a 3x larger Attention-Encoder-Decoder baseline. We also show that this way, an LLM architecture can learn to represent and reproduce the flow of time; and that a pre-trained 7B LLM can be fine-tuned to do reasonably well on this task.",2024-06-13T20:20:29Z,2024-06-13T20:20:29Z,http://arxiv.org/abs/2406.09569v1,http://arxiv.org/pdf/2406.09569v1,"cs.CL, cs.AI, cs.SD, eess.AS"
Exploiting and Securing ML Solutions in Near-RT RIC: A Perspective of an   xApp,"Thusitha Dayaratne, Viet Vo, Shangqi Lai, Sharif Abuadbba, Blake Haydon, Hajime Suzuki, Xingliang Yuan, Carsten Rudolph","Open Radio Access Networks (O-RAN) are emerging as a disruptive technology, revolutionising traditional mobile network architecture and deployments in the current 5G and the upcoming 6G era. Disaggregation of network architecture, inherent support for AI/ML workflows, cloud-native principles, scalability, and interoperability make O-RAN attractive to network providers for beyond-5G and 6G deployments. Notably, the ability to deploy custom applications, including Machine Learning (ML) solutions as xApps or rApps on the RAN Intelligent Controllers (RICs), has immense potential for network function and resource optimisation. However, the openness, nascent standards, and distributed architecture of O-RAN and RICs introduce numerous vulnerabilities exploitable through multiple attack vectors, which have not yet been fully explored. To address this gap and ensure robust systems before large-scale deployments, this work analyses the security of ML-based applications deployed on the RIC platform. We focus on potential attacks, defence mechanisms, and pave the way for future research towards a more robust RIC platform.",2024-06-18T06:12:57Z,2024-06-18T06:12:57Z,http://arxiv.org/abs/2406.12299v1,http://arxiv.org/pdf/2406.12299v1,"cs.CR, cs.NI, cs.SY, eess.SY"
Straight Through Gumbel Softmax Estimator based Bimodal Neural   Architecture Search for Audio-Visual Deepfake Detection,"Aravinda Reddy PN, Raghavendra Ramachandra, Krothapalli Sreenivasa Rao, Pabitra Mitra, Vinod Rathod","Deepfakes are a major security risk for biometric authentication. This technology creates realistic fake videos that can impersonate real people, fooling systems that rely on facial features and voice patterns for identification. Existing multimodal deepfake detectors rely on conventional fusion methods, such as majority rule and ensemble voting, which often struggle to adapt to changing data characteristics and complex patterns. In this paper, we introduce the Straight-through Gumbel-Softmax (STGS) framework, offering a comprehensive approach to search multimodal fusion model architectures. Using a two-level search approach, the framework optimizes the network architecture, parameters, and performance. Initially, crucial features were efficiently identified from backbone networks, whereas within the cell structure, a weighted fusion operation integrated information from various sources. An architecture that maximizes the classification performance is derived by varying parameters such as temperature and sampling time. The experimental results on the FakeAVCeleb and SWAN-DF datasets demonstrated an impressive AUC value 94.4\% achieved with minimal model parameters.",2024-06-19T09:26:22Z,2024-06-19T09:26:22Z,http://arxiv.org/abs/2406.13384v1,http://arxiv.org/pdf/2406.13384v1,"cs.SD, cs.CV, cs.MM, eess.AS"
Joint vs Sequential Speaker-Role Detection and Automatic Speech   Recognition for Air-traffic Control,"Alexander Blatt, Aravind Krishnan, Dietrich Klakow","Utilizing air-traffic control (ATC) data for downstream natural-language processing tasks requires preprocessing steps. Key steps are the transcription of the data via automatic speech recognition (ASR) and speaker diarization, respectively speaker role detection (SRD) to divide the transcripts into pilot and air-traffic controller (ATCO) transcripts. While traditional approaches take on these tasks separately, we propose a transformer-based joint ASR-SRD system that solves both tasks jointly while relying on a standard ASR architecture. We compare this joint system against two cascaded approaches for ASR and SRD on multiple ATC datasets. Our study shows in which cases our joint system can outperform the two traditional approaches and in which cases the other architectures are preferable. We additionally evaluate how acoustic and lexical differences influence all architectures and show how to overcome them for our joint architecture.",2024-06-19T21:11:01Z,2024-06-19T21:11:01Z,http://arxiv.org/abs/2406.13842v1,http://arxiv.org/pdf/2406.13842v1,"cs.CL, cs.SD, eess.AS"
Safety-Critical Edge Robotics Architecture with Bounded End-to-End   Latency,"Gautam Gala, Tilmann Unte, Luiz Maia, Johannes Kühbacher, Isser Kadusale, Mohammad Ibrahim Alkoudsi, Gerhard Fohler, Sebastian Altmeyer","Edge computing processes data near its source, reducing latency and enhancing security compared to traditional cloud computing while providing its benefits. This paper explores edge computing for migrating an existing safety-critical robotics use case from an onboard dedicated hardware solution. We propose an edge robotics architecture based on Linux, Docker containers, Kubernetes, and a local wireless area network based on the TTWiFi protocol. Inspired by previous work on real-time cloud, we complement the architecture with a resource management and orchestration layer to help Linux manage, and Kubernetes orchestrate the system-wide shared resources (e.g., caches, memory bandwidth, and network). Our architecture aims to ensure the fault-tolerant and predictable execution of robotic applications (e.g., path planning) on the edge while upper-bounding the end-to-end latency and ensuring the best possible quality of service without jeopardizing safety and security.",2024-06-20T15:11:22Z,2024-06-21T07:35:02Z,http://arxiv.org/abs/2406.14391v2,http://arxiv.org/pdf/2406.14391v2,"cs.RO, cs.DC, cs.ET, D.2.11; C.4; J.7"
GMM-ResNext: Combining Generative and Discriminative Models for Speaker   Verification,"Hui Yan, Zhenchun Lei, Changhong Liu, Yong Zhou","With the development of deep learning, many different network architectures have been explored in speaker verification. However, most network architectures rely on a single deep learning architecture, and hybrid networks combining different architectures have been little studied in ASV tasks. In this paper, we propose the GMM-ResNext model for speaker verification. Conventional GMM does not consider the score distribution of each frame feature over all Gaussian components and ignores the relationship between neighboring speech frames. So, we extract the log Gaussian probability features based on the raw acoustic features and use ResNext-based network as the backbone to extract the speaker embedding. GMM-ResNext combines Generative and Discriminative Models to improve the generalization ability of deep learning models and allows one to more easily specify meaningful priors on model parameters. A two-path GMM-ResNext model based on two gender-related GMMs has also been proposed. The Experimental results show that the proposed GMM-ResNext achieves relative improvements of 48.1\% and 11.3\% in EER compared with ResNet34 and ECAPA-TDNN on VoxCeleb1-O test set.",2024-07-03T14:14:18Z,2024-07-03T14:14:18Z,http://arxiv.org/abs/2407.03135v1,http://arxiv.org/pdf/2407.03135v1,"cs.SD, cs.AI, cs.HC, eess.AS"
Stable Weight Updating: A Key to Reliable PDE Solutions Using Deep   Learning,"A. Noorizadegan, R. Cavoretto, D. L. Young, C. S. Chen","Background: Deep learning techniques, particularly neural networks, have revolutionized computational physics, offering powerful tools for solving complex partial differential equations (PDEs). However, ensuring stability and efficiency remains a challenge, especially in scenarios involving nonlinear and time-dependent equations. Methodology: This paper introduces novel residual-based architectures, namely the Simple Highway Network and the Squared Residual Network, designed to enhance stability and accuracy in physics-informed neural networks (PINNs). These architectures augment traditional neural networks by incorporating residual connections, which facilitate smoother weight updates and improve backpropagation efficiency. Results: Through extensive numerical experiments across various examples including linear and nonlinear, time-dependent and independent PDEs we demonstrate the efficacy of the proposed architectures. The Squared Residual Network, in particular, exhibits robust performance, achieving enhanced stability and accuracy compared to conventional neural networks. These findings underscore the potential of residual-based architectures in advancing deep learning for PDEs and computational physics applications.",2024-07-10T05:20:43Z,2024-07-10T05:20:43Z,http://arxiv.org/abs/2407.07375v1,http://arxiv.org/pdf/2407.07375v1,"cs.AI, cs.NA, math.NA"
"Integrating Base Station with Intelligent Surface for 6G Wireless   Networks: Architectures, Design Issues, and Future Directions","Yuwei Huang, Lipeng Zhu, Rui Zhang","Intelligent surface (IS) is envisioned as a promising technology for the sixth-generation (6G) wireless networks, which can effectively reconfigure the wireless propagation environment via dynamically controllable signal reflection/transmission. In particular, integrating passive intelligent surface (IS) into the base station (BS) is a novel solution to enhance the wireless network throughput and coverage both cost-effectively and energyefficiently. In this article, we provide an overview of IS-integrated BSs for wireless networks, including their motivations, practical architectures, and main design issues. Moreover, numerical results are presented to compare the performance of different IS-integrated BS architectures as well as the conventional BS without IS. Finally, promising directions are pointed out to stimulate future research on IS-BS/terminal integration in wireless networks.",2024-06-21T07:28:06Z,2024-11-19T01:30:30Z,http://arxiv.org/abs/2407.10986v2,http://arxiv.org/pdf/2407.10986v2,"eess.SP, cs.NI"
Differentiable Quantum Architecture Search in Asynchronous Quantum   Reinforcement Learning,Samuel Yen-Chi Chen,"The emergence of quantum reinforcement learning (QRL) is propelled by advancements in quantum computing (QC) and machine learning (ML), particularly through quantum neural networks (QNN) built on variational quantum circuits (VQC). These advancements have proven successful in addressing sequential decision-making tasks. However, constructing effective QRL models demands significant expertise due to challenges in designing quantum circuit architectures, including data encoding and parameterized circuits, which profoundly influence model performance. In this paper, we propose addressing this challenge with differentiable quantum architecture search (DiffQAS), enabling trainable circuit parameters and structure weights using gradient-based optimization. Furthermore, we enhance training efficiency through asynchronous reinforcement learning (RL) methods facilitating parallel training. Through numerical simulations, we demonstrate that our proposed DiffQAS-QRL approach achieves performance comparable to manually-crafted circuit architectures across considered environments, showcasing stability across diverse scenarios. This methodology offers a pathway for designing QRL models without extensive quantum knowledge, ensuring robust performance and fostering broader application of QRL.",2024-07-25T17:11:00Z,2024-07-25T17:11:00Z,http://arxiv.org/abs/2407.18202v1,http://arxiv.org/pdf/2407.18202v1,"quant-ph, cs.AI, cs.DC, cs.LG, cs.NE"
HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for   Transformer Acceleration,"Pratyush Dhingra, Janardhan Rao Doppa, Partha Pratim Pande","Transformers have revolutionized deep learning and generative modeling to enable unprecedented advancements in natural language processing tasks and beyond. However, designing hardware accelerators for executing transformer models is challenging due to the wide variety of computing kernels involved in the transformer architecture. Existing accelerators are either inadequate to accelerate end-to-end transformer models or suffer notable thermal limitations. In this paper, we propose the design of a three-dimensional heterogeneous architecture referred to as HeTraX specifically optimized to accelerate end-to-end transformer models. HeTraX employs hardware resources aligned with the computational kernels of transformers and optimizes both performance and energy. Experimental results show that HeTraX outperforms existing state-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while ensuring thermally feasibility.",2024-08-06T18:48:01Z,2024-08-06T18:48:01Z,http://arxiv.org/abs/2408.03397v1,http://arxiv.org/pdf/2408.03397v1,"cs.AR, cs.LG, B.0"
Modular Hypernetworks for Scalable and Adaptive Deep MIMO Receivers,"Tomer Raviv, Nir Shlezinger","Deep neural networks (DNNs) were shown to facilitate the operation of uplink multiple-input multiple-output (MIMO) receivers, with emerging architectures augmenting modules of classic receiver processing. Current designs consider static DNNs, whose architecture is fixed and weights are pre-trained. This induces a notable challenge, as the resulting MIMO receiver is suitable for a given configuration, i.e., channel distribution and number of users, while in practice these parameters change frequently with network variations and users leaving and joining the network. In this work, we tackle this core challenge of DNN-aided MIMO receivers. We build upon the concept of hypernetworks, augmenting the receiver with a pre-trained deep model whose purpose is to update the weights of the DNN-aided receiver upon instantaneous channel variations. We design our hypernetwork to augment modular deep receivers, leveraging their modularity to have the hypernetwork adapt not only the weights, but also the architecture. Our modular hypernetwork leads to a DNN-aided receiver whose architecture and resulting complexity adapts to the number of users, in addition to channel variations, without retraining. Our numerical studies demonstrate superior error-rate performance of modular hypernetworks in time-varying channels compared to static pre-trained receivers, while providing rapid adaptivity and scalability to network variations.",2024-08-21T18:12:54Z,2024-08-21T18:12:54Z,http://arxiv.org/abs/2408.11920v1,http://arxiv.org/pdf/2408.11920v1,"cs.IT, eess.SP, math.IT"
Facilitating AI and System Operator Synergy: Active Learning-Enhanced   Digital Twin Architecture for Day-Ahead Load Forecasting,"Costas Mylonas, Titos Georgoulakis, Magda Foti","In this paper, we introduce a synergistic approach between artificial intelligence and system operators through an innovative digital twin architecture, integrated with an active learning framework, to enhance short-term load forecasting. Central to this architecture is the incorporation of sophisticated data pipelines, facilitating the real-time ingestion, processing and analysis of grid-related data. Utilizing a recurrent neural network architecture, our model generates day-ahead load forecasts together with prediction confidence intervals, strengthening system operator trust in the model's predictive reliability and enhancing their ability to respond to evolving grid conditions effectively. The active learning framework iteratively refines the predictions by incorporating real-time feedback based on forecast uncertainty, utilizing newly available data to continuously enhance forecasting accuracy and confidence. This AI-assisted strategy is exemplified in a case study of the Greek transmission system. It demonstrates the potential to transform short-term load forecasting, thereby increasing the reliability and operational efficiency of modern power grids. This approach marks a significant step forward in the digitalization and intelligent management of power systems.",2024-08-31T07:02:27Z,2024-08-31T07:02:27Z,http://arxiv.org/abs/2409.00368v1,http://arxiv.org/pdf/2409.00368v1,"eess.SY, cs.SY"
Low Layer Functional Split Management in 5G and Beyond: Architecture and   Self-adaptation,"Jordi Pérez-Romero, Oriol Sallent, David Campoy, Antoni Gelonch, Xavier Gelabert, Bleron Klaiqi","Radio Access Network (RAN) disaggregation is emerging as a key trend in beyond 5G, as it offers new opportunities for more flexible deployments and intelligent network management. A relevant problem in disaggregated RAN is the functional split selection, which dynamically decides which baseband (BB) functions of a base station are kept close to the radio units and which ones are centralized. In this context, this paper firstly presents an architectural framework for supporting this concept relying on the O-RAN architecture. Then, the paper analyzes how the functional split can be optimized to adapt to the different load conditions while minimizing energy costs.",2024-09-03T08:37:43Z,2024-09-03T08:37:43Z,http://arxiv.org/abs/2409.01701v1,http://arxiv.org/pdf/2409.01701v1,"cs.NI, eess.SP"
Comparing One- and Two-way Quantum Repeater Architectures,"Prateek Mantri, Kenneth Goodenough, Don Towsley","Quantum repeaters are an essential building block for realizing long-distance quantum communications. However, due to the fragile nature of quantum information, these repeaters suffer from loss and operational errors. Prior works have classified repeaters into three broad categories based on their use of probabilistic or near-deterministic methods to mitigate these errors. Besides differences in classical communication times, these approaches also vary in technological complexity, with near-deterministic methods requiring more advanced technology. Recent increases in the number of available memories, and introduction of entanglement generation through multiplexing motivate a re-comparison of one-way and two-way repeater architectures. In this work, we propose a novel protocol that optimizes multiplexed elementary link generation and distillation in memory-unconstrained 'connection-oriented' two-way repeaters to boost the entanglement generation rates. We introduce a recursive formulation to derive the probability distribution of the number of Bell pairs in multiplexed two-way repeater architectures, compatible with probabilistic $n$-to-$k$ distillation protocols. We then compare the performance of this new protocol with one-way schemes in the parameter regime where one-way schemes have previously been shown to be advantageous, and find that the multiplexed two-way protocol provides better performance with lower resource and technology requirements.",2024-09-10T01:55:01Z,2024-09-10T01:55:01Z,http://arxiv.org/abs/2409.06152v1,http://arxiv.org/pdf/2409.06152v1,"quant-ph, cs.NI"
Pushing Joint Image Denoising and Classification to the Edge,"Thomas C Markhorst, Jan C van Gemert, Osman S Kayhan","In this paper, we jointly combine image classification and image denoising, aiming to enhance human perception of noisy images captured by edge devices, like low-light security cameras. In such settings, it is important to retain the ability of humans to verify the automatic classification decision and thus jointly denoise the image to enhance human perception. Since edge devices have little computational power, we explicitly optimize for efficiency by proposing a novel architecture that integrates the two tasks. Additionally, we alter a Neural Architecture Search (NAS) method, which searches for classifiers to search for the integrated model while optimizing for a target latency, classification accuracy, and denoising performance. The NAS architectures outperform our manually designed alternatives in both denoising and classification, offering a significant improvement to human perception. Our approach empowers users to construct architectures tailored to domains like medical imaging, surveillance systems, and industrial inspections.",2024-09-13T16:01:27Z,2024-09-13T16:01:27Z,http://arxiv.org/abs/2409.08943v1,http://arxiv.org/pdf/2409.08943v1,"cs.CV, eess.IV"
TBDM-Net: Bidirectional Dense Networks with Gender Information for   Speech Emotion Recognition,"Vlad Striletchi, Cosmin Striletchi, Adriana Stan","This paper presents a novel deep neural network-based architecture tailored for Speech Emotion Recognition (SER). The architecture capitalises on dense interconnections among multiple layers of bidirectional dilated convolutions. A linear kernel dynamically fuses the outputs of these layers to yield the final emotion class prediction. This innovative architecture is denoted as TBDM-Net: Temporally-Aware Bi-directional Dense Multi-Scale Network. We conduct a comprehensive performance evaluation of TBDM-Net, including an ablation study, across six widely-acknowledged SER datasets for unimodal speech emotion recognition. Additionally, we explore the influence of gender-informed emotion prediction by appending either golden or predicted gender labels to the architecture's inputs or predictions. The implementation of TBDM-Net is accessible at: https://github.com/adrianastan/tbdm-net",2024-09-16T07:36:14Z,2024-09-16T07:36:14Z,http://arxiv.org/abs/2409.10056v1,http://arxiv.org/pdf/2409.10056v1,"eess.AS, cs.SD"
WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency,"Pranav Jeevan, Neeraj Nixon, Amit Sethi","Recent advancements in single image super-resolution have been predominantly driven by token mixers and transformer architectures. WaveMixSR utilized the WaveMix architecture, employing a two-dimensional discrete wavelet transform for spatial token mixing, achieving superior performance in super-resolution tasks with remarkable resource efficiency. In this work, we present an enhanced version of the WaveMixSR architecture by (1) replacing the traditional transpose convolution layer with a pixel shuffle operation and (2) implementing a multistage design for higher resolution tasks ($4\times$). Our experiments demonstrate that our enhanced model -- WaveMixSR-V2 -- outperforms other architectures in multiple super-resolution tasks, achieving state-of-the-art for the BSD100 dataset, while also consuming fewer resources, exhibits higher parameter efficiency, lower latency and higher throughput. Our code is available at https://github.com/pranavphoenix/WaveMixSR.",2024-09-16T04:16:52Z,2024-10-30T15:16:43Z,http://arxiv.org/abs/2409.10582v3,http://arxiv.org/pdf/2409.10582v3,"eess.IV, cs.AI, cs.CV, cs.LG, I.2.10; I.4.0; I.4.1; I.4.2; I.4.6; I.4.7; I.4.8; I.4.9; I.4.10;
  I.2.10; I.5.1; I.5.2; I.5.4; I.4.3; I.4.4; I.4.5"
A High-Throughput Hardware Accelerator for Lempel-Ziv 4 Compression   Algorithm,"Tao Chen, Suwen Song, Zhongfeng Wang","This paper delves into recent hardware implementations of the Lempel-Ziv 4 (LZ4) algorithm, highlighting two key factors that limit the throughput of single-kernel compressors. Firstly, the actual parallelism exhibited in single-kernel designs falls short of the theoretical potential. Secondly, the clock frequency is constrained due to the presence of the feedback loops. To tackle these challenges, we propose a novel scheme that restricts each parallelization window to a single match, thus elevating the level of actual parallelism. Furthermore, by restricting the maximum match length, we eliminate the feedback loops within the architecture, enabling a significant boost in throughput. Finally, we present a high-speed hardware architecture. The implementation results demonstrate that the proposed architecture achieves a throughput of up to 16.10 Gb/s, exhibiting a 2.648x improvement over the start-of-the-art. The new design only results in an acceptable compression ratio reduction ranging from 4.93% to 11.68% with various numbers of hash table entries, compared to the LZ4 compression ratio achieved by official software implementations disclosed on GitHub.",2024-09-19T03:27:22Z,2024-09-19T03:27:22Z,http://arxiv.org/abs/2409.12433v1,http://arxiv.org/pdf/2409.12433v1,"cs.AR, eess.SP"
The Impact of Feature Embedding Placement in the Ansatz of a Quantum   Kernel in QSVMs,"Ilmo Salmenperä, Ilmars Kuhtarskis, Arianne Meijer van de Griend, Jukka K. Nurminen","Designing a useful feature map for a quantum kernel is a critical task when attempting to achieve an advantage over classical machine learning models. The choice of circuit architecture, i.e. how feature-dependent gates should be interwoven with other gates is a relatively unexplored problem and becomes very important when using a model of quantum kernels called Quantum Embedding Kernels (QEK). We study and categorize various architectural patterns in QEKs and show that existing architectural styles do not behave as the literature supposes. We also produce a novel alternative architecture based on the old ones and show that it performs equally well while containing fewer gates than its older counterparts.",2024-09-20T01:25:13Z,2024-09-20T01:25:13Z,http://arxiv.org/abs/2409.13147v1,http://arxiv.org/pdf/2409.13147v1,"quant-ph, cs.AI"
A Contract Theory for Layered Control Architectures,"Manuel Mazo Jr., Will Compton, Max H. Cohen, Aaron D. Ames","Autonomous systems typically leverage layered control architectures with a combination of discrete and continuous models operating at different timescales. As a result, layered systems form a new class of hybrid systems composed of systems operating on a diverse set of continuous and discrete signals. This paper formalizes the notion of a layered (hierarchical) control architecture through a theory of relations between its layers. This theory enables us to formulate contracts within layered control systems -- these define interfaces between layers and isolate the design of each layer, guaranteeing that composition of contracts at each layer results in a contract capturing the desired system-wide specification. Thus, the proposed theory yields the ability to analyze layered control architectures via a compositional approach.",2024-09-23T10:55:49Z,2024-09-23T10:55:49Z,http://arxiv.org/abs/2409.14902v1,http://arxiv.org/pdf/2409.14902v1,"eess.SY, cs.SY, math.OC"
Digital Twin for O-RAN Towards 6G,"Huan X. Nguyen, Kexuan Sun, Duc To, Quoc-Tuan Vien, Tuan Anh Le","In future wireless systems of beyond 5G and 6G, addressing diverse applications with varying quality requirements is essential. Open Radio Access Network (O-RAN) architectures offer the potential for dynamic resource adaptation based on traffic demands. However, achieving real-time resource orchestration remains a challenge. Simultaneously, Digital Twin (DT) technology holds promise for testing and analysing complex systems, offering a unique platform for addressing dynamic operation and automation in O-RAN architectures. Yet, developing DTs for complex 5G/6G networks poses challenges, including data exchanges, ML model training data availability, network dynamics, processing power limitations, interdisciplinary collaboration needs, and a lack of standardized methodologies. This paper provides an overview of Open RAN architecture, trend and challenges, proposing the DT concepts for O-RAN with solution examples showcasing its integration into the framework.",2024-10-03T19:55:20Z,2024-10-03T19:55:20Z,http://arxiv.org/abs/2410.02954v1,http://arxiv.org/pdf/2410.02954v1,"cs.NI, cs.ET, eess.SP"
Leveraging Internet Principles to Build a Quantum Network,"Leonardo Bacciottini, Aparimit Chandra, Matheus Guedes De Andrade, Nitish K. Panigrahy, Shahrooz Pouryousef, Nageswara S. V. Rao, Emily Van Milligen, Gayane Vardoyan, Don Towsley","Designing an operational architecture for the Quantum Internet is a challenging task in light of both fundamental limitations imposed by the laws of physics and technological constraints. Here, we propose a method to abstract away most of the quantum-specific elements and formulate a best-effort quantum network architecture based on packet-switching, akin to that of the classical Internet. Such reframing provides an opportunity to exploit the many tools and protocols available and well-understood within the Internet. As an illustration, we tailor and adapt classical congestion control and active queue management protocols to quantum networks, comprising an architecture wherein quantum end- and intermediate nodes effectively regulate demand and resource utilization, respectively. Results show that these classical networking tools can be effectively used to combat quantum memory decoherence and keep end-to-end fidelity around a target value.",2024-10-11T16:55:10Z,2024-10-11T16:55:10Z,http://arxiv.org/abs/2410.08980v1,http://arxiv.org/pdf/2410.08980v1,"quant-ph, cs.NI"
Architectural Solutions for High-Speed Data Processing Demands of CERN   LHC Detectors with FPGA and High-Level Synthesis,"Sergei Devadze, Christine Elizabeth Nielsen, Dmitri Mihhailov, Peeter Ellervee","The planned high-luminosity upgrade of the Large Hadron Collider (LHC) at CERN will bring much higher data rates that are far above the capabilities of currently installed software-based data processing systems. Therefore, new methods must be used to facilitate on-the-fly extraction of scientifically significant information from the immense flow of data produced by LHC particle detectors. This paper focuses on implementation of a tau lepton triggering algorithm in FPGA. Due to the algorithm's complexity and strict technical requirements, its implementation in FPGA fabric becomes a particularly challenging task. The paper presents a study of algorithm development with the help of High-Level Synthesis (HLS) technique that can generate hardware description from C++ code. Various architectural solutions and optimizations that were tried out during the design architecture exploration process are also discussed in the paper.",2024-10-27T12:55:46Z,2024-10-27T12:55:46Z,http://arxiv.org/abs/2410.20430v1,http://arxiv.org/pdf/2410.20430v1,"cs.AR, physics.ins-det"
Digital requirements engineering with an INCOSE-derived SysML meta-model,"James S. Wheaton, Daniel R. Herber","Traditional requirements engineering tools do not readily access the SysML-defined system architecture model, often resulting in ad-hoc duplication of model elements that lacks the connectivity and expressive detail possible in a SysML-defined model. Further integration of requirements engineering activities with MBSE contributes to the Authoritative Source of Truth while facilitating deep access to system architecture model elements for V&V activities. We explore the application of MBSE to requirements engineering by extending the Model-Based Structured Requirement SysML Profile to comply with the INCOSE Guide to Writing Requirements while conforming to the ISO/IEC/IEEE 29148 standard requirement statement patterns. Rules, Characteristics, and Attributes were defined in SysML according to the Guide to facilitate requirements definition, verification & validation. The resulting SysML Profile was applied in two system architecture models at NASA Jet Propulsion Laboratory, allowing us to assess its applicability and value in real-world project environments. Initial results indicate that INCOSE-derived Model-Based Structured Requirements may rapidly improve requirement expression quality while complementing the NASA Systems Engineering Handbook checklist and guidance, but typical requirement management activities still have challenges related to automation and support in the system architecture modeling software.",2024-10-12T03:06:13Z,2024-10-12T03:06:13Z,http://arxiv.org/abs/2410.21288v1,http://arxiv.org/pdf/2410.21288v1,"cs.SE, cs.SY, eess.SY"
Prototyping O-RAN Enabled UAV Experimentation for the AERPAW Testbed,"Joshua Moore, Aly Sabri Abdalla, Charles Ueltschey, Vuk Marojevic","The Open Radio Access Network (O-RAN) architecture is reshaping the telecommunications landscape by enhancing network flexibility, openness, and intelligence. This paper establishes the requirements, evaluates the design tradeoffs, and introduces a scalable architecture and prototype of an open-source O-RAN experimentation platform within the Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW), an at scale testbed that integrates unmanned aerial vehicles (UAVs) with advanced wireless network technologies, offering experimentation in both outdoor testbed and emulation via a custom digital twin (DT). Through a series of aerial experiments, we evaluate FlexRIC, an open-source RAN Intelligent Controller, within the AERPAW hardware-software platform for network data monitoring, providing valuable insights into the proposed integration and revealing opportunities for leveraging O-RAN to create custom service based optimizations for cellular connected UAVs. We discuss the challenges and potential use cases of this integration and demonstrate the use of a generative artificial intelligence model for generating realistic data based on collected real-world data to support AERPAW's DT.",2024-11-06T16:21:19Z,2024-11-06T16:21:19Z,http://arxiv.org/abs/2411.04027v1,http://arxiv.org/pdf/2411.04027v1,"cs.NI, cs.AR, cs.SY, eess.SY"
The sampling complexity of learning invertible residual neural networks,"Yuanyuan Li, Philipp Grohs, Philipp Petersen","In recent work it has been shown that determining a feedforward ReLU neural network to within high uniform accuracy from point samples suffers from the curse of dimensionality in terms of the number of samples needed. As a consequence, feedforward ReLU neural networks are of limited use for applications where guaranteed high uniform accuracy is required.   We consider the question of whether the sampling complexity can be improved by restricting the specific neural network architecture. To this end, we investigate invertible residual neural networks which are foundational architectures in deep learning and are widely employed in models that power modern generative methods. Our main result shows that the residual neural network architecture and invertibility do not help overcome the complexity barriers encountered with simpler feedforward architectures. Specifically, we demonstrate that the computational complexity of approximating invertible residual neural networks from point samples in the uniform norm suffers from the curse of dimensionality. Similar results are established for invertible convolutional Residual neural networks.",2024-11-08T10:00:40Z,2024-11-08T10:00:40Z,http://arxiv.org/abs/2411.05453v1,http://arxiv.org/pdf/2411.05453v1,"stat.ML, cs.LG"
Architectural Patterns for Designing Quantum Artificial Intelligence   Systems,"Mykhailo Klymenko, Thong Hoang, Xiwei Xu, Zhenchang Xing, Muhammad Usman, Qinghua Lu, Liming Zhu","Utilising quantum computing technology to enhance artificial intelligence systems is expected to improve training and inference times, increase robustness against noise and adversarial attacks, and reduce the number of parameters without compromising accuracy. However, moving beyond proof-of-concept or simulations to develop practical applications of these systems while ensuring high software quality faces significant challenges due to the limitations of quantum hardware and the underdeveloped knowledge base in software engineering for such systems. In this work, we have conducted a systematic mapping study to identify the challenges and solutions associated with the software architecture of quantum-enhanced artificial intelligence systems. The results of the systematic mapping study reveal several architectural patterns that describe how quantum components can be integrated into inference engines, as well as middleware patterns that facilitate communication between classical and quantum components. Each pattern realises a trade-off between various software quality attributes, such as efficiency, scalability, trainability, simplicity, portability, and deployability. The outcomes of this work have been compiled into a catalogue of architectural patterns.",2024-11-14T05:09:07Z,2024-12-17T02:15:28Z,http://arxiv.org/abs/2411.10487v3,http://arxiv.org/pdf/2411.10487v3,"cs.SE, quant-ph, D.2.11; D.2.m; I.2.m"
Retinal Vessel Segmentation via Neuron Programming,"Tingting Wu, Ruyi Min, Peixuan Song, Hengtao Guo, Tieyong Zeng, Feng-Lei Fan","The accurate segmentation of retinal blood vessels plays a crucial role in the early diagnosis and treatment of various ophthalmic diseases. Designing a network model for this task requires meticulous tuning and extensive experimentation to handle the tiny and intertwined morphology of retinal blood vessels. To tackle this challenge, Neural Architecture Search (NAS) methods are developed to fully explore the space of potential network architectures and go after the most powerful one. Inspired by neuronal diversity which is the biological foundation of all kinds of intelligent behaviors in our brain, this paper introduces a novel and foundational approach to neural network design, termed ``neuron programming'', to automatically search neuronal types into a network to enhance a network's representation ability at the neuronal level, which is complementary to architecture-level enhancement done by NAS. Additionally, to mitigate the time and computational intensity of neuron programming, we develop a hypernetwork that leverages the search-derived architectural information to predict optimal neuronal configurations. Comprehensive experiments validate that neuron programming can achieve competitive performance in retinal blood segmentation, demonstrating the strong potential of neuronal diversity in medical image analysis.",2024-11-17T16:03:30Z,2024-11-17T16:03:30Z,http://arxiv.org/abs/2411.11110v1,http://arxiv.org/pdf/2411.11110v1,"eess.IV, cs.CV"
Microsegmented Cloud Network Architecture Using Open-Source Tools for a   Zero Trust Foundation,"Sunil Arora, John Hastings","This paper presents a multi-cloud networking architecture built on zero trust principles and micro-segmentation to provide secure connectivity with authentication, authorization, and encryption in transit. The proposed design includes the multi-cloud network to support a wide range of applications and workload use cases, compute resources including containers, virtual machines, and cloud-native services, including IaaS (Infrastructure as a Service (IaaS), PaaS (Platform as a service). Furthermore, open-source tools provide flexibility, agility, and independence from locking to one vendor technology. The paper provides a secure architecture with micro-segmentation and follows zero trust principles to solve multi-fold security and operational challenges.",2024-11-19T01:58:40Z,2024-11-19T01:58:40Z,http://arxiv.org/abs/2411.12162v1,http://arxiv.org/pdf/2411.12162v1,"cs.CR, cs.DC, cs.NI, cs.SY, eess.SY, K.6.5; D.4.6; C.2.1; C.2.3; C.2.4"
Hermes: A General-Purpose Proxy-Enabled Networking Architecture,"Behrooz Farkiani, Fan Liu, Ke Yang, John DeHart, Jyoti Parwatikar, Patrick Crowley","We introduce Hermes, a general-purpose networking architecture built on an overlay of reconfigurable proxies. Hermes delegates networking responsibilities from applications and services to the overlay proxies. It employs a range of proxying and tunneling techniques, utilizes HTTP as its core component, and incorporates assisting components to facilitate service delivery, enhance communication, and improve end-users' experience. To substantiate these benefits, we prototyped Hermes and demonstrated its ability to efficiently address service and communication challenges. We showed that Hermes enables end-to-end solutions for compatibility with legacy applications and protocols and reliable delivery in highly disadvantaged networking conditions. Furthermore, Hermes demonstrated its ability to provide end-to-end, business-logic-driven handling of general IP traffic and to serve as a communication pipeline for Named Data Networking, facilitating the development and adoption of future networking architectures.",2024-11-20T19:21:42Z,2024-11-20T19:21:42Z,http://arxiv.org/abs/2411.13668v1,http://arxiv.org/pdf/2411.13668v1,"cs.NI, cs.PF, C.2.1"
Barriers on the EDGE: A scalable CBF architecture over EDGE for safe   aerial-ground multi-agent coordination,"Viswa Narayanan Sankaranarayanan, Achilleas Santi Seisa, Akshit Saradagi, Sumeet Satpute, George Nikolakopoulos","In this article, we address the problem of designing a scalable control architecture for a safe coordinated operation of a multi-agent system with aerial (UAVs) and ground robots (UGVs) in a confined task space. The proposed method uses Control Barrier Functions (CBFs) to impose constraints associated with (i) collision avoidance between agents, (ii) landing of UAVs on mobile UGVs, and (iii) task space restriction. Further, to account for the rapid increase in the number of constraints for a single agent with the increasing number of agents, the proposed architecture uses a centralized-decentralized Edge cluster, where a centralized node (Watcher) activates the relevant constraints, reducing the need for high onboard processing and network complexity. The distributed nodes run the controller locally to overcome latency and network issues. The proposed Edge architecture is experimentally validated using multiple aerial and ground robots in a confined environment performing a coordinated operation.",2024-11-25T17:44:02Z,2024-11-25T17:44:02Z,http://arxiv.org/abs/2411.16608v1,http://arxiv.org/pdf/2411.16608v1,"cs.RO, cs.SY, eess.SY"
A Novel Q-stem Connected Architecture for Beyond-Diagonal Reconfigurable   Intelligent Surfaces,"Xiaohua Zhou, Tianyu Fang, Yijie Mao","Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has garnered significant research interest recently due to its ability to generalize existing reconfigurable intelligent surface (RIS) architectures and provide enhanced performance through flexible inter-connection among RIS elements. However, current BD-RIS designs often face challenges related to high circuit complexity and computational complexity, and there is limited study on the trade-off between system performance and circuit complexity. To address these issues, in this work, we propose a novel BD-RIS architecture named Q-stem connected RIS that integrates the characteristics of existing single connected, tree connected, and fully connected BD-RIS, facilitating an effective trade-off between system performance and circuit complexity. Additionally, we propose two algorithms to design the RIS scattering matrix for a Q-stem connected RIS aided multi-user broadcast channels, namely, a low-complexity least squares (LS) algorithm and a suboptimal LS-based quasi-Newton algorithm. Simulations show that the proposed architecture is capable of attaining the sum channel gain achieved by fully connected RIS while reducing the circuit complexity. Moreover, the proposed LS-based quasi-Newton algorithm significantly outperforms the baselines, while the LS algorithm provides comparable performance with a substantial reduction in computational complexity.",2024-11-27T16:23:03Z,2024-11-27T16:23:03Z,http://arxiv.org/abs/2411.18480v1,http://arxiv.org/pdf/2411.18480v1,"cs.IT, eess.SP, math.IT"
Structure-Guided Input Graph for GNNs facing Heterophily,"Victor M. Tenorio, Madeline Navarro, Samuel Rey, Santiago Segarra, Antonio G. Marques","Graph Neural Networks (GNNs) have emerged as a promising tool to handle data exhibiting an irregular structure. However, most GNN architectures perform well on homophilic datasets, where the labels of neighboring nodes are likely to be the same. In recent years, an increasing body of work has been devoted to the development of GNN architectures for heterophilic datasets, where labels do not exhibit this low-pass behavior. In this work, we create a new graph in which nodes are connected if they share structural characteristics, meaning a higher chance of sharing their labels, and then use this new graph in the GNN architecture. To do this, we compute the k-nearest neighbors graph according to distances between structural features, which are either (i) role-based, such as degree, or (ii) global, such as centrality measures. Experiments show that the labels are smoother in this newly defined graph and that the performance of GNN architectures improves when using this alternative structure.",2024-12-02T17:52:33Z,2024-12-02T17:52:33Z,http://arxiv.org/abs/2412.01757v1,http://arxiv.org/pdf/2412.01757v1,"cs.LG, eess.SP"
Distributed Intelligent System Architecture for UAV-Assisted Monitoring   of Wind Energy Infrastructure,"Serhii Svystun, Oleksandr Melnychenko, Pavlo Radiuk, Oleg Savenko, Andrii Lysyi","With the rapid development of green energy, the efficiency and reliability of wind turbines are key to sustainable renewable energy production. For that reason, this paper presents a novel intelligent system architecture designed for the dynamic collection and real-time processing of visual data to detect defects in wind turbines. The system employs advanced algorithms within a distributed framework to enhance inspection accuracy and efficiency using unmanned aerial vehicles (UAVs) with integrated visual and thermal sensors. An experimental study conducted at the ""Staryi Sambir-1"" wind power plant in Ukraine demonstrates the system's effectiveness, showing a significant improvement in defect detection accuracy (up to 94%) and a reduction in inspection time per turbine (down to 1.5 hours) compared to traditional methods. The results show that the proposed intelligent system architecture provides a scalable and reliable solution for wind turbine maintenance, contributing to the durability and performance of renewable energy infrastructure.",2024-12-12T15:53:58Z,2024-12-12T15:53:58Z,http://arxiv.org/abs/2412.09387v1,http://arxiv.org/pdf/2412.09387v1,"cs.RO, cs.AI, cs.SY, eess.SY, I.4.8; I.2.10; I.5.4; I.2.9"
Structurally Consistent MRI Colorization using Cross-modal Fusion   Learning,"Mayuri Mathur, Anav Chaudhary, Saurabh Kumar Gupta, Ojaswa Sharma","Medical image colorization can greatly enhance the interpretability of the underlying imaging modality and provide insights into human anatomy. The objective of medical image colorization is to transfer a diverse spectrum of colors distributed across human anatomy from Cryosection data to source MRI data while retaining the structures of the MRI. To achieve this, we propose a novel architecture for structurally consistent color transfer to the source MRI data. Our architecture fuses segmentation semantics of Cryosection images for stable contextual colorization of various organs in MRI images. For colorization, we neither require precise registration between MRI and Cryosection images, nor segmentation of MRI images. Additionally, our architecture incorporates a feature compression-and-activation mechanism to capture organ-level global information and suppress noise, enabling the distinction of organ-specific data in MRI scans for more accurate and realistic organ-specific colorization. Our experiments demonstrate that our architecture surpasses the existing methods and yields better quantitative and qualitative results.",2024-12-12T06:40:14Z,2024-12-12T06:40:14Z,http://arxiv.org/abs/2412.10452v1,http://arxiv.org/pdf/2412.10452v1,"eess.IV, cs.AI, cs.CV"
Kolmogorov GAM Networks are all you need!,"Sarah Polson, Vadim Sokolov","Kolmogorov GAM (K-GAM) networks are shown to be an efficient architecture for training and inference. They are an additive model with an embedding that is independent of the function of interest. They provide an alternative to the transformer architecture. They are the machine learning version of Kolmogorov's Superposition Theorem (KST) which provides an efficient representations of a multivariate function. Such representations have use in machine learning for encoding dictionaries (a.k.a. ""look-up"" tables). KST theory also provides a representation based on translates of the K\""oppen function. The goal of our paper is to interpret this representation in a machine learning context for applications in Artificial Intelligence (AI). Our architecture is equivalent to a topological embedding which is independent of the function together with an additive layer that uses a Generalized Additive Model (GAM). This provides a class of learning procedures with far fewer parameters than current deep learning algorithms. Implementation can be parallelizable which makes our algorithms computationally attractive. To illustrate our methodology, we use the Iris data from statistical learning. We also show that our additive model with non-linear embedding provides an alternative to transformer architectures which from a statistical viewpoint are kernel smoothers. Additive KAN models therefore provide a natural alternative to transformers. Finally, we conclude with directions for future research.",2025-01-01T02:46:00Z,2025-01-01T02:46:00Z,http://arxiv.org/abs/2501.00704v1,http://arxiv.org/pdf/2501.00704v1,"cs.LG, stat.CO"
RealTime Health Monitoring Using 5G Networks: A Deep Learning-Based   Architecture for Remote Patient Care,Iqra Batool,"Remote patient monitoring is crucial in modern healthcare, but current systems struggle with real-time analysis and prediction of vital signs. This paper presents a novel architecture combining deep learning with 5G network capabilities to enable real-time vital sign monitoring and prediction. The proposed system utilizes a hybrid CNN-LSTM model optimized for edge deployment, paired with 5G Ultra-Reliable Low-Latency Communication (URLLC) for efficient data transmission. The architecture achieves end-to-end latency of 14.4ms while maintaining 96.5% prediction accuracy across multiple vital signs. Our system shows significant improvements over existing solutions, reducing latency by 47% and increasing prediction accuracy by 4.2% compared to current state-of-the-art systems. Performance evaluations conducted over three months with data from 1000 patients validate the system's reliability and scalability in clinical settings. The results demonstrate that integrating deep learning with 5G technology can effectively address the challenges of real-time patient monitoring, leading to early detection of deteriorating conditions and improved clinical outcomes. This research establishes a framework for reliable, real-time vital sign monitoring and prediction in digital healthcare.",2025-01-02T03:17:10Z,2025-01-02T03:17:10Z,http://arxiv.org/abs/2501.01027v1,http://arxiv.org/pdf/2501.01027v1,"cs.NI, eess.SP"
Neural Architecture Codesign for Fast Physics Applications,"Jason Weitz, Dmitri Demler, Luke McDermott, Nhan Tran, Javier Duarte","We develop a pipeline to streamline neural architecture codesign for physics applications to reduce the need for ML expertise when designing models for novel tasks. Our method employs neural architecture search and network compression in a two-stage approach to discover hardware efficient models. This approach consists of a global search stage that explores a wide range of architectures while considering hardware constraints, followed by a local search stage that fine-tunes and compresses the most promising candidates. We exceed performance on various tasks and show further speedup through model compression techniques such as quantization-aware-training and neural network pruning. We synthesize the optimal models to high level synthesis code for FPGA deployment with the hls4ml library. Additionally, our hierarchical search space provides greater flexibility in optimization, which can easily extend to other tasks and domains. We demonstrate this with two case studies: Bragg peak finding in materials science and jet classification in high energy physics, achieving models with improved accuracy, smaller latencies, or reduced resource utilization relative to the baseline models.",2025-01-09T19:00:03Z,2025-01-09T19:00:03Z,http://arxiv.org/abs/2501.05515v1,http://arxiv.org/pdf/2501.05515v1,"cs.LG, cond-mat.mtrl-sci, hep-ex, physics.ins-det"
BLR-MoE: Boosted Language-Routing Mixture of Experts for Domain-Robust   Multilingual E2E ASR,"Guodong Ma, Wenxuan Wang, Lifeng Zhou, Yuting Yang, Yuke Li, Binbin Du","Recently, the Mixture of Expert (MoE) architecture, such as LR-MoE, is often used to alleviate the impact of language confusion on the multilingual ASR (MASR) task. However, it still faces language confusion issues, especially in mismatched domain scenarios. In this paper, we decouple language confusion in LR-MoE into confusion in self-attention and router. To alleviate the language confusion in self-attention, based on LR-MoE, we propose to apply attention-MoE architecture for MASR. In our new architecture, MoE is utilized not only on feed-forward network (FFN) but also on self-attention. In addition, to improve the robustness of the LID-based router on language confusion, we propose expert pruning and router augmentation methods. Combining the above, we get the boosted language-routing MoE (BLR-MoE) architecture. We verify the effectiveness of the proposed BLR-MoE in a 10,000-hour MASR dataset.",2025-01-22T02:55:11Z,2025-01-22T02:55:11Z,http://arxiv.org/abs/2501.12602v1,http://arxiv.org/pdf/2501.12602v1,"cs.CL, cs.SD, eess.AS"
Hierarchical Recording Architecture for Three-Dimensional Magnetic   Recording,"Yugen Jian, Ke Luo, Jincai Chen, Xuanyao Fong","Three-dimensional magnetic recording (3DMR) is a highly promising approach to achieving ultra-large data storage capacity in hard disk drives. One of the greatest challenges for 3DMR lies in performing sequential and correct writing of bits into the multi-layer recording medium. In this work, we have proposed a hierarchical recording architecture based on layered heat-assisted writing with a multi-head array. The feasibility of the architecture is validated in a dual-layer 3DMR system with FePt-based thin films via micromagnetic simulation. Our results reveal the magnetization reversal mechanism of the grains, ultimately attaining appreciable switching probability and medium signal-to-noise ratio (SNR) for each layer. In particular, an optimal head-to-head distance is identified as the one that maximizes the medium SNR. Optimizing the system's noise resistance will improve the overall SNR and allow for a smaller optimal head-to-head distance, which can pave the way for scaling 3DMR to more recording layers.",2025-01-27T13:49:07Z,2025-01-27T13:49:07Z,http://arxiv.org/abs/2501.16053v1,http://arxiv.org/pdf/2501.16053v1,"cs.AR, physics.app-ph"
Comprehensive Formal Verification of Observational Correctness for the   CHERIoT-Ibex Processor,"Louis-Emile Ploix, Alasdair Armstrong, Tom Melham, Ray Lin, Haolong Wang, Anastasia Courtney","The CHERI architecture equips conventional RISC ISAs with significant architectural extensions that provide a hardware-enforced mechanism for memory protection and software compartmentalisation. Architectural capabilities replace conventional integer pointers with memory addresses bound to permissions constraining their use. We present the first comprehensive formal verification of a capability extended RISC-V processor with internally 'compressed' capabilities - a concise encoding of capabilities with some resemblance to floating point number representations.   The reference model for RTL correctness is a minor variant of the full and definitive ISA description written in the Sail ISA specification language. This is made accessible to formal verification tools by a prototype flow for translation of Sail into SystemVerilog. Our verification demonstrates a methodology for establishing that the processor always produces a stream of interactions with memory that is identical to that specified in Sail, when started in the same initial state. We additionally establish liveness. This abstract, microarchitecture-independent observational correctness property provides a comprehensive and clear assurance of functional correctness for the CHERIoT-Ibex processor's observable interactions with memory.",2025-02-07T08:12:02Z,2025-02-07T08:12:02Z,http://arxiv.org/abs/2502.04738v1,http://arxiv.org/pdf/2502.04738v1,"cs.AR, B.6.2; J.6"
AI Agentic workflows and Enterprise APIs: Adapting API architectures for   the age of AI agents,"Vaibhav Tupe, Shrinath Thube","The rapid advancement of Generative AI has catalyzed the emergence of autonomous AI agents, presenting unprecedented challenges for enterprise computing infrastructures. Current enterprise API architectures are predominantly designed for human-driven, predefined interaction patterns, rendering them ill-equipped to support intelligent agents' dynamic, goal-oriented behaviors. This research systematically examines the architectural adaptations for enterprise APIs to support AI agentic workflows effectively. Through a comprehensive analysis of existing API design paradigms, agent interaction models, and emerging technological constraints, the paper develops a strategic framework for API transformation. The study employs a mixed-method approach, combining theoretical modeling, comparative analysis, and exploratory design principles to address critical challenges in standardization, performance, and intelligent interaction. The proposed research contributes a conceptual model for next-generation enterprise APIs that can seamlessly integrate with autonomous AI agent ecosystems, offering significant implications for future enterprise computing architectures.",2025-01-22T05:55:16Z,2025-01-22T05:55:16Z,http://arxiv.org/abs/2502.17443v1,http://arxiv.org/pdf/2502.17443v1,"cs.SE, cs.AI, I.2.0; D.2.11; D.2.12; K.6.5; I.2.11"
Enhancing Speech Quality through the Integration of BGRU and Transformer   Architectures,"Souliman Alghnam, Mohammad Alhussien, Khaled Shaheen","Speech enhancement plays an essential role in improving the quality of speech signals in noisy environments. This paper investigates the efficacy of integrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models for speech enhancement tasks. Through a comprehensive experimental evaluation, our study demonstrates the superiority of this hybrid architecture over traditional methods and standalone models. The combined BGRU-Transformer framework excels in capturing temporal dependencies and learning complex signal patterns, leading to enhanced noise reduction and improved speech quality. Results show significant performance gains compared to existing approaches, highlighting the potential of this integrated model in real-world applications. The seamless integration of BGRU and Transformer architectures not only enhances system robustness but also opens the road for advanced speech processing techniques. This research contributes to the ongoing efforts in speech enhancement technology and sets a solid foundation for future investigations into optimizing model architectures, exploring many application scenarios, and advancing the field of speech processing in noisy environments.",2025-02-25T07:18:35Z,2025-02-25T07:18:35Z,http://arxiv.org/abs/2502.17911v1,http://arxiv.org/pdf/2502.17911v1,"cs.SD, cs.AI, eess.AS"
Accessing LLMs for Front-end Software Architecture Knowledge,"L. P. Franciscatto Guerra, N. Ernst","Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools.",2025-02-26T19:33:35Z,2025-02-26T19:33:35Z,http://arxiv.org/abs/2502.19518v1,http://arxiv.org/pdf/2502.19518v1,"cs.SE, cs.AI, D.2; I.2"
Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking   Function Variation,"Pavel Rumiantsev, Mark Coates","Neural Architecture Search (NAS) is a powerful automatic alternative to manual design of a neural network. In the zero-shot version, a fast ranking function is used to compare architectures without training them. The outputs of the ranking functions often vary significantly due to different sources of randomness, including the evaluated architecture's weights' initialization or the batch of data used for calculations. A common approach to addressing the variation is to average a ranking function output over several evaluations. We propose taking into account the variation in a different manner, by viewing the ranking function output as a random variable representing a proxy performance metric. During the search process, we strive to construct a stochastic ordering of the performance metrics to determine the best architecture. Our experiments show that the proposed stochastic ordering can effectively boost performance of a search on standard benchmark search spaces.",2025-02-27T01:01:22Z,2025-02-27T01:01:22Z,http://arxiv.org/abs/2502.19657v1,http://arxiv.org/pdf/2502.19657v1,"cs.LG, stat.ML"
ServoLNN: Lagrangian Neural Networks Driven by Servomechanisms,"Brandon Johns, Zhuomin Zhou, Elahe Abdi","Combining deep learning with classical physics facilitates the efficient creation of accurate dynamical models. In a recent class of neural network, Lagrangian mechanics is hard-coded into the architecture, and training the network learns the given system. However, the current architectures do not facilitate the modelling of dynamical systems that are driven by servomechanisms (e.g. servomotors, stepper motors, current sources, volumetric pumps). This article presents ServoLNN, a new architecture to model dynamical systems that are driven by servomechanisms. ServoLNN is compatible for use in real-time applications, where the driving motion is known only just-in-time. A PyTorch implementation of ServoLNN is provided. The derivations and results reveal the occurrence of a possible family of solutions that the training may converge on. The effect of the family of solutions on the predicted physical quantities is explored, as is the resolution to reduce the family of solutions to a single solution. Resultantly, the architecture can simultaneously accurately find the energies, power, rate of work, mass matrix, generalised accelerations, generalised forces, and the generalised forces that drive the servomechanisms.",2025-02-27T06:21:17Z,2025-02-27T06:21:17Z,http://arxiv.org/abs/2502.19802v1,http://arxiv.org/pdf/2502.19802v1,"cs.LG, cs.RO, math.DS, I.2.9; I.6.5; J.2"
Unsupervised Basis Function Adaptation for Reinforcement Learning,"Edward W. Barker, Charl J. Ras","When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. This method is ""unsupervised"" in the sense that it makes no direct reference to reward or the VF estimate. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   A common method of scoring a VF estimate is to weight the squared Bellman error of each state-action by the probability of that state-action occurring. Adopting this scoring method, and assuming $S$ states, we demonstrate theoretically that - provided (1) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\log_2{S}\ln{S}$ or greater, (2) the policy and transition function are close to deterministic, and (3) the prior for the transition function is uniformly distributed - our algorithm, used in conjunction with a suitable RL algorithm, can guarantee a score which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X \log_2S)$ space complexity and negligible time complexity. The results take advantage of certain properties of the stationary distributions of Markov chains.",2017-03-03T03:24:03Z,2017-03-03T03:24:03Z,http://arxiv.org/abs/1703.01026v1,http://arxiv.org/pdf/1703.01026v1,"cs.AI, cs.LG, stat.ML"
Learning Transferable Architectures for Scalable Image Recognition,"Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le","Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the ""NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or ""cell"") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named ""NASNet architecture"". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.",2017-07-21T18:10:26Z,2018-04-11T05:12:21Z,http://arxiv.org/abs/1707.07012v4,http://arxiv.org/pdf/1707.07012v4,"cs.CV, cs.LG, stat.ML"
Beyond Finite Layer Neural Networks: Bridging Deep Architectures and   Numerical Differential Equations,"Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong","In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ($>50$\%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.",2017-10-27T13:19:59Z,2020-03-23T04:20:58Z,http://arxiv.org/abs/1710.10121v3,http://arxiv.org/pdf/1710.10121v3,"cs.CV, cs.LG, stat.ML"
"Reinforcement Learning Architectures: SAC, TAC, and ESAC","Ala'eddin Masadeh, Zhengdao Wang, Ahmed E. Kamal","The trend is to implement intelligent agents capable of analyzing available information and utilize it efficiently. This work presents a number of reinforcement learning (RL) architectures; one of them is designed for intelligent agents. The proposed architectures are called selector-actor-critic (SAC), tuner-actor-critic (TAC), and estimator-selector-actor-critic (ESAC). These architectures are improved models of a well known architecture in RL called actor-critic (AC). In AC, an actor optimizes the used policy, while a critic estimates a value function and evaluate the optimized policy by the actor. SAC is an architecture equipped with an actor, a critic, and a selector. The selector determines the most promising action at the current state based on the last estimate from the critic. TAC consists of a tuner, a model-learner, an actor, and a critic. After receiving the approximated value of the current state-action pair from the critic and the learned model from the model-learner, the tuner uses the Bellman equation to tune the value of the current state-action pair. ESAC is proposed to implement intelligent agents based on two ideas, which are lookahead and intuition. Lookahead appears in estimating the values of the available actions at the next state, while the intuition appears in maximizing the probability of selecting the most promising action. The newly added elements are an underlying model learner, an estimator, and a selector. The model learner is used to approximate the underlying model. The estimator uses the approximated value function, the learned underlying model, and the Bellman equation to estimate the values of all actions at the next state. The selector is used to determine the most promising action at the next state, which will be used by the actor to optimize the used policy. Finally, the results show the superiority of ESAC compared with the other architectures.",2020-04-05T18:31:24Z,2020-04-05T18:31:24Z,http://arxiv.org/abs/2004.02274v1,http://arxiv.org/pdf/2004.02274v1,"cs.LG, eess.SP, stat.ML"
How to Build a Graph-Based Deep Learning Architecture in Traffic Domain:   A Survey,"Jiexia Ye, Juanjuan Zhao, Kejiang Ye, Chengzhong Xu","In recent years, various deep learning architectures have been proposed to solve complex challenges (e.g. spatial dependency, temporal dependency) in traffic domain, which have achieved satisfactory performance. These architectures are composed of multiple deep learning techniques in order to tackle various challenges in traffic tasks. Traditionally, convolution neural networks (CNNs) are utilized to model spatial dependency by decomposing the traffic network as grids. However, many traffic networks are graph-structured in nature. In order to utilize such spatial information fully, it's more appropriate to formulate traffic networks as graphs mathematically. Recently, various novel deep learning techniques have been developed to process graph data, called graph neural networks (GNNs). More and more works combine GNNs with other deep learning techniques to construct an architecture dealing with various challenges in a complex traffic task, where GNNs are responsible for extracting spatial correlations in traffic network. These graph-based architectures have achieved state-of-the-art performance. To provide a comprehensive and clear picture of such emerging trend, this survey carefully examines various graph-based deep learning architectures in many traffic applications. We first give guidelines to formulate a traffic problem based on graph and construct graphs from various kinds of traffic datasets. Then we decompose these graph-based architectures to discuss their shared deep learning techniques, clarifying the utilization of each technique in traffic tasks. What's more, we summarize some common traffic challenges and the corresponding graph-based deep learning solutions to each challenge. Finally, we provide benchmark datasets, open source codes and future research directions in this rapidly growing field.",2020-05-24T09:07:55Z,2020-10-11T03:26:00Z,http://arxiv.org/abs/2005.11691v6,http://arxiv.org/pdf/2005.11691v6,"eess.SP, cs.LG"
VLSI Implementation of TDC Architectures Used in PET Imaging Systems,"Mehmet Akif Ozdemir, Ali Tangel","Positron emission tomography (PET) is a medical imaging method based on the measurement of concentrations of positron-emitting radionuclides in a living body. In the PET imaging system, glucose is labeled with a positron-emitting radionuclide and injected intravenously. Then, the positrons move through the tissue and collide with the electrons of the cells in which they interact. As a result of this interaction, two gamma rays are emitted in the opposite direction. Gama rays emitted from cancerous tissue that has retained radioactive glucose are detected through ring-shaped detectors. And the detected signals are converted into an electrical response. Subsequently, these responses are sampled with electronic circuits and recorded as histogram matrix to generate the image set. The gamma rays may not reach the detectors located in the opposite position in equal time. In PETs having TOF characteristics, it is aimed to obtain better positioning information by a method based on the principle of measuring the difference between the reach time of the two photons to detectors. The measurement of the flight time is carried out with TDC structures. The measurement of this time difference at the ps level is directly related to the spatial resolution of the PET system. In this study, 45 nm CMOS VLSI simulations of TDC structures that have various architectural approaches were performed for use in PET systems. With the designed TDC architectures, two gamma photons time reach to detectors have been simulated and the time difference has been successfully digitized. In addition, various performance metrics such as input and output voltages, time resolutions, measurement ranges, and power analysis of TDC architectures have been determined. Proposed Vernier oscillator-based TDC architecture has been reached 25 ps time resolution with a low power consumption of 1.62681 mW at 1V supply voltage.",2020-06-10T19:00:06Z,2020-06-10T19:00:06Z,http://arxiv.org/abs/2006.06034v1,http://arxiv.org/pdf/2006.06034v1,"eess.SP, cs.AR, cs.SY, eess.SY"
Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks,"Shoukang Hu, Xurong Xie, Mingyu Cui, Jiajun Deng, Shansong Liu, Jianwei Yu, Mengzhe Geng, Xunying Liu, Helen Meng","State-of-the-art automatic speech recognition (ASR) system development is data and computation intensive. The optimal design of deep neural networks (DNNs) for these systems often require expert knowledge and empirical evaluation. In this paper, a range of neural architecture search (NAS) techniques are used to automatically learn two types of hyper-parameters of factored time delay neural networks (TDNN-Fs): i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These techniques include the differentiable neural architecture search (DARTS) method integrating architecture learning with lattice-free MMI training; Gumbel-Softmax and pipelined DARTS methods reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to balance the trade-off between performance and system complexity. Parameter sharing among TDNN-F architectures allows an efficient search over up to 7^28 different systems. Statistically significant word error rate (WER) reductions of up to 1.2% absolute and relative model size reduction of 31% were obtained over a state-of-the-art 300-hour Switchboard corpus trained baseline LF-MMI TDNN-F system featuring speed perturbation, i-Vector and learning hidden unit contribution (LHUC) based speaker adaptation as well as RNNLM rescoring. Performance contrasts on the same task against recent end-to-end systems reported in the literature suggest the best NAS auto-configured system achieves state-of-the-art WERs of 9.9% and 11.1% on the NIST Hub5' 00 and Rt03s test sets respectively with up to 96% model size reduction. Further analysis using Bayesian learning shows that ...",2022-01-08T07:52:01Z,2022-03-29T02:47:44Z,http://arxiv.org/abs/2201.03943v3,http://arxiv.org/pdf/2201.03943v3,"eess.AS, cs.SD"
Toward Refactoring of DMARF and GIPSY Case Studies -- a Team 4   SOEN6471-S14 Project Report,"Afshin Somani, Ahmad Al-Sheikh Hassan, Anurag Reddy Pedditi, Challa Sai Sukesh Reddy, Vijay Nag Ranga, Saravanan Iyyaswamy Srinivasan, Hongyo Lao, Zhu Zhili","Software Quality is a major concern in software engineering development in order to be competitive. Such a quality can be achieved by a possible technique called Refactoring where the systems external behavior of the system is not changed. Initially we present our work by analyzing the case studies of ongoing researches of DMARF and GIPSY by understanding their needs and requirements involving the major components in their respective systems. Later sections illustrate the conceptual architecture of these case studies, for this we have referenced the original architecture to draw the important candidate concepts presented in the system, and analyzing their associations with other concepts in the system and then compared this conceptual architecture with the original architectures. Later the document throws light on identifying the code smells exist in the architectures to find them and resolve to minimize the deeper problems. JDeodorant, SonarQube are the tools which we go across for identification and analyzing the source code quality, both these tools are available as an IDE plugin or as an open source platforms. Next is to identify the design patterns exist in the architectures along with their importance and need for existence in respective systems. Finally, the implication is towards introducing refactoring methods onto the smells which have been identified and possibly refactor them accordingly by applying appropriate refactoring methods and showcasing the respective tests to ensure that changes in the architecture does not change the behavior much.",2014-12-23T17:42:20Z,2014-12-23T17:42:20Z,http://arxiv.org/abs/1412.7534v1,http://arxiv.org/pdf/1412.7534v1,"cs.SE, D.2; K.6; H.5.2"
Deep Learning architectures for generalized immunofluorescence based   nuclear image segmentation,"Florian Kromp, Lukas Fischer, Eva Bozsaky, Inge Ambros, Wolfgang Doerr, Sabine Taschner-Mandl, Peter Ambros, Allan Hanbury","Separating and labeling each instance of a nucleus (instance-aware segmentation) is the key challenge in segmenting single cell nuclei on fluorescence microscopy images. Deep Neural Networks can learn the implicit transformation of a nuclear image into a probability map indicating the class membership of each pixel (nucleus or background), but the use of post-processing steps to turn the probability map into a labeled object mask is error-prone. This especially accounts for nuclear images of tissue sections and nuclear images across varying tissue preparations. In this work, we aim to evaluate the performance of state-of-the-art deep learning architectures to segment nuclei in fluorescence images of various tissue origins and sample preparation types without post-processing. We compare architectures that operate on pixel to pixel translation and an architecture that operates on object detection and subsequent locally applied segmentation. In addition, we propose a novel strategy to create artificial images to extend the training set. We evaluate the influence of ground truth annotation quality, image scale and segmentation complexity on segmentation performance. Results show that three out of four deep learning architectures (U-Net, U-Net with ResNet34 backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the sample preparation types and tissue origins with satisfactory segmentation performance. Mask R-CNN, an architecture designed to address instance aware segmentation tasks, outperforms other architectures. Equal nuclear mean size, consistent nuclear annotations and the use of artificially generated images result in overall acceptable precision and recall across different tissues and sample preparation types.",2019-07-30T14:23:29Z,2019-07-30T14:23:29Z,http://arxiv.org/abs/1907.12975v1,http://arxiv.org/pdf/1907.12975v1,"cs.CV, cs.LG, q-bio.TO"
Inferring Convolutional Neural Networks' accuracies from their   architectural characterizations,"Duc Hoang, Jesse Hamer, Gabriel N. Perdue, Steven R. Young, Jonathan Miller, Anushree Ghosh","Convolutional Neural Networks (CNNs) have shown strong promise for analyzing scientific data from many domains including particle imaging detectors. However, the challenge of choosing the appropriate network architecture (depth, kernel shapes, activation functions, etc.) for specific applications and different data sets is still poorly understood. In this paper, we study the relationships between a CNN's architecture and its performance by proposing a systematic language that is useful for comparison between different CNN's architectures before training time. We characterize CNN's architecture by different attributes, and demonstrate that the attributes can be predictive of the networks' performance in two specific computer vision-based physics problems -- event vertex finding and hadron multiplicity classification in the MINERvA experiment at Fermi National Accelerator Laboratory. In doing so, we extract several architectural attributes from optimized networks' architecture for the physics problems, which are outputs of a model selection algorithm called Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL). We use machine learning models to predict whether a network can perform better than a certain threshold accuracy before training. The models perform 16-20% better than random guessing. Additionally, we found an coefficient of determination of 0.966 for an Ordinary Least Squares model in a regression on accuracy over a large population of networks.",2020-01-07T16:41:58Z,2020-01-10T03:52:48Z,http://arxiv.org/abs/2001.02160v2,http://arxiv.org/pdf/2001.02160v2,"cs.CV, cs.LG, hep-ex"
NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and   Size,"Xuanyi Dong, Lu Liu, Katarzyna Musial, Bogdan Gabrys","Neural architecture search (NAS) has attracted a lot of attention and has been illustrated to bring tangible benefits in a large number of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under different search spaces and training setups. This makes the overall performance of the algorithms to some extent incomparable and the improvement from a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a unified benchmark on searching for both topology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space. We also show the versatility of NATS-Bench by benchmarking 13 recent state-of-the-art NAS algorithms on it. All logs and diagnostic information trained using the same setup for each candidate are provided. This facilitates a much larger community of researchers to focus on developing better NAS algorithms in a more comparable and computationally cost friendly environment. All codes are publicly available at: https://xuanyidong.com/assets/projects/NATS-Bench.",2020-08-28T21:34:56Z,2021-01-26T02:33:39Z,http://arxiv.org/abs/2009.00437v6,http://arxiv.org/pdf/2009.00437v6,"cs.LG, stat.ML"
HeM3D: Heterogeneous Manycore Architecture Based on Monolithic 3D   Vertical Integration,"Aqeeb Iqbal Arka, Biresh Kumar Joardar, Ryan Gary Kim, Dae Hyun Kim, Janardhan Rao Doppa, Partha Pratim Pande","Heterogeneous manycore architectures are the key to efficiently execute compute- and data-intensive applications. Through silicon via (TSV)-based 3D manycore system is a promising solution in this direction as it enables integration of disparate computing cores on a single system. However, the achievable performance of conventional through-silicon-via (TSV)-based 3D systems is ultimately bottlenecked by the horizontal wires (wires in each planar die). Moreover, current TSV 3D architectures suffer from thermal limitations. Hence, TSV-based architectures do not realize the full potential of 3D integration. Monolithic 3D (M3D) integration, a breakthrough technology to achieve - More Moore and More Than Moore - and opens up the possibility of designing cores and associated network routers using multiple layers by utilizing monolithic inter-tier vias (MIVs) and hence, reducing the effective wire length. Compared to TSV-based 3D ICs, M3D offers the true benefits of vertical dimension for system integration: the size of a MIV used in M3D is over 100x smaller than a TSV. In this work, we demonstrate how M3D-enabled vertical core and uncore elements offer significant performance and thermal improvements in manycore heterogeneous architectures compared to its TSV-based counterpart. To overcome the difficult optimization challenges due to the large design space and complex interactions among the heterogeneous components (CPU, GPU, Last Level Cache, etc.) in an M3D-based manycore chip, we leverage novel design-space exploration algorithms to trade-off different objectives. The proposed M3D-enabled heterogeneous architecture, called HeM3D, outperforms its state-of-the-art TSV-equivalent counterpart by up to 18.3% in execution time while being up to 19 degrees Celcius cooler.",2020-11-30T21:23:41Z,2020-12-08T02:07:11Z,http://arxiv.org/abs/2012.00102v2,http://arxiv.org/pdf/2012.00102v2,"cs.AR, C.2"
Low-Precision Hardware Architectures Meet Recommendation Model Inference   at Scale,"Zhaoxia, Deng, Jongsoo Park, Ping Tak Peter Tang, Haixin Liu, Jie, Yang, Hector Yuen, Jianyu Huang, Daya Khudia, Xiaohan Wei, Ellie Wen, Dhruv Choudhary, Raghuraman Krishnamoorthi, Carole-Jean Wu, Satish Nadathur, Changkyu Kim, Maxim Naumov, Sam Naghshineh, Mikhail Smelyanskiy","Tremendous success of machine learning (ML) and the unabated growth in ML model complexity motivated many ML-specific designs in both CPU and accelerator architectures to speed up the model inference. While these architectures are diverse, highly optimized low-precision arithmetic is a component shared by most. Impressive compute throughputs are indeed often exhibited by these architectures on benchmark ML models. Nevertheless, production models such as recommendation systems important to Facebook's personalization services are demanding and complex: These systems must serve billions of users per month responsively with low latency while maintaining high prediction accuracy, notwithstanding computations with many tens of billions parameters per inference. Do these low-precision architectures work well with our production recommendation systems? They do. But not without significant effort. We share in this paper our search strategies to adapt reference recommendation models to low-precision hardware, our optimization of low-precision compute kernels, and the design and development of tool chain so as to maintain our models' accuracy throughout their lifespan during which topic trends and users' interests inevitably evolve. Practicing these low-precision technologies helped us save datacenter capacities while deploying models with up to 5X complexity that would otherwise not be deployed on traditional general-purpose CPUs. We believe these lessons from the trenches promote better co-design between hardware architecture and software engineering and advance the state of the art of ML in industry.",2021-05-26T16:42:33Z,2021-05-26T16:42:33Z,http://arxiv.org/abs/2105.12676v1,http://arxiv.org/pdf/2105.12676v1,"cs.LG, cs.AR, cs.IR, cs.NA, cs.PF, math.NA"
Enabling Cell-Free Massive MIMO Systems with Wireless Millimeter Wave   Fronthaul,"Umut Demirhan, Ahmed Alkhateeb","Cell-free massive MIMO systems have promising data rate and uniform coverage gains. These systems, however, typically rely on optical fiber based fronthaul for the communication between the central processing unit (CPU) and the distributed access points (APs), which increases the infrastructure cost, leads to high installation time, and limits the deployment flexibility and adaptability. To address these challenges, this paper proposes two architectures for cell-free massive MIMO systems based on wireless fronthaul that is operating at a \textit{higher-band} compared to the access links: (i) A wireless-only fronthaul architecture where the CPU has a wireless fronthaul link to each AP, and (ii) a mixed-fronthaul architecture where the CPU has a wireless link to each cluster of APs that are connected together via optical fibers. These dual-band architectures ensure high-data rate fronthaul and provide high capability to synchronize the distributed APs. Further, the wireless fronthaul reduces the infrastructure cost and installation time, and enhances the flexibility, adaptability, and scalability of the network deployment. To investigate the achievable data rates with the proposed architectures, we formulate the end-to-end data rate optimization problem accounting for the various practical aspects of the fronthaul and access links. Then, we develop a low-complexity yet efficient joint beamforming and resource allocation solution for the proposed architectures based on user-centric AP grouping. With this solution, we show that the proposed architectures can achieve comparable data rates to those obtained with optical fiber-based fronthaul under realistic assumptions on the fronthaul bandwidth, hardware constraints, and deployment scenarios. This highlights a promising path for realizing the cell-free massive MIMO gains in practice while reducing the infrastructure and deployment overhead.",2021-10-05T03:20:09Z,2022-08-08T01:13:52Z,http://arxiv.org/abs/2110.01798v3,http://arxiv.org/pdf/2110.01798v3,"cs.IT, eess.SP, math.IT"
A Timing-Based Framework for Designing Resilient Cyber-Physical Systems   under Safety Constraint,"Abdullah Al Maruf, Luyao Niu, Andrew Clark, J. Sukarno Mertoguno, Radha Poovendran","Cyber-physical systems (CPS) are required to satisfy safety constraints in various application domains such as robotics, industrial manufacturing systems, and power systems. Faults and cyber attacks have been shown to cause safety violations, which can damage the system and endanger human lives. Resilient architectures have been proposed to ensure safety of CPS under such faults and attacks via methodologies including redundancy and restarting from safe operating conditions. The existing resilient architectures for CPS utilize different mechanisms to guarantee safety, and currently there is no approach to compare them. Moreover, the analysis and design undertaken for CPS employing one architecture is not readily extendable to another. In this paper, we propose a timing-based framework for CPS employing various resilient architectures and develop a common methodology for safety analysis and computation of control policies and design parameters. Using the insight that the cyber subsystem operates in one out of a finite number of statuses, we first develop a hybrid system model that captures CPS adopting any of these architectures. Based on the hybrid system, we formulate the problem of joint computation of control policies and associated timing parameters for CPS to satisfy a given safety constraint and derive sufficient conditions for the solution. Utilizing the derived conditions, we provide an algorithm to compute control policies and timing parameters relevant to the employed architecture. We also note that our solution can be applied to a wide class of CPS with polynomial dynamics and also allows incorporation of new architectures. We verify our proposed framework by performing a case study on adaptive cruise control of vehicles.",2022-08-30T14:01:48Z,2022-09-01T17:30:14Z,http://arxiv.org/abs/2208.14282v2,http://arxiv.org/pdf/2208.14282v2,"eess.SY, cs.SY"
Empowering the 6G Cellular Architecture with Open RAN,"Michele Polese, Mischa Dohler, Falko Dressler, Melike Erol-Kantarci, Rittwik Jana, Raymond Knopp, Tommaso Melodia","Innovation and standardization in 5G have brought advancements to every facet of the cellular architecture. This ranges from the introduction of new frequency bands and signaling technologies for the radio access network (RAN), to a core network underpinned by micro-services and network function virtualization (NFV). However, like any emerging technology, the pace of real-world deployments does not instantly match the pace of innovation. To address this discrepancy, one of the key aspects under continuous development is the RAN with the aim of making it more open, adaptive, functional, and easy to manage. In this paper, we highlight the transformative potential of embracing novel cellular architectures by transitioning from conventional systems to the progressive principles of Open RAN. This promises to make 6G networks more agile, cost-effective, energy-efficient, and resilient. It opens up a plethora of novel use cases, ranging from ubiquitous support for autonomous devices to cost-effective expansions in regions previously underserved. The principles of Open RAN encompass: (i) a disaggregated architecture with modular and standardized interfaces; (ii) cloudification, programmability and orchestration; and (iii) AI-enabled data-centric closed-loop control and automation. We first discuss the transformative role Open RAN principles have played in the 5G era. Then, we adopt a system-level approach and describe how these Open RAN principles will support 6G RAN and architecture innovation. We qualitatively discuss potential performance gains that Open RAN principles yield for specific 6G use cases. For each principle, we outline the steps that research, development and standardization communities ought to take to make Open RAN principles central to next-generation cellular network designs.",2023-12-05T13:20:04Z,2023-12-05T13:20:04Z,http://arxiv.org/abs/2312.02746v1,http://arxiv.org/pdf/2312.02746v1,"cs.NI, eess.SP"
Nuclei Instance Segmentation of Cryosectioned H&E Stained Histological   Images using Triple U-Net Architecture,"Zarif Ahmed, Chowdhury Nur E Alam Siddiqi, Fardifa Fathmiul Alam, Tasnim Ahmed, Tareque Mohmud Chowdhury","Nuclei instance segmentation is crucial in oncological diagnosis and cancer pathology research. H&E stained images are commonly used for medical diagnosis, but pre-processing is necessary before using them for image processing tasks. Two principal pre-processing methods are formalin-fixed paraffin-embedded samples (FFPE) and frozen tissue samples (FS). While FFPE is widely used, it is time-consuming, while FS samples can be processed quickly. Analyzing H&E stained images derived from fast sample preparation, staining, and scanning can pose difficulties due to the swift process, which can result in the degradation of image quality. This paper proposes a method that leverages the unique optical characteristics of H&E stained images. A three-branch U-Net architecture has been implemented, where each branch contributes to the final segmentation results. The process includes applying watershed algorithm to separate overlapping regions and enhance accuracy. The Triple U-Net architecture comprises an RGB branch, a Hematoxylin branch, and a Segmentation branch. This study focuses on a novel dataset named CryoNuSeg. The results obtained through robust experiments outperform the state-of-the-art results across various metrics. The benchmark score for this dataset is AJI 52.5 and PQ 47.7, achieved through the implementation of U-Net Architecture. However, the proposed Triple U-Net architecture achieves an AJI score of 67.41 and PQ of 50.56. The proposed architecture improves more on AJI than other evaluation metrics, which further justifies the superiority of the Triple U-Net architecture over the baseline U-Net model, as AJI is a more strict evaluation metric. The use of the three-branch U-Net model, followed by watershed post-processing, significantly surpasses the benchmark scores, showing substantial improvement in the AJI score",2024-04-19T16:36:21Z,2024-04-19T16:36:21Z,http://arxiv.org/abs/2404.12986v1,http://arxiv.org/pdf/2404.12986v1,"eess.IV, cs.CV"
FlexiBit: Fully Flexible Precision Bit-parallel Accelerator Architecture   for Arbitrary Mixed Precision AI,"Faraz Tahmasebi, Yian Wang, Benji Y. H. Huang, Hyoukjun Kwon","Recent research has shown that large language models (LLMs) can utilize low-precision floating point (FP) quantization to deliver high efficiency while maintaining original model accuracy. In particular, recent works have shown the effectiveness of non-power-of-two precisions, such as FP6 and FP5, and diverse sensitivity to low-precision arithmetic of LLM layers, which motivates mixed precision arithmetic including non-power-of-two precisions in LLMs. Although low-precision algorithmically leads to low computational overheads, such benefits cannot be fully exploited due to hardware constraints that support a limited set of power-of-two precisions (e.g., FP8, 16, 32, and 64 in NVIDIA H100 Tensor Core). In addition, the hardware compute units are designed to support standard formats (e.g., E4M3 and E5M2 for FP8). Such practices require re-designing the hardware whenever new precision and format emerge, which leads to high hardware replacement costs to exploit the benefits of new precisions and formats. Therefore, in this paper, we propose a new accelerator architecture, FlexiBit, which efficiently supports FP and INT arithmetic in arbitrary precisions and formats. Unlike previous bit-serial designs, which also provide flexibility but at the cost of performance due to its bit-wise temporal processing nature, FlexiBit's architecture enables bit-parallel processing of any precision and format without compute unit underutilization. FlexiBit's new capability to exploit non-power of two precision and format led to 1.66x and 1.62x higher performance per area on GPT-3 in FP6 targeting a cloud-scale accelerator, compared to a Tensor Core-like architecture and a state-of-the-art bit-parallel flexible precision accelerator, BitFusion, respectively. Also, the bit-parallel nature of FlexiBit's architecture led to 3.9x higher performance/area compared to a state-of-the-art bit-serial architecture.",2024-11-27T05:16:26Z,2024-11-27T05:16:26Z,http://arxiv.org/abs/2411.18065v1,http://arxiv.org/pdf/2411.18065v1,"cs.AR, C.1.3; C.1.4; C.3; I.2"
Beyond Diagonal RIS in Multiuser MIMO: Graph Theoretic Modeling and   Optimal Architectures with Low Complexity,"Zheyu Wu, Bruno Clerckx","Reconfigurable intelligent surfaces (RIS) is regarded as a key enabler of wave/analog-domain beamforming, processing, and computing in future wireless communication systems. Recently, Beyond Diagonal RIS (BD-RIS) has been proposed as a generalization of conventional RIS, offering enhanced design flexibility thanks to the presence of tunable impedances that connect RIS elements. However, increased interconnections lead to high circuit complexity, which poses a significant practical challenge. In this paper, we address the fundamental open question: What is the class of BD-RIS architectures that achieves the optimal performance in a RIS-aided multiuser multi-input multi-output (MIMO) system? By modeling BD-RIS architectures using graph theory, we identify a class of BD-RIS architectures that achieves the optimal performance--matching that of fully-connected RIS--while maintaining low circuit complexity. Our result holds for a broad class of performance metrics, including the commonly used sum channel gain/sum-rate/energy efficiency maximization, transmit power minimization, and the information-theoretic capacity region. The number of tunable impedances in the proposed class is ${O}(N\min\{D,N/2\})$, where $N$ denotes the number of RIS elements and $D$ is the degree of freedom of the multiuser MIMO channel, i.e., the minimum between the number of transmit antennas and the total number of received antennas across all users. Since $D$ is much smaller than $N$ in practice, the complexity scales as ${O}(ND)$, which is substantially lower than the ${O}(N^2)$ complexity of fully-connected RIS. We further introduce two novel BD-RIS architectures--band-connected RIS and stem-connected RIS--and show that they belong to the optimal architecture class under certain conditions. Simulation results validate the optimality and enhanced performance-complexity tradeoff of our proposed architecture.",2025-02-23T09:05:26Z,2025-02-23T09:05:26Z,http://arxiv.org/abs/2502.16509v1,http://arxiv.org/pdf/2502.16509v1,"cs.IT, eess.SP, math.IT"
Pattern Reification as the Basis for Description-Driven Systems,"Florida Estrella, Zsolt Kovacs, Jean-Marie Le Goff, Richard McClatchey, Tony Solomonides, Norbert Toth","One of the main factors driving object-oriented software development for information systems is the requirement for systems to be tolerant to change. To address this issue in designing systems, this paper proposes a pattern-based, object-oriented, description-driven system (DDS) architecture as an extension to the standard UML four-layer meta-model. A DDS architecture is proposed in which aspects of both static and dynamic systems behavior can be captured via descriptive models and meta-models. The proposed architecture embodies four main elements - firstly, the adoption of a multi-layered meta-modeling architecture and reflective meta-level architecture, secondly the identification of four data modeling relationships that can be made explicit such that they can be modified dynamically, thirdly the identification of five design patterns which have emerged from practice and have proved essential in providing reusable building blocks for data management, and fourthly the encoding of the structural properties of the five design patterns by means of one fundamental pattern, the Graph pattern. A practical example of this philosophy, the CRISTAL project, is used to demonstrate the use of description-driven data objects to handle system evolution.",2004-02-12T14:25:14Z,2004-02-12T14:25:14Z,http://arxiv.org/abs/cs/0402024v1,http://arxiv.org/pdf/cs/0402024v1,"cs.DB, cs.SE, H2.4,J.3"
VXA: A Virtual Architecture for Durable Compressed Archives,Bryan Ford,"Data compression algorithms change frequently, and obsolete decoders do not always run on new hardware and operating systems, threatening the long-term usability of content archived using those algorithms. Re-encoding content into new formats is cumbersome, and highly undesirable when lossy compression is involved. Processor architectures, in contrast, have remained comparatively stable over recent decades. VXA, an archival storage system designed around this observation, archives executable decoders along with the encoded content it stores. VXA decoders run in a specialized virtual machine that implements an OS-independent execution environment based on the standard x86 architecture. The VXA virtual machine strictly limits access to host system services, making decoders safe to run even if an archive contains malicious code. VXA's adoption of a ""native"" processor architecture instead of type-safe language technology allows reuse of existing ""hand-optimized"" decoders in C and assembly language, and permits decoders access to performance-enhancing architecture features such as vector processing instructions. The performance cost of VXA's virtualization is typically less than 15% compared with the same decoders running natively. The storage cost of archived decoders, typically 30-130KB each, can be amortized across many archived files sharing the same compression method.",2006-03-18T16:31:33Z,2006-03-18T16:31:33Z,http://arxiv.org/abs/cs/0603073v1,http://arxiv.org/pdf/cs/0603073v1,"cs.DL, cs.IR, H.3.7; H.1.1; D.4.5; E.5"
Circuit Design for A Measurement-Based Quantum Carry-Lookahead Adder,"Agung Trisetyarso, Rodney Van Meter","We present the design and evaluation of a quantum carry-lookahead adder (QCLA) using measurement-based quantum computation (MBQC), called MBQCLA. QCLA was originally designed for an abstract, concurrent architecture supporting long-distance communication, but most realistic architectures heavily constrain communication distances. The quantum carry-lookahead adder is faster than a quantum ripple-carry adder; QCLA has logarithmic depth while ripple adders have linear depth. MBQCLA utilizes MBQC's ability to transfer quantum states in unit time to accelerate addition. MBQCLA breaks the latency limit of addition circuits in nearest neighbor-only architectures : compared to the $\Theta(n)$ limit on circuit depth for linear nearest-neighbor architectures, it can reach $\Theta(log n)$ depth. MBQCLA is an order of magnitude faster than a ripple-carry adder when adding registers longer than 100 qubits, but requires a cluster state that is an order of magnitude larger. The cluster state resources can be classified as computation and communication; for the unoptimized form, $\approx$ 88 % of the resources are used for communication. Hand optimization of horizontal communication costs results in a $\approx$ 12% reduction in spatial resources for the in-place MBQCLA circuit. For comparison, a graph state quantum carry-lookahead adder (GSQCLA) uses only $\approx$ 9 % of the spatial resources of the MBQCLA.",2009-03-04T12:32:01Z,2009-10-14T04:45:05Z,http://arxiv.org/abs/0903.0748v2,http://arxiv.org/pdf/0903.0748v2,"quant-ph, cs.AR"
Scale Invariance of Immune System Response Rates and Times: Perspectives   on Immune System Architecture and Implications for Artificial Immune Systems,"Soumya Banerjee, Melanie Moses","Most biological rates and times decrease systematically with organism body size. We use an ordinary differential equation (ODE) model of West Nile Virus in birds to show that pathogen replication rates decline with host body size, but natural immune system (NIS) response rates do not change systematically with body size. This is surprising since the NIS has to search for small quantities of pathogens through larger physical spaces in larger organisms, and also respond by producing larger absolute quantities of antibody in larger organisms. We call this scale-invariant detection and response. We hypothesize that the NIS has evolved an architecture to efficiently neutralize pathogens. We investigate a range of architectures using an Agent Based Model (ABM). We find that a sub-modular NIS architecture, in which lymph node number and size both increase sublinearly with body size, efficiently balances the tradeoff between local pathogen detection and global response using antibodies. This leads to nearly scale-invariant detection and response, consistent with experimental data. Similar to the NIS, physical space and resources are also important constraints on Artificial Immune Systems (AIS), especially distributed systems applications used to connect low-powered sensors using short-range wireless communication. We show that AIS problems, like distributed robot control, will also require a sub-modular architecture to efficiently balance the tradeoff between local search for a solution and global response or proliferation of the solution between different components. This research has wide applicability in other distributed systems AIS applications.",2010-08-08T04:47:36Z,2010-08-08T04:47:36Z,http://arxiv.org/abs/1008.1380v1,http://arxiv.org/pdf/1008.1380v1,"q-bio.QM, cs.DC, math.OC"
Load Distribution Composite Design Pattern for Genetic Algorithm-Based   Autonomic Computing Systems,"Vishnuvardhan Mannava, T. Ramesh","Current autonomic computing systems are ad hoc solutions that are designed and implemented from the scratch. When designing software, in most cases two or more patterns are to be composed to solve a bigger problem. A composite design patterns shows a synergy that makes the composition more than just the sum of its parts which leads to ready-made software architectures. As far as we know, there are no studies on composition of design patterns for autonomic computing domain. In this paper we propose pattern-oriented software architecture for self-optimization in autonomic computing system using design patterns composition and multi objective evolutionary algorithms that software designers and/or programmers can exploit to drive their work. Main objective of the system is to reduce the load in the server by distributing the population to clients. We used Case Based Reasoning, Database Access, and Master Slave design patterns. We evaluate the effectiveness of our architecture with and without design patterns compositions. The use of composite design patterns in the architecture and quantitative measurements are presented. A simple UML class diagram is used to describe the architecture.",2012-09-08T17:39:46Z,2012-09-08T17:39:46Z,http://arxiv.org/abs/1209.1734v1,http://arxiv.org/pdf/1209.1734v1,"cs.SE, cs.DC, cs.NE, D.2.11; D.2.10; D.3.3; I.2.8"
Parallel Interleaver Design for a High Throughput HSPA+/LTE   Multi-Standard Turbo Decoder,"Guohui Wang, Hao Shen, Yang Sun, Joseph R. Cavallaro, Aida Vosoughi, Yuanbin Guo","To meet the evolving data rate requirements of emerging wireless communication technologies, many parallel architectures have been proposed to implement high throughput turbo decoders. However, concurrent memory reading/writing in parallel turbo decoding architectures leads to severe memory conflict problem, which has become a major bottleneck for high throughput turbo decoders. In this paper, we propose a flexible and efficient VLSI architecture to solve the memory conflict problem for highly parallel turbo decoders targeting multi-standard 3G/4G wireless communication systems. To demonstrate the effectiveness of the proposed parallel interleaver architecture, we implemented an HSPA+/LTE/LTE-Advanced multi-standard turbo decoder with a 45nm CMOS technology. The implemented turbo decoder consists of 16 Radix-4 MAP decoder cores, and the chip core area is 2.43 mm^2. When clocked at 600 MHz, this turbo decoder can achieve a maximum decoding throughput of 826 Mbps in the HSPA+ mode and 1.67 Gbps in the LTE/LTE-Advanced mode, exceeding the peak data rate requirements for both standards.",2014-03-15T06:23:28Z,2014-03-26T04:48:54Z,http://arxiv.org/abs/1403.3759v3,http://arxiv.org/pdf/1403.3759v3,"cs.IT, cs.AR, cs.DC, math.IT, C.2.1; B.7.1; E.4; C.1.4; C.1.2"
A Percolation based M2M Networking Architecture for Data Transmission   and Routing,"Jihua Lu, Jianping An, Xiangming Li, Jie Yang, Lei Yang","We propose a percolation based M2M networking architecture and its data transmission method. The proposed network architecture can be server-free and router-free, which allows us to operate routing efficiently with percolations based on six degrees of separation theory in small world network modeling. The data transmission can be divided into two phases: routing and data transmission phases. In the routing phase, probe packets will be transmitted and forwarded in the network thus multiple paths are selected and performed based on the constriction of the maximum hop number. In the second phase, the information will be encoded, say, with the fountain codes, and transmitted using the paths generated in the first phase. In such a way, an efficient routing and data transmission mechanism can be built, which allow us to construct a low-cost, flexible and ubiquitous network. Such a networking architecture and data transmission can be used in many M2M communications, such as the stub network of internet of things, and deep space networking, and so on.",2014-03-31T18:47:17Z,2014-05-08T11:48:38Z,http://arxiv.org/abs/1403.8123v2,http://arxiv.org/pdf/1403.8123v2,"cs.NI, 94C30"
Regularization for Design,"Nikolai Matni, Venkat Chandrasekaran","When designing controllers for large-scale systems, the architectural aspects of the controller such as the placement of actuators, sensors, and the communication links between them can no longer be taken as given. The task of designing this architecture is now as important as the design of the control laws themselves. By interpreting controller synthesis (in a model matching setup) as the solution of a particular linear inverse problem, we view the challenge of obtaining a controller with a desired architecture as one of finding a structured solution to an inverse problem. Building on this conceptual connection, we formulate and analyze a framework called \textit{Regularization for Design (RFD)}, in which we augment the variational formulations of controller synthesis problems with convex penalty functions that induce a desired controller architecture. The resulting regularized formulations are convex optimization problems that can be solved efficiently, these convex programs provide a unified computationally tractable approach for the simultaneous co-design of a structured optimal controller and the actuation, sensing and communication architecture required to implement it. Further, these problems are natural control-theoretic analogs of prominent approaches such as the Lasso, the Group Lasso, the Elastic Net, and others that are employed in statistical modeling. In analogy to that literature, we show that our approach identifies optimally structured controllers under a suitable condition on a ""signal-to-noise"" type ratio.",2014-04-08T00:00:54Z,2015-08-02T18:51:03Z,http://arxiv.org/abs/1404.1972v3,http://arxiv.org/pdf/1404.1972v3,"math.OC, cs.SY"
Towards Automating the Construction & Maintenance of Attack Trees: a   Feasibility Study,Stéphane Paul,"Security risk management can be applied on well-defined or existing systems; in this case, the objective is to identify existing vulnerabilities, assess the risks and provide for the adequate countermeasures. Security risk management can also be applied very early in the system's development life-cycle, when its architecture is still poorly defined; in this case, the objective is to positively influence the design work so as to produce a secure architecture from the start. The latter work is made difficult by the uncertainties on the architecture and the multiple round-trips required to keep the risk assessment study and the system architecture aligned. This is particularly true for very large projects running over many years. This paper addresses the issues raised by those risk assessment studies performed early in the system's development life-cycle. Based on industrial experience, it asserts that attack trees can help solve the human cognitive scalability issue related to securing those large, continuously-changing system-designs. However, big attack trees are difficult to build, and even more difficult to maintain. This paper therefore proposes a systematic approach to automate the construction and maintenance of such big attack trees, based on the system's operational and logical architectures, the system's traditional risk assessment study and a security knowledge database.",2014-04-08T01:46:40Z,2014-04-08T01:46:40Z,http://arxiv.org/abs/1404.1986v1,http://arxiv.org/pdf/1404.1986v1,"cs.CR, K6.5"
Enhanced Multi-Parameter Cognitive Architecture for Future Wireless   Communications,"Kaiqing Zhang, Feifei Gao, Qihui Wu","The very original concept of cognitive radio (CR) raised by Mitola targets at all the environment parameters, including those in physical layer, MAC layer, application layer as well as the information extracted from reasoning. Hence the first CR is also referred to as ""full cognitive radio"". However, due to its difficult implementation, FCC and Simon Haykin separately proposed a much more simplified definition, in which CR mainly detects one single parameter, i.e., spectrum occupancy, and is also called as ""spectrum sensing cognitive radio"". With the rapid development of wireless communication, the infrastructure of a wireless system becomes much more complicated while the functionality at every node is desired to be as intelligent as possible, say the self-organized capability in the approaching 5G cellular networks. It is then interesting to re-look into Mitola's definition and think whether one could, besides obtaining the ""on/off"" status of the licensed user only, achieve more parameters in a cognitive way. In this article, we propose a new cognitive architecture targeting at multiple parameters in future cellular networks, which is a one step further towards the ""full cognition"" compared to the most existing CR research. The new architecture is elaborated in detailed stages, and three representative examples are provided based on the recent research progress to illustrate the feasibility as well as the validity of the proposed architecture.",2014-11-22T16:07:29Z,2015-03-06T12:38:10Z,http://arxiv.org/abs/1411.6137v2,http://arxiv.org/pdf/1411.6137v2,"cs.IT, cs.NI, math.IT"
Flexible Queueing Architectures,"John N. Tsitsiklis, Kuang Xu","We study a multi-server model with $n$ flexible servers and $n$ queues, connected through a bipartite graph, where the level of flexibility is captured by the graph's average degree, $d_n$. Applications in content replication in data centers, skill-based routing in call centers, and flexible supply chains are among our main motivations.   We focus on the scaling regime where the system size $n$ tends to infinity, while the overall traffic intensity stays fixed. We show that a large capacity region and an asymptotically vanishing queueing delay are simultaneously achievable even under limited flexibility ($d_n \ll n$). Our main results demonstrate that, when $d_n\gg \ln n$, a family of expander-graph-based flexibility architectures has a capacity region that is within a constant factor of the maximum possible, while simultaneously ensuring a diminishing queueing delay for all arrival rate vectors in the capacity region. Our analysis is centered around a new class of virtual-queue-based scheduling policies that rely on dynamically constructed job-to-server assignments on the connectivity graph. For comparison, we also analyze a natural family of modular architectures, which is simpler but has provably weaker performance.",2015-05-28T11:25:04Z,2017-02-06T08:44:35Z,http://arxiv.org/abs/1505.07648v2,http://arxiv.org/pdf/1505.07648v2,"math.PR, cs.NI, cs.PF"
Optimized Configurable Architectures for Scalable Soft-Input Soft-Output   MIMO Detectors with 256-QAM,"Mohammad M. Mansour, Louay M. A. Jalloul","This paper presents an optimized low-complexity and high-throughput multiple-input multiple-output (MIMO) signal detector core for detecting spatially-multiplexed data streams. The core architecture supports various layer configurations up to 4, while achieving near-optimal performance, as well as configurable modulation constellations up to 256-QAM on each layer. The core is capable of operating as a soft-input soft-output log-likelihood ratio (LLR) MIMO detector which can be used in the context of iterative detection and decoding. High area-efficiency is achieved via algorithmic and architectural optimizations performed at two levels. First, distance computations and slicing operations for an optimal 2-layer maximum a posteriori (MAP) MIMO detector are optimized to eliminate the use of multipliers and reduce the overhead of slicing in the presence of soft-input LLRs. We show that distances can be easily computed using elementary addition operations, while optimal slicing is done via efficient comparisons with soft decision boundaries, resulting in a simple feed-forward pipelined architecture. Second, to support more layers, an efficient channel decomposition scheme is presented that reduces the detection of multiple layers into multiple 2-layer detection subproblems, which map onto the 2-layer core with a slight modification using a distance accumulation stage and a post-LLR processing stage. Various architectures are accordingly developed to achieve a desired detection throughput and run-time reconfigurability by time-multiplexing of one or more component cores. The proposed core is applied as well to design an optimal multi-user MIMO detector for LTE. The core occupies an area of 1.58MGE and achieves a throughput of 733 Mbps for 256-QAM when synthesized in 90 nm CMOS.",2015-06-08T07:50:48Z,2015-06-08T07:50:48Z,http://arxiv.org/abs/1506.04644v1,http://arxiv.org/pdf/1506.04644v1,"cs.IT, math.IT"
"On higher order computations, rewiring the connectome, and non-von   Neumann computer architecture",Stanislaw Ambroszkiewicz,"Structural plasticity in the brain (i.e. rewiring the connectome) may be viewed as mechanisms for dynamic reconfiguration of neural circuits. First order computations in the brain are done by static neural circuits, whereas higher order computations are done by dynamic reconfigurations of the links (synapses) between the neural circuits. Static neural circuits correspond to first order computable functions. Synapse creation (activation) between them correspond to the mathematical notion of function composition. Functionals are higher order functions that take functions as their arguments. The construction of functionals is based on dynamic reconfigurations of function compositions. Perhaps the functionals correspond to rewiring mechanisms of the connectome. The architecture of human mind is different than the von Neumann computer architecture. Higher order computations in the human brain (based on functionals) may suggest a non-von Neumann computer architecture, a challenge posed by John Backus in 1977 \cite{Backus}. The presented work is a substantial extension and revision of the paper published in Proc. ICANN2016.",2016-03-07T20:10:16Z,2020-10-03T21:31:08Z,http://arxiv.org/abs/1603.02238v4,http://arxiv.org/pdf/1603.02238v4,"cs.NE, q-bio.NC, 92B20, F.1.1"
Channel Estimation for Hybrid Architecture Based Wideband Millimeter   Wave Systems,"Kiran Venugopal, Ahmed Alkhateeb, Nuria González Prelcic, Robert W. Heath Jr","Hybrid analog and digital precoding allows millimeter wave (mmWave) systems to achieve both array and multiplexing gain. The design of the hybrid precoders and combiners, though, is usually based on knowledge of the channel. Prior work on mmWave channel estimation with hybrid architectures focused on narrowband channels. Since mmWave systems will be wideband with frequency selectivity, it is vital to develop channel estimation solutions for hybrid architectures based wideband mmWave systems. In this paper, we develop a sparse formulation and compressed sensing based solutions for the wideband mmWave channel estimation problem for hybrid architectures. First, we leverage the sparse structure of the frequency selective mmWave channels and formulate the channel estimation problem as a sparse recovery in both time and frequency domains. Then, we propose explicit channel estimation techniques for purely time or frequency domains and for combined time/frequency domains. Our solutions are suitable for both SC-FDE and OFDM systems. Simulation results show that the proposed solutions achieve good channel estimation quality, while requiring small training overhead. Leveraging the hybrid architecture at the transceivers gives further improvement in estimation error performance and achievable rates.",2016-11-09T19:03:30Z,2016-11-13T22:00:06Z,http://arxiv.org/abs/1611.03046v2,http://arxiv.org/pdf/1611.03046v2,"cs.IT, math.IT"
Capacity and Trainability in Recurrent Neural Networks,"Jasmine Collins, Jascha Sohl-Dickstein, David Sussillo","Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.",2016-11-29T22:13:20Z,2017-03-03T17:39:34Z,http://arxiv.org/abs/1611.09913v3,http://arxiv.org/pdf/1611.09913v3,"stat.ML, cs.AI, cs.LG, cs.NE"
A 588 Gbps LDPC Decoder Based on Finite-Alphabet Message Passing,"Reza Ghanaatian, Alexios Balatsoukas-Stimming, Christoph Muller, Michael Meidlinger, Gerald Matz, Adam Teman, Andreas Burg","An ultra-high throughput low-density parity check (LDPC) decoder with an unrolled full-parallel architecture is proposed, which achieves the highest decoding throughput compared to previously reported LDPC decoders in the literature. The decoder benefits from a serial message-transfer approach between the decoding stages to alleviate the well-known routing congestion problem in parallel LDPC decoders. Furthermore, a finite-alphabet message passing algorithm is employed to replace the variable node update rule of the standard min-sum decoder with look-up tables, which are designed in a way that maximizes the mutual information between decoding messages. The proposed algorithm results in an architecture with reduced bit-width messages, leading to a significantly higher decoding throughput and to a lower area as compared to a min-sum decoder when serial message-transfer is used. The architecture is placed and routed for the standard min-sum reference decoder and for the proposed finite-alphabet decoder using a custom pseudo-hierarchical backend design strategy to further alleviate routing congestions and to handle the large design. Post-layout results show that the finite-alphabet decoder with the serial message-transfer architecture achieves a throughput as large as 588 Gbps with an area of 16.2 mm$^2$ and dissipates an average power of 22.7 pJ per decoded bit in a 28 nm FD-SOI library. Compared to the reference min-sum decoder, this corresponds to 3.1 times smaller area and 2 times better energy efficiency.",2017-03-16T18:01:55Z,2017-12-30T20:34:46Z,http://arxiv.org/abs/1703.05769v2,http://arxiv.org/pdf/1703.05769v2,"cs.AR, cs.IT, math.IT"
DeepArchitect: Automatically Designing and Training Deep Architectures,"Renato Negrinho, Geoff Gordon","In deep learning, performance is strongly affected by the choice of architecture and hyperparameters. While there has been extensive work on automatic hyperparameter optimization for simple spaces, complex spaces such as the space of deep architectures remain largely unexplored. As a result, the choice of architecture is done manually by the human expert through a slow trial and error process guided mainly by intuition. In this paper we describe a framework for automatically designing and training deep models. We propose an extensible and modular language that allows the human expert to compactly represent complex search spaces over architectures and their hyperparameters. The resulting search spaces are tree-structured and therefore easy to traverse. Models can be automatically compiled to computational graphs once values for all hyperparameters have been chosen. We can leverage the structure of the search space to introduce different model search algorithms, such as random search, Monte Carlo tree search (MCTS), and sequential model-based optimization (SMBO). We present experiments comparing the different algorithms on CIFAR-10 and show that MCTS and SMBO outperform random search. In addition, these experiments show that our framework can be used effectively for model discovery, as it is possible to describe expressive search spaces and discover competitive models without much effort from the human expert. Code for our framework and experiments has been made publicly available.",2017-04-28T02:48:38Z,2017-04-28T02:48:38Z,http://arxiv.org/abs/1704.08792v1,http://arxiv.org/pdf/1704.08792v1,"stat.ML, cs.LG"
Stable Architectures for Deep Neural Networks,"Eldad Haber, Lars Ruthotto","Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks.   The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.",2017-05-09T14:13:18Z,2019-02-16T16:13:21Z,http://arxiv.org/abs/1705.03341v3,http://arxiv.org/pdf/1705.03341v3,"cs.LG, cs.NA, math.NA, math.OC, 68T05, 65L09, 49N90, I.2.6"
Convolutional Neural Network Architectures for Signals Supported on   Graphs,"Fernando Gama, Antonio G. Marques, Geert Leus, Alejandro Ribeiro","Two architectures that generalize convolutional neural networks (CNNs) for the processing of signals supported on graphs are introduced. We start with the selection graph neural network (GNN), which replaces linear time invariant filters with linear shift invariant graph filters to generate convolutional features and reinterprets pooling as a possibly nonlinear subsampling stage where nearby nodes pool their information in a set of preselected sample nodes. A key component of the architecture is to remember the position of sampled nodes to permit computation of convolutional features at deeper layers. The second architecture, dubbed aggregation GNN, diffuses the signal through the graph and stores the sequence of diffused components observed by a designated node. This procedure effectively aggregates all components into a stream of information having temporal structure to which the convolution and pooling stages of regular CNNs can be applied. A multinode version of aggregation GNNs is further introduced for operation in large scale graphs. An important property of selection and aggregation GNNs is that they reduce to conventional CNNs when particularized to time signals reinterpreted as graph signals in a circulant graph. Comparative numerical analyses are performed in a source localization application over synthetic and real-world networks. Performance is also evaluated for an authorship attribution problem and text category classification. Multinode aggregation GNNs are consistently the best performing GNN architecture.",2018-05-01T03:04:31Z,2018-12-07T01:17:25Z,http://arxiv.org/abs/1805.00165v2,http://arxiv.org/pdf/1805.00165v2,"eess.SP, cs.AI, cs.LG, stat.ML"
Low-Cost Parameterizations of Deep Convolutional Neural Networks,"Eran Treister, Lars Ruthotto, Michal Sharoni, Sapir Zafrani, Eldad Haber","Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities. Commonly, the convolution operators couple features from all channels. For wide networks, this leads to immense computational cost in the training of and prediction with CNNs. In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity. We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN. Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties. Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs. Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators. We outline in our experiments that the proposed architectures, although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.",2018-05-20T20:17:47Z,2018-10-03T06:10:32Z,http://arxiv.org/abs/1805.07821v3,http://arxiv.org/pdf/1805.07821v3,"cs.NA, math.NA"
Deep Neural Networks with Multi-Branch Architectures Are Less Non-Convex,"Hongyang Zhang, Junru Shao, Ruslan Salakhutdinov","Several recently proposed architectures of neural networks such as ResNeXt, Inception, Xception, SqueezeNet and Wide ResNet are based on the designing idea of having multiple branches and have demonstrated improved performance in many applications. We show that one cause for such success is due to the fact that the multi-branch architecture is less non-convex in terms of duality gap. The duality gap measures the degree of intrinsic non-convexity of an optimization problem: smaller gap in relative value implies lower degree of intrinsic non-convexity. The challenge is to quantitatively measure the duality gap of highly non-convex problems such as deep neural networks. In this work, we provide strong guarantees of this quantity for two classes of network architectures. For the neural networks with arbitrary activation functions, multi-branch architecture and a variant of hinge loss, we show that the duality gap of both population and empirical risks shrinks to zero as the number of branches increases. This result sheds light on better understanding the power of over-parametrization where increasing the network width tends to make the loss surface less non-convex. For the neural networks with linear activation function and $\ell_2$ loss, we show that the duality gap of empirical risk is zero. Our two results work for arbitrary depths and adversarial data, while the analytical techniques might be of independent interest to non-convex optimization more broadly. Experiments on both synthetic and real-world datasets validate our results.",2018-06-06T14:16:36Z,2018-06-21T15:51:46Z,http://arxiv.org/abs/1806.01845v2,http://arxiv.org/pdf/1806.01845v2,"cs.LG, stat.ML"
Novel Architecture of Pipeline Radix 2 power of 2 SDF FFT Based on   Digit-Slicing Technique,"Yazan Samir Algnabi, Furat A. Aldaamee, Rozita Teymourzadeh","The prevalent need for very high-speed digital signals processing in wireless communications has driven the communications system to high-performance levels. The objective of this paper is to propose a novel structure for efficient implementation for the Fast Fourier Transform (FFT) processor to meet the requirement for high-speed wireless communication system standards. Based on the algorithm, architecture analysis, the design of pipeline Radix 2power of 2 SDF FFT processor based on digit-slicing Multiplier-Less is proposed. Furthermore, this paper proposed an optimal constant multiplication arithmetic design to multiply a fixed point input selectively by one of the several present twiddle factor constants. The proposed architecture was simulated using MATLAB software and the Field Programmable Gate Array (FPGA) Virtex 4 was targeted to synthesis the proposed architecture. The design was tested in real hardware of TLA5201 logic analyzer and the ISE synthesis report results the high speed of 669.277 MHz with the total equivalent gate count of 14,854. Meanwhile, It can be found as significant improvement over Radix 22 DIF SDF FFT processor and can be concluded that the proposed pipeline Radix 22 DIF SDF FFT processor based on digit-slicing multiplier-less is an enable in solving problems that affect the most high-speed wireless communication systems capability in FFT and possesses huge potentials for future related works and research areas.",2018-06-10T06:05:40Z,2018-06-10T06:05:40Z,http://arxiv.org/abs/1806.04573v1,http://arxiv.org/pdf/1806.04573v1,"eess.SP, cs.AR"
From probabilistic graphical models to generalized tensor networks for   supervised learning,"Ivan Glasser, Nicola Pancotti, J. Ignacio Cirac","Tensor networks have found a wide use in a variety of applications in physics and computer science, recently leading to both theoretical insights as well as practical algorithms in machine learning. In this work we explore the connection between tensor networks and probabilistic graphical models, and show that it motivates the definition of generalized tensor networks where information from a tensor can be copied and reused in other parts of the network. We discuss the relationship between generalized tensor network architectures used in quantum physics, such as string-bond states, and architectures commonly used in machine learning. We provide an algorithm to train these networks in a supervised-learning context and show that they overcome the limitations of regular tensor networks in higher dimensions, while keeping the computation efficient. A method to combine neural networks and tensor networks as part of a common deep learning architecture is also introduced. We benchmark our algorithm for several generalized tensor network architectures on the task of classifying images and sounds, and show that they outperform previously introduced tensor-network algorithms. The models we consider also have a natural implementation on a quantum computer and may guide the development of near-term quantum machine learning architectures.",2018-06-15T13:47:50Z,2019-12-03T11:28:46Z,http://arxiv.org/abs/1806.05964v2,http://arxiv.org/pdf/1806.05964v2,"quant-ph, cond-mat.str-el, cs.LG, stat.ML"
Low Complexity Full Duplex MIMO: Novel Analog Cancellation Architectures   and Transceiver Design,"George C. Alexandropoulos, Melissa Duarte","Incorporating full duplex operation in Multiple Input Multiple Output (MIMO) systems provides the potential of boosting throughput performance. However, the hardware complexity of the analog self-interference canceller in emerging full duplex MIMO designs mostly scales with the number of transmit and receive antennas, thus exploiting the benefits of analog cancellation becomes impractical for full duplex MIMO transceivers, even for moderate number of antennas. In this paper, we present two novel architectures for the analog canceller comprising of reduced number of cancellation elements, compared to the state of the art, and simple multiplexers for efficient signal routing among the transmit and receive radio frequency chains. One architecture is based on analog taps (tap refers to a line of fixed delay, variable phase shifter, and attenuator) and the other on AUXiliary (AUX) Transmitters (TXs) that locally generate the cancellation signal. In contrast to the available analog cancellation architectures, the values for each tap or each AUX TX and the configuration of the multiplexers are jointly designed with the digital transmit and receive beamforming filters according to certain performance objectives. Focusing on a narrowband flat fading channel model as an example, we present a general optimization framework for the joint design of analog self-interference cancellation and digital beamforming. We also detail the sum rate optimization objective together with its derived solution for the latter architectural components. Representative computer simulation results demonstrate the superiority both in terms of hardware complexity and achievable performance of the proposed low complexity full duplex MIMO schemes over the lately available ones.",2018-09-22T09:10:49Z,2018-09-22T09:10:49Z,http://arxiv.org/abs/1809.09474v1,http://arxiv.org/pdf/1809.09474v1,"cs.IT, math.IT"
Efficient Hybrid Network Architectures for Extremely Quantized Neural   Networks Enabling Intelligence at the Edge,"Indranil Chakraborty, Deboleena Roy, Aayush Ankit, Kaushik Roy","The recent advent of `Internet of Things' (IOT) has increased the demand for enabling AI-based edge computing. This has necessitated the search for efficient implementations of neural networks in terms of both computations and storage. Although extreme quantization has proven to be a powerful tool to achieve significant compression over full-precision networks, it can result in significant degradation in performance. In this work, we propose extremely quantized hybrid network architectures with both binary and full-precision sections to emulate the classification performance of full-precision networks while ensuring significant energy efficiency and memory compression. We explore several hybrid network architectures and analyze the performance of the networks in terms of accuracy, energy efficiency and memory compression. We perform our analysis on ResNet and VGG network architectures. Among the proposed network architectures, we show that the hybrid networks with full-precision residual connections emerge as the optimum by attaining accuracies close to full-precision networks while achieving excellent memory compression, up to 21.8x in case of VGG-19. This work demonstrates an effective way of hybridizing networks which achieve performance close to full-precision networks while attaining significant compression, furthering the feasibility of using such networks for energy-efficient neural computing in IOT-based edge devices.",2019-02-01T17:16:05Z,2019-02-01T17:16:05Z,http://arxiv.org/abs/1902.00460v1,http://arxiv.org/pdf/1902.00460v1,"cs.LG, stat.ML"
Which Neural Network Architecture matches Human Behavior in Artificial   Grammar Learning?,"Andrea Alamia, Victor Gauducheau, Dimitri Paisios, Rufin VanRullen","In recent years artificial neural networks achieved performance close to or better than humans in several domains: tasks that were previously human prerogatives, such as language processing, have witnessed remarkable improvements in state of the art models. One advantage of this technological boost is to facilitate comparison between different neural networks and human performance, in order to deepen our understanding of human cognition. Here, we investigate which neural network architecture (feed-forward vs. recurrent) matches human behavior in artificial grammar learning, a crucial aspect of language acquisition. Prior experimental studies proved that artificial grammars can be learnt by human subjects after little exposure and often without explicit knowledge of the underlying rules. We tested four grammars with different complexity levels both in humans and in feedforward and recurrent networks. Our results show that both architectures can 'learn' (via error back-propagation) the grammars after the same number of training sequences as humans do, but recurrent networks perform closer to humans than feedforward ones, irrespective of the grammar complexity level. Moreover, similar to visual processing, in which feedforward and recurrent architectures have been related to unconscious and conscious processes, our results suggest that explicit learning is best modeled by recurrent architectures, whereas feedforward networks better capture the dynamics involved in implicit learning.",2019-02-13T11:02:39Z,2019-02-13T11:02:39Z,http://arxiv.org/abs/1902.04861v1,http://arxiv.org/pdf/1902.04861v1,"q-bio.NC, cs.HC"
Nonmodular architectures of cognitive systems based on active inference,"Manuel Baltieri, Christopher L. Buckley","In psychology and neuroscience it is common to describe cognitive systems as input/output devices where perceptual and motor functions are implemented in a purely feedforward, open-loop fashion. On this view, perception and action are often seen as encapsulated modules with limited interaction between them. While embodied and enactive approaches to cognitive science have challenged the idealisation of the brain as an input/output device, we argue that even the more recent attempts to model systems using closed-loop architectures still heavily rely on a strong separation between motor and perceptual functions. Previously, we have suggested that the mainstream notion of modularity strongly resonates with the separation principle of control theory. In this work we present a minimal model of a sensorimotor loop implementing an architecture based on the separation principle. We link this to popular formulations of perception and action in the cognitive sciences, and show its limitations when, for instance, external forces are not modelled by an agent. These forces can be seen as variables that an agent cannot directly control, i.e., a perturbation from the environment or an interference caused by other agents. As an alternative approach inspired by embodied cognitive science, we then propose a nonmodular architecture based on the active inference framework. We demonstrate the robustness of this architecture to unknown external inputs and show that the mechanism with which this is achieved in linear models is equivalent to integral control.",2019-03-22T15:00:25Z,2019-03-22T15:00:25Z,http://arxiv.org/abs/1903.09542v1,http://arxiv.org/pdf/1903.09542v1,"q-bio.NC, cs.AI"
V-NAS: Neural Architecture Search for Volumetric Medical Image   Segmentation,"Zhuotun Zhu, Chenxi Liu, Dong Yang, Alan Yuille, Daguang Xu","Deep learning algorithms, in particular 2D and 3D fully convolutional neural networks (FCNs), have rapidly become the mainstream methodology for volumetric medical image segmentation. However, 2D convolutions cannot fully leverage the rich spatial information along the third axis, while 3D convolutions suffer from the demanding computation and high GPU memory consumption. In this paper, we propose to automatically search the network architecture tailoring to volumetric medical image segmentation problem. Concretely, we formulate the structure learning as differentiable neural architecture search, and let the network itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each layer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas dataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon (MSD) Challenge. Our method, named V-NAS, consistently outperforms other state-of-the-arts on the segmentation task of both normal organ (NIH Pancreas) and abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the power of chosen architecture. Moreover, the searched architecture on one dataset can be well generalized to other datasets, which demonstrates the robustness and practical use of our proposed method.",2019-06-06T21:07:40Z,2019-08-04T22:04:33Z,http://arxiv.org/abs/1906.02817v2,http://arxiv.org/pdf/1906.02817v2,"eess.IV, cs.CV"
Hardware Aware Neural Network Architectures using FbNet,"Sai Vineeth Kalluru Srinivas, Harideep Nair, Vinay Vidyasagar","We implement a differentiable Neural Architecture Search (NAS) method inspired by FBNet for discovering neural networks that are heavily optimized for a particular target device. The FBNet NAS method discovers a neural network from a given search space by optimizing over a loss function which accounts for accuracy and target device latency. We extend this loss function by adding an energy term. This will potentially enhance the ``hardware awareness"" and help us find a neural network architecture that is optimal in terms of accuracy, latency and energy consumption, given a target device (Raspberry Pi in our case). We name our trained child architecture obtained at the end of search process as Hardware Aware Neural Network Architecture (HANNA). We prove the efficacy of our approach by benchmarking HANNA against two other state-of-the-art neural networks designed for mobile/embedded applications, namely MobileNetv2 and CondenseNet for CIFAR-10 dataset. Our results show that HANNA provides a speedup of about 2.5x and 1.7x, and reduces energy consumption by 3.8x and 2x compared to MobileNetv2 and CondenseNet respectively. HANNA is able to provide such significant speedup and energy efficiency benefits over the state-of-the-art baselines at the cost of a tolerable 4-5% drop in accuracy.",2019-06-17T18:34:01Z,2019-06-17T18:34:01Z,http://arxiv.org/abs/1906.07214v1,http://arxiv.org/pdf/1906.07214v1,"cs.CV, stat.ML"
Multi-Cell Sparse Activity Detection for Massive Random Access: Massive   MIMO versus Cooperative MIMO,"Zhilin Chen, Foad Sohrabi, Wei Yu","This paper considers sparse device activity detection for cellular machine-type communications with non-orthogonal signatures using the approximate message passing algorithm. This paper compares two network architectures, massive multiple-input multiple-output (MIMO) and cooperative MIMO, in terms of their effectiveness in overcoming inter-cell interference. In the massive MIMO architecture, each base station (BS) detects only the users from its own cell while treating inter-cell interference as noise. In the cooperative MIMO architecture, each BS detects the users from neighboring cells as well; the detection results are then forwarded in the form of log-likelihood ratio (LLR) to a central unit where final decisions are made. This paper analytically characterizes the probabilities of false alarm and missed detection for both architectures. Numerical results validate the analytic characterization and show that as the number of antennas increases, a massive MIMO system effectively drives the detection error to zero, while as the cooperation size increases, the cooperative MIMO architecture mainly improves the cell-edge user performance. Moreover, this paper studies the effect of LLR quantization to account for the finite-capacity fronthaul. Numerical simulations of a practical scenario suggest that in that specific case cooperating three BSs in a cooperative MIMO system achieves about the same cell-edge detection reliability as a non-cooperative massive MIMO system with four times the number of antennas per BS.",2019-06-22T19:57:47Z,2019-06-22T19:57:47Z,http://arxiv.org/abs/1906.09494v1,http://arxiv.org/pdf/1906.09494v1,"cs.IT, math.IT"
Likelihood Contribution based Multi-scale Architecture for Generative   Flows,"Hari Prasanna Das, Pieter Abbeel, Costas J. Spanos","Deep generative modeling using flows has gained popularity owing to the tractable exact log-likelihood estimation with efficient training and synthesis process. However, flow models suffer from the challenge of having high dimensional latent space, the same in dimension as the input space. An effective solution to the above challenge as proposed by Dinh et al. (2016) is a multi-scale architecture, which is based on iterative early factorization of a part of the total dimensions at regular intervals. Prior works on generative flow models involving a multi-scale architecture perform the dimension factorization based on static masking. We propose a novel multi-scale architecture that performs data-dependent factorization to decide which dimensions should pass through more flow layers. To facilitate the same, we introduce a heuristic based on the contribution of each dimension to the total log-likelihood which encodes the importance of the dimensions. Our proposed heuristic is readily obtained as part of the flow training process, enabling the versatile implementation of our likelihood contribution based multi-scale architecture for generic flow models. We present such implementations for several state-of-the-art flow models and demonstrate improvements in log-likelihood score and sampling quality on standard image benchmarks. We also conduct ablation studies to compare the proposed method with other options for dimension factorization.",2019-08-05T15:14:18Z,2022-01-27T07:39:34Z,http://arxiv.org/abs/1908.01686v3,http://arxiv.org/pdf/1908.01686v3,"cs.LG, stat.ML"
Unequally Sub-connected Architecture for Hybrid Beamforming in Massive   MIMO Systems,"Nhan Thanh Nguyen, Kyungchun Lee","A variety of hybrid analog-digital beamforming architectures have recently been proposed for massive multiple-input multiple-output (MIMO) systems to reduce energy consumption and the cost of implementation. In the analog processing network of these architectures, the practical sub-connected structure requires lower power consumption and hardware complexity than the fully connected structure but cannot fully exploit the beamforming gains, which leads to a loss in overall performance. In this work, we propose a novel unequal sub-connected architecture for hybrid combining at the receiver of a massive MIMO system that employs unequal numbers of antennas in sub-antenna arrays. The optimal design of the proposed architecture is analytically derived, and includes antenna allocation and channel ordering schemes. Simulation results show that an enhancement of up to 10% can be attained in the total achievable rate by unequally assigning antennas to sub-arrays in the sub-connected system at the cost of a marginal increase in power consumption. Furthermore, in order to reduce the computational complexity involved in finding the optimal number of antennas connected to each radio frequency (RF) chain, we propose three low-complexity antenna allocation algorithms. The simulation results show that they can yield a significant reduction in complexity while achieving near-optimal performance.",2019-08-27T07:20:33Z,2019-08-27T07:20:33Z,http://arxiv.org/abs/1908.10056v1,http://arxiv.org/pdf/1908.10056v1,"cs.IT, eess.SP, math.IT"
Towards modular and programmable architecture search,"Renato Negrinho, Darshan Patil, Nghia Le, Daniel Ferreira, Matthew Gormley, Geoffrey Gordon","Neural architecture search methods are able to find high performance deep learning architectures with minimal effort from an expert. However, current systems focus on specific use-cases (e.g. convolutional image classifiers and recurrent language models), making them unsuitable for general use-cases that an expert might wish to write. Hyperparameter optimization systems are general-purpose but lack the constructs needed for easy application to architecture search. In this work, we propose a formal language for encoding search spaces over general computational graphs. The language constructs allow us to write modular, composable, and reusable search space encodings and to reason about search space design. We use our language to encode search spaces from the architecture search literature. The language allows us to decouple the implementations of the search space and the search algorithm, allowing us to expose search spaces to search algorithms through a consistent interface. Our experiments show the ease with which we can experiment with different combinations of search spaces and search algorithms without having to implement each combination from scratch. We release an implementation of our language with this paper.",2019-09-30T00:18:56Z,2019-09-30T00:18:56Z,http://arxiv.org/abs/1909.13404v1,http://arxiv.org/pdf/1909.13404v1,"cs.LG, cs.AI, stat.ML"
The Z1: Architecture and Algorithms of Konrad Zuse's First Computer,Raul Rojas,"This paper provides the first comprehensive description of the Z1, the mechanical computer built by the German inventor Konrad Zuse in Berlin from 1936 to 1938. The paper describes the main structural elements of the machine, the high-level architecture, and the dataflow between components. The computer could perform the four basic arithmetic operations using floating-point numbers. Instructions were read from punched tape. A program consisted of a sequence of arithmetical operations, intermixed with memory store and load instructions, interrupted possibly by input and output operations. Numbers were stored in a mechanical memory. The machine did not include conditional branching in the instruction set. While the architecture of the Z1 is similar to the relay computer Zuse finished in 1941 (the Z3) there are some significant differences. The Z1 implements operations as sequences of microinstructions, as in the Z3, but does not use rotary switches as micro-steppers. The Z1 uses a digital incrementer and a set of conditions which are translated into microinstructions for the exponent and mantissa units, as well as for the memory blocks. Microinstructions select one out of 12 layers in a machine with a 3D mechanical structure of binary mechanical elements. The exception circuits for mantissa zero, necessary for normalized floating-point, were lacking; they were first implemented in the Z3. The information for this article was extracted from careful study of the blueprints drawn by Zuse for the reconstruction of the Z1 for the German Technology Museum in Berlin, from some letters, and from sketches in notebooks. Although the machine has been in exhibition since 1989 (non-operational), no detailed high-level description of the machine's architecture had been available. This paper fills that gap.",2014-06-07T11:23:31Z,2014-06-07T11:23:31Z,http://arxiv.org/abs/1406.1886v1,http://arxiv.org/pdf/1406.1886v1,"cs.AR, 68Mxx, K.2; B.0"
Leveraging One-hop Information in Massive MIMO Full-Duplex Wireless   Systems,"Wenzhuo Ouyang, Jingwen Bai, Ashutosh Sabharwal","We consider a single-cell massive MIMO full-duplex wireless communication system, where the base-station (BS) is equipped with a large number of antennas. We consider the setup where the single-antenna mobile users operate in half- duplex, while each antenna at the BS is capable of full-duplex transmissions, i.e., it can transmit and receive simultaneously using the same frequency spectrum. The fundamental challenge in this system is intra-cell inter-node interference, generated by the transmissions of uplink users to the receptions at the downlink users. The key operational challenge is estimating and aggregating inter-mobile channel estimates, which can potentially overwhelm any gains from full-duplex operation.   In this work, we propose a scalable and distributed scheme to optimally manage the inter-node interference by utilizing a ""one- hop information architecture"". In this architecture, the BS only needs to know the signal-to-interference-plus-noise ratio (SINR) from the downlink users. Each uplink user needs its own SINR, along with a weighted signal-plus-noise metric from its one-hop neighboring downlink users, which are the downlink users that it interferes with. The proposed one-hop information architecture does not require any network devices to comprehensively gather the vast inter-node interference channel knowledge, and hence significantly reduces the overhead. Based on the one-hop information architecture, we design a distributed power control algorithm and implement such architecture using overheard feedback information. We show that, in typical asymptotic regimes with many users and antennas, the proposed distributed power control scheme improves the overall network utility and reduces the transmission power of the uplink users.",2015-09-02T01:31:40Z,2015-09-02T01:31:40Z,http://arxiv.org/abs/1509.00539v1,http://arxiv.org/pdf/1509.00539v1,"cs.IT, math.IT"
A future mobile packet core network based on ip-in-ip protocol,"Mohammad Al Shinwan, Kim Chul-Soo","The current Evolved Packet Core (EPC) 4th generation (4G) mobile network architecture features complicated control plane protocols and requires expensive equipment. Data delivery in the mobile packet core is performed based on a centralized mobility anchor between eNode B (eNB) elements and the network gateways. The mobility anchor is performed based on General Packet Radio Service tunnelling protocol (GTP), which has numerous drawbacks, including high tunnelling overhead and suboptimal routing between mobile devices on the same network. To address these challenges, here we describe new mobile core architecture for future mobile networks. The proposed scheme is based on IP encapsulated within IP (IP-in-IP) for mobility management and data delivery. In this scheme, the core network functions via layer 3 switching (L3S), and data delivery is implemented based on IP-in-IP routing, thus eliminating the GTP tunnelling protocol. For handover between eNB elements located near to one another, we propose the creation of a tunnel that maintains data delivery to mobile devices until the new eNB element updates the route with the gateway, which prevents data packet loss during handover. For this, we propose Generic Routing Encapsulation (GRE) tunnelling protocol. We describe the results of numerical analyses and simulation results showing that the proposed network core architecture provides superior performance compared with the current 4G architecture in terms of handover delay, tunnelling overhead and total transmission delay.",2018-10-12T08:42:24Z,2018-10-12T08:42:24Z,http://arxiv.org/abs/1810.05405v1,http://arxiv.org/pdf/1810.05405v1,"cs.NI, 68M10"
Unsupervised Feature Learning Architecture with Multi-clustering   Integration RBM,"Jielei Chu, Hongjun Wang, Jing Liu, Zhiguo Gong, Tianrui Li","In this paper, we present a novel unsupervised feature learning architecture, which consists of a multi-clustering integration module and a variant of RBM termed multi-clustering integration RBM (MIRBM). In the multi-clustering integration module, we apply three unsupervised K-means, affinity propagation and spectral clustering algorithms to obtain three different clustering partitions (CPs) without any background knowledge or label. Then, an unanimous voting strategy is used to generate a local clustering partition (LCP). The novel MIRBM model is a core feature encoding part of the proposed unsupervised feature learning architecture. The novelty of it is that the LCP as an unsupervised guidance is integrated into one step contrastive divergence (CD1) learning to guide the distribution of the hidden layer features. For the instance in the same LCP cluster, the hidden and reconstructed hidden layer features of the MIRBM model in the proposed architecture tend to constrict together in the training process. Meanwhile, each LCP center tends to disperse from each other as much as possible in the hidden and reconstructed hidden layer during training. The experiments demonstrate that the proposed unsupervised feature learning architecture has more powerful feature representation and generalization capability than the state-of-the-art graph regularized RBM (GraphRBM) for clustering tasks in the Microsoft Research Asia Multimedia (MSRA-MM)2.0 dataset.",2018-12-05T12:56:27Z,2020-04-02T08:52:24Z,http://arxiv.org/abs/1812.01967v2,http://arxiv.org/pdf/1812.01967v2,"cs.LG, stat.ML"
GP-CNAS: Convolutional Neural Network Architecture Search with Genetic   Programming,"Yiheng Zhu, Yichen Yao, Zili Wu, Yujie Chen, Guozheng Li, Haoyuan Hu, Yinghui Xu","Convolutional neural networks (CNNs) are effective at solving difficult problems like visual recognition, speech recognition and natural language processing. However, performance gain comes at the cost of laborious trial-and-error in designing deeper CNN architectures. In this paper, a genetic programming (GP) framework for convolutional neural network architecture search, abbreviated as GP-CNAS, is proposed to automatically search for optimal CNN architectures. GP-CNAS encodes CNNs as trees where leaf nodes (GP terminals) are selected residual blocks and non-leaf nodes (GP functions) specify the block assembling procedure. Our tree-based representation enables easy design and flexible implementation of genetic operators. Specifically, we design a dynamic crossover operator that strikes a balance between exploration and exploitation, which emphasizes CNN complexity at early stage and CNN diversity at later stage. Therefore, the desired CNN architecture with balanced depth and width can be found within limited trials. Moreover, our GP-CNAS framework is highly compatible with other manually-designed and NAS-generated block types as well. Experimental results on the CIFAR-10 dataset show that GP-CNAS is competitive among the state-of-the-art automatic and semi-automatic NAS algorithms.",2018-11-26T17:44:15Z,2018-11-26T17:44:15Z,http://arxiv.org/abs/1812.07611v1,http://arxiv.org/pdf/1812.07611v1,"cs.NE, stat.ML"
Single Flux Quantum Based Ultrahigh Speed Spiking Neuromorphic Processor   Architecture,"Ali Bozbey, Mustafa Altay Karamuftuoglu, Sasan Razmkhah, Murat Ozbayoglu","Artificial neural networks inspired by brain operations can improve the possibilities of solving complex problems more efficiently. Today's computing hardware, on the other hand, is mainly based on von Neumann architecture and CMOS technology, which is inefficient at implementing neural networks. For the first time, we propose an ultrahigh speed, spiking neuromorphic processor architecture built upon single flux quantum (SFQ) based artificial neurons (JJ-Neuron). Proposed architecture has the potential to provide higher performance and power efficiency over the state of the art including CMOS, memristors and nanophotonics devices. JJ-Neuron has the ultrafast spiking capability, trainability with commodity design software even after fabrication and compatibility with commercial CMOS and SFQ foundry services. We experimentally demonstrate the soma part of the JJ-Neuron for various activation functions together with peripheral SFQ logic gates. Then, the neural network is trained for the IRIS dataset and we have shown 100% match with the results of the offline training with 1.2x${10}^{10}$ synaptic operations per second (SOPS) and 8.57x${10}^{11}$ SOPS/W performance and power efficiency, respectively. In addition, scalability for ${10}^{18}$ SOPS and ${10}^{17}$ SOPS/W is shown which is at least five orders of magnitude more efficient than the state of the art CMOS circuits and one order of magnitude more efficient than estimations of nanophotonics-based architectures.",2018-12-26T16:08:03Z,2020-07-06T20:04:38Z,http://arxiv.org/abs/1812.10354v3,http://arxiv.org/pdf/1812.10354v3,"cs.ET, physics.app-ph"
Energy-Aware Neural Architecture Optimization with Fast Splitting   Steepest Descent,"Dilin Wang, Meng Li, Lemeng Wu, Vikas Chandra, Qiang Liu","Designing energy-efficient networks is of critical importance for enabling state-of-the-art deep learning in mobile and edge settings where the computation and energy budgets are highly limited. Recently, Liu et al. (2019) framed the search of efficient neural architectures into a continuous splitting process: it iteratively splits existing neurons into multiple off-springs to achieve progressive loss minimization, thus finding novel architectures by gradually growing the neural network. However, this method was not specifically tailored for designing energy-efficient networks, and is computationally expensive on large-scale benchmarks. In this work, we substantially improve Liu et al. (2019) in two significant ways: 1) we incorporate the energy cost of splitting different neurons to better guide the splitting process, thereby discovering more energy-efficient network architectures; 2) we substantially speed up the splitting process of Liu et al. (2019), which requires expensive eigen-decomposition, by proposing a highly scalable Rayleigh-quotient stochastic gradient algorithm. Our fast algorithm allows us to reduce the computational cost of splitting to the same level of typical back-propagation updates and enables efficient implementation on GPU. Extensive empirical results show that our method can train highly accurate and energy-efficient networks on challenging datasets such as ImageNet, improving a variety of baselines, including the pruning-based methods and expert-designed architectures.",2019-10-07T21:45:17Z,2020-07-08T20:58:06Z,http://arxiv.org/abs/1910.03103v3,http://arxiv.org/pdf/1910.03103v3,"cs.LG, stat.ML"
"Green Deep Reinforcement Learning for Radio Resource Management:   Architecture, Algorithm Compression and Challenge","Zhiyong Du, Yansha Deng, Weisi Guo, Arumugam Nallanathan, Qihui Wu","AI heralds a step-change in the performance and capability of wireless networks and other critical infrastructures. However, it may also cause irreversible environmental damage due to their high energy consumption. Here, we address this challenge in the context of 5G and beyond, where there is a complexity explosion in radio resource management (RRM). On the one hand, deep reinforcement learning (DRL) provides a powerful tool for scalable optimization for high dimensional RRM problems in a dynamic environment. On the other hand, DRL algorithms consume a high amount of energy over time and risk compromising progress made in green radio research. This paper reviews and analyzes how to achieve green DRL for RRM via both architecture and algorithm innovations. Architecturally, a cloud based training and distributed decision-making DRL scheme is proposed, where RRM entities can make lightweight deep local decisions whilst assisted by on-cloud training and updating. On the algorithm level, compression approaches are introduced for both deep neural networks and the underlying Markov Decision Processes, enabling accurate low-dimensional representations of challenges. To scale learning across geographic areas, a spatial transfer learning scheme is proposed to further promote the learning efficiency of distributed DRL entities by exploiting the traffic demand correlations. Together, our proposed architecture and algorithms provide a vision for green and on-demand DRL capability.",2019-10-11T09:51:15Z,2019-10-11T09:51:15Z,http://arxiv.org/abs/1910.05054v1,http://arxiv.org/pdf/1910.05054v1,"cs.LG, cs.AI, cs.NI, eess.SP"
Generative Neural Network based Spectrum Sharing using Linear Sum   Assignment Problems,"Ahmed B. Zaky, Joshua Zhexue Huang, KaishunWu, Basem M. ElHalawany","Spectrum management and resource allocation (RA) problems are challenging and critical in a vast number of research areas such as wireless communications and computer networks. The traditional approaches for solving such problems usually consume time and memory, especially for large size problems. Recently different machine learning approaches have been considered as potential promising techniques for combinatorial optimization problems, especially the generative model of the deep neural networks. In this work, we propose a resource allocation deep autoencoder network, as one of the promising generative models, for enabling spectrum sharing in underlay device-to-device (D2D) communication by solving linear sum assignment problems (LSAPs). Specifically, we investigate the performance of three different architectures for the conditional variational autoencoders (CVAE). The three proposed architecture are the convolutional neural network (CVAE-CNN) autoencoder, the feed-forward neural network (CVAE-FNN) autoencoder, and the hybrid (H-CVAE) autoencoder. The simulation results show that the proposed approach could be used as a replacement of the conventional RA techniques, such as the Hungarian algorithm, due to its ability to find solutions of LASPs of different sizes with high accuracy and very fast execution time. Moreover, the simulation results reveal that the accuracy of the proposed hybrid autoencoder architecture outperforms the other proposed architectures and the state-of-the-art DNN techniques.",2019-10-12T07:05:07Z,2019-10-12T07:05:07Z,http://arxiv.org/abs/1910.05510v1,http://arxiv.org/pdf/1910.05510v1,"cs.LG, cs.NI, stat.ML"
BANANAS: Bayesian Optimization with Neural Architectures for Neural   Architecture Search,"Colin White, Willie Neiswanger, Yash Savani","Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance.   In this work, we give a thorough analysis of the ""BO + neural predictor"" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.",2019-10-25T17:35:49Z,2020-11-02T15:28:47Z,http://arxiv.org/abs/1910.11858v3,http://arxiv.org/pdf/1910.11858v3,"cs.LG, cs.NE, stat.ML"
STConvS2S: Spatiotemporal Convolutional Sequence to Sequence Network for   Weather Forecasting,"Rafaela Castro, Yania M. Souto, Eduardo Ogasawara, Fabio Porto, Eduardo Bezerra","Applying machine learning models to meteorological data brings many opportunities to the Geosciences field, such as predicting future weather conditions more accurately. In recent years, modeling meteorological data with deep neural networks has become a relevant area of investigation. These works apply either recurrent neural networks (RNN) or some hybrid approach mixing RNN and convolutional neural networks (CNN). In this work, we propose STConvS2S (Spatiotemporal Convolutional Sequence to Sequence Network), a deep learning architecture built for learning both spatial and temporal data dependencies using only convolutional layers. Our proposed architecture resolves two limitations of convolutional networks to predict sequences using historical data: (1) they violate the temporal order during the learning process and (2) they require the lengths of the input and output sequences to be equal. Computational experiments using air temperature and rainfall data from South America show that our architecture captures spatiotemporal context and that it outperforms or matches the results of state-of-the-art architectures for forecasting tasks. In particular, one of the variants of our proposed architecture is 23% better at predicting future sequences and five times faster at training than the RNN-based model used as a baseline.",2019-11-30T05:19:04Z,2020-11-10T02:00:23Z,http://arxiv.org/abs/1912.00134v4,http://arxiv.org/pdf/1912.00134v4,"cs.LG, stat.ML"
VINNAS: Variational Inference-based Neural Network Architecture Search,"Martin Ferianc, Hongxiang Fan, Miguel Rodrigues","In recent years, neural architecture search (NAS) has received intensive scientific and industrial interest due to its capability of finding a neural architecture with high accuracy for various artificial intelligence tasks such as image classification or object detection. In particular, gradient-based NAS approaches have become one of the more popular approaches thanks to their computational efficiency during the search. However, these methods often experience a mode collapse, where the quality of the found architectures is poor due to the algorithm resorting to choosing a single operation type for the entire network, or stagnating at a local minima for various datasets or search spaces.   To address these defects, we present a differentiable variational inference-based NAS method for searching sparse convolutional neural networks. Our approach finds the optimal neural architecture by dropping out candidate operations in an over-parameterised supergraph using variational dropout with automatic relevance determination prior, which makes the algorithm gradually remove unnecessary operations and connections without risking mode collapse. The evaluation is conducted through searching two types of convolutional cells that shape the neural network for classifying different image datasets. Our method finds diverse network cells, while showing state-of-the-art accuracy with up to almost 2 times fewer non-zero parameters.",2020-07-12T21:47:35Z,2021-01-14T21:26:57Z,http://arxiv.org/abs/2007.06103v5,http://arxiv.org/pdf/2007.06103v5,"cs.LG, cs.CV, stat.ML"
Deep Neural-Kernel Machines,Siamak Mehrkanoon,"In this chapter we review the main literature related to the recent advancement of deep neural-kernel architecture, an approach that seek the synergy between two powerful class of models, i.e. kernel-based models and artificial neural networks. The introduced deep neural-kernel framework is composed of a hybridization of the neural networks architecture and a kernel machine. More precisely, for the kernel counterpart the model is based on Least Squares Support Vector Machines with explicit feature mapping. Here we discuss the use of one form of an explicit feature map obtained by random Fourier features. Thanks to this explicit feature map, in one hand bridging the two architectures has become more straightforward and on the other hand one can find the solution of the associated optimization problem in the primal, therefore making the model scalable to large scale datasets. We begin by introducing a neural-kernel architecture that serves as the core module for deeper models equipped with different pooling layers. In particular, we review three neural-kernel machines with average, maxout and convolutional pooling layers. In average pooling layer the outputs of the previous representation layers are averaged. The maxout layer triggers competition among different input representations and allows the formation of multiple sub-networks within the same model. The convolutional pooling layer reduces the dimensionality of the multi-scale output representations. Comparison with neural-kernel model, kernel based models and the classical neural networks architecture have been made and the numerical experiments illustrate the effectiveness of the introduced models on several benchmark datasets.",2020-07-13T19:46:29Z,2020-07-19T11:03:51Z,http://arxiv.org/abs/2007.06655v2,http://arxiv.org/pdf/2007.06655v2,"cs.LG, stat.ML, I.2; I.5"
U-Net Based Architecture for an Improved Multiresolution Segmentation in   Medical Images,"Simindokht Jahangard, Mohammad Hossein Zangooei, Maysam Shahedi","Purpose: Manual medical image segmentation is an exhausting and time-consuming task along with high inter-observer variability. In this study, our objective is to improve the multi-resolution image segmentation performance of U-Net architecture. Approach: We have proposed a fully convolutional neural network for image segmentation in a multi-resolution framework. We used U-Net as the base architecture and modified that to improve its image segmentation performance. In the proposed architecture (mrU-Net), the input image and its down-sampled versions were used as the network inputs. We added more convolution layers to extract features directly from the down-sampled images. We trained and tested the network on four different medical datasets, including skin lesion photos, lung computed tomography (CT) images (LUNA dataset), retina images (DRIVE dataset), and prostate magnetic resonance (MR) images (PROMISE12 dataset). We compared the performance of mrU-Net to U-Net under similar training and testing conditions. Results: Comparing the results to manual segmentation labels, mrU-Net achieved average Dice similarity coefficients of 70.6%, 97.9%, 73.6%, and 77.9% for the skin lesion, LUNA, DRIVE, and PROMISE12 segmentation, respectively. For the skin lesion, LUNA, and DRIVE datasets, mrU-Net outperformed U-Net with significantly higher accuracy and for the PROMISE12 dataset, both networks achieved similar accuracy. Furthermore, using mrU-Net led to a faster training rate on LUNA and DRIVE datasets when compared to U-Net. Conclusions: The striking feature of the proposed architecture is its higher capability in extracting image-derived features compared to U-Net. mrU-Net illustrated a faster training rate and slightly more accurate image segmentation compared to U-Net.",2020-07-16T10:19:01Z,2020-07-17T06:17:20Z,http://arxiv.org/abs/2007.08238v2,http://arxiv.org/pdf/2007.08238v2,"eess.IV, cs.CV"
Transform Network Architectures for Deep Learning based End-to-End   Image/Video Coding in Subsampled Color Spaces,"Hilmi E. Egilmez, Ankitesh K. Singh, Muhammed Coban, Marta Karczewicz, Yinhao Zhu, Yang Yang, Amir Said, Taco S. Cohen","Most of the existing deep learning based end-to-end image/video coding (DLEC) architectures are designed for non-subsampled RGB color format. However, in order to achieve a superior coding performance, many state-of-the-art block-based compression standards such as High Efficiency Video Coding (HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for YUV 4:2:0 format, where U and V components are subsampled by considering the human visual system. This paper investigates various DLEC designs to support YUV 4:2:0 format by comparing their performance against the main profiles of HEVC and VVC standards under a common evaluation framework. Moreover, a new transform network architecture is proposed to improve the efficiency of coding YUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the proposed architecture significantly outperforms naive extensions of existing architectures designed for RGB format and achieves about 10% average BD-rate improvement over the intra-frame coding in HEVC.",2021-02-27T06:47:27Z,2021-08-27T07:44:13Z,http://arxiv.org/abs/2103.01760v2,http://arxiv.org/pdf/2103.01760v2,"eess.IV, cs.AI, cs.CV, cs.LG, cs.MM"
Fisher Task Distance and Its Application in Neural Architecture Search,"Cat P. Le, Mohammadreza Soltani, Juncheng Dong, Vahid Tarokh","We formulate an asymmetric (or non-commutative) distance between tasks based on Fisher Information Matrices, called Fisher task distance. This distance represents the complexity of transferring the knowledge from one task to another. We provide a proof of consistency for our distance through theorems and experiments on various classification tasks from MNIST, CIFAR-10, CIFAR-100, ImageNet, and Taskonomy datasets. Next, we construct an online neural architecture search framework using the Fisher task distance, in which we have access to the past learned tasks. By using the Fisher task distance, we can identify the closest learned tasks to the target task, and utilize the knowledge learned from these related tasks for the target task. Here, we show how the proposed distance between a target task and a set of learned tasks can be used to reduce the neural architecture search space for the target task. The complexity reduction in search space for task-specific architectures is achieved by building on the optimized architectures for similar tasks instead of doing a full search and without using this side information. Experimental results for tasks in MNIST, CIFAR-10, CIFAR-100, ImageNet datasets demonstrate the efficacy of the proposed approach and its improvements, in terms of the performance and the number of parameters, over other gradient-based search methods, such as ENAS, DARTS, PC-DARTS.",2021-03-23T20:43:31Z,2022-04-30T04:40:37Z,http://arxiv.org/abs/2103.12827v5,http://arxiv.org/pdf/2103.12827v5,"cs.LG, eess.IV, stat.ML"
Multistatic Scatter Radio Sensor Networks for Extended Coverage,"Panos N. Alevizos, Konstantinos Tountas, Aggelos Bletsas","Scatter radio, i.e., communication by means of reflection, has been recently proposed as a viable ultra-low power solution for wireless sensor networks (WSNs). This work offers a detailed comparison between monostatic and multistatic scatter radio architectures. In monostatic architecture, the reader consists of both the illuminating transmitter and the receiver of signals scattered back from the sensors. The multistatic architecture includes several ultra-low cost illuminating carrier emitters and a single reader. Maximum-likelihood coherent and noncoherent bit error rate (BER), diversity order, average information and energy outage probability comparison is performed, under dyadic Nakagami fading, filling a gap in the literature. It is found that: (i) diversity order, BER, and tag location-independent performance bounds of multistatic architecture outperform monostatic, (ii) energy outage due to radio frequency (RF) harvesting for passive tags, is less frequent in multistatic than monostatic architecture, and (iii) multistatic coverage is higher than monostatic. Furthermore, a proof-of-concept {digital} multistatic, scatter radio WSN with a single receiver, four low-cost emitters and multiple ambiently-powered, low-bitrate tags, perhaps the first of its kind, is experimentally demonstrated (at $13$ dBm transmission power), covering an area of $3500$ m$^2$. Research findings are applicable in the industries of WSNs, radio frequency identification (RFID), and emerging Internet-of-Things.",2017-06-09T18:56:42Z,2018-04-11T14:10:38Z,http://arxiv.org/abs/1706.03091v3,http://arxiv.org/pdf/1706.03091v3,"cs.IT, math.IT"
Intriguing Properties of Adversarial Examples,"Ekin D. Cubuk, Barret Zoph, Samuel S. Schoenholz, Quoc V. Le","It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \emph{and} black box attacks compared to previous attempts.",2017-11-08T06:54:49Z,2017-11-08T06:54:49Z,http://arxiv.org/abs/1711.02846v1,http://arxiv.org/pdf/1711.02846v1,"stat.ML, cs.LG"
Multi-User Frequency-Selective Hybrid MIMO Demonstrated Using 60 GHz RF   Modules,"Steve Blandino, Claude Desset, Cheng-Ming Chen, Andre Bourdoux, Sofie Pollin","Given the high throughput requirement for 5G, merging millimeter wave technologies and multi-user MIMO seems a very promising strategy. As hardware limitations impede to realize a full digital architecture, hybrid MIMO architectures, using digital precoding and phased antenna arrays, are considered a feasible solution to implement multi-user MIMO at millimeter wave. However, real channel propagation and hardware non-idealities can significantly degrade the performance of such systems. Experimenting the new architecture is thus crucial to confirm and to support system design. Nevertheless, hybrid MIMO systems are not yet understood as the effects of the wide channel bandwidths at millimeter wave, the non-ideal RF front end as well as the imperfections of the analog beamforming are often neglected. In this paper, we present a 60 GHz MU-MIMO testbed using phased antenna arrays at both transmitter and receiver. The base station equipped with a 32 phased antenna array allocates simultaneously two users. We show that frequency selective hybrid precoding can efficiently suppress inter-user interference enabling spatial multiplexing in interference limited scenario doubling the throughput compared to a SISO scenario and compensating the frequency fluctuation of the channel. In addition, we report an EVM constellation improvement of 6 dB when comparing the hybrid MIMO architecture with a fully analog architecture.",2017-11-08T14:36:21Z,2018-04-09T12:59:00Z,http://arxiv.org/abs/1711.02968v3,http://arxiv.org/pdf/1711.02968v3,"cs.IT, math.IT"
Residual Gated Graph ConvNets,"Xavier Bresson, Thomas Laurent","Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures. Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.",2017-11-20T21:28:40Z,2018-04-24T08:19:32Z,http://arxiv.org/abs/1711.07553v2,http://arxiv.org/pdf/1711.07553v2,"cs.LG, stat.ML"
Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music   Retrieval,"Yi Yu, Suhua Tang, Francisco Raposo, Lei Chen","Little research focuses on cross-modal correlation learning where temporal structures of different data modalities such as audio and lyrics are taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Different modality data are converted to the same canonical space where inter modal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study on understanding the correlation between language and music audio through deep architectures for learning the paired temporal correlation of audio and lyrics. Pre-trained Doc2vec model followed by fully-connected layers (fully-connected deep neural network) is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: i) pre-trained CNN followed by fully-connected layers is investigated for representing music audio. ii) We further suggest an end-to-end architecture that simultaneously trains convolutional layers and fully-connected layers to better learn temporal structures of music audio. Particularly, our end-to-end deep architecture contains two properties: simultaneously implementing feature learning and cross-modal correlation learning, and learning joint representation by considering temporal structures. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.",2017-11-24T14:21:46Z,2017-11-29T01:15:53Z,http://arxiv.org/abs/1711.08976v2,http://arxiv.org/pdf/1711.08976v2,"cs.IR, cs.SD, eess.AS"
Two-Stage Convolutional Neural Network Architecture for Lung Nodule   Detection,"Haichao Cao, Hong Liu, Enmin Song, Guangzhi Ma, Xiangyang Xu, Renchao Jin, Tengying Liu, Chih-Cheng Hung","Early detection of lung cancer is an effective way to improve the survival rate of patients. It is a critical step to have accurate detection of lung nodules in computed tomography (CT) images for the diagnosis of lung cancer. However, due to the heterogeneity of the lung nodules and the complexity of the surrounding environment, robust nodule detection has been a challenging task. In this study, we propose a two-stage convolutional neural network (TSCNN) architecture for lung nodule detection. The CNN architecture in the first stage is based on the improved UNet segmentation network to establish an initial detection of lung nodules. Simultaneously, in order to obtain a high recall rate without introducing excessive false positive nodules, we propose a novel sampling strategy, and use the offline hard mining idea for training and prediction according to the proposed cascaded prediction method. The CNN architecture in the second stage is based on the proposed dual pooling structure, which is built into three 3D CNN classification networks for false positive reduction. Since the network training requires a significant amount of training data, we adopt a data augmentation method based on random mask. Furthermore, we have improved the generalization ability of the false positive reduction model by means of ensemble learning. The proposed method has been experimentally verified on the LUNA dataset. Experimental results show that the proposed TSCNN architecture can obtain competitive detection performance.",2019-05-09T05:21:40Z,2019-05-09T05:21:40Z,http://arxiv.org/abs/1905.03445v1,http://arxiv.org/pdf/1905.03445v1,"cs.CV, eess.IV"
VLSI Computational Architectures for the Arithmetic Cosine Transform,"N. Rajapaksha, A. Madanayake, R. J. Cintra, J. Adikari, V. S. Dimitrov","The discrete cosine transform (DCT) is a widely-used and important signal processing tool employed in a plethora of applications. Typical fast algorithms for nearly-exact computation of DCT require floating point arithmetic, are multiplier intensive, and accumulate round-off errors. Recently proposed fast algorithm arithmetic cosine transform (ACT) calculates the DCT exactly using only additions and integer constant multiplications, with very low area complexity, for null mean input sequences. The ACT can also be computed non-exactly for any input sequence, with low area complexity and low power consumption, utilizing the novel architecture described. However, as a trade-off, the ACT algorithm requires 10 non-uniformly sampled data points to calculate the 8-point DCT. This requirement can easily be satisfied for applications dealing with spatial signals such as image sensors and biomedical sensor arrays, by placing sensor elements in a non-uniform grid. In this work, a hardware architecture for the computation of the null mean ACT is proposed, followed by a novel architectures that extend the ACT for non-null mean signals. All circuits are physically implemented and tested using the Xilinx XC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for performance assessment.",2017-10-30T19:06:19Z,2017-10-30T19:06:19Z,http://arxiv.org/abs/1710.11200v1,http://arxiv.org/pdf/1710.11200v1,"cs.AR, cs.DS, cs.MM, math.NA, stat.ME"
Cloud-based MPC with Encrypted Data,"Andreea B. Alexandru, Manfred Morari, George J. Pappas","This paper explores the privacy of cloud outsourced Model Predictive Control (MPC) for a linear system with input constraints. In our cloud-based architecture, a client sends her private states to the cloud who performs the MPC computation and returns the control inputs. In order to guarantee that the cloud can perform this computation without obtaining anything about the client's private data, we employ a partially homomorphic cryptosystem. We propose protocols for two cloud-MPC architectures motivated by the current developments in the Internet of Things: a client-server architecture and a two-server architecture. In the first case, a control input for the system is privately computed by the cloud server, with the assistance of the client. In the second case, the control input is privately computed by two independent, non-colluding servers, with no additional requirements from the client. We prove that the proposed protocols preserve the privacy of the client's data and of the resulting control input. Furthermore, we compute bounds on the errors introduced by encryption. We present numerical simulations for the two architectures and discuss the trade-off between communication, MPC performance and privacy.",2018-03-27T04:27:47Z,2018-09-19T13:14:07Z,http://arxiv.org/abs/1803.09891v2,http://arxiv.org/pdf/1803.09891v2,"math.OC, cs.CR, cs.SY"
GSA-DenseNet121-COVID-19: a Hybrid Deep Learning Architecture for the   Diagnosis of COVID-19 Disease based on Gravitational Search Optimization   Algorithm,"Dalia Ezzat, Aboul ell Hassanien, Hassan Aboul Ella","In this paper, a novel approach called GSA-DenseNet121-COVID-19 based on a hybrid convolutional neural network (CNN) architecture is proposed using an optimization algorithm. The CNN architecture that was used is called DenseNet121 and the optimization algorithm that was used is called the gravitational search algorithm (GSA). The GSA is adapted to determine the best values for the hyperparameters of the DenseNet121 architecture, and to achieve a high level of accuracy in diagnosing COVID-19 disease through chest x-ray image analysis. The obtained results showed that the proposed approach was able to correctly classify 98% of the test set. To test the efficacy of the GSA in setting the optimum values for the hyperparameters of DenseNet121, it was compared to another optimization algorithm called social ski driver (SSD). The comparison results demonstrated the efficacy of the proposed GSA-DenseNet121-COVID-19 and its ability to better diagnose COVID-19 disease than the SSD-DenseNet121 as the second was able to diagnose only 94% of the test set. As well as, the proposed approach was compared to an approach based on a CNN architecture called Inception-v3 and the manual search method for determining the values of the hyperparameters. The results of the comparison showed that the GSA-DenseNet121 was able to beat the other approach, as the second was able to classify only 95% of the test set samples.",2020-04-09T13:30:11Z,2020-04-09T13:30:11Z,http://arxiv.org/abs/2004.05084v1,http://arxiv.org/pdf/2004.05084v1,"eess.IV, cs.NE"
Federated Transfer Learning for EEG Signal Classification,"Ce Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, Cuntai Guan","The success of deep learning (DL) methods in the Brain-Computer Interfaces (BCI) field for classification of electroencephalographic (EEG) recordings has been restricted by the lack of large datasets. Privacy concerns associated with EEG signals limit the possibility of constructing a large EEG-BCI dataset by the conglomeration of multiple small ones for jointly training machine learning models. Hence, in this paper, we propose a novel privacy-preserving DL architecture named federated transfer learning (FTL) for EEG classification that is based on the federated learning framework. Working with the single-trial covariance matrix, the proposed architecture extracts common discriminative information from multi-subject EEG data with the help of domain adaptation techniques. We evaluate the performance of the proposed architecture on the PhysioNet dataset for 2-class motor imagery classification. While avoiding the actual data sharing, our FTL approach achieves 2% higher classification accuracy in a subject-adaptive analysis. Also, in the absence of multi-subject data, our architecture provides 6% better accuracy compared to other state-of-the-art DL architectures.",2020-04-26T09:03:19Z,2021-01-25T17:01:40Z,http://arxiv.org/abs/2004.12321v5,http://arxiv.org/pdf/2004.12321v5,"cs.LG, eess.SP, I.5.4"
Exploring the Loss Landscape in Neural Architecture Search,"Colin White, Sam Nolen, Yash Savani","Neural architecture search (NAS) has seen a steep rise in interest over the last few years. Many algorithms for NAS consist of searching through a space of architectures by iteratively choosing an architecture, evaluating its performance by training it, and using all prior evaluations to come up with the next choice. The evaluation step is noisy - the final accuracy varies based on the random initialization of the weights. Prior work has focused on devising new search algorithms to handle this noise, rather than quantifying or understanding the level of noise in architecture evaluations. In this work, we show that (1) the simplest hill-climbing algorithm is a powerful baseline for NAS, and (2), when the noise in popular NAS benchmark datasets is reduced to a minimum, hill-climbing to outperforms many popular state-of-the-art algorithms. We further back up this observation by showing that the number of local minima is substantially reduced as the noise decreases, and by giving a theoretical characterization of the performance of local search in NAS. Based on our findings, for NAS research we suggest (1) using local search as a baseline, and (2) denoising the training pipeline when possible.",2020-05-06T17:09:16Z,2021-06-16T17:41:03Z,http://arxiv.org/abs/2005.02960v3,http://arxiv.org/pdf/2005.02960v3,"cs.LG, stat.ML"
GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy   Efficient Inference,"Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, Andreas Moshovos","Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (1) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (2) replaces most multiply-accumulations with additions, and (3) reduces the off-chip traffic by amplifying on-chip memory capacity.",2020-05-08T03:59:53Z,2020-09-27T00:09:30Z,http://arxiv.org/abs/2005.03842v2,http://arxiv.org/pdf/2005.03842v2,"cs.LG, cs.AR, stat.ML"
A Review on Impact of Bloom Filter on Named Data Networking: The Future   Internet Architecture,"Sabuzima Nayak, Ripon Patgiri, Angana Borah","Today is the era of smart devices. Through the smart devices, people remain connected with systems across the globe even in mobile state. Hence, the current Internet is facing scalability issue. Therefore, leaving IP based Internet behind due to scalability, the world is moving to the Future Internet Architecture, called Named Data Networking (NDN). Currently, the number of nodes connected to the Internet is in billions. And, the number of requests sent is in millions per second. NDN handles such huge numbers by modifying the IP architecture to meet the current requirements. NDN is scalable, produces less traffic and congestion, provides high level security, saves bandwidth, efficiently utilizes multiple network interfaces and have many more functionalities. Similarly, Bloom Filter is the only good choice to deploy in various modules of NDN to handle the huge number of packets. Bloom Filter is a simple probabilistic data structure for the membership query. This article presents a detailed discussion on the role of Bloom Filter in implementing NDN. The article includes a precise discussion on Bloom Filter and the main components of the NDN architecture, namely, packet, content store, forward information base and pending interest table are also discussed briefly.",2020-04-07T13:38:51Z,2020-04-07T13:38:51Z,http://arxiv.org/abs/2005.06965v1,http://arxiv.org/pdf/2005.06965v1,"cs.NI, 41-02, 68-02, 68M10, 68M11, 68M12, C.2; E.1; F.2"
Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture   Search,"Aditya Rawal, Joel Lehman, Felipe Petroski Such, Jeff Clune, Kenneth O. Stanley","Neural Architecture Search (NAS) explores a large space of architectural motifs -- a compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands of domain-specific data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available. Our hope is that this work can inspire a new research direction in studying the performance of extracted components of models in an alternative controlled setting.",2020-05-27T00:12:06Z,2020-05-27T00:12:06Z,http://arxiv.org/abs/2005.13092v1,http://arxiv.org/pdf/2005.13092v1,"cs.LG, stat.ML"
Non-Euclidean Universal Approximation,"Anastasis Kratsios, Eugene Bilokopytov","Modifications to a neural network's input and output layers are often required to accommodate the specificities of most practical learning tasks. However, the impact of such changes on architecture's approximation capabilities is largely not understood. We present general conditions describing feature and readout maps that preserve an architecture's ability to approximate any continuous functions uniformly on compacts. As an application, we show that if an architecture is capable of universal approximation, then modifying its final layer to produce binary values creates a new architecture capable of deterministically approximating any classifier. In particular, we obtain guarantees for deep CNNs and deep feed-forward networks. Our results also have consequences within the scope of geometric deep learning. Specifically, when the input and output spaces are Cartan-Hadamard manifolds, we obtain geometrically meaningful feature and readout maps satisfying our criteria. Consequently, commonly used non-Euclidean regression models between spaces of symmetric positive definite matrices are extended to universal DNNs. The same result allows us to show that the hyperbolic feed-forward networks, used for hierarchical learning, are universal. Our result is also used to show that the common practice of randomizing all but the last two layers of a DNN produces a universal family of functions with probability one. We also provide conditions on a DNN's first (resp. last) few layer's connections and activation function which guarantee that these layers can have a width equal to the input (resp. output) space's dimension while not negatively affecting the architecture's approximation capabilities.",2020-06-03T15:38:57Z,2020-11-07T15:40:00Z,http://arxiv.org/abs/2006.02341v3,http://arxiv.org/pdf/2006.02341v3,"cs.LG, cs.NE, math.DG, math.GN, stat.ML, 8T07, 68T05, 41A65, 46T99, 46T10, 54C35, I.2.6"
A Comprehensive Survey of Neural Architecture Search: Challenges and   Solutions,"Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, Xin Wang","Deep learning has made breakthroughs and substantial in many fields due to its powerful automatic representation capabilities. It has been proven that neural architecture design is crucial to the feature representation of data and the final performance. However, the design of the neural architecture heavily relies on the researchers' prior knowledge and experience. And due to the limitations of human' inherent knowledge, it is difficult for people to jump out of their original thinking paradigm and design an optimal model. Therefore, an intuitive idea would be to reduce human intervention as much as possible and let the algorithm automatically design the neural architecture. Neural Architecture Search (NAS) is just such a revolutionary algorithm, and the related research work is complicated and rich. Therefore, a comprehensive and systematic survey on the NAS is essential. Previously related surveys have begun to classify existing work mainly based on the key components of NAS: search space, search strategy, and evaluation strategy. While this classification method is more intuitive, it is difficult for readers to grasp the challenges and the landmark work involved. Therefore, in this survey, we provide a new perspective: beginning with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then providing solutions for subsequent related research work. Besides, we conduct a detailed and comprehensive analysis, comparison, and summary of these works. Finally, we provide some possible future research directions.",2020-06-01T13:08:03Z,2021-03-02T08:35:02Z,http://arxiv.org/abs/2006.02903v3,http://arxiv.org/pdf/2006.02903v3,"cs.LG, stat.ML"
A non-causal FFTNet architecture for speech enhancement,"Muhammed PV Shifas, Nagaraj Adiga, Vassilis Tsiaras, Yannis Stylianou","In this paper, we suggest a new parallel, non-causal and shallow waveform domain architecture for speech enhancement based on FFTNet, a neural network for generating high quality audio waveform. In contrast to other waveform based approaches like WaveNet, FFTNet uses an initial wide dilation pattern. Such an architecture better represents the long term correlated structure of speech in the time domain, where noise is usually highly non-correlated, and therefore it is suitable for waveform domain based speech enhancement. To further strengthen this feature of FFTNet, we suggest a non-causal FFTNet architecture, where the present sample in each layer is estimated from the past and future samples of the previous layer. By suggesting a shallow network and applying non-causality within certain limits, the suggested FFTNet for speech enhancement (SE-FFTNet) uses much fewer parameters compared to other neural network based approaches for speech enhancement like WaveNet and SEGAN. Specifically, the suggested network has considerably reduced model parameters: 32% fewer compared to WaveNet and 87% fewer compared to SEGAN. Finally, based on subjective and objective metrics, SE-FFTNet outperforms WaveNet in terms of enhanced signal quality, while it provides equally good performance as SEGAN. A Tensorflow implementation of the architecture is provided at 1 .",2020-06-08T10:49:04Z,2020-06-08T10:49:04Z,http://arxiv.org/abs/2006.04469v1,http://arxiv.org/pdf/2006.04469v1,"eess.AS, cs.CL, cs.SD"
Self-organization of multi-layer spiking neural networks,"Guruprasad Raghavan, Cong Lin, Matt Thomson","Living neural networks in our brains autonomously self-organize into large, complex architectures during early development to result in an organized and functional organic computational device. A key mechanism that enables the formation of complex architecture in the developing brain is the emergence of traveling spatio-temporal waves of neuronal activity across the growing brain. Inspired by this strategy, we attempt to efficiently self-organize large neural networks with an arbitrary number of layers into a wide variety of architectures. To achieve this, we propose a modular tool-kit in the form of a dynamical system that can be seamlessly stacked to assemble multi-layer neural networks. The dynamical system encapsulates the dynamics of spiking units, their inter/intra layer interactions as well as the plasticity rules that control the flow of information between layers. The key features of our tool-kit are (1) autonomous spatio-temporal waves across multiple layers triggered by activity in the preceding layer and (2) Spike-timing dependent plasticity (STDP) learning rules that update the inter-layer connectivity based on wave activity in the connecting layers. Our framework leads to the self-organization of a wide variety of architectures, ranging from multi-layer perceptrons to autoencoders. We also demonstrate that emergent waves can self-organize spiking network architecture to perform unsupervised learning, and networks can be coupled with a linear classifier to perform classification on classic image datasets like MNIST. Broadly, our work shows that a dynamical systems framework for learning can be used to self-organize large computational devices.",2020-06-12T01:44:48Z,2020-06-12T01:44:48Z,http://arxiv.org/abs/2006.06902v1,http://arxiv.org/pdf/2006.06902v1,"cs.NE, cs.LG, q-bio.NC"
Evaluation of Neural Architectures Trained with Square Loss vs   Cross-Entropy in Classification Tasks,"Like Hui, Mikhail Belkin","Modern neural architectures for classification tasks are trained using the cross-entropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be well-founded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks.   We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.",2020-06-12T17:00:49Z,2021-10-23T00:36:12Z,http://arxiv.org/abs/2006.07322v5,http://arxiv.org/pdf/2006.07322v5,"cs.LG, stat.ML"
Theory-Inspired Path-Regularized Differential Network Architecture   Search,"Pan Zhou, Caiming Xiong, Richard Socher, Steven C. H. Hoi","Despite its high search efficiency, differential architecture search (DARTS) often selects network architectures with dominated skip connections which lead to performance degradation. However, theoretical understandings on this issue remain absent, hindering the development of more advanced methods in a principled way. In this work, we solve this problem by theoretically analyzing the effects of various types of operations, e.g. convolution, skip connection and zero operation, to the network optimization. We prove that the architectures with more skip connections can converge faster than the other candidates, and thus are selected by DARTS. This result, for the first time, theoretically and explicitly reveals the impact of skip connections to fast network optimization and its competitive advantage over other types of operations in DARTS. Then we propose a theory-inspired path-regularized DARTS that consists of two key modules: (i) a differential group-structured sparse binary gate introduced for each operation to avoid unfair competition among operations, and (ii) a path-depth-wise regularization used to incite search exploration for deep architectures that often converge slower than shallow ones as shown in our theory and are not well explored during the search. Experimental results on image classification tasks validate its advantages.",2020-06-30T05:28:23Z,2020-10-12T12:12:55Z,http://arxiv.org/abs/2006.16537v2,http://arxiv.org/pdf/2006.16537v2,"cs.LG, cs.CV, math.OC, stat.ML"
Automated Enterprise Architecture Model Mining,"Peter Hillmann, Erik Heiland, Andreas Karcher","Metadata are like the steam engine of the 21st century, driving businesses and offer multiple enhancements. Nevertheless, many companies are unaware that these data can be used efficiently to improve their own operation. This is where the Enterprise Architecture Framework comes in. It empowers an organisation to get a clear view of their business, application, technical and physical layer. This modelling approach is an established method for organizations to take a deeper look into their structure and processes. The development of such models requires a great deal of effort, is carried out manually by interviewing stakeholders and requires continuous maintenance. Our new approach enables the automated mining of Enterprise Architecture models. The system uses common technologies to collect the metadata based on network traffic, log files and other information in an organisation. Based on this, the new approach generates EA models with the desired views points. Furthermore, a rule and knowledge-based reasoning is used to obtain a holistic overview. This offers a strategic decision support from business structure over process design up to planning the appropriate support technology. Therefore, it forms the base for organisations to act in an agile way. The modelling can be performed in different modelling languages, including ArchiMate and the Nato Architecture Framework (NAF). The designed approach is already evaluated on a small company with multiple services and an infrastructure with several nodes.",2021-08-15T09:01:57Z,2021-08-15T09:01:57Z,http://arxiv.org/abs/2108.06696v1,http://arxiv.org/pdf/2108.06696v1,"cs.IR, cs.CR, cs.NI, cs.SY, eess.SY"
Towards Deep and Efficient: A Deep Siamese Self-Attention Fully   Efficient Convolutional Network for Change Detection in VHR Images,"Hongruixuan Chen, Chen Wu, Bo Du","Recently, FCNs have attracted widespread attention in the CD field. In pursuit of better CD performance, it has become a tendency to design deeper and more complicated FCNs, which inevitably brings about huge numbers of parameters and an unbearable computational burden. With the goal of designing a quite deep architecture to obtain more precise CD results while simultaneously decreasing parameter numbers to improve efficiency, in this work, we present a very deep and efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the numerous parameters associated with deep architecture, an efficient convolution consisting of depth-wise convolution and group convolution with a channel shuffle mechanism is introduced to replace standard convolutional layers. In terms of the specific network architecture, EffCDNet does not use mainstream UNet-like architecture, but rather adopts the architecture with a very deep encoder and a lightweight decoder. In the very deep encoder, two very deep siamese streams stacked by efficient convolution first extract two highly representative and informative feature maps from input image-pairs. Subsequently, an efficient ASPP module is designed to capture multi-scale change information. In the lightweight decoder, a recurrent criss-cross self-attention (RCCA) module is applied to efficiently utilize non-local similar feature representations to enhance discriminability for each pixel, thus effectively separating the changed and unchanged regions. Moreover, to tackle the optimization problem in confused pixels, two novel loss functions based on information entropy are presented. On two challenging CD datasets, our approach outperforms other SOTA FCN-based methods, with only benchmark-level parameter numbers and quite low computational overhead.",2021-08-18T14:02:38Z,2021-08-18T14:02:38Z,http://arxiv.org/abs/2108.08157v1,http://arxiv.org/pdf/2108.08157v1,"cs.CV, cs.LG, eess.IV"
Study of the Utility of Text Classification Based Software Architecture   Recovery Method RELAX for Maintenance,"Daniel Link, Kamonphop Srisopha, Barry Boehm","Background. The software architecture recovery method RELAX produces a concern-based architectural view of a software system graphically and textually from that system's source code. The method has been implemented in software which can be run on subject systems whose source code is written in Java. Aims. Our aim was to find out whether the availability of architectural views produced by RELAX can help maintainers who are new to a project in becoming productive with development tasks sooner, and find out how they felt about working in such an environment. Method. We conducted a user study with nine participants. They were subjected to a controlled experiment in which maintenance success and speed with and without access to RELAX recovery results were compared to each other. Results. We have observed that employing architecture views produced by RELAX helped participants reduce time to get started on maintenance tasks by a factor of 5.38 or more. While most participants were unable to finish their tasks within the allotted time when they did not have recovery results available, all of them finished them successfully when they did. Additionally, participants reported that these views were easy to understand, helped them to learn the system's structure and enabled them to compare different versions of the system. Conclusions. In the speedup experienced to the start of maintenance experienced by the participants as well as in their experience-based opinions, RELAX has shown itself to be a valuable help that could form the basis for further tools that specifically support the development process with a focus on maintenance.",2021-08-30T23:40:29Z,2021-08-30T23:40:29Z,http://arxiv.org/abs/2108.13553v1,http://arxiv.org/pdf/2108.13553v1,"cs.SE, D.2.11"
Approximate Neural Architecture Search via Operation Distribution   Learning,"Xingchen Wan, Binxin Ru, Pedro M. Esperança, Fabio M. Carlucci","The standard paradigm in Neural Architecture Search (NAS) is to search for a fully deterministic architecture with specific operations and connections. In this work, we instead propose to search for the optimal operation distribution, thus providing a stochastic and approximate solution, which can be used to sample architectures of arbitrary length. We propose and show, that given an architectural cell, its performance largely depends on the ratio of used operations, rather than any specific connection pattern in typical search spaces; that is, small changes in the ordering of the operations are often irrelevant. This intuition is orthogonal to any specific search strategy and can be applied to a diverse set of NAS algorithms. Through extensive validation on 4 data-sets and 4 NAS techniques (Bayesian optimisation, differentiable search, local search and random search), we show that the operation distribution (1) holds enough discriminating power to reliably identify a solution and (2) is significantly easier to optimise than traditional encodings, leading to large speed-ups at little to no cost in performance. Indeed, this simple intuition significantly reduces the cost of current approaches and potentially enable NAS to be used in a broader range of applications.",2021-11-08T17:38:29Z,2021-11-08T17:38:29Z,http://arxiv.org/abs/2111.04670v1,http://arxiv.org/pdf/2111.04670v1,"cs.LG, cs.CV, stat.ML"
A Multi-criteria Approach to Evolve Sparse Neural Architectures for   Stock Market Forecasting,"Faizal Hafiz, Jan Broekaert, Davide La Torre, Akshya Swain","This study proposes a new framework to evolve efficacious yet parsimonious neural architectures for the movement prediction of stock market indices using technical indicators as inputs. In the light of a sparse signal-to-noise ratio under the Efficient Market hypothesis, developing machine learning methods to predict the movement of a financial market using technical indicators has shown to be a challenging problem. To this end, the neural architecture search is posed as a multi-criteria optimization problem to balance the efficacy with the complexity of architectures. In addition, the implications of different dominant trading tendencies which may be present in the pre-COVID and within-COVID time periods are investigated. An $\epsilon-$ constraint framework is proposed as a remedy to extract any concordant information underlying the possibly conflicting pre-COVID data. Further, a new search paradigm, Two-Dimensional Swarms (2DS) is proposed for the multi-criteria neural architecture search, which explicitly integrates sparsity as an additional search dimension in particle swarms. A detailed comparative evaluation of the proposed approach is carried out by considering genetic algorithm and several combinations of empirical neural design rules with a filter-based feature selection method (mRMR) as baseline approaches. The results of this study convincingly demonstrate that the proposed approach can evolve parsimonious networks with better generalization capabilities.",2021-11-15T19:44:10Z,2021-11-15T19:44:10Z,http://arxiv.org/abs/2111.08060v1,http://arxiv.org/pdf/2111.08060v1,"cs.NE, q-fin.ST"
Physics Informed Neural Networks for Control Oriented Thermal Modeling   of Buildings,"Gargya Gokhale, Bert Claessens, Chris Develder","This paper presents a data-driven modeling approach for developing control-oriented thermal models of buildings. These models are developed with the objective of reducing energy consumption costs while controlling the indoor temperature of the building within required comfort limits. To combine the interpretability of white/gray box physics models and the expressive power of neural networks, we propose a physics informed neural network approach for this modeling task. Along with measured data and building parameters, we encode the neural networks with the underlying physics that governs the thermal behavior of these buildings. Thus, realizing a model that is guided by physics, aids in modeling the temporal evolution of room temperature and power consumption as well as the hidden state, i.e., the temperature of building thermal mass for subsequent time steps. The main research contributions of this work are: (1) we propose two variants of physics informed neural network architectures for the task of control-oriented thermal modeling of buildings, (2) we show that training these architectures is data-efficient, requiring less training data compared to conventional, non-physics informed neural networks, and (3) we show that these architectures achieve more accurate predictions than conventional neural networks for longer prediction horizons. We test the prediction performance of the proposed architectures using simulated and real-word data to demonstrate (2) and (3) and show that the proposed physics informed neural network architectures can be used for this control-oriented modeling problem.",2021-11-23T18:27:54Z,2022-03-21T12:36:01Z,http://arxiv.org/abs/2111.12066v2,http://arxiv.org/pdf/2111.12066v2,"eess.SP, cs.LG, cs.SY, eess.SY"
"Assessing Policy, Loss and Planning Combinations in Reinforcement   Learning using a New Modular Architecture","Tiago Gaspar Oliveira, Arlindo L. Oliveira","The model-based reinforcement learning paradigm, which uses planning algorithms and neural network models, has recently achieved unprecedented results in diverse applications, leading to what is now known as deep reinforcement learning. These agents are quite complex and involve multiple components, factors that can create challenges for research. In this work, we propose a new modular software architecture suited for these types of agents, and a set of building blocks that can be easily reused and assembled to construct new model-based reinforcement learning agents. These building blocks include planning algorithms, policies, and loss functions.   We illustrate the use of this architecture by combining several of these building blocks to implement and test agents that are optimized to three different test environments: Cartpole, Minigrid, and Tictactoe. One particular planning algorithm, made available in our implementation and not previously used in reinforcement learning, which we called averaged minimax, achieved good results in the three tested environments.   Experiments performed with this architecture have shown that the best combination of planning algorithm, policy, and loss function is heavily problem dependent. This result provides evidence that the proposed architecture, which is modular and reusable, is useful for reinforcement learning researchers who want to study new environments and techniques.",2022-01-08T18:30:25Z,2022-01-08T18:30:25Z,http://arxiv.org/abs/2201.02874v1,http://arxiv.org/pdf/2201.02874v1,"cs.LG, cs.AI, 49L20, I.2.6; I.2.8"
Dual-Tasks Siamese Transformer Framework for Building Damage Assessment,"Hongruixuan Chen, Edoardo Nemni, Sofia Vallecorsa, Xi Li, Chen Wu, Lars Bromley","Accurate and fine-grained information about the extent of damage to buildings is essential for humanitarian relief and disaster response. However, as the most commonly used architecture in remote sensing interpretation tasks, Convolutional Neural Networks (CNNs) have limited ability to model the non-local relationship between pixels. Recently, Transformer architecture first proposed for modeling long-range dependency in natural language processing has shown promising results in computer vision tasks. Considering the frontier advances of Transformer architecture in the computer vision field, in this paper, we present the first attempt at designing a Transformer-based damage assessment architecture (DamFormer). In DamFormer, a siamese Transformer encoder is first constructed to extract non-local and representative deep features from input multitemporal image-pairs. Then, a multitemporal fusion module is designed to fuse information for downstream tasks. Finally, a lightweight dual-tasks decoder aggregates multi-level features for final prediction. To the best of our knowledge, it is the first time that such a deep Transformer-based network is proposed for multitemporal remote sensing interpretation tasks. The experimental results on the large-scale damage assessment dataset xBD demonstrate the potential of the Transformer-based architecture.",2022-01-26T14:11:16Z,2022-05-28T13:01:11Z,http://arxiv.org/abs/2201.10953v2,http://arxiv.org/pdf/2201.10953v2,"cs.CV, cs.LG, eess.IV"
UrbanFlow: Designing Comfortable Outdoor Areas,"Daoming Liu, Florian Rist, Helmut Pottmann, Dominik Michels","Design decisions in urban planning have to be made with particular carefulness as the resulting constraints are binding for the whole architectural design that follows. In this context, investigating and optimizing the airflow in urban environments is critical to design comfortable outdoor areas as unwanted effects such as windy areas and the formation of heat pockets have to be avoided. Our UrbanFlow framework enables interactive architectural design allowing for decision making based on simulating urban flow. Compared to real-time fluid flow simulation, enabling interactive architecture design poses an even higher computational efficiency challenge as evaluating a design by simulation usually requires hundreds of time steps. This is addressed based on a highly efficient Eulerian fluid simulator in which we incorporate a unified porosity model which is devised to encode digital urban models containing objects such as buildings and trees. UrbanFlow is equipped with an optimization routine enabling the direct computation of design adaptations improving livability and comfort for given parameterized architectural designs. To ensure convergence of the optimization process, instead of the classical Navier-Stokes equations, the Reynolds-averaged Navier-Stokes equations are solved as this can be done on a relatively coarse grid and allows for the decoupling of the effects of turbulent eddies which are taken into account using a separate turbulence model. As we demonstrate on a real-world example taken from an ongoing architectural competition, this results in a fast convergence of the optimization process which computes a design adaptation avoiding heat pockets as well as uncomfortable windy areas.",2022-04-03T16:55:23Z,2022-04-03T16:55:23Z,http://arxiv.org/abs/2204.01117v1,http://arxiv.org/pdf/2204.01117v1,"cs.GR, physics.flu-dyn"
A physics-informed deep neural network for surrogate modeling in   classical elasto-plasticity,"Mahdad Eghbalian, Mehdi Pouragha, Richard Wan","In this work, we present a deep neural network architecture that can efficiently approximate classical elasto-plastic constitutive relations. The network is enriched with crucial physics aspects of classical elasto-plasticity, including additive decomposition of strains into elastic and plastic parts, and nonlinear incremental elasticity. This leads to a Physics-Informed Neural Network (PINN) surrogate model named here as Elasto-Plastic Neural Network (EPNN). Detailed analyses show that embedding these physics into the architecture of the neural network facilitates a more efficient training of the network with less training data, while also enhancing the extrapolation capability for loading regimes outside the training data. The architecture of EPNN is model and material-independent, i.e. it can be adapted to a wide range of elasto-plastic material types, including geomaterials and metals; and experimental data can potentially be directly used in training the network. To demonstrate the robustness of the proposed architecture, we adapt its general framework to the elasto-plastic behavior of sands. We use synthetic data generated from material point simulations based on a relatively advanced dilatancy-based constitutive model for granular materials to train the neural network. The superiority of EPNN over regular neural network architectures is explored through predicting unseen strain-controlled loading paths for sands with different initial densities.",2022-04-26T05:58:13Z,2022-04-26T05:58:13Z,http://arxiv.org/abs/2204.12088v1,http://arxiv.org/pdf/2204.12088v1,"cs.LG, cs.CE, 74C05, 65N99, J.2; I.6.5"
N2N Learning: Network to Network Compression via Policy Gradient   Reinforcement Learning,"Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, Kris M. Kitani","While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger `teacher' network as input and outputs a compressed `student' network derived from the `teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large `teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input `teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller `teacher' networks can be used to rapidly speed up training on larger `teacher' networks.",2017-09-18T16:26:53Z,2017-12-17T11:46:06Z,http://arxiv.org/abs/1709.06030v2,http://arxiv.org/pdf/1709.06030v2,"cs.LG, stat.ML"
A Hardware-Software Blueprint for Flexible Deep Learning Specialization,"Thierry Moreau, Tianqi Chen, Luis Vega, Jared Roesch, Eddie Yan, Lianmin Zheng, Josh Fromm, Ziheng Jiang, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy","Specialized Deep Learning (DL) acceleration stacks, designed for a specific set of frameworks, model architectures, operators, and data types, offer the allure of high performance while sacrificing flexibility. Changes in algorithms, models, operators, or numerical systems threaten the viability of specialized hardware accelerators. We propose VTA, a programmable deep learning architecture template designed to be extensible in the face of evolving workloads. VTA achieves this flexibility via a parametrizable architecture, two-level ISA, and a JIT compiler. The two-level ISA is based on (1) a task-ISA that explicitly orchestrates concurrent compute and memory tasks and (2) a microcode-ISA which implements a wide variety of operators with single-cycle tensor-tensor operations. Next, we propose a runtime system equipped with a JIT compiler for flexible code-generation and heterogeneous execution that enables effective use of the VTA architecture. VTA is integrated and open-sourced into Apache TVM, a state-of-the-art deep learning compilation stack that provides flexibility for diverse models and divergent hardware backends. We propose a flow that performs design space exploration to generate a customized hardware architecture and software operator library that can be leveraged by mainstream learning frameworks. We demonstrate our approach by deploying optimized deep learning models used for object classification and style transfer on edge-class FPGAs.",2018-07-11T15:19:30Z,2019-04-23T00:50:43Z,http://arxiv.org/abs/1807.04188v3,http://arxiv.org/pdf/1807.04188v3,"cs.LG, cs.DC, stat.ML"
Assessing the Scalability of Biologically-Motivated Deep Learning   Algorithms and Architectures,"Sergey Bartunov, Adam Santoro, Blake A. Richards, Luke Marris, Geoffrey E. Hinton, Timothy Lillicrap","The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.",2018-07-12T12:53:50Z,2018-11-20T14:26:44Z,http://arxiv.org/abs/1807.04587v2,http://arxiv.org/pdf/1807.04587v2,"cs.LG, cs.AI, cs.NE, stat.ML"
Error Forward-Propagation: Reusing Feedforward Connections to Propagate   Errors in Deep Learning,"Adam A. Kohan, Edward A. Rietman, Hava T. Siegelmann","We introduce Error Forward-Propagation, a biologically plausible mechanism to propagate error feedback forward through the network. Architectural constraints on connectivity are virtually eliminated for error feedback in the brain; systematic backward connectivity is not used or needed to deliver error feedback. Feedback as a means of assigning credit to neurons earlier in the forward pathway for their contribution to the final output is thought to be used in learning in the brain. How the brain solves the credit assignment problem is unclear. In machine learning, error backpropagation is a highly successful mechanism for credit assignment in deep multilayered networks. Backpropagation requires symmetric reciprocal connectivity for every neuron. From a biological perspective, there is no evidence of such an architectural constraint, which makes backpropagation implausible for learning in the brain. This architectural constraint is reduced with the use of random feedback weights. Models using random feedback weights require backward connectivity patterns for every neuron, but avoid symmetric weights and reciprocal connections. In this paper, we practically remove this architectural constraint, requiring only a backward loop connection for effective error feedback. We propose reusing the forward connections to deliver the error feedback by feeding the outputs into the input receiving layer. This mechanism, Error Forward-Propagation, is a plausible basis for how error feedback occurs deep in the brain independent of and yet in support of the functionality underlying intricate network architectures. We show experimentally that recurrent neural networks with two and three hidden layers can be trained using Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving $1.90\%$ and $11\%$ generalization errors respectively.",2018-08-09T21:52:10Z,2018-08-09T21:52:10Z,http://arxiv.org/abs/1808.03357v1,http://arxiv.org/pdf/1808.03357v1,"cs.NE, q-bio.NC"
Decentralized Equalization with Feedforward Architectures for Massive   MU-MIMO,"Charles Jeon, Kaipeng Li, Joseph R. Cavallaro, Christoph Studer","Linear data-detection algorithms that build on zero forcing (ZF) or linear minimum mean-square error (L-MMSE) equalization achieve near-optimal spectral efficiency in massive multi-user multiple-input multiple-output (MU-MIMO) systems. Such algorithms, however, typically rely on centralized processing at the base-station (BS) which results in (i) excessive interconnect and chip input/output (I/O) data rates and (ii) high computational complexity. Decentralized baseband processing (DBP) partitions the BS antenna array into independent clusters that are associated with separate radio-frequency circuitry and computing fabrics in order to overcome the limitations of centralized processing. In this paper, we investigate decentralized equalization with feedforward architectures that minimize the latency bottlenecks of existing DBP solutions. We propose two distinct architectures with different interconnect and I/O bandwidth requirements that fuse the local equalization results of each cluster in a feedforward network. For both architectures, we consider maximum ratio combining, ZF, L-MMSE, and a nonlinear equalization algorithm that relies on approximate message passing, and we analyze the associated post-equalization signal-to-noise-and-interference-ratio (SINR). We provide reference implementation results on a multi graphics processing unit (GPU) system which demonstrate that decentralized equalization with feedforward architectures enables throughputs in the Gb/s regime and incurs no or only a small performance loss compared to centralized solutions.",2018-08-13T21:19:39Z,2019-06-20T17:36:44Z,http://arxiv.org/abs/1808.04473v2,http://arxiv.org/pdf/1808.04473v2,"cs.IT, eess.SP, math.IT"
Universality and individuality in neural dynamics across large   populations of recurrent networks,"Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, David Sussillo","Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold---the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics---often appears universal across all architectures.",2019-07-19T15:35:38Z,2019-12-04T20:43:41Z,http://arxiv.org/abs/1907.08549v2,http://arxiv.org/pdf/1907.08549v2,"q-bio.NC, cs.NE"
AutoShrink: A Topology-aware NAS for Discovering Efficient Neural   Architecture,"Tunhou Zhang, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Li, Yiran Chen","Resource is an important constraint when deploying Deep Neural Networks (DNNs) on mobile and edge devices. Existing works commonly adopt the cell-based search approach, which limits the flexibility of network patterns in learned cell structures. Moreover, due to the topology-agnostic nature of existing works, including both cell-based and node-based approaches, the search process is time consuming and the performance of found architecture may be sub-optimal. To address these problems, we propose AutoShrink, a topology-aware Neural Architecture Search(NAS) for searching efficient building blocks of neural architectures. Our method is node-based and thus can learn flexible network patterns in cell structures within a topological search space. Directed Acyclic Graphs (DAGs) are used to abstract DNN architectures and progressively optimize the cell structure through edge shrinking. As the search space intrinsically reduces as the edges are progressively shrunk, AutoShrink explores more flexible search space with even less search time. We evaluate AutoShrink on image classification and language tasks by crafting ShrinkCNN and ShrinkRNN models. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34% Multiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of state-of-the-art (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are crafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting time of SOTA CNN and RNN models, respectively.",2019-11-21T02:40:00Z,2019-11-21T02:40:00Z,http://arxiv.org/abs/1911.09251v1,http://arxiv.org/pdf/1911.09251v1,"cs.LG, stat.ML"
A Permutation-Equivariant Neural Network Architecture For Auction Design,"Jad Rahme, Samy Jelassi, Joan Bruna, S. Matthew Weinberg","Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. Theoretical approaches to the problem have hit some limits in the past decades and analytical solutions are known for only a few simple settings. Computational approaches to the problem through the use of LPs have their own set of limitations. Building on the success of deep learning, a new approach was recently proposed by Duetting et al. (2019) in which the auction is modeled by a feed-forward neural network and the design problem is framed as a learning problem. The neural architectures used in that work are general purpose and do not take advantage of any of the symmetries the problem could present, such as permutation equivariance. In this work, we consider auction design problems that have permutation-equivariant symmetry and construct a neural architecture that is capable of perfectly recovering the permutation-equivariant optimal mechanism, which we show is not possible with the previous architecture. We demonstrate that permutation-equivariant architectures are not only capable of recovering previous results, they also have better generalization properties.",2020-03-02T00:37:36Z,2021-10-25T15:41:56Z,http://arxiv.org/abs/2003.01497v4,http://arxiv.org/pdf/2003.01497v4,"cs.GT, cs.LG, stat.ML"
Cluster Pruning: An Efficient Filter Pruning Method for Edge AI Vision   Applications,"Chinthaka Gamanayake, Lahiru Jayasinghe, Benny Ng, Chau Yuen","Even though the Convolutional Neural Networks (CNN) has shown superior results in the field of computer vision, it is still a challenging task to implement computer vision algorithms in real-time at the edge, especially using a low-cost IoT device due to high memory consumption and computation complexities in a CNN. Network compression methodologies such as weight pruning, filter pruning, and quantization are used to overcome the above mentioned problem. Even though filter pruning methodology has shown better performances compared to other techniques, irregularity of the number of filters pruned across different layers of a CNN might not comply with majority of the neural computing hardware architectures. In this paper, a novel greedy approach called cluster pruning has been proposed, which provides a structured way of removing filters in a CNN by considering the importance of filters and the underlying hardware architecture. The proposed methodology is compared with the conventional filter pruning algorithm on Pascal-VOC open dataset, and Head-Counting dataset, which is our own dataset developed to detect and count people entering a room. We benchmark our proposed method on three hardware architectures, namely CPU, GPU, and Intel Movidius Neural Computer Stick (NCS) using the popular SSD-MobileNet and SSD-SqueezeNet neural network architectures used for edge-AI vision applications. Results demonstrate that our method outperforms the conventional filter pruning methodology, using both datasets on above mentioned hardware architectures. Furthermore, a low cost IoT hardware setup consisting of an Intel Movidius-NCS is proposed to deploy an edge-AI application using our proposed pruning methodology.",2020-03-05T06:20:09Z,2020-03-05T06:20:09Z,http://arxiv.org/abs/2003.02449v1,http://arxiv.org/pdf/2003.02449v1,"cs.CV, cs.LG, eess.IV"
Thermodynamic Cost of Edge Detection in Artificial Neural   Network(ANN)-Based Processors,"Seçkin Barışık, İlke Ercan","Architecture-based heat dissipation analyses allow us to reveal fundamental sources of inefficiency in a given processor and thereby provide us with road-maps to design less dissipative computing schemes independent of technology-base used to implement them. In this work, we study architectural-level contributions to energy dissipation in an Artificial Neural Network (ANN)-based processor that is trained to perform edge-detection task. We compare the training and information processing cost of ANN to that of conventional architectures and algorithms using 64-pixel binary image. Our results reveal the inherent efficiency advantages of an ANN network trained for specific tasks over general-purpose processors based on von Neumann architecture. We also compare the proposed performance improvements to that of Cellular Array Processors (CAPs) and illustrate the reduction in dissipation for special purpose processors. Lastly, we calculate the change in dissipation as a result of input data structure and show the effect of randomness on energetic cost of information processing. The results we obtained provide a basis for comparison for task-based fundamental energy efficiency analyses for a range of processors and therefore contribute to the study of architecture-level descriptions of processors and thermodynamic cost calculations based on physics of computation.",2020-03-18T13:02:10Z,2020-10-29T10:15:49Z,http://arxiv.org/abs/2003.08196v2,http://arxiv.org/pdf/2003.08196v2,"eess.IV, cs.NE"
GAN Compression: Efficient Architectures for Interactive Conditional   GANs,"Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, Song Han","Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more compute-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method finds efficient architectures via neural architecture search. To accelerate the search process, we decouple the model training and search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings, network architectures, and learning methods. Without losing image quality, we reduce the computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by 9x, paving the way for interactive image synthesis.",2020-03-19T17:59:05Z,2021-11-11T03:45:16Z,http://arxiv.org/abs/2003.08936v4,http://arxiv.org/pdf/2003.08936v4,"cs.CV, eess.IV"
Optimizing Co-flows Scheduling and Routing in Data Centre Networks for   Big Data Applications,"Sanaa Hamid Mohamed, Ali Hammadi, Taisir E. H. El-Gorashi, Jaafar Mohamed Hashim Elmirghani","This paper optimizes the scheduling and routing of the co-flows of MapReduce shuffling phase in state-of-the-art and proposed Passive Optical Networking (PON)-based Data Centre Network (DCN) architectures. A time-slotted Mixed Integer Linear Programming (MILP) model is developed and used for the optimization with the objective of minimizing either the total energy consumption or the completion time. The DCN architectures include four state-of-the-art electronic switching architectures which are spine-leaf, Fat-tree, BCube, and DCell data centres. The proposed PON-based DCN architectures include two designs that utilize ports in Optical Line Terminal (OLT) line cards for inter and possibly intra data centre networking in addition to passive interconnects for the intra data centre networking between different PON groups (i.e. racks) within a PON cell (i.e. number of PON groups connected to a single OLT port). The first design is a switch-centric design that uses two Arrayed Waveguide Grating Routers (AWGRs) and the second is a server-centric design. The study also considers different traffic patterns defined according to the distribution of map and reduce tasks in the servers and data skewness.",2020-08-08T11:32:09Z,2020-08-08T11:32:09Z,http://arxiv.org/abs/2008.03497v1,http://arxiv.org/pdf/2008.03497v1,"cs.NI, eess.SP"
S-vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based   on Transformer Encoder,"N J Metilda Sagaya Mary, S Umesh, Sandesh V Katta","One of the most popular speaker embeddings is x-vectors, which are obtained from an architecture that gradually builds a larger temporal context with layers. In this paper, we propose to derive speaker embeddings from Transformer's encoder trained for speaker classification. Self-attention, on which Transformer's encoder is built, attends to all the features over the entire utterance and might be more suitable in capturing the speaker characteristics in an utterance. We refer to the speaker embeddings obtained from the proposed speaker classification model as s-vectors to emphasize that they are obtained from an architecture that heavily relies on self-attention. Through experiments, we demonstrate that s-vectors perform better than x-vectors. In addition to the s-vectors, we also propose a new architecture based on Transformer's encoder for speaker verification as a replacement for speaker verification based on conventional probabilistic linear discriminant analysis (PLDA). This architecture is inspired by the next sentence prediction task of bidirectional encoder representations from Transformers (BERT), and we feed the s-vectors of two utterances to verify whether they belong to the same speaker. We name this architecture the Transformer encoder speaker authenticator (TESA). Our experiments show that the performance of s-vectors with TESA is better than s-vectors with conventional PLDA-based speaker verification.",2020-08-11T12:23:21Z,2021-12-12T09:08:01Z,http://arxiv.org/abs/2008.04659v2,http://arxiv.org/pdf/2008.04659v2,"eess.AS, cs.SD"
RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty   Quantification in Medical Image Segmentation,"Marc Gantenbein, Ertunc Erdil, Ender Konukoglu","Quantifying segmentation uncertainty has become an important issue in medical image analysis due to the inherent ambiguity of anatomical structures and its pathologies. Recently, neural network-based uncertainty quantification methods have been successfully applied to various problems. One of the main limitations of the existing techniques is the high memory requirement during training; which limits their application to processing smaller field-of-views (FOVs) and/or using shallower architectures. In this paper, we investigate the effect of using reversible blocks for building memory-efficient neural network architectures for quantification of segmentation uncertainty. The reversible architecture achieves memory saving by exactly computing the activations from the outputs of the subsequent layers during backpropagation instead of storing the activations for each layer. We incorporate the reversible blocks into a recently proposed architecture called PHiSeg that is developed for uncertainty quantification in medical image segmentation. The reversible architecture, RevPHiSeg, allows training neural networks for quantifying segmentation uncertainty on GPUs with limited memory and processing larger FOVs. We perform experiments on the LIDC-IDRI dataset and an in-house prostate dataset, and present comparisons with PHiSeg. The results demonstrate that RevPHiSeg consumes ~30% less memory compared to PHiSeg while achieving very similar segmentation accuracy.",2020-08-16T21:16:19Z,2020-08-18T08:18:44Z,http://arxiv.org/abs/2008.06999v2,http://arxiv.org/pdf/2008.06999v2,"eess.IV, cs.CV"
Graph Neural Network Architecture Search for Molecular Property   Prediction,"Shengli Jiang, Prasanna Balaprakash","Predicting the properties of a molecule from its structure is a challenging task. Recently, deep learning methods have improved the state of the art for this task because of their ability to learn useful features from the given data. By treating molecule structure as graphs, where atoms and bonds are modeled as nodes and edges, graph neural networks (GNNs) have been widely used to predict molecular properties. However, the design and development of GNNs for a given data set rely on labor-intensive design and tuning of the network architectures. Neural architecture search (NAS) is a promising approach to discover high-performing neural network architectures automatically. To that end, we develop an NAS approach to automate the design and development of GNNs for molecular property prediction. Specifically, we focus on automated development of message-passing neural networks (MPNNs) to predict the molecular properties of small molecules in quantum mechanics and physical chemistry data sets from the MoleculeNet benchmark. We demonstrate the superiority of the automatically discovered MPNNs by comparing them with manually designed GNNs from the MoleculeNet benchmark. We study the relative importance of the choices in the MPNN search space, demonstrating that customizing the architecture is critical to enhancing performance in molecular property prediction and that the proposed approach can perform customization automatically with minimal manual effort.",2020-08-27T15:30:57Z,2020-08-27T15:30:57Z,http://arxiv.org/abs/2008.12187v1,http://arxiv.org/pdf/2008.12187v1,"cs.LG, q-bio.BM, stat.ML"
DANCE: Differentiable Accelerator/Network Co-Exploration,"Kanghyun Choi, Deokki Hong, Hojae Yoon, Joonsang Yu, Youngsok Kim, Jinho Lee","To cope with the ever-increasing computational demand of the DNN execution, recent neural architecture search (NAS) algorithms consider hardware cost metrics into account, such as GPU latency. To further pursue a fast, efficient execution, DNN-specialized hardware accelerators are being designed for multiple purposes, which far-exceeds the efficiency of the GPUs. However, those hardware-related metrics have been proven to exhibit non-linear relationships with the network architectures. Therefore it became a chicken-and-egg problem to optimize the network against the accelerator, or to optimize the accelerator against the network. In such circumstances, this work presents DANCE, a differentiable approach towards the co-exploration of the hardware accelerator and network architecture design. At the heart of DANCE is a differentiable evaluator network. By modeling the hardware evaluation software with a neural network, the relation between the accelerator architecture and the hardware metrics becomes differentiable, allowing the search to be performed with backpropagation. Compared to the naive existing approaches, our method performs co-exploration in a significantly shorter time, while achieving superior accuracy and hardware cost metrics.",2020-09-14T07:43:27Z,2021-02-16T04:41:17Z,http://arxiv.org/abs/2009.06237v3,http://arxiv.org/pdf/2009.06237v3,"cs.LG, cs.AI, cs.AR, stat.ML"
Bringing Network Coding into SDN: A Case-study for Highly Meshed   Heterogeneous Communications,"Alejandro Cohen, Homa Esfahanizadeh, Bruno Sousa, João P. Vilela, Miguel Luís, Duarte Raposo, Francois Michel, Susana Sargento, Muriel Médard","Modern communications have moved away from point-to-point models to increasingly heterogeneous network models. In this article, we propose a novel controller-based protocol to deploy adaptive causal network coding in heterogeneous and highly-meshed communication networks. Specifically, we consider using Software-Defined-Network (SDN) as the main controller. We first present an architecture for the highly-meshed heterogeneous multi-source multi-destination networks that represents the practical communication networks encountered in the fifth generation of wireless networks (5G) and beyond. Next, we present a promising solution to deploy network coding over the new architecture. In fact, we investigate how to generalize adaptive and causal random linear network coding (AC-RLNC), proposed for multipath multi-hop (MP-MH) communication channels, to a protocol for the new multi-source multi-destination network architecture using controller. To this end, we present a modularized implementation of AC-RLNC solution where the modules work together in a distributed fashion and perform the AC-RLNC technology. We also present a new controller-based setting through which the network coding modules can communicate and can attain their required information. Finally, we briefly discuss how the proposed architecture and network coding solution provide a good opportunity for future technologies, e.g., distributed coded computation and storage, mmWave communication environments, and innovative and efficient security features.",2020-10-01T12:20:08Z,2020-10-01T12:20:08Z,http://arxiv.org/abs/2010.00343v1,http://arxiv.org/pdf/2010.00343v1,"cs.NI, cs.IT, math.IT"
AgEBO-Tabular: Joint Neural Architecture and Hyperparameter Search with   Autotuned Data-Parallel Training for Tabular Data,"Romain Egele, Prasanna Balaprakash, Venkatram Vishwanath, Isabelle Guyon, Zhengying Liu","Developing high-performing predictive models for large tabular data sets is a challenging task. The state-of-the-art methods are based on expert-developed model ensembles from different supervised learning methods. Recently, automated machine learning (AutoML) is emerging as a promising approach to automate predictive model development. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural network architectures concurrently and improves the accuracy of the generated models iteratively. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training is a promising approach that can address this issue, its use within NAS is difficult. For different data sets, the data-parallel training settings such as the number of parallel processes, learning rate, and batch size need to be adapted to achieve high accuracy and reduction in training time. To that end, we have developed AgEBO-Tabular, an approach to combine aging evolution (AgE), a parallel NAS method that searches over neural architecture space, and an asynchronous Bayesian optimization method for tuning the hyperparameters of the data-parallel training simultaneously. We demonstrate the efficacy of the proposed method to generate high-performing neural network models for large tabular benchmark data sets. Furthermore, we demonstrate that the automatically discovered neural network models using our method outperform the state-of-the-art AutoML ensemble models in inference speed by two orders of magnitude while reaching similar accuracy values.",2020-10-30T16:28:48Z,2021-10-26T12:23:43Z,http://arxiv.org/abs/2010.16358v2,http://arxiv.org/pdf/2010.16358v2,"cs.LG, cs.NE, stat.ML"
DR-Unet104 for Multimodal MRI brain tumor segmentation,"Jordan Colman, Lei Zhang, Wenting Duan, Xujiong Ye","In this paper we propose a 2D deep residual Unet with 104 convolutional layers (DR-Unet104) for lesion segmentation in brain MRIs. We make multiple additions to the Unet architecture, including adding the 'bottleneck' residual block to the Unet encoder and adding dropout after each convolution block stack. We verified the effect of introducing the regularisation of dropout with small rate (e.g. 0.2) on the architecture, and found a dropout of 0.2 improved the overall performance compared to no dropout, or a dropout of 0.5. We evaluated the proposed architecture as part of the Multimodal Brain Tumor Segmentation (BraTS) 2020 Challenge and compared our method to DeepLabV3+ with a ResNet-V2-152 backbone. We found that the DR-Unet104 achieved a mean dice score coefficient of 0.8862, 0.6756 and 0.6721 for validation data, whole tumor, enhancing tumor and tumor core respectively, an overall improvement on 0.8770, 0.65242 and 0.68134 achieved by DeepLabV3+. Our method produced a final mean DSC of 0.8673, 0.7514 and 0.7983 on whole tumor, enhancing tumor and tumor core on the challenge's testing data. We produced a competitive lesion segmentation architecture, despite only 2D convolutions, having the added benefit that it can be used on lower power computers than a 3D architecture. The source code and trained model for this work is openly available at https://github.com/jordan-colman/DR-Unet104.",2020-11-04T01:24:26Z,2021-05-04T14:25:49Z,http://arxiv.org/abs/2011.02840v2,http://arxiv.org/pdf/2011.02840v2,"eess.IV, cs.CV, cs.LG"
Disentangling Quantum and Classical Contributions in Hybrid Quantum   Machine Learning Architectures,"Michael Kölle, Jonas Maurer, Philipp Altmann, Leo Sünkel, Jonas Stein, Claudia Linnhoff-Popien","Quantum computing offers the potential for superior computational capabilities, particularly for data-intensive tasks. However, the current state of quantum hardware puts heavy restrictions on input size. To address this, hybrid transfer learning solutions have been developed, merging pre-trained classical models, capable of handling extensive inputs, with variational quantum circuits. Yet, it remains unclear how much each component -- classical and quantum -- contributes to the model's results. We propose a novel hybrid architecture: instead of utilizing a pre-trained network for compression, we employ an autoencoder to derive a compressed version of the input data. This compressed data is then channeled through the encoder part of the autoencoder to the quantum component. We assess our model's classification capabilities against two state-of-the-art hybrid transfer learning architectures, two purely classical architectures and one quantum architecture. Their accuracy is compared across four datasets: Banknote Authentication, Breast Cancer Wisconsin, MNIST digits, and AudioMNIST. Our research suggests that classical components significantly influence classification in hybrid transfer learning, a contribution often mistakenly ascribed to the quantum element. The performance of our model aligns with that of a variational quantum circuit using amplitude embedding, positioning it as a feasible alternative.",2023-11-09T18:13:50Z,2024-01-13T11:02:53Z,http://arxiv.org/abs/2311.05559v2,http://arxiv.org/pdf/2311.05559v2,"quant-ph, cs.CV, cs.LG"
DenRAM: Neuromorphic Dendritic Architecture with RRAM for Efficient   Temporal Processing with Delays,"Simone DAgostino, Filippo Moro, Tristan Torchet, Yigit Demirag, Laurent Grenouillet, Giacomo Indiveri, Elisa Vianello, Melika Payvand","An increasing number of neuroscience studies are highlighting the importance of spatial dendritic branching in pyramidal neurons in the brain for supporting non-linear computation through localized synaptic integration. In particular, dendritic branches play a key role in temporal signal processing and feature detection, using coincidence detection (CD) mechanisms, made possible by the presence of synaptic delays that align temporally disparate inputs for effective integration. Computational studies on spiking neural networks further highlight the significance of delays for CD operations, enabling spatio-temporal pattern recognition within feed-forward neural networks without the need for recurrent architectures. In this work, we present DenRAM, the first realization of a spiking neural network with analog dendritic circuits, integrated into a 130nm technology node coupled with resistive memory (RRAM) technology. DenRAM's dendritic circuits use the RRAM devices to implement both delays and synaptic weights in the network. By configuring the RRAM devices to reproduce bio-realistic timescales, and through exploiting their heterogeneity, we experimentally demonstrate DenRAM's capability to replicate synaptic delay profiles, and efficiently implement CD for spatio-temporal pattern recognition. To validate the architecture, we conduct comprehensive system-level simulations on two representative temporal benchmarks, highlighting DenRAM's resilience to analog hardware noise, and its superior accuracy compared to recurrent architectures with an equivalent number of parameters. DenRAM not only brings rich temporal processing capabilities to neuromorphic architectures, but also reduces the memory footprint of edge devices, provides high accuracy on temporal benchmarks, and represents a significant step-forward in low-power real-time signal processing technologies.",2023-12-14T14:08:45Z,2023-12-14T14:08:45Z,http://arxiv.org/abs/2312.08960v1,http://arxiv.org/pdf/2312.08960v1,"cs.ET, eess.SP"
BRYT: Data Rich Analytics Based Computer Architecture for A New Paradigm   of Chip Design,"Ian McDougall, Shayne Wadle, Harish Batchu, Michael Davies, Karthikeyan Sankaralingam","Motivated by the end of Moore's Law and Dennard Scaling which necessitate architectural efficiency as the means for improved capability for the next decade or two, this paper introduces a new data-rich paradigm of chip design for the semi-conductor industry. The goal is to enable monitoring chip hardware behavior in the field, at real-time speeds with no slowdowns, with minimal power overheads and obtain insights on chip behavior and workloads. We posit that, such extensive amounts of data would allow better and more capable architectures addressing three problems: obfuscated hardware, obfuscated software, and inability of A/B testing for hardware ideas. This paper implements the first version of the paradigm with a system architecture and the concept of an analYtics Processing Unit (YPU). We perform 4 case studies, and implement an RTL level prototype. Across the case studies we show a YPU with area overhead $<1 \%$ at 7nm, and overall power consumption of $<25 mW$ is able to create previously inconceivable analysis: per-instruction cycles stacks of arbitrary programs, evaluating instruction prefetchers in the wild before deployment, fine-grained cycle-by-cycle utilization of hardware modules, and histograms of tensor-value distributions of DL models.",2023-12-20T21:14:28Z,2024-10-23T22:03:52Z,http://arxiv.org/abs/2312.13428v2,http://arxiv.org/pdf/2312.13428v2,"cs.AR, C.1.m; B.m; C.m"
Efficient Architecture Search via Bi-level Data Pruning,"Chongjun Tu, Peng Ye, Weihao Lin, Hancheng Ye, Chong Yu, Tao Chen, Baopu Li, Wanli Ouyang","Improving the efficiency of Neural Architecture Search (NAS) is a challenging but significant task that has received much attention. Previous works mainly adopted the Differentiable Architecture Search (DARTS) and improved its search strategies or modules to enhance search efficiency. Recently, some methods have started considering data reduction for speedup, but they are not tightly coupled with the architecture search process, resulting in sub-optimal performance. To this end, this work pioneers an exploration into the critical role of dataset characteristics for DARTS bi-level optimization, and then proposes a novel Bi-level Data Pruning (BDP) paradigm that targets the weights and architecture levels of DARTS to enhance efficiency from a data perspective. Specifically, we introduce a new progressive data pruning strategy that utilizes supernet prediction dynamics as the metric, to gradually prune unsuitable samples for DARTS during the search. An effective automatic class balance constraint is also integrated into BDP, to suppress potential class imbalances resulting from data-efficient algorithms. Comprehensive evaluations on the NAS-Bench-201 search space, DARTS search space, and MobileNet-like search space validate that BDP reduces search costs by over 50% while achieving superior performance when applied to baseline DARTS. Besides, we demonstrate that BDP can harmoniously integrate with advanced DARTS variants, like PC-DARTS and \b{eta}-DARTS, offering an approximately 2 times speedup with minimal performance compromises.",2023-12-21T02:48:44Z,2023-12-21T02:48:44Z,http://arxiv.org/abs/2312.14200v1,http://arxiv.org/pdf/2312.14200v1,"cs.CV, 68T05(Primary)"
"Training Neural Networks with Internal State, Unconstrained   Connectivity, and Discrete Activations",Alexander Grushin,"Today's most powerful machine learning approaches are typically designed to train stateless architectures with predefined layers and differentiable activation functions. While these approaches have led to unprecedented successes in areas such as natural language processing and image recognition, the trained models are also susceptible to making mistakes that a human would not. In this paper, we take the view that true intelligence may require the ability of a machine learning model to manage internal state, but that we have not yet discovered the most effective algorithms for training such models. We further postulate that such algorithms might not necessarily be based on gradient descent over a deep architecture, but rather, might work best with an architecture that has discrete activations and few initial topological constraints (such as multiple predefined layers). We present one attempt in our ongoing efforts to design such a training algorithm, applied to an architecture with binary activations and only a single matrix of weights, and show that it is able to form useful representations of natural language text, but is also limited in its ability to leverage large quantities of training data. We then provide ideas for improving the algorithm and for designing other training algorithms for similar architectures. Finally, we discuss potential benefits that could be gained if an effective training algorithm is found, and suggest experiments for evaluating whether these benefits exist in practice.",2023-12-22T01:19:08Z,2023-12-22T01:19:08Z,http://arxiv.org/abs/2312.14359v1,http://arxiv.org/pdf/2312.14359v1,"cs.LG, cs.NE, 68T07, I.2.6"
Dynamic MIMO Architecture Design for Near-Field Communications,"Zheng Zhang, Yuanwei Liu, Zhaolin Wang, Jian Chen, Tony Q. S. Quek","A novel dynamic hybrid beamforming architecture is proposed to achieve the spatial multiplexing-power consumption tradeoff for near-field multiple-input multiple-output (MIMO) networks, where each radio frequency (RF) chain is connected to each antenna using a couple of independent phase shifters to reduce the number of required RF chains. Based on this architecture, an optimization problem is formulated that maximizes the sum of achievable rates while minimizing the hardware power consumption. Both continuous and discrete phase shifters are considered. 1) For continuous phase shifters, a weighted minimum mean-square error-based two-stage (WMMSE-TS) algorithm is proposed, where the same performance as the optimal fully-digital beamformer can be achieved by the proposed hybrid beamformer even if the number of RF chains equals the number of data streams. 2) For discrete phase shifters, a penalty-based layered iterative (PLI) algorithm is proposed. The closed-form analog and baseband digital beamformers are derived in each iteration. Simulation results demonstrate that: 1) the proposed dynamic beamforming architecture outperforms the conventional fixed hybrid beamforming architecture in terms of spatial multiplexing-power consumption tradeoff, and 2) the proposed algorithms achieve better performance than the other baseline schemes.",2023-12-25T15:41:01Z,2023-12-25T15:41:01Z,http://arxiv.org/abs/2312.15757v1,http://arxiv.org/pdf/2312.15757v1,"cs.IT, eess.SP, math.IT"
A Modifiable Architectural Design for Commercial Greenhouses Energy   Economic Dispatch Testbed,"Christian Skafte Beck Clausen, Bo Nørregaard Jørgensen, Zheng Grace Ma","Facing economic challenges due to the diverse objectives of businesses, and consumers, commercial greenhouses strive to minimize energy costs while addressing CO2 emissions. This scenario is intensified by rising energy costs and the global imperative to curtail CO2 emissions. To address these dynamic economic challenges, this paper proposes an architectural design for an energy economic dispatch testbed for commercial greenhouses. Utilizing the Attribute-Driven De-sign method, core architectural components of a software-in-the-loop testbed are proposed which emphasizes modularity and careful consideration of the multi-objective optimization problem. This approach extends prior research by implementing a modular multi-objective optimization framework in Java. The results demonstrate the successful integration of the CO2 reduction objective within the modular architecture with minimal effort. The multi-objective optimization output can also be employed to examine cost and CO2 objectives, ultimately serving as a valuable decision-support tool. The novel testbed architecture and a modular approach can tackle the multi-objective optimization problem and enable commercial greenhouses to navigate the intricate landscape of energy cost and CO2 emissions management.",2024-01-08T13:36:31Z,2024-01-08T13:36:31Z,http://arxiv.org/abs/2401.03888v1,http://arxiv.org/pdf/2401.03888v1,"cs.SE, cs.SY, eess.SY"
Quantum Architecture Search with Unsupervised Representation Learning,"Yize Sun, Zixin Wu, Yunpu Ma, Volker Tresp","Unsupervised representation learning presents new opportunities for advancing Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ) devices. QAS is designed to optimize quantum circuits for Variational Quantum Algorithms (VQAs). Most QAS algorithms tightly couple the search space and search algorithm, typically requiring the evaluation of numerous quantum circuits, resulting in high computational costs and limiting scalability to larger quantum circuits. Predictor-based QAS algorithms mitigate this issue by estimating circuit performance based on structure or embedding. However, these methods often demand time-intensive labeling to optimize gate parameters across many circuits, which is crucial for training accurate predictors. Inspired by the classical neural architecture search algorithm Arch2vec, we investigate the potential of unsupervised representation learning for QAS without relying on predictors. Our framework decouples unsupervised architecture representation learning from the search process, enabling the learned representations to be applied across various downstream tasks. Additionally, it integrates an improved quantum circuit graph encoding scheme, addressing the limitations of existing representations and enhancing search efficiency. This predictor-free approach removes the need for large labeled datasets. During the search, we employ REINFORCE and Bayesian Optimization to explore the latent representation space and compare their performance against baseline methods. Our results demonstrate that the framework efficiently identifies high-performing quantum circuits with fewer search iterations.",2024-01-21T19:53:17Z,2024-10-23T15:30:50Z,http://arxiv.org/abs/2401.11576v3,http://arxiv.org/pdf/2401.11576v3,"quant-ph, cs.LG"
New Insight in Cervical Cancer Diagnosis Using Convolution Neural   Network Architecture,"Ach. Khozaimi, Wayan Firdaus Mahmudy","The Pap smear is a screening method for early cervical cancer diagnosis. The selection of the right optimizer in the convolutional neural network (CNN) model is key to the success of the CNN in image classification, including the classification of cervical cancer Pap smear images. In this study, stochastic gradient descent (SGD), RMSprop, Adam, AdaGrad, AdaDelta, Adamax, and Nadam optimizers were used to classify cervical cancer Pap smear images from the SipakMed dataset. Resnet-18, Resnet-34, and VGG-16 are the CNN architectures used in this study, and each architecture uses a transfer-learning model. Based on the test results, we conclude that the transfer learning model performs better on all CNNs and optimization techniques and that in the transfer learning model, the optimization has little influence on the training of the model. Adamax, with accuracy values of 72.8% and 66.8%, had the best accuracy for the VGG-16 and Resnet-18 architectures, respectively. Resnet-34 had 54.0%. This is 0.034% lower than Nadam. Overall, Adamax is a suitable optimizer for CNN in cervical cancer classification on Resnet-18, Resnet-34, and VGG-16 architectures. This study provides new insights into the configuration of CNN models for Pap smear image analysis.",2024-10-23T10:11:39Z,2024-10-23T10:11:39Z,http://arxiv.org/abs/2410.17735v1,http://arxiv.org/pdf/2410.17735v1,"eess.IV, cs.AI, cs.CV"
Optimal Equivariant Architectures from the Symmetries of Matrix-Element   Likelihoods,"Daniel Maître, Vishal S. Ngairangbam, Michael Spannowsky","The Matrix-Element Method (MEM) has long been a cornerstone of data analysis in high-energy physics. It leverages theoretical knowledge of parton-level processes and symmetries to evaluate the likelihood of observed events. In parallel, the advent of geometric deep learning has enabled neural network architectures that incorporate known symmetries directly into their design, leading to more efficient learning. This paper presents a novel approach that combines MEM-inspired symmetry considerations with equivariant neural network design for particle physics analysis. Even though Lorentz invariance and permutation invariance overall reconstructed objects are the largest and most natural symmetry in the input domain, we find that they are sub-optimal in most practical search scenarios. We propose a longitudinal boost-equivariant message-passing neural network architecture that preserves relevant discrete symmetries. We present numerical studies demonstrating MEM-inspired architectures achieve new state-of-the-art performance in distinguishing di-Higgs decays to four bottom quarks from the QCD background, with enhanced sample and parameter efficiencies. This synergy between MEM and equivariant deep learning opens new directions for physics-informed architecture design, promising more powerful tools for probing physics beyond the Standard Model.",2024-10-24T08:56:37Z,2024-10-24T08:56:37Z,http://arxiv.org/abs/2410.18553v1,http://arxiv.org/pdf/2410.18553v1,"hep-ph, cs.LG, hep-ex, physics.data-an"
QADL: Prototype of Quantum Architecture Description Language,"Muhammad Waseem, Tommi Mikkonen, Aakash Ahmad, Muhammad Taimoor Khan, Majid Haghparast, Vlad Stirbu, Peng Liang","Quantum Software (QSW) uses the principles of quantum mechanics, specifically programming quantum bits (qubits) that manipulate quantum gates, to implement quantum computing systems. QSW has become a specialized field of software development, requiring specific notations, languages, patterns, and tools for mapping the behavior of qubits and the structure of quantum gates to components and connectors of QSW architectures. To support declarative modeling of QSW, we aim to enable architecture-driven development, where software engineers can design, program, and evaluate quantum software systems by abstracting complex details through high-level components and connectors. We introduce QADL (Quantum Architecture Description Language), which provides a specification language, design space, and execution environment for architecting QSW. Inspired by classical ADLs, QADL offers (1) a graphical interface to specify and design QSW components, (2) a parser for syntactical correctness, and (3) an execution environment by integrating QADL with IBM Qiskit. The initial evaluation of QADL is based on usability assessments by a team of quantum physicists and software engineers, using quantum algorithms such as Quantum Teleportation and Grover's Search. QADL offers a pioneering specification language and environment for QSW architecture.   A demo is available at https://youtu.be/xaplHH_3NtQ.",2024-10-13T19:09:38Z,2024-10-13T19:09:38Z,http://arxiv.org/abs/2410.19770v1,http://arxiv.org/pdf/2410.19770v1,"quant-ph, cs.SE"
Random Heterogeneous Neurochaos Learning Architecture for Data   Classification,"Remya Ajai A S, Nithin Nagaraj","Inspired by the human brain's structure and function, Artificial Neural Networks (ANN) were developed for data classification. However, existing Neural Networks, including Deep Neural Networks, do not mimic the brain's rich structure. They lack key features such as randomness and neuron heterogeneity, which are inherently chaotic in their firing behavior. Neurochaos Learning (NL), a chaos-based neural network, recently employed one-dimensional chaotic maps like Generalized L\""uroth Series (GLS) and Logistic map as neurons. For the first time, we propose a random heterogeneous extension of NL, where various chaotic neurons are randomly placed in the input layer, mimicking the randomness and heterogeneous nature of human brain networks. We evaluated the performance of the newly proposed Random Heterogeneous Neurochaos Learning (RHNL) architectures combined with traditional Machine Learning (ML) methods. On public datasets, RHNL outperformed both homogeneous NL and fixed heterogeneous NL architectures in nearly all classification tasks. RHNL achieved high F1 scores on the Wine dataset (1.0), Bank Note Authentication dataset (0.99), Breast Cancer Wisconsin dataset (0.99), and Free Spoken Digit Dataset (FSDD) (0.98). These RHNL results are among the best in the literature for these datasets. We investigated RHNL performance on image datasets, where it outperformed stand-alone ML classifiers. In low training sample regimes, RHNL was the best among stand-alone ML. Our architecture bridges the gap between existing ANN architectures and the human brain's chaotic, random, and heterogeneous properties. We foresee the development of several novel learning algorithms centered around Random Heterogeneous Neurochaos Learning in the coming days.",2024-10-30T18:00:14Z,2024-10-30T18:00:14Z,http://arxiv.org/abs/2410.23351v1,http://arxiv.org/pdf/2410.23351v1,"cs.LG, F.2.2, I.2.7, F.2.2; I.2.7"
Optimizing Multi-level Magic State Factories for Fault-Tolerant Quantum   Architectures,"Allyson Silva, Artur Scherer, Zak Webb, Abdullah Khalid, Bohdan Kulchytskyy, Mia Kramer, Kevin Nguyen, Xiangzhou Kong, Gebremedhin A. Dagnew, Yumeng Wang, Huy Anh Nguyen, Katiemarie Olfert, Pooya Ronagh","We propose a novel technique for optimizing a modular fault-tolerant quantum computing architecture, taking into account any desired space-time trade--offs between the number of physical qubits and the fault-tolerant execution time of a quantum algorithm. We consider a concept architecture comprising a dedicated zone as a multi-level magic state factory and a core processor for efficient logical operations, forming a supply chain network for production and consumption of magic states. Using a heuristic algorithm, we solve the multi-objective optimization problem of minimizing space and time subject to a user-defined error budget for the success of the computation, taking the performance of various fault-tolerant protocols such as quantum memory, state preparation, magic state distillation, code growth, and logical operations into account. As an application, we show that physical quantum resource estimation reduces to a simple model involving a small number of key parameters, namely, the circuit volume, the error prefactors ($\mu$) and error suppression rates ($\Lambda$) of the fault-tolerant protocols, and an allowed slowdown factor ($\beta$). We show that, in the proposed architecture, $10^5$--$10^8$ physical qubits are required for quantum algorithms with $T$-counts in the range $10^6$--$10^{15}$ and logical qubit counts in the range $10^2$--$10^4$, when run on quantum computers with quantum memory $\Lambda$ in the range 3--10, for all slowdown factors $\beta \geq 0.2$.",2024-11-06T21:25:34Z,2024-11-06T21:25:34Z,http://arxiv.org/abs/2411.04270v1,http://arxiv.org/pdf/2411.04270v1,"quant-ph, cs.AR, math.OC"
AmpliNetECG12: A lightweight SoftMax-based relativistic amplitude   amplification architecture for 12 lead ECG classification,Shreya Srivastava,"The urgent need to promptly detect cardiac disorders from 12-lead Electrocardiograms using limited computations is motivated by the heart's fast and complex electrical activity and restricted computational power of portable devices. Timely and precise diagnoses are crucial since delays might significantly impact patient health outcomes. This research presents a novel deep-learning architecture that aims to diagnose heart abnormalities quickly and accurately. We devised a new activation function called aSoftMax, designed to improve the visibility of ECG deflections. The proposed activation function is used with Convolutional Neural Network architecture to includes kernel weight sharing across the ECG's various leads. This innovative method thoroughly generalizes the global 12-lead ECG features and minimizes the model's complexity by decreasing the trainable parameters. aSoftMax, combined with enhanced CNN architecture yielded AmpliNetECG12, we obtain exceptional accuracy of 84% in diagnosing cardiac disorders. AmpliNetECG12 shows outstanding prediction ability when used with the CPSC2018 dataset for arrhythmia classification. The model attains an F1-score of 80.71% and a ROC-AUC score of 96.00%, with 280,000 trainable parameters which signifies the lightweight yet efficient nature of AmpliNetECG12. The stochastic characteristics of aSoftMax, a fundamental element of AmpliNetECG12, improve prediction accuracy and also increasse the model's interpretability. This feature enhances comprehension of important ECG segments in different forms of arrhythmias, establishing a new standard of explainable architecture for cardiac disorder classification.",2024-11-21T07:28:24Z,2024-11-21T07:28:24Z,http://arxiv.org/abs/2411.13903v1,http://arxiv.org/pdf/2411.13903v1,"eess.SP, cs.AI, cs.LG"
Beautimeter: Harnessing GPT for Assessing Architectural and Urban Beauty   based on the 15 Properties of Living Structure,Bin Jiang,"Beautimeter is a new tool powered by generative pre-trained transformer (GPT) technology, designed to evaluate architectural and urban beauty. Rooted in Christopher Alexander's theory of centers, this work builds on the idea that all environments possess, to varying degrees, an innate sense of life. Alexander identified 15 fundamental properties, such as levels of scale and thick boundaries, that characterize living structure, which Beautimeter uses as a basis for its analysis. By integrating GPT's advanced natural language processing capabilities, Beautimeter assesses the extent to which a structure embodies these 15 properties, enabling a nuanced evaluation of architectural and urban aesthetics. Using ChatGPT, the tool helps users generate insights into the perceived beauty and coherence of spaces. We conducted a series of case studies, evaluating images of architectural and urban environments, as well as carpets, paintings, and other artifacts. The results demonstrate Beautimeter's effectiveness in analyzing aesthetic qualities across diverse contexts. Our findings suggest that by leveraging GPT technology, Beautimeter offers architects, urban planners, and designers a powerful tool to create spaces that resonate deeply with people. This paper also explores the implications of such technology for architecture and urban design, highlighting its potential to enhance both the design process and the assessment of built environments. Keywords: Living structure, structural beauty, Christopher Alexander, AI in Design, human centered design",2024-11-28T12:14:24Z,2025-01-18T10:29:00Z,http://arxiv.org/abs/2411.19094v2,http://arxiv.org/pdf/2411.19094v2,"physics.soc-ph, cs.AI"
ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient   Liver Segmentation,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy","The growing need for accurate and efficient 3D identification of tumors, particularly in liver segmentation, has spurred considerable research into deep learning models. While many existing architectures offer strong performance, they often face challenges such as overfitting and excessive computational costs. An adjustable and flexible architecture that strikes a balance between time efficiency and model complexity remains an unmet requirement. In this paper, we introduce proKAN, a progressive stacking methodology for Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike traditional architectures, proKAN dynamically adjusts its complexity by progressively adding KAN blocks during training, based on overfitting behavior. This approach allows the network to stop growing when overfitting is detected, preventing unnecessary computational overhead while maintaining high accuracy. Additionally, proKAN utilizes KAN's learnable activation functions modeled through B-splines, which provide enhanced flexibility in learning complex relationships in 3D medical data. Our proposed architecture achieves state-of-the-art performance in liver segmentation tasks, outperforming standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The dynamic nature of proKAN ensures efficient training times and high accuracy without the risk of overfitting. Furthermore, proKAN provides better interpretability by allowing insight into the decision-making process through its learnable coefficients. The experimental results demonstrate a significant improvement in accuracy, Dice score, and time efficiency, making proKAN a compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,http://arxiv.org/pdf/2412.19713v1,"eess.IV, cs.CV, cs.LG"
H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and   Stereo Semantic Segmentation in Intracardiac Catheters,"Pedram Fekri, Mehrdad Zadeh, Javad Dargahi","The success rate of catheterization procedures is closely linked to the sensory data provided to the surgeon. Vision-based deep learning models can deliver both tactile and visual information in a sensor-free manner, while also being cost-effective to produce. Given the complexity of these models for devices with limited computational resources, research has focused on force estimation and catheter segmentation separately. However, there is a lack of a comprehensive architecture capable of simultaneously segmenting the catheter from two different angles and estimating the applied forces in 3D. To bridge this gap, this work proposes a novel, lightweight, multi-input, multi-output encoder-decoder-based architecture. It is designed to segment the catheter from two points of view and concurrently measure the applied forces in the x, y, and z directions. This network processes two simultaneous X-Ray images, intended to be fed by a biplane fluoroscopy system, showing a catheter's deflection from different angles. It uses two parallel sub-networks with shared parameters to output two segmentation maps corresponding to the inputs. Additionally, it leverages stereo vision to estimate the applied forces at the catheter's tip in 3D. The architecture features two input channels, two classification heads for segmentation, and a regression head for force estimation through a single end-to-end architecture. The output of all heads was assessed and compared with the literature, demonstrating state-of-the-art performance in both segmentation and force estimation. To the best of the authors' knowledge, this is the first time such a model has been proposed",2024-12-31T15:55:13Z,2024-12-31T15:55:13Z,http://arxiv.org/abs/2501.00514v1,http://arxiv.org/pdf/2501.00514v1,"eess.IV, cs.AI, cs.CV, cs.LG, cs.RO"
Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria   Shaping Modern Artificial Neural Network Architectures,"Ziyuan Huang, Mark Newman, Maria Vaida, Srikar Bellur, Roozbeh Sadeghian, Andrew Siu, Hui Wang, Kevin Huggins","This study examined the viability of enhancing the prediction accuracy of artificial neural networks (ANNs) in image classification tasks by developing ANNs with evolution patterns similar to those of biological neural networks. ResNet is a widely used family of neural networks with both deep and wide variants; therefore, it was selected as the base model for our investigation. The aim of this study is to improve the image classification performance of ANNs via a novel approach inspired by the biological nervous system architecture of planarians, which comprises a brain and two nerve cords. We believe that the unique neural architecture of planarians offers valuable insights into the performance enhancement of ANNs. The proposed planarian neural architecture-based neural network was evaluated on the CIFAR-10 and CIFAR-100 datasets. Our results indicate that the proposed method exhibits higher prediction accuracy than the baseline neural network models in image classification tasks. These findings demonstrate the significant potential of biologically inspired neural network architectures in improving the performance of ANNs in a wide range of applications.",2025-01-08T18:59:36Z,2025-01-08T18:59:36Z,http://arxiv.org/abs/2501.04700v1,http://arxiv.org/pdf/2501.04700v1,"cs.NE, cs.AI, cs.CV, cs.LG, 68T07"
Evaluation of MQTT Bridge Architectures in a Cross-Organizational   Context,"Keila Lima, Tosin Daniel Oyetoyan, Rogardt Heldal, Wilhelm Hasselbring","The latest surveys estimate an increasing number of connected Internet-of-Things (IoT) devices (around 16 billion) despite the sector's shortage of manufacturers. All these devices deployed into the wild will collect data to guide decision-making that can be made automatically by other systems, humans, or hybrid approaches. In this work, we conduct an initial investigation of benchmark configuration options for IoT Platforms that process data ingested by such devices in real-time using the MQTT protocol. We identified metrics and related MQTT configurable parameters in the system's component deployment for an MQTT bridge architecture. For this purpose, we benchmark a real-world IoT platform's operational data flow design to monitor the surrounding environment remotely. We consider the MQTT broker solution and the system's real-time ingestion and bridge processing portion of the platform to be the system under test. In the benchmark, we investigate two architectural deployment options for the bridge component to gain insights into the latency and reliability of MQTT bridge deployments in which data is provided in a cross-organizational context. Our results indicate that the number of bridge components, MQTT packet sizes, and the topic name can impact the quality attributes in IoT architectures using MQTT protocol.",2025-01-24T19:24:36Z,2025-01-24T19:24:36Z,http://arxiv.org/abs/2501.14890v1,http://arxiv.org/pdf/2501.14890v1,"cs.SE, D.2.11"
Embracing Reconfigurable Antennas in the Tri-hybrid MIMO Architecture   for 6G and Beyond,"Miguel Rodrigo Castellanos, Siyun Yang, Chan-Byoung Chae, Robert W. Heath Jr","Multiple-input multiple-output (MIMO) communication has led to immense enhancements in data rates and efficient spectrum management. The evolution of MIMO, though, has been accompanied by increased hardware complexity and array sizes, causing the system power consumption to increase. Despite past advances in power-efficient hybrid architectures, new solutions are needed to enable extremely large-scale MIMO deployments for 6G and beyond. In this paper, we introduce a novel architecture that integrates low-power reconfigurable antennas with both digital and analog precoding. This \emph{tri-hybrid} approach addresses key limitations in traditional and hybrid MIMO systems by improving power consumption and adds a new layer for signal processing. We provide an analysis of the proposed architecture and compare its performance with existing solutions, including fully-digital and hybrid MIMO systems. The results demonstrate significant improvements in energy efficiency, highlighting the potential of the tri-hybrid system to meet the growing demands of future wireless networks. We conclude the paper with a summary of design and implementation challenges, including the need for technological advancements in reconfigurable array hardware and tunable antenna parameters.",2025-01-28T01:09:50Z,2025-02-23T01:08:00Z,http://arxiv.org/abs/2501.16610v2,http://arxiv.org/pdf/2501.16610v2,"cs.IT, cs.ET, cs.NI, math.IT"
Modular RADAR: An Immune System Inspired Search and Response Strategy   for Distributed Systems,"Soumya Banerjee, Melanie Moses","The Natural Immune System (NIS) is a distributed system that solves challenging search and response problems while operating under constraints imposed by physical space and resource availability. Remarkably, NIS search and response times do not scale appreciably with the physical size of the animal in which its search is conducted. Many distributed systems are engineered to solve analogous problems, and the NIS demonstrates how such engineered systems can achieve desirable scalability. We hypothesize that the architecture of the NIS, composed of a hierarchical decentralized detection network of lymph nodes (LN) facilitates efficient search and response. A sub-modular architecture in which LN numbers and size both scale with organism size is shown to efficiently balance tradeoffs between local antigen detection and global antibody production, leading to nearly scale-invariant detection and response. We characterize the tradeoffs as balancing local and global communication and show that similar tradeoffs exist in distributed systems like LN inspired artificial immune system (AIS) applications and peer-to-peer (P2P) systems. Taking inspiration from the architecture of the NIS, we propose a modular RADAR (Robust Adaptive Decentralized search with Automated Response) strategy for distributed systems. We demonstrate how two existing distributed systems (a LN inspired multi-robot control application and a P2P system) can be improved by a modular RADAR strategy. Such a sub-modular architecture is shown to balance the tradeoffs between local communication (within artificial LNs and P2P clusters) and global communication (between artificial LNs and P2P clusters), leading to efficient search and response.",2010-06-17T07:58:01Z,2010-06-17T07:58:01Z,http://arxiv.org/abs/1006.3394v1,http://arxiv.org/pdf/1006.3394v1,"cs.DC, q-bio.PE"
"Achieving ""Massive MIMO"" Spectral Efficiency with a Not-so-Large Number   of Antennas","Hoon Huh, Giuseppe Caire, Haralabos C. Papadopoulos, Sean A. Ramprashad","The main focus and contribution of this paper is a novel network-MIMO TDD architecture that achieves spectral efficiencies comparable with ""Massive MIMO"", with one order of magnitude fewer antennas per active user per cell. The proposed architecture is based on a family of network-MIMO schemes defined by small clusters of cooperating base stations, zero-forcing multiuser MIMO precoding with suitable inter-cluster interference constraints, uplink pilot signals reuse across cells, and frequency reuse. The key idea consists of partitioning the users population into geographically determined ""bins"", such that all users in the same bin are statistically equivalent, and use the optimal network-MIMO architecture in the family for each bin. A scheduler takes care of serving the different bins on the time-frequency slots, in order to maximize a desired network utility function that captures some desired notion of fairness. This results in a mixed-mode network-MIMO architecture, where different schemes, each of which is optimized for the served user bin, are multiplexed in time-frequency. In order to carry out the performance analysis and the optimization of the proposed architecture in a clean and computationally efficient way, we consider the large-system regime where the number of users, the number of antennas, and the channel coherence block length go to infinity with fixed ratios. The performance predicted by the large-system asymptotic analysis matches very well the finite-dimensional simulations. Overall, the system spectral efficiency obtained by the proposed architecture is similar to that achieved by ""Massive MIMO"", with a 10-fold reduction in the number of antennas at the base stations (roughly, from 500 to 50 antennas).",2011-07-19T23:28:37Z,2011-09-13T01:52:51Z,http://arxiv.org/abs/1107.3862v2,http://arxiv.org/pdf/1107.3862v2,"cs.IT, math.IT"
A Row-parallel 8$\times$8 2-D DCT Architecture Using Algebraic Integer   Based Exact Computation,"A. Madanayake, R. J. Cintra, D. Onen, V. S. Dimitrov, N. T. Rajapaksha, L. T. Bruton, A. Edirisuriya","An algebraic integer (AI) based time-multiplexed row-parallel architecture and two final-reconstruction step (FRS) algorithms are proposed for the implementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The architecture directly realizes an error-free 2-D DCT without using FRSs between row-column transforms, leading to an 8$\times$8 2-D DCT which is entirely free of quantization errors in AI basis. As a result, the user-selectable accuracy for each of the coefficients in the FRS facilitates each of the 64 coefficients to have its precision set independently of others, avoiding the leakage of quantization noise between channels as is the case for published DCT designs. The proposed FRS uses two approaches based on (i) optimized Dempster-Macleod multipliers and (ii) expansion factor scaling. This architecture enables low-noise high-dynamic range applications in digital video processing that requires full control of the finite-precision computation of the 2-D DCT. The proposed architectures and FRS techniques are experimentally verified and validated using hardware implementations that are physically realized and verified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using the two proposed FRS schemes, have been designed, simulated, physically implemented and measured. The maximum clock rate and block-rate achieved among 8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a pixel rate of 8$\times$307.787$\approx$2.462 GHz if eventually embedded in a real-time video-processing system. The equivalent frame rate is about 1187.35 Hz for the image size of 1920$\times$1080. All implementations are functional on a Xilinx Virtex-6 XC6VLX240T FPGA device.",2015-02-14T16:14:05Z,2015-02-14T16:14:05Z,http://arxiv.org/abs/1502.04221v1,http://arxiv.org/pdf/1502.04221v1,"cs.AR, cs.DM, math.NT, stat.CO, stat.ME"
End-to-End Architecture Modularisation and Slicing for Next Generation   Networks,"Xueli An, Riccardo Trivisonno, Hans Einsiedler, Dirk von Hugo, Kay Haensge, Xiaofeng Huang, Qing Shen, Daniel Corujo, Kashif Mahmood, Dirk Trossen, Marco Liebsch, Filipe Leitao, Cao-Thanh Phan, Frederic Klamm","The journey towards the deployment of next generation networks has recently accelerated, driven by the joint effort of research and standards organisations. Despite this fact, the overall picture is still unclear as prioritization and understanding on several key concepts are not yet agreed by major vendors and network providers. Network Slicing is one of the central topics of the debate, and it is expected to become the key feature of next generation networks, providing the flexibility required to support the variety of 5G use cases and business. Network slices are seen as network operator business, offering the possibility to provide flexible services and even infrastructures to vertical industries and classical Telco customers alike. Another key ingredient is the Architecture Modularisation concept, discussed in this paper and regarded by the authors as the essential design principle to build a flexible network architecture natively supporting Network Slicing. According to this concept, conventional monolithic network functions, often corresponding to physical network elements in the existing systems, are to split into basic building blocks defined with the proper granularity, allowing the definition of different logical architectures (i.e. different Network Slices). In this paper, we further discuss a modularisation methodology as a criteria to define the right set of basic building blocks. Defined through this proposed methodology, the set of basic building blocks and the relating interfacing model are discussed. The paper concludes by proposing a modular 5G network architecture as candidate for next generation network standards.",2016-11-02T12:12:31Z,2016-11-02T12:12:31Z,http://arxiv.org/abs/1611.00566v1,http://arxiv.org/pdf/1611.00566v1,"cs.NI, 68M10"
WhatsApp security and role of metadata in preserving privacy,"Nidhi Rastogi, James Hendler","WhatsApp messenger is arguably the most popular mobile app available on all smart-phones. Over one billion people worldwide for free messaging, calling, and media sharing use it. In April 2016, WhatsApp switched to a default end-to-end encrypted service. This means that all messages (SMS), phone calls, videos, audios, and any other form of information exchanged cannot be read by any unauthorized entity since WhatsApp. In this paper we analyze the WhatsApp messaging platform and critique its security architecture along with a focus on its privacy preservation mechanisms. We report that the Signal Protocol, which forms the basis of WhatsApp end-to-end encryption, does offer protection against forward secrecy, and MITM to a large extent. Finally, we argue that simply encrypting the end-to-end channel cannot preserve privacy. The metadata can reveal just enough information to show connections between people, their patterns, and personal information. This paper elaborates on the security architecture of WhatsApp and performs an analysis on the various protocols used. This enlightens us on the status quo of the app security and what further measures can be used to fill existing gaps without compromising the usability. We start by describing the following (i) important concepts that need to be understood to properly understand security, (ii) the security architecture, (iii) security evaluation, (iv) followed by a summary of our work. Some of the important concepts that we cover in this paper before evaluating the architecture are - end-to-end encryption (E2EE), signal protocol, and curve25519. The description of the security architecture covers key management, end-to-end encryption in WhatsApp, Authentication Mechanism, Message Exchange, and finally the security evaluation. We then cover importance of metadata and role it plays in conserving privacy with respect to whatsapp.",2017-01-24T11:21:33Z,2017-01-24T11:21:33Z,http://arxiv.org/abs/1701.06817v1,http://arxiv.org/pdf/1701.06817v1,"cs.CR, cs.NI, cs.SI, C.2.0; D.4.6"
Plexus Convolutional Neural Network (PlexusNet): A novel neural network   architecture for histologic image analysis,"Okyaz Eminaga, Mahmoud Abbas, Christian Kunder, Andreas M. Loening, Jeanne Shen, James D. Brooks, Curtis P. Langlotz, Daniel L. Rubin","Different convolutional neural network (CNN) models have been tested for their application in histological image analyses. However, these models are prone to overfitting due to their large parameter capacity, requiring more data or valuable computational resources for model training. Given these limitations, we introduced a novel architecture (termed PlexusNet). We utilized 310 Hematoxylin and Eosin stained (H&E) annotated histological images of prostate cancer cases from TCGA-PRAD and Stanford University and 398 H&E whole slides images from the Camelyon 2016 challenge. PlexusNet-architecture -derived models were compared to models derived from several existing ""state of the art"" architectures. We measured discrimination accuracy, calibration, and clinical utility. An ablation study was conducted to study the effect of each component of PlexusNet on model performance. A well-fitted PlexusNet-based model delivered comparable classification performance (AUC: 0.963) in distinguishing prostate cancer from healthy tissues, although it was at least 23 times smaller, had a better model calibration and clinical utility than the comparison models. A separate smaller PlexusNet model accurately detected slides with breast cancer metastases (AUC: 0.978); it helped reduce the slide number to examine by 43.8% without consequences, although its parameter capacity was 200 times smaller than ResNet18. We found that the partitioning of the development set influences the model calibration for all models. However, with PlexusNet architecture, we could achieve comparable well-calibrated models trained on different partitions. In conclusion, PlexusNet represents a novel model architecture for histological image analysis that achieves classification performance comparable to other models while providing orders-of-magnitude parameter reduction.",2019-08-24T01:29:34Z,2020-06-03T04:43:21Z,http://arxiv.org/abs/1908.09067v2,http://arxiv.org/pdf/1908.09067v2,"q-bio.QM, cs.AI, cs.CV, eess.IV, q-bio.TO"
Stabilizing Transformers for Reinforcement Learning,"Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, Raia Hadsell","Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.",2019-10-13T20:02:15Z,2019-10-13T20:02:15Z,http://arxiv.org/abs/1910.06764v1,http://arxiv.org/pdf/1910.06764v1,"cs.LG, cs.AI, stat.ML"
NAS evaluation is frustratingly hard,"Antoine Yang, Pedro M. Esperança, Fabio M. Carlucci","Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of $8$ NAS methods on $5$ datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method's relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macro-structure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between $8$ and $20$ cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls. The code used is available at https://github.com/antoyang/NAS-Benchmark.",2019-12-28T21:24:12Z,2020-02-13T22:10:12Z,http://arxiv.org/abs/1912.12522v3,http://arxiv.org/pdf/1912.12522v3,"cs.LG, cs.CV, stat.ML"
EfficientTDNN: Efficient Architecture Search for Speaker Recognition,"Rui Wang, Zhihua Wei, Haoran Duan, Shouling Ji, Yang Long, Zhen Hong","Convolutional neural networks (CNNs), such as the time-delay neural network (TDNN), have shown their remarkable capability in learning speaker embedding. However, they meanwhile bring a huge computational cost in storage size, processing, and memory. Discovering the specialized CNN that meets a specific constraint requires a substantial effort of human experts. Compared with hand-designed approaches, neural architecture search (NAS) appears as a practical technique in automating the manual architecture design process and has attracted increasing interest in spoken language processing tasks such as speaker recognition. In this paper, we propose EfficientTDNN, an efficient architecture search framework consisting of a TDNN-based supernet and a TDNN-NAS algorithm. The proposed supernet introduces temporal convolution of different ranges of the receptive field and feature aggregation of various resolutions from different layers to TDNN. On top of it, the TDNN-NAS algorithm quickly searches for the desired TDNN architecture via weight-sharing subnets, which surprisingly reduces computation while handling the vast number of devices with various resources requirements. Experimental results on the VoxCeleb dataset show the proposed EfficientTDNN enables approximate $10^{13}$ architectures concerning depth, kernel, and width. Considering different computation constraints, it achieves a 2.20% equal error rate (EER) with 204M multiply-accumulate operations (MACs), 1.41% EER with 571M MACs as well as 0.94% EER with 1.45G MACs. Comprehensive investigations suggest that the trained supernet generalizes subnets not sampled during training and obtains a favorable trade-off between accuracy and efficiency.",2021-03-25T03:28:07Z,2022-06-18T09:35:24Z,http://arxiv.org/abs/2103.13581v5,http://arxiv.org/pdf/2103.13581v5,"eess.AS, cs.AI"
Attention-Based Neural Networks for Chroma Intra Prediction in Video   Coding,"Marc Górriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta Mrak","Neural networks can be successfully used to improve several modules of advanced video coding schemes. In particular, compression of colour components was shown to greatly benefit from usage of machine learning models, thanks to the design of appropriate attention-based architectures that allow the prediction to exploit specific samples in the reference region. However, such architectures tend to be complex and computationally intense, and may be difficult to deploy in a practical video coding pipeline. This work focuses on reducing the complexity of such methodologies, to design a set of simplified and cost-effective attention-based architectures for chroma intra-prediction. A novel size-agnostic multi-model approach is proposed to reduce the complexity of the inference process. The resulting simplified architecture is still capable of outperforming state-of-the-art methods. Moreover, a collection of simplifications is presented in this paper, to further reduce the complexity overhead of the proposed prediction architecture. Thanks to these simplifications, a reduction in the number of parameters of around 90% is achieved with respect to the original attention-based methodologies. Simplifications include a framework for reducing the overhead of the convolutional operations, a simplified cross-component processing model integrated into the original architecture, and a methodology to perform integer-precision approximations with the aim to obtain fast and hardware-aware implementations. The proposed schemes are integrated into the Versatile Video Coding (VVC) prediction pipeline, retaining compression efficiency of state-of-the-art chroma intra-prediction methods based on neural networks, while offering different directions for significantly reducing coding complexity.",2021-02-09T18:01:22Z,2021-02-09T18:01:22Z,http://arxiv.org/abs/2102.04993v1,http://arxiv.org/pdf/2102.04993v1,"eess.IV, cs.CC, cs.CV, cs.LG, cs.MM"
Deformable image registration with deep network priors: a study on   longitudinal PET images,"Constance Fourcade, Ludovic Ferrer, Noemie Moreau, Gianmarco Santini, Aishlinn Brennan, Caroline Rousseau, Marie Lacombe, Vincent Fleury, Mathilde Colombié, Pascal Jézéquel, Mario Campone, Mathieu Rubeaux, Diana Mateus","Longitudinal image registration is challenging and has not yet benefited from major performance improvements thanks to deep-learning. Inspired by Deep Image Prior, this paper introduces a different use of deep architectures as regularizers to tackle the image registration question. We propose a subject-specific deformable registration method called MIRRBA, relying on a deep pyramidal architecture to be the prior parametric model constraining the deformation field. Diverging from the supervised learning paradigm, MIRRBA does not require a learning database, but only the pair of images to be registered to optimize the network's parameters and provide a deformation field. We demonstrate the regularizing power of deep architectures and present new elements to understand the role of the architecture in deep learning methods for registration. Hence, to study the impact of the network parameters, we ran our method with different architectural configurations on a private dataset of 110 metastatic breast cancer full-body PET images with manual segmentations of the brain, bladder and metastatic lesions. We compared it against conventional iterative registration approaches and supervised deep learning-based models. Global and local registration accuracies were evaluated using the detection rate and the Dice score respectively, while registration realism was evaluated using the Jacobian's determinant. Moreover, we computed the ability of the different methods to shrink vanishing lesions with the disappearing rate. MIRRBA significantly improves the organ and lesion Dice scores of supervised models. Regarding the disappearing rate, MIRRBA more than doubles the best performing conventional approach SyNCC score. Our work therefore proposes an alternative way to bridge the performance gap between conventional and deep learning-based methods and demonstrates the regularizing power of deep architectures.",2021-11-22T10:58:14Z,2022-03-30T22:13:30Z,http://arxiv.org/abs/2111.11873v2,http://arxiv.org/pdf/2111.11873v2,"eess.IV, cs.CV"
"A wearable sensor vest for social humanoid robots with GPGPU, IoT, and   modular software architecture","Mohsen Jafarzadeh, Stephen Brooks, Shimeng Yu, Balakrishnan Prabhakaran, Yonas Tadesse","Currently, most social robots interact with their surroundings and humans through sensors that are integral parts of the robots, which limits the usability of the sensors, human-robot interaction, and interchangeability. A wearable sensor garment that fits many robots is needed in many applications. This article presents an affordable wearable sensor vest, and an open-source software architecture with the Internet of Things (IoT) for social humanoid robots. The vest consists of touch, temperature, gesture, distance, vision sensors, and a wireless communication module. The IoT feature allows the robot to interact with humans locally and over the Internet. The designed architecture works for any social robot that has a general-purpose graphics processing unit (GPGPU), I2C/SPI buses, Internet connection, and the Robotics Operating System (ROS). The modular design of this architecture enables developers to easily add/remove/update complex behaviors. The proposed software architecture provides IoT technology, GPGPU nodes, I2C and SPI bus mangers, audio-visual interaction nodes (speech to text, text to speech, and image understanding), and isolation between behavior nodes and other nodes. The proposed IoT solution consists of related nodes in the robot, a RESTful web service, and user interfaces. We used the HTTP protocol as a means of two-way communication with the social robot over the Internet. Developers can easily edit or add nodes in C, C++, and Python programming languages. Our architecture can be used for designing more sophisticated behaviors for social humanoid robots.",2022-01-06T18:57:35Z,2022-01-06T18:57:35Z,http://arxiv.org/abs/2201.02192v1,http://arxiv.org/pdf/2201.02192v1,"cs.RO, cs.AI, cs.HC, cs.SY, eess.SY, 68T40, I.2.9"
Vertex finding in neutrino-nucleus interaction: A Model Architecture   Comparison,"F. Akbar, A. Ghosh, S. Young, S. Akhter, Z. Ahmad Dar, V. Ansari, M. V. Ascencio, M. Sajjad Athar, A. Bodek, J. L. Bonilla, A. Bravar, H. Budd, G. Caceres, T. Cai, M. F. Carneiro, G. A. Díaz, J. Felix, L. Fields, A. Filkins, R. Fine, P. K. Gaura, R. Gran, D. A. Harris, D. Jena, S. Jena, J. Kleykamp, A. Klustová, D. Last, A. Lozano, X. G. Lu, E. Maher, S. Manly, W. A. Mann, K. S. McFarland, B. Messerly, J. Miller, O. Moreno, J. G. Morfín, J. K. Nelson, C. Nguyen, A. Olivier, V. Paolone, G. N. Perdue, K. J. Plows, M. A. Ramírez, D. Ruterbories, H. Su, V. S. Syrotenko, A. V. Waldron, B. Yaeggy, L. Zazueta","We compare different neural network architectures for Machine Learning (ML) algorithms designed to identify the neutrino interaction vertex position in the MINERvA detector. The architectures developed and optimized by hand are compared with the architectures developed in an automated way using the package ""Multi-node Evolutionary Neural Networks for Deep Learning"" (MENNDL), developed at Oak Ridge National Laboratory (ORNL). The two architectures resulted in a similar performance which suggests that the systematics associated with the optimized network architecture are small. Furthermore, we find that while the domain expert hand-tuned network was the best performer, the differences were negligible and the auto-generated networks performed well. There is always a trade-off between human, and computer resources for network optimization and this work suggests that automated optimization, assuming resources are available, provides a compelling way to save significant expert time.",2022-01-07T16:18:35Z,2022-01-07T16:18:35Z,http://arxiv.org/abs/2201.02523v1,http://arxiv.org/pdf/2201.02523v1,"hep-ex, physics.data-an, physics.ins-det"
GAUSS: Guided Encoder-Decoder Architecture for Hyperspectral Unmixing   with Spatial Smoothness,"Yasiru Ranasinghe, Kavinga Weerasooriya, Roshan Godaliyadda, Vijitha Herath, Parakrama Ekanayake, Dhananjaya Jayasundara, Lakshitha Ramanayake, Neranjan Senarath, Dulantha Wickramasinghe","In recent hyperspectral unmixing (HU) literature, the application of deep learning (DL) has become more prominent, especially with the autoencoder (AE) architecture. We propose a split architecture and use a pseudo-ground truth for abundances to guide the `unmixing network' (UN) optimization. Preceding the UN, an `approximation network' (AN) is proposed, which will improve the association between the centre pixel and its neighbourhood. Hence, it will accentuate spatial correlation in the abundances as its output is the input to the UN and the reference for the `mixing network' (MN). In the Guided Encoder-Decoder Architecture for Hyperspectral Unmixing with Spatial Smoothness (GAUSS), we proposed using one-hot encoded abundances as the pseudo-ground truth to guide the UN; computed using the k-means algorithm to exclude the use of prior HU methods. Furthermore, we release the single-layer constraint on MN by introducing the UN generated abundances in contrast to the standard AE for HU. Secondly, we experimented with two modifications on the pre-trained network using the GAUSS method. In GAUSS$_\textit{blind}$, we have concatenated the UN and the MN to back-propagate the reconstruction error gradients to the encoder. Then, in the GAUSS$_\textit{prime}$, abundance results of a signal processing (SP) method with reliable abundance results were used as the pseudo-ground truth with the GAUSS architecture. According to quantitative and graphical results for four experimental datasets, the three architectures either transcended or equated the performance of existing HU algorithms from both DL and SP domains.",2022-04-16T04:23:47Z,2022-12-20T16:35:44Z,http://arxiv.org/abs/2204.07713v2,http://arxiv.org/pdf/2204.07713v2,"cs.CV, eess.IV"
FTT-NAS: Discovering Fault-Tolerant Convolutional Neural Architecture,"Xuefei Ning, Guangjun Ge, Wenshuo Li, Zhenhua Zhu, Yin Zheng, Xiaoming Chen, Zhen Gao, Yu Wang, Huazhong Yang","With the fast evolvement of embedded deep-learning computing systems, applications powered by deep learning are moving from the cloud to the edge. When deploying neural networks (NNs) onto the devices under complex environments, there are various types of possible faults: soft errors caused by cosmic radiation and radioactive impurities, voltage instability, aging, temperature variations, and malicious attackers. Thus the safety risk of deploying NNs is now drawing much attention. In this paper, after the analysis of the possible faults in various types of NN accelerators, we formalize and implement various fault models from the algorithmic perspective. We propose Fault-Tolerant Neural Architecture Search (FT-NAS) to automatically discover convolutional neural network (CNN) architectures that are reliable to various faults in nowadays devices. Then we incorporate fault-tolerant training (FTT) in the search process to achieve better results, which is referred to as FTT-NAS. Experiments on CIFAR-10 show that the discovered architectures outperform other manually designed baseline architectures significantly, with comparable or fewer floating-point operations (FLOPs) and parameters. Specifically, with the same fault settings, F-FTT-Net discovered under the feature fault model achieves an accuracy of 86.2% (VS. 68.1% achieved by MobileNet-V2), and W-FTT-Net discovered under the weight fault model achieves an accuracy of 69.6% (VS. 60.8% achieved by ResNet-20). By inspecting the discovered architectures, we find that the operation primitives, the weight quantization range, the capacity of the model, and the connection pattern have influences on the fault resilience capability of NN models.",2020-03-20T17:17:14Z,2021-04-12T16:15:18Z,http://arxiv.org/abs/2003.10375v2,http://arxiv.org/pdf/2003.10375v2,"eess.SP, cs.LG, stat.ML"
NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural   Network Synthesis,"Anjul Tyagi, Cong Xie, Klaus Mueller","Recent advancements in the area of deep learning have shown the effectiveness of very large neural networks in several applications. However, as these deep neural networks continue to grow in size, it becomes more and more difficult to configure their many parameters to obtain good results. Presently, analysts must experiment with many different configurations and parameter settings, which is labor-intensive and time-consuming. On the other hand, the capacity of fully automated techniques for neural network architecture search is limited without the domain knowledge of human experts. To deal with the problem, we formulate the task of neural network architecture optimization as a graph space exploration, based on the one-shot architecture search technique. In this approach, a super-graph of all candidate architectures is trained in one-shot and the optimal neural network is identified as a sub-graph. In this paper, we present a framework that allows analysts to effectively build the solution sub-graph space and guide the network search by injecting their domain knowledge. Starting with the network architecture space composed of basic neural network components, analysts are empowered to effectively select the most promising components via our one-shot search scheme. Applying this technique in an iterative manner allows analysts to converge to the best performing neural network architecture for a given application. During the exploration, analysts can use their domain knowledge aided by cues provided from a scatterplot visualization of the search space to edit different components and guide the search for faster convergence. We designed our interface in collaboration with several deep learning researchers and its final effectiveness is evaluated with a user study and two case studies.",2020-09-28T01:48:45Z,2022-08-06T02:22:25Z,http://arxiv.org/abs/2009.13008v3,http://arxiv.org/pdf/2009.13008v3,"cs.LG, cs.CV, cs.HC, stat.ML"
Fast meningioma segmentation in T1-weighted MRI volumes using a   lightweight 3D deep learning architecture,"David Bouget, André Pedersen, Sayied Abdol Mohieb Hosainey, Johanna Vanel, Ole Solheim, Ingerid Reinertsen","Automatic and consistent meningioma segmentation in T1-weighted MRI volumes and corresponding volumetric assessment is of use for diagnosis, treatment planning, and tumor growth evaluation. In this paper, we optimized the segmentation and processing speed performances using a large number of both surgically treated meningiomas and untreated meningiomas followed at the outpatient clinic. We studied two different 3D neural network architectures: (i) a simple encoder-decoder similar to a 3D U-Net, and (ii) a lightweight multi-scale architecture (PLS-Net). In addition, we studied the impact of different training schemes. For the validation studies, we used 698 T1-weighted MR volumes from St. Olav University Hospital, Trondheim, Norway. The models were evaluated in terms of detection accuracy, segmentation accuracy and training/inference speed. While both architectures reached a similar Dice score of 70% on average, the PLS-Net was more accurate with an F1-score of up to 88%. The highest accuracy was achieved for the largest meningiomas. Speed-wise, the PLS-Net architecture tended to converge in about 50 hours while 130 hours were necessary for U-Net. Inference with PLS-Net takes less than a second on GPU and about 15 seconds on CPU. Overall, with the use of mixed precision training, it was possible to train competitive segmentation models in a relatively short amount of time using the lightweight PLS-Net architecture. In the future, the focus should be brought toward the segmentation of small meningiomas (less than 2ml) to improve clinical relevance for automatic and early diagnosis as well as speed of growth estimates.",2020-10-14T12:26:53Z,2020-10-14T12:26:53Z,http://arxiv.org/abs/2010.07002v1,http://arxiv.org/pdf/2010.07002v1,"eess.IV, cs.CV, cs.LG, I.4.6; J.3"
Quantum computing with magnetic atoms in optical lattices of reduced   periodicity,"Boris Ravaine, Andrei Derevianko, P. R. Berman","We investigate the feasibility of combining Raman optical lattices with a quantum computing architecture based on lattice-confined magnetically interacting neutral atoms. A particular advantage of the standing Raman field lattices comes from reduced interatomic separations leading to increased interatomic interactions and improved multi-qubit gate performance. Specifically, we analyze a $J=3/2$ Zeeman system placed in $% \sigma _{+}-\sigma_{-}$ Raman fields which exhibit $\lambda /4$ periodicity. We find that the resulting CNOT gate operations times are in the order of millisecond. We also investigate motional and magnetic-field induced decoherences specific to the proposed architecture.",2006-06-20T06:31:43Z,2006-06-24T22:28:15Z,http://arxiv.org/abs/quant-ph/0606162v2,http://arxiv.org/pdf/quant-ph/0606162v2,"quant-ph, physics.atom-ph"
Reliable Memories Built from Unreliable Components Based on Expander   Graphs,"Shashi Kiran Chilappagari, Bane Vasic","In this paper, memories built from components subject to transient faults are considered. A fault-tolerant memory architecture based on low-density parity-check codes is proposed and the existence of reliable memories for the adversarial failure model is proved. The proof relies on the expansion property of the underlying Tanner graph of the code. An equivalence between the Taylor-Kuznetsov (TK) scheme and Gallager B algorithm is established and the results are extended to the independent failure model. It is also shown that the proposed memory architecture has lower redundancy compared to the TK scheme. The results are illustrated with specific numerical examples.",2007-05-01T00:07:46Z,2007-05-01T00:07:46Z,http://arxiv.org/abs/0705.0044v1,http://arxiv.org/pdf/0705.0044v1,"cs.IT, math.IT"
Resource Adaptive Agents in Interactive Theorem Proving,"Christoph Benzmueller, Volker Sorge","We introduce a resource adaptive agent mechanism which supports the user in interactive theorem proving. The mechanism uses a two layered architecture of agent societies to suggest appropriate commands together with possible command argument instantiations. Experiments with this approach show that its effectiveness can be further improved by introducing a resource concept. In this paper we provide an abstract view on the overall mechanism, motivate the necessity of an appropriate resource concept and discuss its realization within the agent architecture.",2009-01-23T05:29:09Z,2009-01-23T05:29:09Z,http://arxiv.org/abs/0901.3585v1,http://arxiv.org/pdf/0901.3585v1,"cs.LO, cs.AI, I.2.11; I.2.3; F.4.1; D.4.7; H.3.4"
An exploration of CUDA and CBEA for a gravitational wave data-analysis   application (Einstein@Home),"Jens Breitbart, Gaurav Khanna","We present a detailed approach for making use of two new computer hardware architectures -- CBEA and CUDA -- for accelerating a scientific data-analysis application (Einstein@Home). Our results suggest that both the architectures suit the application quite well and the achievable performance in the same software developmental time-frame, is nearly identical.",2009-04-12T12:43:04Z,2009-06-29T17:48:44Z,http://arxiv.org/abs/0904.1826v2,http://arxiv.org/pdf/0904.1826v2,"gr-qc, physics.comp-ph"
Certificate-based Single Sign-On Mechanism for Multi-Platform   Distributed Systems,"Magyari Attila, Genge Bela, Haller Piroska","We propose a certificate-based single sign-on mechanism in distributed systems. The proposed security protocols and authentication mechanisms are integrated in a middleware. The novelty of our middleware lies on the use of XPCOM components, this way we provide a different services that can be used on every platform where Mozilla is available. The componen based architecture of the implemented services allows using the authentication components separately.",2009-09-09T07:20:05Z,2009-09-09T07:20:05Z,http://arxiv.org/abs/0909.1640v1,http://arxiv.org/pdf/0909.1640v1,"cs.CR, cs.NI, D.4.2"
Proceedings Ninth International Workshop on the Foundations of   Coordination Languages and Software Architectures,"MohammadReza Mousavi, Gwen Salaün","This volume contains the proceedings of FOCLASA 2010, the 9th International Workshop on the Foundations of Coordination Languages and Software Architectures. FOCLASA 2010 was held in Paris, France on July 30th, 2010 as a satellite event of the 21st International Conference on Concurrency Theory, CONCUR 2010. The papers presented in this proceedings tackle different issues that are currently central to our community, namely software adaptation, sensor networks, distributed control, non-functional aspects of coordination such as resources, timing and stochastics.",2010-07-28T14:39:30Z,2010-07-28T14:39:30Z,http://arxiv.org/abs/1007.4993v1,http://arxiv.org/pdf/1007.4993v1,"cs.SE, cs.DC, cs.LO, cs.PL, D.2.11; D.3.1; D.2.4; F.3.1; F.3.2"
A simple circuit with dynamic logic architecture of basic logic gates,"I. Campos-Canton, J. A. Pecina-Sanchez, E. Campos-Canton, H. C. Rosu","We report experimental results obtained with a circuit possessing dynamic logic architecture based on one of the theoretical schemes proposed by H. Peng and collaborators in 2008. The schematic diagram of the electronic circuit and its implementation to get different basic logic gates are displayed and discussed. In particular, we show explicitly how to get the electronic NOR, NAND, and XOR gates. The proposed electronic circuit is easy to build because it employs only resistors, operational amplifiers and comparators",2010-12-23T18:16:10Z,2010-12-23T18:16:10Z,http://arxiv.org/abs/1012.5270v1,http://arxiv.org/pdf/1012.5270v1,"cond-mat.other, cs.OH"
Scalability of spin FPGA: A Reconfigurable Architecture based on spin   MOSFET,"Tetsufumi Tanamoto, Hideyuki Sugiyama, Tomoaki Inokuchi, Takao Marukame, Mizue Ishikawa, Kazutaka Ikegami, Yoshiaki Saito","Scalability of Field Programmable Gate Array (FPGA) using spin MOSFET (spin FPGA) with magnetocurrent (MC) ratio in the range of 100% to 1000% is discussed for the first time. Area and speed of million-gate spin FPGA are numerically benchmarked with CMOS FPGA for 22nm, 32nm and 45nm technologies including 20% transistor size variation. We show that area is reduced and speed is increased in spin FPGA owing to the nonvolatile memory function of spin MOSFET.",2011-04-08T06:24:47Z,2011-04-08T06:24:47Z,http://arxiv.org/abs/1104.1493v1,http://arxiv.org/pdf/1104.1493v1,"cond-mat.mes-hall, cs.AR"
Introducing Sourcements,"J. A. Bergstra, G. P. A. J. Delen, S. F. M. van Vlijmen","Sourcing processes are discussed at a high abstraction level. A dedicated terminology is developed concerning general aspects of sourcing. The term sourcement is coined to denote a building block for sourcing. No- tions of allocation, functional architecture and allocational architecture, equilibrium, and configuration are discussed. Limitations of the concept of outsourcing are outlined. This theoretical work is meant to serve as a point of departure for the subsequent development of a detailed theory of sourcing and sourcing transformations, which can be a tool for dealing with practical applica- tions.",2011-07-23T12:12:12Z,2011-07-23T12:12:12Z,http://arxiv.org/abs/1107.4684v1,http://arxiv.org/pdf/1107.4684v1,"cs.SE, cs.GL, K.6.0; J.4; H.4.0; D.0"
Network Coding Meets Information-Centric Networking,"Marie-Jose Montpetit, Cedric Westphal, Dirk Trossen","The focus of user behavior in the Internet has changed over the recent years towards being driven by exchanging and accessing information. Many advances in networking technologies have utilized this change by focusing on the content of an exchange rather than the endpoints exchanging the content. Network coding and information centric networking are two examples of these technology trends, each being developed largely independent so far. This paper brings these areas together in an evolutionary as well as explorative setting for a new internetworking architecture. We outline opportunities for applying network coding in a novel and performance-enhancing way that could eventually push forward the case for information centric network itself.",2012-01-11T19:58:30Z,2012-01-11T19:58:30Z,http://arxiv.org/abs/1201.2387v1,http://arxiv.org/pdf/1201.2387v1,"cs.NI, 68M10, 90B18, C.2.1; H.3.5"
Partial-MDS Codes and their Application to RAID Type of Architectures,"Mario Blaum, James Lee Hafner, Steven Hetzler","A family of codes with a natural two-dimensional structure is presented, inspired by an application of RAID type of architectures whose units are solid state drives (SSDs). Arrays of SSDs behave differently to arrays of hard disk drives (HDDs), since hard errors in sectors are common and traditional RAID approaches (like RAID 5 or RAID 6) may be either insufficient or excessive. An efficient solution to this problem is given by the new codes presented, called partial-MDS (PMDS) codes.",2012-05-04T16:08:26Z,2014-09-11T16:45:24Z,http://arxiv.org/abs/1205.0997v2,http://arxiv.org/pdf/1205.0997v2,"cs.IT, math.IT"
MITRA: A Meta-Model for Information Flow in Trust and Reputation   Architectures,"Eugen Staab, Guillaume Muller","We propose MITRA, a meta-model for the information flow in (computational) trust and reputation architectures. On an abstract level, MITRA describes the information flow as it is inherent in prominent trust and reputation models from the literature. We use MITRA to provide a structured comparison of these models. This makes it possible to get a clear overview of the complex research area. Furthermore, by doing so, we identify interesting new approaches for trust and reputation modeling that so far have not been investigated.",2012-07-02T14:34:05Z,2012-07-02T14:34:05Z,http://arxiv.org/abs/1207.0405v1,http://arxiv.org/pdf/1207.0405v1,"cs.MA, 68T42, I.2.11"
Recursive Descriptions of Polar Codes,"Noam Presman, Simon Litsyn","Polar codes are recursive general concatenated codes. This property motivates a recursive formalization of the known decoding algorithms: Successive Cancellation, Successive Cancellation with Lists and Belief Propagation. Using such description allows an easy development of these algorithms for arbitrary polarizing kernels. Hardware architectures for these decoding algorithms are also described in a recursive way, both for Arikan's standard polar codes and for arbitrary polarizing kernels.",2012-09-21T13:46:26Z,2015-06-18T17:22:42Z,http://arxiv.org/abs/1209.4818v3,http://arxiv.org/pdf/1209.4818v3,"cs.IT, cs.AR, math.IT"
Separating Topology and Geometry in Space Planning,"Benachir Medjdoub, Bernard Yannou","We are dealing with the problem of space layout planning here. We present an architectural conceptual CAD approach. Starting with design specifications in terms of constraints over spaces, a specific enumeration heuristics leads to a complete set of consistent conceptual design solutions named topological solutions. These topological solutions which do not presume any precise definitive dimension correspond to the sketching step that an architect carries out from the Design specifications on a preliminary design phase in architecture.",2013-03-16T20:12:49Z,2013-03-16T20:12:49Z,http://arxiv.org/abs/1303.4017v1,http://arxiv.org/pdf/1303.4017v1,"cs.AI, physics.med-ph"
Constant-Factor Optimization of Quantum Adders on 2D Quantum   Architectures,"Mehdi Saeedi, Alireza Shafaei, Massoud Pedram","Quantum arithmetic circuits have practical applications in various quantum algorithms. In this paper, we address quantum addition on 2-dimensional nearest-neighbor architectures based on the work presented by Choi and Van Meter (JETC 2012). To this end, we propose new circuit structures for some basic blocks in the adder, and reduce communication overhead by adding concurrency to consecutive blocks and also by parallel execution of expensive Toffoli gates. The proposed optimizations reduce total depth from $140\sqrt n+k_1$ to $92\sqrt n+k_2$ for constants $k_1,k_2$ and affect the computation fidelity considerably.",2013-04-01T19:40:39Z,2013-04-01T19:40:39Z,http://arxiv.org/abs/1304.0432v1,http://arxiv.org/pdf/1304.0432v1,"quant-ph, cs.ET"
Fast Polar Decoders: Algorithm and Implementation,"Gabi Sarkis, Pascal Giard, Alexander Vardy, Claude Thibeault, Warren J. Gross","Polar codes provably achieve the symmetric capacity of a memoryless channel while having an explicit construction. This work aims to increase the throughput of polar decoder hardware by an order of magnitude relative to the state of the art successive-cancellation decoder. We present an algorithm, architecture, and FPGA implementation of a gigabit-per-second polar decoder.",2013-07-26T20:07:04Z,2013-12-09T23:38:49Z,http://arxiv.org/abs/1307.7154v2,http://arxiv.org/pdf/1307.7154v2,"cs.AR, cs.IT, math.IT"
SWIFT: Fast algorithms for multi-resolution SPH on multi-core   architectures,"Pedro Gonnet, Matthieu Schaller, Tom Theuns, Aidan B. G. Chalk","This paper describes a novel approach to neighbour-finding in Smoothed Particle Hydrodynamics (SPH) simulations with large dynamic range in smoothing length. This approach is based on hierarchical cell decompositions, sorted interactions, and a task-based formulation. It is shown to be faster than traditional tree-based codes, and to scale better than domain decomposition-based approaches on shared-memory parallel architectures such as multi-cores.",2013-09-15T17:53:42Z,2013-09-15T17:53:42Z,http://arxiv.org/abs/1309.3783v1,http://arxiv.org/pdf/1309.3783v1,"astro-ph.IM, astro-ph.CO, cs.DC"
MIMO Beamforming in Millimeter-Wave Directional Wi-Fi,"Keang-Po Ho, Shi Cheng, Jianhan Liu","Beamforming is indispensable in the operation of 60-GHz millimeter-wave directional multi-gigabit Wi-Fi. Simple power method and its extensions enable the transmitting and receiving antenna arrays to form a beam for single spatial stream. To further improve the spectral efficiency in future 60-GHz directional Wi-Fi, alternating least square (ALS) algorithm can form multiple beams between the transmitter and receiver for multi-input-multi-output (MIMO) operations. For both shared and split MIMO architecture, the ALS beamforming algorithm can be operated in both frequency-flat and frequency-selective channels. In the split architecture, MIMO beamforming approximately maximizes the capacity of the beam-formed MIMO channel.",2014-03-30T03:14:56Z,2014-03-30T03:14:56Z,http://arxiv.org/abs/1403.7697v1,http://arxiv.org/pdf/1403.7697v1,"cs.IT, math.IT"
Efficient and Scalable Algorithms for Smoothed Particle Hydrodynamics on   Hybrid Shared/Distributed-Memory Architectures,Pedro Gonnet,"This paper describes a new fast and implicitly parallel approach to neighbour-finding in multi-resolution Smoothed Particle Hydrodynamics (SPH) simulations. This new approach is based on hierarchical cell decompositions and sorted interactions, within a task-based formulation. It is shown to be faster than traditional tree-based codes, and to scale better than domain decomposition-based approaches on hybrid shared/distributed-memory parallel architectures, e.g. clusters of multi-cores, achieving a $40\times$ speedup over the Gadget-2 simulation code.",2014-04-08T20:45:22Z,2014-04-08T20:45:22Z,http://arxiv.org/abs/1404.2303v1,http://arxiv.org/pdf/1404.2303v1,"cs.DC, astro-ph.IM, physics.comp-ph"
A Vision Architecture,Christoph von der Malsburg,"We are offering a particular interpretation (well within the range of experimentally and theoretically accepted notions) of neural connectivity and dynamics and discuss it as the data-and-process architecture of the visual system. In this interpretation the permanent connectivity of cortex is an overlay of well-structured networks, nets, which are formed on the slow time-scale of learning by self-interaction of the network under the influence of sensory input, and which are selectively activated on the fast perceptual time-scale. Nets serve as an explicit, hierarchically structured representation of visual structure in the various sub-modalities, as constraint networks favouring mutually consistent sets of latent variables and as projection mappings to deal with invariance.",2014-07-07T09:41:47Z,2014-07-07T09:41:47Z,http://arxiv.org/abs/1407.1642v1,http://arxiv.org/pdf/1407.1642v1,"q-bio.NC, I.2.10; I.4.8; I.5.1"
A Mixtures-of-Experts Framework for Multi-Label Classification,"Charmgil Hong, Iyad Batal, Milos Hauskrecht","We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.",2014-09-16T16:52:14Z,2014-09-16T16:52:14Z,http://arxiv.org/abs/1409.4698v1,http://arxiv.org/pdf/1409.4698v1,"cs.LG, I.2.6"
High-Order Finite-differences on multi-threaded architectures using OCCA,"David S. Medina, Amik St-Cyr, Timothy Warburton","High-order finite-difference methods are commonly used in wave propagators for industrial subsurface imaging algorithms. Computational aspects of the reduced linear elastic vertical transversely isotropic propagator are considered. Thread parallel algorithms suitable for implementing this propagator on multi-core and many-core processing devices are introduced. Portability is addressed through the use of the \OCCA runtime programming interface. Finally, performance results are shown for various architectures on a representative synthetic test case.",2014-10-02T18:15:22Z,2014-10-02T18:15:22Z,http://arxiv.org/abs/1410.1387v1,http://arxiv.org/pdf/1410.1387v1,"math.NA, cs.MS, cs.NA"
Staggered Dslash Performance on Intel Xeon Phi Architecture,"Ruizi Li, Steven Gottlieb","The conjugate gradient (CG) algorithm is among the most essential and time consuming parts of lattice calculations with staggered quarks. We test the performance of CG and dslash, the key step in the CG algorithm, on the Intel Xeon Phi, also known as the Many Integrated Core (MIC) architecture. We try different parallelization strategies using MPI, OpenMP, and the vector processing units (VPUs).",2014-11-08T05:55:11Z,2014-11-08T05:55:11Z,http://arxiv.org/abs/1411.2087v1,http://arxiv.org/pdf/1411.2087v1,"hep-lat, physics.comp-ph"
Hardware Architecture for Single Iteration Reconstruction Algorithm,"Andjela Draganic, Irena Orovic, Nedjeljko Lekic, Milos Dakovic, Srdjan Stankovic","A hardware architecture for the single iteration algorithm is proposed in this paper. Single iteration algorithm enables reconstruction of the full signal when small number of signal samples is available. The algorithm is based on the threshold calculation, and allows distinguishing between signal components and noise that appears as a consequence of missing samples. The proposed system for hardware realization is divided into three parts, each part with different functionality. The system is suitable for the FPGA realization. Realization of the blocks for which there are no standard components in FPGA, is discussed as well.",2015-02-22T14:24:09Z,2015-02-22T14:24:09Z,http://arxiv.org/abs/1502.06230v1,http://arxiv.org/pdf/1502.06230v1,"cs.IT, math.IT"
Performance Evaluation Of Qos In Wimax Network,"Ahmed Hassan M. Hassan, Elrasheed Ismail M. Zayid, Mohammed Altayeb Awad, Ahmed Salah Mohammed, Samreen Tarig Hassan","OPNET Modeler is used to simulate the architecture and to calculate the performance criteria (i.e. throughput, delay and data dropped) that slightly concerned in network estimation. It is concluded that our models shorten the time quite a bit for obtaining the performance measures of an end-to-end delay as well as throughput can be used as an effective tool for this purpose.",2015-06-16T10:28:59Z,2015-06-16T10:28:59Z,http://arxiv.org/abs/1506.04902v1,http://arxiv.org/pdf/1506.04902v1,"cs.NI, cs.IT, math.IT"
Performance Analysis of an Astrophysical Simulation Code on the Intel   Xeon Phi Architecture,"Vahid Noormofidi, Susan R. Atlas, Huaiyu Duan","We have developed the astrophysical simulation code XFLAT to study neutrino oscillations in supernovae. XFLAT is designed to utilize multiple levels of parallelism through MPI, OpenMP, and SIMD instructions (vectorization). It can run on both CPU and Xeon Phi co-processors based on the Intel Many Integrated Core Architecture (MIC). We analyze the performance of XFLAT on configurations with CPU only, Xeon Phi only and both CPU and Xeon Phi. We also investigate the impact of I/O and the multi-node performance of XFLAT on the Xeon Phi-equipped Stampede supercomputer at the Texas Advanced Computing Center (TACC).",2015-10-07T23:02:00Z,2015-10-07T23:02:00Z,http://arxiv.org/abs/1510.02163v1,http://arxiv.org/pdf/1510.02163v1,"cs.DC, cs.CE, D.1.3; I.6.8; D.2.8"
A Source-Channel Separation Theorem with Application to the Source   Broadcast Problem,"Kia Khezeli, Jun Chen","A converse method is developed for the source broadcast problem. Specifically, it is shown that the separation architecture is optimal for a variant of the source broadcast problem and the associated source-channel separation theorem can be leveraged, via a reduction argument, to establish a necessary condition for the original problem, which unifies several existing results in the literature. Somewhat surprisingly, this method, albeit based on the source-channel separation theorem, can be used to prove the optimality of non-separation based schemes and determine the performance limits in certain scenarios where the separation architecture is suboptimal.",2016-02-06T19:35:19Z,2016-02-06T19:35:19Z,http://arxiv.org/abs/1602.02294v1,http://arxiv.org/pdf/1602.02294v1,"cs.IT, math.IT"
Optimized Polynomial Evaluation with Semantic Annotations,"Daniel Rubio Bonilla, Colin W. Glass, Jan Kuper","In this paper we discuss how semantic annotations can be used to introduce mathematical algorithmic information of the underlying imperative code to enable compilers to produce code transformations that will enable better performance. By using this approaches not only good performance is achieved, but also better programmability, maintainability and portability across different hardware architectures. To exemplify this we will use polynomial equations of different degrees.",2016-03-04T16:13:24Z,2016-03-11T11:31:59Z,http://arxiv.org/abs/1603.01520v3,http://arxiv.org/pdf/1603.01520v3,"cs.PL, cs.CL, B.1.4"
Generalized surface codes and packing of logical qubits,"Nicolas Delfosse, Pavithran Iyer, David Poulin","We consider a notion of relative homology (and cohomology) for surfaces with two types of boundaries. Using this tool, we study a generalization of Kitaev's code based on surfaces with mixed boundaries. This construction includes both Bravyi and Kitaev's and Freedman and Meyer's extension of Kitaev's toric code. We argue that our generalization offers a denser storage of quantum information. In a planar architecture, we obtain a three-fold overhead reduction over the standard architecture consisting of a punctured square lattice.",2016-06-22T21:24:23Z,2016-06-22T21:24:23Z,http://arxiv.org/abs/1606.07116v1,http://arxiv.org/pdf/1606.07116v1,"quant-ph, cs.IT, math.IT"
Deep vs. shallow networks : An approximation theory perspective,"Hrushikesh Mhaskar, Tomaso Poggio","The paper briefy reviews several recent results on hierarchical architectures for learning from examples, that may formally explain the conditions under which Deep Convolutional Neural Networks perform much better in function approximation problems than shallow, one-hidden layer architectures. The paper announces new results for a non-smooth activation function - the ReLU function - used in present-day neural networks, as well as for the Gaussian networks. We propose a new definition of relative dimension to encapsulate different notions of sparsity of a function class that can possibly be exploited by deep networks but not by shallow ones to drastically reduce the complexity required for approximation and learning.",2016-08-10T20:02:40Z,2016-08-10T20:02:40Z,http://arxiv.org/abs/1608.03287v1,http://arxiv.org/pdf/1608.03287v1,"cs.LG, math.FA"
A Practical Quantum Instruction Set Architecture,"Robert S. Smith, Michael J. Curtis, William J. Zeng","We introduce an abstract machine architecture for classical/quantum computations---including compilation---along with a quantum instruction language called Quil for explicitly writing these computations. With this formalism, we discuss concrete implementations of the machine and non-trivial algorithms targeting them. The introduction of this machine dovetails with ongoing development of quantum computing technology, and makes possible portable descriptions of recent classical/quantum algorithms.",2016-08-11T03:17:57Z,2017-02-17T18:44:52Z,http://arxiv.org/abs/1608.03355v2,http://arxiv.org/pdf/1608.03355v2,"quant-ph, cs.ET, cs.PL"
"Learning to Communicate: Channel Auto-encoders, Domain Specific   Regularizers, and Attention","Timothy J O'Shea, Kiran Karra, T. Charles Clancy",We address the problem of learning efficient and adaptive ways to communicate binary information over an impaired channel. We treat the problem as reconstruction optimization through impairment layers in a channel autoencoder and introduce several new domain-specific regularizing layers to emulate common channel impairments. We also apply a radio transformer network based attention model on the input of the decoder to help recover canonical signal representations. We demonstrate some promising initial capacity results from this architecture and address several remaining challenges before such a system could become practical.,2016-08-23T07:41:31Z,2016-08-23T07:41:31Z,http://arxiv.org/abs/1608.06409v1,http://arxiv.org/pdf/1608.06409v1,"cs.LG, cs.IT, cs.NI, math.IT"
Towards a New Interpretation of Separable Convolutions,Tapabrata Ghosh,"In recent times, the use of separable convolutions in deep convolutional neural network architectures has been explored. Several researchers, most notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in their deep architectures and have demonstrated state of the art or close to state of the art performance. However, the underlying mechanism of action of separable convolutions are still not fully understood. Although their mathematical definition is well understood as a depthwise convolution followed by a pointwise convolution, deeper interpretations such as the extreme Inception hypothesis (Chollet, 2016) have failed to provide a thorough explanation of their efficacy. In this paper, we propose a hybrid interpretation that we believe is a better model for explaining the efficacy of separable convolutions.",2017-01-16T23:57:33Z,2017-01-16T23:57:33Z,http://arxiv.org/abs/1701.04489v1,http://arxiv.org/pdf/1701.04489v1,"cs.LG, stat.ML"
Temporal Overdrive Recurrent Neural Network,"Filippo Maria Bianchi, Michael Kampffmeyer, Enrico Maiorino, Robert Jenssen","In this work we present a novel recurrent neural network architecture designed to model systems characterized by multiple characteristic timescales in their dynamics. The proposed network is composed by several recurrent groups of neurons that are trained to separately adapt to each timescale, in order to improve the system identification process. We test our framework on time series prediction tasks and we show some promising, preliminary results achieved on synthetic data. To evaluate the capabilities of our network, we compare the performance with several state-of-the-art recurrent architectures.",2017-01-18T17:37:35Z,2017-01-18T17:37:35Z,http://arxiv.org/abs/1701.05159v1,http://arxiv.org/pdf/1701.05159v1,"cs.NE, math.DS"
Effects of the optimisation of the margin distribution on generalisation   in deep architectures,"Lech Szymanski, Brendan McCane, Wei Gao, Zhi-Hua Zhou","Despite being so vital to success of Support Vector Machines, the principle of separating margin maximisation is not used in deep learning. We show that minimisation of margin variance and not maximisation of the margin is more suitable for improving generalisation in deep architectures. We propose the Halfway loss function that minimises the Normalised Margin Variance (NMV) at the output of a deep learning models and evaluate its performance against the Softmax Cross-Entropy loss on the MNIST, smallNORB and CIFAR-10 datasets.",2017-04-19T08:31:20Z,2017-04-19T08:31:20Z,http://arxiv.org/abs/1704.05646v1,http://arxiv.org/pdf/1704.05646v1,"cs.LG, 68T05, I.2.6; I.5.1"
Grounded Recurrent Neural Networks,"Ankit Vani, Yacine Jernite, David Sontag","In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process ""grounding""). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.",2017-05-23T23:17:49Z,2017-05-23T23:17:49Z,http://arxiv.org/abs/1705.08557v1,http://arxiv.org/pdf/1705.08557v1,"stat.ML, cs.CL, cs.LG, cs.NE"
Synthesizing Deep Neural Network Architectures using Biological Synaptic   Strength Distributions,"A. H. Karimi, M. J. Shafiee, A. Ghodsi, A. Wong","In this work, we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions, and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a CNN with convolutional layer synaptic strengths drawn from biologically-inspired distributions such as log-normal or correlated center-surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures that do not require many data samples to learn, and can sidestep current training procedures while maintaining or boosting modelling performance.",2017-07-01T01:30:21Z,2017-07-01T01:30:21Z,http://arxiv.org/abs/1707.00081v1,http://arxiv.org/pdf/1707.00081v1,"cs.NE, cs.AI, cs.CV, stat.ML"
A classical groupoid model for quantum networks,"David J. Reutter, Jamie Vicary","We give a mathematical analysis of a new type of classical computer network architecture, intended as a model of a new technology that has recently been proposed in industry. Our approach is based on groubits, generalizations of classical bits based on groupoids. This network architecture allows the direct execution of a number of protocols that are usually associated with quantum networks, including teleportation, dense coding and secure key distribution.",2017-07-04T13:16:57Z,2019-03-27T18:21:35Z,http://arxiv.org/abs/1707.00966v4,http://arxiv.org/pdf/1707.00966v4,"quant-ph, math.CT"
Lessons learnt from the NetIDE project: Taking SDN programming to the   next level,"Pedro A. Aranda Gutierrez, Roberto Doriguzzi-Corin, Elisa Rojas","SDN promises to overcome vendor lock-in by enabling a multi-vendor hardware and software ecosystem in operator networks. However, we observe that this is currently not happening. A framework allowing to compose SDN applications combining different frameworks can help revert the trend. In this paper, we analyze the challenges in the current SDN landscape and then present the multi-controller SDN framework developed by the NetIDE project. Our architecture supports different SDN southbound protocols and we have implemented a proof of concept using the OpenFlow protocol, which has given us a greater insight on its shortcomings.",2017-07-29T21:31:28Z,2017-07-29T21:31:28Z,http://arxiv.org/abs/1707.09558v1,http://arxiv.org/pdf/1707.09558v1,"cs.NI, C.2; C.2.1"
Sum-Product Graphical Models,"Mattia Desana, Christoph Schnörr","This paper introduces a new probabilistic architecture called Sum-Product Graphical Model (SPGM). SPGMs combine traits from Sum-Product Networks (SPNs) and Graphical Models (GMs): Like SPNs, SPGMs always enable tractable inference using a class of models that incorporate context specific independence. Like GMs, SPGMs provide a high-level model interpretation in terms of conditional independence assumptions and corresponding factorizations. Thus, the new architecture represents a class of probability distributions that combines, for the first time, the semantics of graphical models with the evaluation efficiency of SPNs. We also propose a novel algorithm for learning both the structure and the parameters of SPGMs. A comparative empirical evaluation demonstrates competitive performances of our approach in density estimation.",2017-08-21T22:23:20Z,2017-08-21T22:23:20Z,http://arxiv.org/abs/1708.06438v1,http://arxiv.org/pdf/1708.06438v1,"stat.ML, cs.LG"
Expectation Learning for Adaptive Crossmodal Stimuli Association,"Pablo Barros, German I. Parisi, Di Fu, Xun Liu, Stefan Wermter","The human brain is able to learn, generalize, and predict crossmodal stimuli. Learning by expectation fine-tunes crossmodal processing at different levels, thus enhancing our power of generalization and adaptation in highly dynamic environments. In this paper, we propose a deep neural architecture trained by using expectation learning accounting for unsupervised learning tasks. Our learning model exhibits a self-adaptable behavior, setting the first steps towards the development of deep learning architectures for crossmodal stimuli association.",2018-01-23T16:47:32Z,2018-01-23T16:47:32Z,http://arxiv.org/abs/1801.07654v1,http://arxiv.org/pdf/1801.07654v1,"cs.LG, cs.AI, cs.SD, q-bio.NC, stat.ML"
Human-Machine Inference Networks For Smart Decision Making:   Opportunities and Challenges,"Aditya Vempaty, Bhavya Kailkhura, Pramod K. Varshney","The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines complementary cognitive strengths of humans and machines in an intelligent manner to tackle various inference tasks and achieves higher performance than either humans or machines by themselves. While inference performance optimization techniques for human-only or sensor-only networks are quite mature, HuMaINs require novel signal processing and machine learning solutions. In this paper, we present an overview of the HuMaINs architecture with a focus on three main issues that include architecture design, inference algorithms including security/privacy challenges, and application areas/use cases.",2018-01-29T17:03:34Z,2018-01-29T17:03:34Z,http://arxiv.org/abs/1801.09626v1,http://arxiv.org/pdf/1801.09626v1,"cs.HC, cs.AI, stat.ML"
Sparse Architectures for Text-Independent Speaker Verification Using   Deep Neural Networks,"Sara Sedighi, Shayan Ramhormozi","Network pruning is of great importance due to the elimination of the unimportant weights or features activated due to the network over-parametrization. Advantages of sparsity enforcement include preventing the overfitting and speedup. Considering a large number of parameters in deep architectures, network compression becomes of critical importance due to the required huge amount of computational power. In this work, we impose structured sparsity for speaker verification which is the validation of the query speaker compared to the speaker gallery. We will show that the mere sparsity enforcement can improve the verification results due to the possible initial overfitting in the network.",2018-05-19T17:35:14Z,2018-08-09T22:42:24Z,http://arxiv.org/abs/1805.07628v2,http://arxiv.org/pdf/1805.07628v2,"cs.SD, eess.AS"
Scoring Lexical Entailment with a Supervised Directional Similarity   Network,"Marek Rei, Daniela Gerz, Ivan Vulić","We present the Supervised Directional Similarity Network (SDSN), a novel neural architecture for learning task-specific transformation functions on top of general-purpose word embeddings. Relying on only a limited amount of supervision from task-specific scores on a subset of the vocabulary, our architecture is able to generalise and transform a general-purpose distributional vector space to model the relation of lexical entailment. Experiments show excellent performance on scoring graded lexical entailment, raising the state-of-the-art on the HyperLex dataset by approximately 25%.",2018-05-23T18:03:40Z,2018-05-23T18:03:40Z,http://arxiv.org/abs/1805.09355v1,http://arxiv.org/pdf/1805.09355v1,"cs.CL, cs.LG, cs.NE, I.2.7; I.2.6; I.5.1"
Network-Connected UAV Communications: Potentials and Challenges,"Haichao Wang, Jinlong Wang, Jin Chen, Yuping Gong, Guoru Ding","This article explores the use of network-connected unmanned aerial vehicle (UAV) communications as a compelling solution to achieve high-rate information transmission and support ultra-reliable UAV remote command and control. We first discuss the use cases of UAVs and the resulting communication requirements, accompanied with a flexible architecture for network-connected UAV communications. Then, the signal transmission and interference characteristics are theoretically analyzed, and subsequently we highlight the design and optimization considerations, including antenna design, non-orthogonal multiple access communications, as well as network selection and association optimization. Finally, case studies are provided to show the feasibility of network-connected UAV communications.",2018-05-29T12:44:12Z,2018-05-29T12:44:12Z,http://arxiv.org/abs/1806.04583v1,http://arxiv.org/pdf/1806.04583v1,"cs.NI, eess.SP"
Primal-dual residual networks,"Christoph Brauer, Dirk Lorenz","In this work, we propose a deep neural network architecture motivated by primal-dual splitting methods from convex optimization. We show theoretically that there exists a close relation between the derived architecture and residual networks, and further investigate this connection in numerical experiments. Moreover, we demonstrate how our approach can be used to unroll optimization algorithms for certain problems with hard constraints. Using the example of speech dequantization, we show that our method can outperform classical splitting methods when both are applied to the same task.",2018-06-15T06:34:34Z,2018-06-15T06:34:34Z,http://arxiv.org/abs/1806.05823v1,http://arxiv.org/pdf/1806.05823v1,"stat.ML, cs.LG, math.OC"
Optimizing deep video representation to match brain activity,"Hugo Richard, Ana Pinho, Bertrand Thirion, Guillaume Charpiat",The comparison of observed brain activity with the statistics generated by artificial intelligence systems is useful to probe brain functional organization under ecological conditions. Here we study fMRI activity in ten subjects watching color natural movies and compute deep representations of these movies with an architecture that relies on optical flow and image content. The association of activity in visual areas with the different layers of the deep architecture displays complexity-related contrasts across visual areas and reveals a striking foveal/peripheral dichotomy.,2018-09-07T12:37:50Z,2018-09-07T12:37:50Z,http://arxiv.org/abs/1809.02440v1,http://arxiv.org/pdf/1809.02440v1,"cs.NE, cs.CV, cs.LG, q-bio.NC"
Internet of NanoThings: Concepts and Applications,"Ebtesam Almazrouei, Raed M. Shubair, Fabrice Saffre","This chapter focuses on Internet of Things from the nanoscale point of view. The chapter starts with section 1 which provides an introduction of nanothings and nanotechnologies. The nanoscale communication paradigms and the different approaches are discussed for nanodevices development. Nanodevice characteristics are discussed and the architecture of wireless nanodevices are outlined. Section 2 describes Internet of NanoThing(IoNT), its network architecture, and the challenges of nanoscale communication which is essential for enabling IoNT. Section 3 gives some practical applications of IoNT. The internet of Bio-NanoThing (IoBNT) and relevant biomedical applications are discussed. Other Applications such as military, industrial, and environmental applications are also outlined.",2018-09-21T02:20:18Z,2018-09-21T02:20:18Z,http://arxiv.org/abs/1809.08914v1,http://arxiv.org/pdf/1809.08914v1,"cs.ET, eess.SP"
WALL-E: An Efficient Reinforcement Learning Research Framework,"Tianbing Xu, Andrew Zhang, Liang Zhao","There are two halves to RL systems: experience collection time and policy learning time. For a large number of samples in rollouts, experience collection time is the major bottleneck. Thus, it is necessary to speed up the rollout generation time with multi-process architecture support. Our work, dubbed WALL-E, utilizes multiple rollout samplers running in parallel to rapidly generate experience. Due to our parallel samplers, we experience not only faster convergence times, but also higher average reward thresholds. For example, on the MuJoCo HalfCheetah-v2 task, with $N = 10$ parallel sampler processes, we are able to achieve much higher average return than those from using only a single process architecture.",2019-01-18T04:54:15Z,2019-01-28T06:25:18Z,http://arxiv.org/abs/1901.06086v2,http://arxiv.org/pdf/1901.06086v2,"cs.LG, stat.ML"
Optimal Vehicle Dynamics and Powertrain Control for Connected and   Automated Vehicles,"Liuhui Zhao, A M Ishtiaque Mahbub, Andreas A. Malikopoulos","The implementation of connected and automated vehicle technologies enables opportunities for a novel computational framework for real-time control actions aimed at optimizing energy consumption and associated benefits. In this paper, we present a two-level control architecture for a connected and automated plug-in hybrid electric vehicle to optimize simultaneously its speed profile and powertrain efficiency. We evaluate the proposed architecture through simulation in a network of vehicles.",2019-03-07T18:21:30Z,2019-05-24T22:29:26Z,http://arxiv.org/abs/1903.03084v2,http://arxiv.org/pdf/1903.03084v2,"math.OC, cs.SY"
Musical Tempo and Key Estimation using Convolutional Neural Networks   with Directional Filters,"Hendrik Schreiber, Meinard Müller","In this article we explore how the different semantics of spectrograms' time and frequency axes can be exploited for musical tempo and key estimation using Convolutional Neural Networks (CNN). By addressing both tasks with the same network architectures ranging from shallow, domain-specific approaches to deep variants with directional filters, we show that axis-aligned architectures perform similarly well as common VGG-style networks developed for computer vision, while being less vulnerable to confounding factors and requiring fewer model parameters.",2019-03-26T12:43:09Z,2019-03-26T12:43:09Z,http://arxiv.org/abs/1903.10839v1,http://arxiv.org/pdf/1903.10839v1,"cs.SD, cs.LG, eess.AS"
Learning Symmetries of Classical Integrable Systems,"Roberto Bondesan, Austen Lamacraft","The solution of problems in physics is often facilitated by a change of variables. In this work we present neural transformations to learn symmetries of Hamiltonian mechanical systems. Maintaining the Hamiltonian structure requires novel network architectures that parametrize symplectic transformations. We demonstrate the utility of these architectures by learning the structure of integrable models. Our work exemplifies the adaptation of neural transformations to a family constrained by more than the condition of invertibility, which we expect to be a common feature of applications of these methods.",2019-06-11T15:11:37Z,2019-06-11T15:11:37Z,http://arxiv.org/abs/1906.04645v1,http://arxiv.org/pdf/1906.04645v1,"physics.comp-ph, cs.LG"
Implicit Deep Learning,"Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, Alicia Y. Tsai","Implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks. Such rules are based on the solution of a fixed-point equation involving a single vector of hidden features, which is thus only implicitly defined. The implicit framework greatly simplifies the notation of deep learning, and opens up many new possibilities, in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization.",2019-08-17T15:36:37Z,2020-08-06T22:10:43Z,http://arxiv.org/abs/1908.06315v4,http://arxiv.org/pdf/1908.06315v4,"cs.LG, math.OC, stat.ML"
Design space exploration of Ferroelectric FET based Processing-in-Memory   DNN Accelerator,"Insik Yoon, Matthew Jerry, Suman Datta, Arijit Raychowdhury","In this letter, we quantify the impact of device limitations on the classification accuracy of an artificial neural network, where the synaptic weights are implemented in a Ferroelectric FET (FeFET) based in-memory processing architecture. We explore a design-space consisting of the resolution of the analog-to-digital converter, number of bits per FeFET cell, and the neural network depth. We show how the system architecture, training models and overparametrization can address some of the device limitations.",2019-08-12T18:26:06Z,2019-08-12T18:26:06Z,http://arxiv.org/abs/1908.07942v1,http://arxiv.org/pdf/1908.07942v1,"cs.ET, cs.NE, eess.SP"
Practical Risk Measures in Reinforcement Learning,"Dotan Di Castro, Joel Oren, Shie Mannor","Practical application of Reinforcement Learning (RL) often involves risk considerations. We study a generalized approximation scheme for risk measures, based on Monte-Carlo simulations, where the risk measures need not necessarily be \emph{coherent}. We demonstrate that, even in simple problems, measures such as the variance of the reward-to-go do not capture the risk in a satisfactory manner. In addition, we show how a risk measure can be derived from model's realizations. We propose a neural architecture for estimating the risk and suggest the risk critic architecture that can be use to optimize a policy under general risk measures. We conclude our work with experiments that demonstrate the efficacy of our approach.",2019-08-22T13:50:31Z,2019-08-22T13:50:31Z,http://arxiv.org/abs/1908.08379v1,http://arxiv.org/pdf/1908.08379v1,"cs.LG, stat.ML"
Set Flow: A Permutation Invariant Normalizing Flow,"Kashif Rasul, Ingmar Schuster, Roland Vollgraf, Urs Bergmann","We present a generative model that is defined on finite sets of exchangeable, potentially high dimensional, data. As the architecture is an extension of RealNVPs, it inherits all its favorable properties, such as being invertible and allowing for exact log-likelihood evaluation. We show that this architecture is able to learn finite non-i.i.d. set data distributions, learn statistical dependencies between entities of the set and is able to train and sample with variable set sizes in a computationally efficient manner. Experiments on 3D point clouds show state-of-the art likelihoods.",2019-09-06T09:00:24Z,2019-09-06T09:00:24Z,http://arxiv.org/abs/1909.02775v1,http://arxiv.org/pdf/1909.02775v1,"cs.LG, stat.ML"
Deep Prediction of Investor Interest: a Supervised Clustering Approach,"Baptiste Barreau, Laurent Carlier, Damien Challet","We propose a novel deep learning architecture suitable for the prediction of investor interest for a given asset in a given time frame. This architecture performs both investor clustering and modelling at the same time. We first verify its superior performance on a synthetic scenario inspired by real data and then apply it to two real-world databases, a publicly available dataset about the position of investors in Spanish stock market and proprietary data from BNP Paribas Corporate and Institutional Banking.",2019-09-11T18:32:06Z,2021-02-26T13:28:01Z,http://arxiv.org/abs/1909.05289v3,http://arxiv.org/pdf/1909.05289v3,"cs.LG, q-fin.CP, stat.ML"
Hybrid Neural Models For Sequence Modelling: The Best Of Three Worlds,"Marco Dinarelli, Loïc Grobol","We propose a neural architecture with the main characteristics of the most successful neural models of the last years: bidirectional RNNs, encoder-decoder, and the Transformer model. Evaluation on three sequence labelling tasks yields results that are close to the state-of-the-art for all tasks and better than it for some of them, showing the pertinence of this hybrid architecture for this kind of tasks.",2019-09-16T10:19:59Z,2019-09-16T10:19:59Z,http://arxiv.org/abs/1909.07102v1,http://arxiv.org/pdf/1909.07102v1,"cs.LG, stat.ML"
Augmented Reality with Hololens: Experiential Architectures Embedded in   the Real World,"Paul Hockett, Tim Ingleby","Early hands-on experiences with the Microsoft Hololens augmented/mixed reality device are reported and discussed, with a general aim of exploring basic 3D visualization. A range of usage cases are tested, including data visualization and immersive data spaces, in-situ visualization of 3D models and full scale architectural form visualization. Ultimately, the Hololens is found to provide a remarkable tool for moving from traditional visualization of 3D objects on a 2D screen, to fully experiential 3D visualizations embedded in the real world.",2016-10-13T22:32:08Z,2016-10-13T22:32:08Z,http://arxiv.org/abs/1610.04281v1,http://arxiv.org/pdf/1610.04281v1,"cs.GR, physics.data-an"
A Quantum Cellular Automata architecture with nearest neighbor   interactions using one quantum gate type,"D. Ntalaperas, N. Konofaos","We propose an architecture based on Quantum cellular Automata which allows the use of only one type of quantum gates per computational step in order to perform nearest neighbor interactions. The model is built in partial steps, each one of them analyzed using nearest neighbor interactions, starting with single qubit operations and continuing with two qubit ones. The effectiveness of the model is tested and valuated by developing a quantum circuit implementing the Quantum Fourier Transform. The important outcome of this validation was that the operations are performed in a local and controlled manner thus reducing the error rate of each computational step.",2016-10-20T14:21:29Z,2016-10-20T14:21:29Z,http://arxiv.org/abs/1610.06426v1,http://arxiv.org/pdf/1610.06426v1,"cs.ET, physics.comp-ph"
Channel capacity of polar coding with a given polar mismatched   successive cancellation decoder,Mine Alsan,"Ar{\i}kan's polar coding, is by now a well studied technique that allows achieving the symmetric capacity of binary input memoryless channels with low complexity encoding and decoding, provided that the polar decoding architecture is used and the decoding metric is matched to the true channel. In this paper, we analyze communication rates that are achievable when the polar coding/decoding architecture is used with the decoder using an incorrect model of the channel. We define the `polar mismatched capacity' as an analogue of the classical mismatched capacity, give an expression for it, and derive bounds on it.",2016-10-24T06:59:45Z,2018-05-07T04:15:04Z,http://arxiv.org/abs/1610.07297v2,http://arxiv.org/pdf/1610.07297v2,"cs.IT, math.IT"
Benchmarks of ResNet Architecture for Atrial Fibrillation Classification,"Roman Khudorozhkov, Dmitry Podvyaznikov","In this work we apply variations of ResNet architecture to the task of atrial fibrillation classification. Variations differ in number of filter after first convolution, ResNet block layout, number of filters in block convolutions and number of ResNet blocks between downsampling operations. We have found a range of model size in which models with quite different configurations show similar performance. It is likely that overall number of parameters plays dominant role in model performance. However, configuration parameters like layout have values that constantly lead to better results, which allows to suggest that these parameters should be defined and fixed in the first place, while others may be varied in a reasonable range to satisfy any existing constraints.",2018-09-30T15:09:42Z,2018-09-30T15:09:42Z,http://arxiv.org/abs/1810.00396v1,http://arxiv.org/pdf/1810.00396v1,"cs.CV, cs.LG, stat.ML"
Community Detection Across Emerging Quantum Architectures,"Ruslan Shaydulin, Hayato Ushijima-Mwesigwa, Ilya Safro, Susan Mniszewski, Yuri Alexeev","One of the roadmap plans for quantum computers is an integration within HPC ecosystems assigning them a role of accelerators for a variety of computationally hard tasks. However, in the near term, quantum hardware will be in a constant state of change. Heading towards solving real-world problems, we advocate development of portable, architecture-agnostic hybrid quantum-classical frameworks and demonstrate one for the community detection problem evaluated using quantum annealing and gate-based universal quantum computation paradigms.",2018-10-01T15:28:30Z,2018-10-01T15:28:30Z,http://arxiv.org/abs/1810.07765v1,http://arxiv.org/pdf/1810.07765v1,"quant-ph, cs.OH"
Hierarchical Approaches for Reinforcement Learning in Parameterized   Action Space,"Ermo Wei, Drew Wicke, Sean Luke","We explore Deep Reinforcement Learning in a parameterized action space. Specifically, we investigate how to achieve sample-efficient end-to-end training in these tasks. We propose a new compact architecture for the tasks where the parameter policy is conditioned on the output of the discrete action policy. We also propose two new methods based on the state-of-the-art algorithms Trust Region Policy Optimization (TRPO) and Stochastic Value Gradient (SVG) to train such an architecture. We demonstrate that these methods outperform the state of the art method, Parameterized Action DDPG, on test domains.",2018-10-23T04:52:53Z,2018-10-23T04:52:53Z,http://arxiv.org/abs/1810.09656v1,http://arxiv.org/pdf/1810.09656v1,"cs.LG, cs.AI, stat.ML"
A Survey on High-Capacity OFDM-based Passive Optical Networks,"Mohammad Ghanbarisabagh, Gobi Vetharatnam, Elias Giacoumidis, Hossein Rouzegar, Nasreddine Mallouki","The exponential growth of demand for high-speed internet and high bandwidth applications such as interactive entertainment in access networks mandates the requirement for higher optical signal speeds. On the other hand, since cost and energy efficiency should be concurrently preserved in access networks, PONs have emerged as a breakthrough solution. This survey shows that future-proof PONs should be supported by an adaptively modulated WDM-OFDM architecture to maximize signal capacity and transmission-reach, while the employment of reflective semiconductor optical amplification and re-modulation can potentially prevent the employment of an additional light source.",2018-11-09T14:49:09Z,2018-11-09T14:49:09Z,http://arxiv.org/abs/1812.04692v1,http://arxiv.org/pdf/1812.04692v1,"eess.SP, cs.NI"
Lattice QCD on upcoming Arm architectures,"Nils Meyer, Dirk Pleiter, Stefan Solbrig, Tilo Wettig","Recently Arm introduced a new instruction set called Scalable Vector Extension (SVE), which supports vector lengths up to 2048 bits. While SVE hardware will not be generally available until about 2021, we believe that future SVE-based architectures will have great potential for Lattice QCD. In this contribution we discuss key aspects of SVE and describe how we implemented SVE in the Grid Lattice QCD framework.",2019-04-08T10:09:22Z,2019-04-08T10:09:22Z,http://arxiv.org/abs/1904.03927v1,http://arxiv.org/pdf/1904.03927v1,"hep-lat, physics.comp-ph"
Disentangling Options with Hellinger Distance Regularizer,"Minsung Hyun, Junyoung Choi, Nojun Kwak","In reinforcement learning (RL), temporal abstraction still remains as an important and unsolved problem. The options framework provided clues to temporal abstraction in the RL, and the option-critic architecture elegantly solved the two problems of finding options and learning RL agents in an end-to-end manner. However, it is necessary to examine whether the options learned through this method play a mutually exclusive role. In this paper, we propose a Hellinger distance regularizer, a method for disentangling options. In addition, we will shed light on various indicators from the statistical point of view to compare with the options learned through the existing option-critic architecture.",2019-04-15T07:43:17Z,2019-04-15T07:43:17Z,http://arxiv.org/abs/1904.06887v1,http://arxiv.org/pdf/1904.06887v1,"cs.LG, stat.ML"
Multigrid for Wilson Clover Fermions in Grid,"Daniel Richtmann, Peter A. Boyle, Tilo Wettig","With the ever-growing number of computing architectures, performance portability is an important aspect of (Lattice QCD) software. The Grid library provides a good framework for writing such code, as it thoroughly separates hardware-specific code from algorithmic functionality and already supports many modern architectures. We describe the implementation of a multigrid solver for Wilson clover fermions in Grid by the RQCD group. We present the features included in our implementation, discuss initial optimization efforts, and compare the performance with another multigrid implementation.",2019-04-18T10:40:34Z,2019-04-18T10:40:34Z,http://arxiv.org/abs/1904.08678v1,http://arxiv.org/pdf/1904.08678v1,"hep-lat, physics.comp-ph"
N2Sky - Neural Networks as Services in the Clouds,"Erich Schikuta, Erwin Mann","We present the N2Sky system, which provides a framework for the exchange of neural network specific knowledge, as neural network paradigms and objects, by a virtual organization environment. It follows the sky computing paradigm delivering ample resources by the usage of federated Clouds. N2Sky is a novel Cloud-based neural network simulation environment, which follows a pure service oriented approach. The system implements a transparent environment aiming to enable both novice and experienced users to do neural network research easily and comfortably. N2Sky is built using the RAVO reference architecture of virtual organizations which allows itself naturally integrating into the Cloud service stack (SaaS, PaaS, and IaaS) of service oriented architectures.",2014-01-10T21:09:36Z,2014-01-10T21:09:36Z,http://arxiv.org/abs/1401.2468v1,http://arxiv.org/pdf/1401.2468v1,"cs.NE, H.3.5; I.2"
VFNet: A Convolutional Architecture for Accent Classification,"Asad Ahmed, Pratham Tangri, Anirban Panda, Dhruv Ramani, Samarjit Karmakar","Understanding accent is an issue which can derail any human-machine interaction. Accent classification makes this task easier by identifying the accent being spoken by a person so that the correct words being spoken can be identified by further processing, since same noises can mean entirely different words in different accents of the same language. In this paper, we present VFNet (Variable Filter Net), a convolutional neural network (CNN) based architecture which captures a hierarchy of features to beat the previous benchmarks of accent classification, through a novel and elegant technique of applying variable filter sizes along the frequency band of the audio utterances.",2019-10-15T13:04:45Z,2019-10-15T13:04:45Z,http://arxiv.org/abs/1910.06697v1,http://arxiv.org/pdf/1910.06697v1,"cs.SD, cs.LG, eess.AS"
Mobile Edge Computing in Unmanned Aerial Vehicle Networks,"Fuhui Zhou, Rose Qingyang Hu, Zan Li, Yuhao Wang","Unmanned aerial vehicle (UAV)-enabled communication networks are promising in the fifth and beyond wireless communication systems. In this paper, we shed light on three UAV-enabled mobile edge computing (MEC) architectures. Those architectures have been receiving ever increasing research attention for improving computation performance and decreasing execution latency by integrating UAV into MEC networks. We present a comprehensive survey for the state-of-the-art research in this domain. Important implementation issues are clarified. Moreover, in order to provide an enlightening guidance for future research directions, key challenges and open issues are discussed.",2019-10-12T05:32:30Z,2019-10-12T05:32:30Z,http://arxiv.org/abs/1910.10523v1,http://arxiv.org/pdf/1910.10523v1,"eess.SP, cs.GT"
Towards Sustainable Architecture: 3D Convolutional Neural Networks for   Computational Fluid Dynamics Simulation and Reverse DesignWorkflow,"Josef Musil, Jakub Knir, Athanasios Vitsas, Irene Gallou",We present a general and flexible approximation model for near real-time prediction of steady turbulent flow in a 3D domain based on residual Convolutional Neural Networks (CNNs). This approach can provide immediate feedback for real-time iterations at the early stage of architectural design. This work-flow is then reversed and offers a designer a tool that generates building volumes based on target wind flow.,2019-11-25T17:48:22Z,2019-11-25T17:48:22Z,http://arxiv.org/abs/1912.02125v1,http://arxiv.org/pdf/1912.02125v1,"cs.GR, eess.IV, I.2.10, I.4.0, J.5, J.6, I.2.10; I.4.0; J.5; J.6"
Machine Translation Evaluation Meets Community Question Answering,"Francisco Guzmán, Lluís Màrquez, Preslav Nakov","We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering. In particular, we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efficiently models complex non-linear interactions. The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture.",2019-12-06T06:35:21Z,2019-12-06T06:35:21Z,http://arxiv.org/abs/1912.02998v1,http://arxiv.org/pdf/1912.02998v1,"cs.CL, cs.IR, cs.LG, 68T50, I.2.7"
"Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS","Petro Liashchynskyi, Pavlo Liashchynskyi","In this paper, we compare the three most popular algorithms for hyperparameter optimization (Grid Search, Random Search, and Genetic Algorithm) and attempt to use them for neural architecture search (NAS). We use these algorithms for building a convolutional neural network (search architecture). Experimental results on CIFAR-10 dataset further demonstrate the performance difference between compared algorithms. The comparison results are based on the execution time of the above algorithms and accuracy of the proposed models.",2019-12-12T16:07:20Z,2019-12-12T16:07:20Z,http://arxiv.org/abs/1912.06059v1,http://arxiv.org/pdf/1912.06059v1,"cs.LG, cs.NE, stat.ML"
A Novel Higher-order Weisfeiler-Lehman Graph Convolution,"Clemens Damke, Vitalik Melnikov, Eyke Hüllermeier","Current GNN architectures use a vertex neighborhood aggregation scheme, which limits their discriminative power to that of the 1-dimensional Weisfeiler-Lehman (WL) graph isomorphism test. Here, we propose a novel graph convolution operator that is based on the 2-dimensional WL test. We formally show that the resulting 2-WL-GNN architecture is more discriminative than existing GNN approaches. This theoretical result is complemented by experimental studies using synthetic and real data. On multiple common graph classification benchmarks, we demonstrate that the proposed model is competitive with state-of-the-art graph kernels and GNNs.",2020-07-01T09:32:01Z,2020-09-21T11:33:32Z,http://arxiv.org/abs/2007.00346v2,http://arxiv.org/pdf/2007.00346v2,"cs.LG, stat.ML"
Using Capsule Neural Network to predict Tuberculosis in lens-free   microscopic images,"Dennis Núñez-Fernández, Lamberto Ballan, Gabriel Jiménez-Avalos, Jorge Coronel, Mirko Zimic","Tuberculosis, caused by a bacteria called Mycobacterium tuberculosis, is one of the most serious public health problems worldwide. This work seeks to facilitate and automate the prediction of tuberculosis by the MODS method and using lens-free microscopy, which is easy to use by untrained personnel. We employ the CapsNet architecture in our collected dataset and show that it has a better accuracy than traditional CNN architectures.",2020-07-05T22:18:13Z,2020-07-05T22:18:13Z,http://arxiv.org/abs/2007.02457v1,http://arxiv.org/pdf/2007.02457v1,"eess.IV, cs.CV"
Quantum Secured Internet Transport,"Bernardo Huberman, Bob Lund, Jing Wang","Quantum computing represents an emerging threat to the public key infrastructure underlying transport layer security (TLS) widely used in the Internet. This paper describes how QKD symmetric keys can be used with TLS to provide quantum computing resistant security for existing Internet applications. We also implement and test a general hybrid key delivery architecture with QKD over long distance fibers between secure sites, and wireless key distribution over short distance within each site Finally we show how this same capability can be extended to a TLS cipher scheme with perfect security.",2020-07-10T17:57:06Z,2020-07-10T17:57:06Z,http://arxiv.org/abs/2007.05522v1,http://arxiv.org/pdf/2007.05522v1,"cs.CR, cs.CY, cs.NI, quant-ph"
A Low Complexity Algorithm and Architecture for Systematic Encoding of   Hermitian Codes,"Rachit Agarwal, Ralf Koetter, Emanuel Popovici","We present an algorithm for systematic encoding of Hermitian codes. For a Hermitian code defined over GF(q^2), the proposed algorithm achieves a run time complexity of O(q^2) and is suitable for VLSI implementation. The encoder architecture uses as main blocks q varying-rate Reed-Solomon encoders and achieves a space complexity of O(q^2) in terms of finite field multipliers and memory elements.",2007-04-04T15:06:14Z,2007-04-05T11:47:23Z,http://arxiv.org/abs/0704.0590v2,http://arxiv.org/pdf/0704.0590v2,"cs.IT, math.IT"
MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural   Networks,Minmin Chen,"We introduce MinimalRNN, a new recurrent neural network architecture that achieves comparable performance as the popular gated RNNs with a simplified structure. It employs minimal updates within RNN, which not only leads to efficient learning and testing but more importantly better interpretability and trainability. We demonstrate that by endorsing the more restrictive update rule, MinimalRNN learns disentangled RNN states. We further examine the learning dynamics of different RNN structures using input-output Jacobians, and show that MinimalRNN is able to capture longer range dependencies than existing RNN architectures.",2017-11-18T01:42:04Z,2018-06-20T02:19:13Z,http://arxiv.org/abs/1711.06788v2,http://arxiv.org/pdf/1711.06788v2,"stat.ML, cs.LG"
Current Mode Neuron for the Memristor based synapse,"Harshit Roy, Mrigank Sharad","Due to many limitations of Von Neumann architecture such as speed, memory bandwidth, efficiency of global interconnects and increase in the application of artificial neural network, researchers have been pushed to look into alternative architectures such as Neuromorphic computing system. Memristors (memristive crossbar memory RCM) are used as synapses due to its high packing density and energy efficiency and CMOS blocks as neurons. The increase in the terminal resistance of the RCM can degrade its energy efficiency and bandwidth. A more energy efficient current mode neuron has been proposed in this paper which can operate at lower voltages as compared to conventional voltage mode neuron circuit.",2019-05-13T22:30:36Z,2019-05-13T22:30:36Z,http://arxiv.org/abs/1905.05307v1,http://arxiv.org/pdf/1905.05307v1,"cs.ET, eess.SP"
Population-based Global Optimisation Methods for Learning Long-term   Dependencies with RNNs,"Bryan Lim, Stefan Zohren, Stephen Roberts","Despite recent innovations in network architectures and loss functions, training RNNs to learn long-term dependencies remains difficult due to challenges with gradient-based optimisation methods. Inspired by the success of Deep Neuroevolution in reinforcement learning (Such et al. 2017), we explore the use of gradient-free population-based global optimisation (PBO) techniques -- training RNNs to capture long-term dependencies in time-series data. Testing evolution strategies (ES) and particle swarm optimisation (PSO) on an application in volatility forecasting, we demonstrate that PBO methods lead to performance improvements in general, with ES exhibiting the most consistent results across a variety of architectures.",2019-05-23T14:55:28Z,2019-05-23T14:55:28Z,http://arxiv.org/abs/1905.09691v1,http://arxiv.org/pdf/1905.09691v1,"stat.ML, cs.LG"
Locality-Promoting Representation Learning,Johannes Schneider,"This work investigates fundamental questions related to learning features in convolutional neural networks (CNN). Empirical findings across multiple architectures such as VGG, ResNet, Inception, DenseNet and MobileNet indicate that weights near the center of a filter are larger than weights on the outside. Current regularization schemes violate this principle. Thus, we introduce Locality-promoting Regularization (LOCO-Reg), which yields accuracy gains across multiple architectures and datasets. We also show theoretically that the empirical finding is a consequence of maximizing feature cohesion under the assumption of spatial locality.",2019-05-25T19:19:14Z,2021-03-29T16:09:40Z,http://arxiv.org/abs/1905.10661v3,http://arxiv.org/pdf/1905.10661v3,"cs.LG, stat.ML"
Comparison of Cloud-Based Ion Trap and Superconducting Quantum Computer   Architectures,"S. Blinov, B. Wu, C. Monroe","Quantum computing represents a radical departure from conventional approaches to information processing, offering the potential for solving problems that can never be approached classically. While large scale quantum computer hardware is still in development, several quantum computing systems have recently become available as commercial cloud services. We compare the performance of these systems on several simple quantum circuits and algorithms, and examine component performance in the context of each system's architecture.",2021-01-31T04:00:48Z,2021-01-31T04:00:48Z,http://arxiv.org/abs/2102.00371v1,http://arxiv.org/pdf/2102.00371v1,"quant-ph, cs.ET"
Deep Hedging under Rough Volatility,"Blanka Horvath, Josef Teichmann, Zan Zuric","We investigate the performance of the Deep Hedging framework under training paths beyond the (finite dimensional) Markovian setup. In particular we analyse the hedging performance of the original architecture under rough volatility models with view to existing theoretical results for those. Furthermore, we suggest parsimonious but suitable network architectures capable of capturing the non-Markoviantity of time-series. Secondly, we analyse the hedging behaviour in these models in terms of P\&L distributions and draw comparisons to jump diffusion models if the the rebalancing frequency is realistically small.",2021-02-03T09:27:16Z,2021-02-03T09:27:16Z,http://arxiv.org/abs/2102.01962v1,http://arxiv.org/pdf/2102.01962v1,"q-fin.CP, cs.CE, 91-08"
Mimetic Neural Networks: A unified framework for Protein Design and   Folding,"Moshe Eliasof, Tue Boesen, Eldad Haber, Chen Keasar, Eran Treister","Recent advancements in machine learning techniques for protein folding motivate better results in its inverse problem -- protein design. In this work we introduce a new graph mimetic neural network, MimNet, and show that it is possible to build a reversible architecture that solves the structure and design problems in tandem, allowing to improve protein design when the structure is better estimated. We use the ProteinNet data set and show that the state of the art results in protein design can be improved, given recent architectures for protein folding.",2021-02-07T18:53:52Z,2021-02-07T18:53:52Z,http://arxiv.org/abs/2102.03881v1,http://arxiv.org/pdf/2102.03881v1,"q-bio.BM, cs.CV, cs.LG"
Learnable MFCCs for Speaker Verification,"Xuechen Liu, Md Sahidullah, Tomi Kinnunen","We propose a learnable mel-frequency cepstral coefficient (MFCC) frontend architecture for deep neural network (DNN) based automatic speaker verification. Our architecture retains the simplicity and interpretability of MFCC-based features while allowing the model to be adapted to data flexibly. In practice, we formulate data-driven versions of the four linear transforms of a standard MFCC extractor -- windowing, discrete Fourier transform (DFT), mel filterbank and discrete cosine transform (DCT). Results reported reach up to 6.7\% (VoxCeleb1) and 9.7\% (SITW) relative improvement in term of equal error rate (EER) from static MFCCs, without additional tuning effort.",2021-02-20T12:16:35Z,2021-02-20T12:16:35Z,http://arxiv.org/abs/2102.10322v1,http://arxiv.org/pdf/2102.10322v1,"cs.SD, cs.LG, eess.AS"
Hybrid CMOS-MQCA Logic Architectures using Multi-Layer Spintronic   Devices,"Jayita Das, Syed M. Alam, Srinath Rajaram, Sanjukta Bhanja",We present a novel hybrid CMOS-MQCA architecture using multi-layer Spintronic devices as computing elements. A feasibility study is presented with 22nm CMOS where new approaches for spin transfer torque induced clocking and read-out scheme for variability-tolerance are introduced. A first-of-its-kind Spintronic device model enables circuit simulation using existing CAD infrastructure. Approximately 70% reduction in energy consumption is observed when compared against conventional field-induced clocking scheme.,2011-02-20T00:27:17Z,2012-03-17T05:09:18Z,http://arxiv.org/abs/1102.4034v2,http://arxiv.org/pdf/1102.4034v2,"cs.ET, cond-mat.mes-hall"
MIMO Architectures for Efficient Communication in Space,Richard J. Barton,"The characteristics of a space-to-space multiple-input, multiple-output (MIMO) communication channel that distinguish it from terrestrial MIMO communication channels are summarized and discussed primarily from an information-theoretic viewpoint. The implications of these characteristics for the design and application of future space-based communication systems are also discussed, and it is shown that in general, either energy-efficient or spectrally-efficient communication in space can only be achieved using a distributed MIMO architecture.",2016-01-14T16:57:56Z,2016-01-20T20:42:41Z,http://arxiv.org/abs/1601.03664v3,http://arxiv.org/pdf/1601.03664v3,"cs.IT, math.IT"
Parametric channel estimation for massive MIMO,"Luc Le Magoarou, Stéphane Paquelet","Channel state information is crucial to achieving the capacity of multi-antenna (MIMO) wireless communication systems. It requires estimating the channel matrix. This estimation task is studied, considering a sparse channel model particularly suited to millimeter wave propagation, as well as a general measurement model taking into account hybrid architectures. The contribution is twofold. First, the Cram{\'e}r-Rao bound in this context is derived. Second, interpretation of the Fisher Information Matrix structure allows to assess the role of system parameters, as well as to propose asymptotically optimal and computationally efficient estimation algorithms.",2017-10-23T11:45:41Z,2018-04-05T06:52:35Z,http://arxiv.org/abs/1710.08214v3,http://arxiv.org/pdf/1710.08214v3,"cs.IT, cs.NI, math.IT"
Online Learning with Gated Linear Networks,"Joel Veness, Tor Lattimore, Avishkar Bhoopchand, Agnieszka Grabska-Barwinska, Christopher Mattern, Peter Toth","This paper describes a family of probabilistic architectures designed for online learning under the logarithmic loss. Rather than relying on non-linear transfer functions, our method gains representational power by the use of data conditioning. We state under general conditions a learnable capacity theorem that shows this approach can in principle learn any bounded Borel-measurable function on a compact subset of euclidean space; the result is stronger than many universality results for connectionist architectures because we provide both the model and the learning procedure for which convergence is guaranteed.",2017-12-05T20:07:59Z,2017-12-05T20:07:59Z,http://arxiv.org/abs/1712.01897v1,http://arxiv.org/pdf/1712.01897v1,"cs.LG, cs.IT, math.IT"
Music Genre Classification with Paralleling Recurrent Convolutional   Neural Network,"Lin Feng, Shenlan Liu, Jianing Yao","Deep learning has been demonstrated its effectiveness and efficiency in music genre classification. However, the existing achievements still have several shortcomings which impair the performance of this classification task. In this paper, we propose a hybrid architecture which consists of the paralleling CNN and Bi-RNN blocks. They focus on spatial features and temporal frame orders extraction respectively. Then the two outputs are fused into one powerful representation of musical signals and fed into softmax function for classification. The paralleling network guarantees the extracting features robust enough to represent music. Moreover, the experiments prove our proposed architecture improve the music genre classification performance and the additional Bi-RNN block is a supplement for CNNs.",2017-12-22T09:49:26Z,2017-12-22T09:49:26Z,http://arxiv.org/abs/1712.08370v1,http://arxiv.org/pdf/1712.08370v1,"cs.SD, cs.IR, eess.AS"
Thermally tunable hybrid photonic architecture for nonlinear optical   circuits,"Marina Radulaski, Ranojoy Bose, Tho Tran, Thomas Van Vaerenbergh, David Kielpinski, Raymond G. Beausoleil","We develop a thermally tunable hybrid photonic platform comprising gallium arsenide (GaAs) photonic crystal cavities, silicon nitride (SiN$_x$) grating couplers and waveguides, and chromium (Cr) microheaters on an integrated photonic chip. The GaAs photonic crystal cavities are evanescently connected to a common bus waveguide, separating the computation and communication layers. The microheaters are designed to continuously and reversibly tune distant photonic crystal cavities to a common resonance. This architecture can be implemented in a coherent optical network for dedicated optical computing and machine learning.",2018-03-01T00:58:22Z,2018-09-19T22:31:36Z,http://arxiv.org/abs/1803.03591v2,http://arxiv.org/pdf/1803.03591v2,"physics.app-ph, cond-mat.mtrl-sci, physics.optics"
"R3Net: Random Weights, Rectifier Linear Units and Robustness for   Artificial Neural Network","Arun Venkitaraman, Alireza M. Javid, Saikat Chatterjee","We consider a neural network architecture with randomized features, a sign-splitter, followed by rectified linear units (ReLU). We prove that our architecture exhibits robustness to the input perturbation: the output feature of the neural network exhibits a Lipschitz continuity in terms of the input perturbation. We further show that the network output exhibits a discrimination ability that inputs that are not arbitrarily close generate output vectors which maintain distance between each other obeying a certain lower bound. This ensures that two different inputs remain discriminable while contracting the distance in the output feature space.",2018-03-12T11:04:17Z,2018-03-12T11:04:17Z,http://arxiv.org/abs/1803.04186v1,http://arxiv.org/pdf/1803.04186v1,"stat.ML, cs.LG"
Mm-wave specific challenges in designing 5G transceiver architectures   and air-interfaces,"Mythri Hunukumbure, Jian Luo, Mario Castaneda, Raffaele DErrico, Per Zetterberg, Ali A. Zaidi, Jaakko Vihriala, Domenico Giustiniano",The mm-wave spectrum will be of significant importance to 5G mobile systems. There are multiple challenges in designing transceiver architectures and air interfaces in this spectrum. This paper is an attempt to explain some of these challenges and their interactions as means of enabling robust system design in near future.,2018-03-15T09:42:17Z,2018-03-15T09:42:17Z,http://arxiv.org/abs/1803.05661v1,http://arxiv.org/pdf/1803.05661v1,"cs.IT, eess.SP, math.IT"
Energy Efficient and Delay Aware Vehicular Edge Cloud,"Amal A. Alahmadi, T. E. H. El-Gorashi, Jaafar M. H. Elmirghani","Vehicular Edge Clouds (VECs) is a new distributed processing paradigm that exploits the revolution in the processing capabilities of vehicles to offer energy efficient services and improved QoS. In this paper we tackle the problem of processing allocation in a cloud-fog-VEC architecture by developing a joint optimization Mixed Integer Linear Programming (MILP) model to minimize power consumption, propagation delay, and queuing delay. The results show that while VEC processing can reduce the power consumption and propagation delay, VEC processing can increase the queuing delay because of the low data rate connectivity between the vehicles and the data source nodes.",2020-04-15T15:55:54Z,2020-04-15T15:55:54Z,http://arxiv.org/abs/2004.07170v1,http://arxiv.org/pdf/2004.07170v1,"cs.NI, eess.SP"
Benchmarking Physical Performance of Neural Inference Circuits,"Dmitri E. Nikonov, Ian A. Young","Numerous neural network circuits and architectures are presently under active research for application to artificial intelligence and machine learning. Their physical performance metrics (area, time, energy) are estimated. Various types of neural networks (artificial, cellular, spiking, and oscillator) are implemented with multiple CMOS and beyond-CMOS (spintronic, ferroelectric, resistive memory) devices. A consistent and transparent methodology is proposed and used to benchmark this comprehensive set of options across several application cases. Promising architecture/device combinations are identified.",2019-07-12T13:57:16Z,2019-07-12T13:57:16Z,http://arxiv.org/abs/1907.05748v1,http://arxiv.org/pdf/1907.05748v1,"cs.ET, physics.app-ph"
DAR-Net: Dynamic Aggregation Network for Semantic Scene Segmentation,"Zongyue Zhao, Min Liu, Karthik Ramani","Traditional grid/neighbor-based static pooling has become a constraint for point cloud geometry analysis. In this paper, we propose DAR-Net, a novel network architecture that focuses on dynamic feature aggregation. The central idea of DAR-Net is generating a self-adaptive pooling skeleton that considers both scene complexity and local geometry features. Providing variable semi-local receptive fields and weights, the skeleton serves as a bridge that connect local convolutional feature extractors and a global recurrent feature integrator. Experimental results on indoor scene datasets show advantages of the proposed approach compared to state-of-the-art architectures that adopt static pooling methods.",2019-07-28T06:23:19Z,2019-12-26T03:13:59Z,http://arxiv.org/abs/1907.12022v2,http://arxiv.org/pdf/1907.12022v2,"cs.CV, cs.LG, I.2.10"
An empirical study of the relation between network architecture and   complexity,"Emir Konuk, Kevin Smith","In this preregistration submission, we propose an empirical study of how networks handle changes in complexity of the data. We investigate the effect of network capacity on generalization performance in the face of increasing data complexity. For this, we measure the generalization error for an image classification task where the number of classes steadily increases. We compare a number of modern architectures at different scales in this setting. The methodology, setup, and hypotheses described in this proposal were evaluated by peer review before experiments were conducted.",2019-11-11T07:45:01Z,2019-11-11T07:45:01Z,http://arxiv.org/abs/1911.04120v1,http://arxiv.org/pdf/1911.04120v1,"cs.LG, stat.ML"
A Neural Architecture for Detecting Confusion in Eye-tracking Data,"Shane Sims, Cristina Conati","Encouraged by the success of deep learning in a variety of domains, we investigate a novel application of its methods on the effectiveness of detecting user confusion in eye-tracking data. We introduce an architecture that uses RNN and CNN sub-models in parallel to take advantage of the temporal and visuospatial aspects of our data. Experiments with a dataset of user interactions with the ValueChart visualization tool show that our model outperforms an existing model based on Random Forests resulting in a 22% improvement in combined sensitivity & specificity.",2020-03-13T18:20:39Z,2020-03-13T18:20:39Z,http://arxiv.org/abs/2003.06434v1,http://arxiv.org/pdf/2003.06434v1,"cs.CV, cs.LG, eess.IV"
A lower bound for the ELBO of the Bernoulli Variational Autoencoder,"Robert Sicks, Ralf Korn, Stefanie Schwaar","We consider a variational autoencoder (VAE) for binary data. Our main innovations are an interpretable lower bound for its training objective, a modified initialization and architecture of such a VAE that leads to faster training, and a decision support for finding the appropriate dimension of the latent space via using a PCA. Numerical examples illustrate our theoretical result and the performance of the new architecture.",2020-03-26T10:59:53Z,2020-03-26T10:59:53Z,http://arxiv.org/abs/2003.11830v1,http://arxiv.org/pdf/2003.11830v1,"cs.LG, math.ST, stat.ML, stat.TH"
Integrated Structured Light Architectures,"Randy Lemons, Wei Liu, Josef C. Frisch, Alan Fry, Joseph Robinson, Steve Smith, Sergio Carbajo","The structural versatility of light underpins an outstanding collection of optical phenomena where both geometrical and topological states of light can dictate how matter will respond or display. Light possesses multiple degrees of freedom such as amplitude, and linear, spin angular, and orbital angular momenta, but the ability to adaptively engineer the spatio-temporal distribution of all these characteristics is primarily curtailed by technologies used to impose any desired structure to light. We describe a foundational demonstration that examines a laser architecture offering integrated spatio-temporal field control and programmability, thereby presenting unique opportunities for generating light by design to exploit its topology.",2020-03-31T17:47:32Z,2020-03-31T17:47:32Z,http://arxiv.org/abs/2003.14400v1,http://arxiv.org/pdf/2003.14400v1,"physics.optics, physics.app-ph, quant-ph"
Cross-stakeholder service orchestration for B5G through capability   provisioning,"Vilho Raisanen, Mohammed Elbamby, Dmitry Petrov","Cross-stakeholder service orchestration is a generalization of 5G network slices which has potential to increase business agility in Beyond 5G (B5G). An architectural framework is proposed which enables domain operators to expose their functionalities towards E2E services as capabilities. Capability orchestration is proposed as a mechanism for exposure. The use of intent-based management for communicating domain owner's business goals to capability orchestration is analyzed. The combination of business goal input and capability orchestration provides a basis for agile monetization of domain resources for domain owners, and a building block for rich end-to-end B5G services.",2020-08-17T08:57:32Z,2020-08-17T08:57:32Z,http://arxiv.org/abs/2008.07162v1,http://arxiv.org/pdf/2008.07162v1,"cs.NI, C.2.3"
SOPI design and analysis for LDN,Michael Luby,"Liquid Data Networking (LDN) is an ICN architecture that is designed to enable the benefits of erasure-code enabled object delivery. A primary contribution of LDN is the introduction of SOPIs, which enables client s to concurrently download encoded data for the same object from multiple edge nodes, optimizes caching efficiency, and enables seamless mobility. This paper provides an enhanced design and analysis of SOPI s.",2020-08-31T00:16:20Z,2020-09-05T00:04:23Z,http://arxiv.org/abs/2008.13300v2,http://arxiv.org/pdf/2008.13300v2,"cs.NI, 94, C.2.1"
An application of Answer Set Programming in Distributed Architectures:   ASP Microservices,"Stefania Costantini, Lorenzo De Lauretis","We propose an approach to the definition of microservices with an Answer Set Programming (ASP) `core', where microservices are a successful abstraction for designing distributed applications as suites of independently deployable interacting components. Such ASP-based components might be employed in distributed architectures related to Cloud Computing or to the Internet of Things (IoT).",2020-09-22T00:50:46Z,2020-09-22T00:50:46Z,http://arxiv.org/abs/2009.10250v1,http://arxiv.org/pdf/2009.10250v1,"cs.AI, cs.LO, F.4.1; I.2.3; I.2.4; I.2.11"
Using Neural Architecture Search for Improving Software Flaw Detection   in Multimodal Deep Learning Models,"Alexis Cooper, Xin Zhou, Scott Heidbrink, Daniel M. Dunlavy","Software flaw detection using multimodal deep learning models has been demonstrated as a very competitive approach on benchmark problems. In this work, we demonstrate that even better performance can be achieved using neural architecture search (NAS) combined with multimodal learning models. We adapt a NAS framework aimed at investigating image classification to the problem of software flaw detection and demonstrate improved results on the Juliet Test Suite, a popular benchmarking data set for measuring performance of machine learning models in this problem domain.",2020-09-22T15:59:21Z,2020-09-22T15:59:21Z,http://arxiv.org/abs/2009.10644v1,http://arxiv.org/pdf/2009.10644v1,"stat.ML, cs.AI, cs.CR, cs.LG"
"Deep learning models for predictive maintenance: a survey, comparison,   challenges and prospect","Oscar Serradilla, Ekhi Zugasti, Urko Zurutuza","Given the growing amount of industrial data spaces worldwide, deep learning solutions have become popular for predictive maintenance, which monitor assets to optimise maintenance tasks. Choosing the most suitable architecture for each use-case is complex given the number of examples found in literature. This work aims at facilitating this task by reviewing state-of-the-art deep learning architectures, and how they integrate with predictive maintenance stages to meet industrial companies' requirements (i.e. anomaly detection, root cause analysis, remaining useful life estimation). They are categorised and compared in industrial applications, explaining how to fill their gaps. Finally, open challenges and future research paths are presented.",2020-10-07T06:28:01Z,2020-10-07T06:28:01Z,http://arxiv.org/abs/2010.03207v1,http://arxiv.org/pdf/2010.03207v1,"cs.LG, I.5; J.2; A.1"
C for a tiny system,"Philipp Klaus Krause, Nicolas Lesser","We have implemented support for Padauk microcontrollers, tiny 8-Bit devices with 60 B to 256 B of RAM, in the Small Device C Compiler (SDCC), showing that the use of (mostly) standard C to program such minimal devices is feasible. We report on our experience and on the difficulties in supporting the hardware multithreading present on some of these devices. To make the devices a better target for C, we propose various enhancements of the architecture, and empirically evaluated their impact on code size.",2020-10-09T15:25:33Z,2024-11-02T10:46:42Z,http://arxiv.org/abs/2010.04633v2,http://arxiv.org/pdf/2010.04633v2,"cs.PL, cs.AR, D.3; C.1"
Stable ResNet,"Soufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, Judith Rousseau","Deep ResNet architectures have achieved state of the art performance on many tasks. While they solve the problem of gradient vanishing, they might suffer from gradient exploding as the depth becomes large (Yang et al. 2017). Moreover, recent results have shown that ResNet might lose expressivity as the depth goes to infinity (Yang et al. 2017, Hayou et al. 2019). To resolve these issues, we introduce a new class of ResNet architectures, called Stable ResNet, that have the property of stabilizing the gradient while ensuring expressivity in the infinite depth limit.",2020-10-24T10:27:24Z,2021-03-18T17:27:53Z,http://arxiv.org/abs/2010.12859v2,http://arxiv.org/pdf/2010.12859v2,"cs.LG, stat.ML"
Generative Tomography Reconstruction,"Matteo Ronchetti, Davide Bacciu","We propose an end-to-end differentiable architecture for tomography reconstruction that directly maps a noisy sinogram into a denoised reconstruction. Compared to existing approaches our end-to-end architecture produces more accurate reconstructions while using less parameters and time. We also propose a generative model that, given a noisy sinogram, can sample realistic reconstructions. This generative model can be used as prior inside an iterative process that, by taking into consideration the physical model, can reduce artifacts and errors in the reconstructions.",2020-10-26T18:22:37Z,2020-11-25T22:05:56Z,http://arxiv.org/abs/2010.14933v2,http://arxiv.org/pdf/2010.14933v2,"eess.IV, cs.CV, cs.LG, cs.NA, math.NA"
Low PAPR waveform design for OFDM SYSTEM based on Convolutional   Auto-Encoder,"Yara Huleihel, Eilam Ben-Dror, Haim H. Permuter","This paper introduces the architecture of a convolutional autoencoder (CAE) for the task of peak-to-average power ratio (PAPR) reduction and waveform design, for orthogonal frequency division multiplexing (OFDM) systems. The proposed architecture integrates a PAPR reduction block and a non-linear high power amplifier (HPA) model. We apply gradual loss learning for multi-objective optimization. We analyze the models performance by examining the bit error rate (BER), the PAPR and the spectral response, and comparing them with common PAPR reduction algorithms.",2020-11-12T12:44:30Z,2020-11-12T12:44:30Z,http://arxiv.org/abs/2011.06349v1,http://arxiv.org/pdf/2011.06349v1,"eess.SY, cs.LG, cs.SY"
High-Throughput Parallel Viterbi Decoder on GPU Tensor Cores,"Alireza Mohammadidoost, Matin Hashemi","Many research works have been performed on implementation of Vitrerbi decoding algorithm on GPU instead of FPGA because this platform provides considerable flexibility in addition to great performance. Recently, the recently-introduced Tensor cores in modern GPU architectures provide incredible computing capability. This paper proposes a novel parallel implementation of Viterbi decoding algorithm based on Tensor cores in modern GPU architectures. The proposed parallel algorithm is optimized to efficiently utilize the computing power of Tensor cores. Experiments show considerable throughput improvements in comparison with previous works.",2020-11-27T06:44:47Z,2020-11-27T06:44:47Z,http://arxiv.org/abs/2011.13579v1,http://arxiv.org/pdf/2011.13579v1,"cs.DC, eess.SP"
Distributed Training and Optimization Of Neural Networks,"Jean-Roch Vlimant, Junqi Yin","Deep learning models are yielding increasingly better performances thanks to multiple factors. To be successful, model may have large number of parameters or complex architectures and be trained on large dataset. This leads to large requirements on computing resource and turn around time, even more so when hyper-parameter optimization is done (e.g search over model architectures). While this is a challenge that goes beyond particle physics, we review the various ways to do the necessary computations in parallel, and put it in the context of high energy physics.",2020-12-03T11:18:46Z,2021-01-15T14:24:22Z,http://arxiv.org/abs/2012.01839v2,http://arxiv.org/pdf/2012.01839v2,"cs.LG, hep-ex"
Mutual Information Decay Curves and Hyper-Parameter Grid Search Design   for Recurrent Neural Architectures,"Abhijit Mahalunkar, John D. Kelleher","We present an approach to design the grid searches for hyper-parameter optimization for recurrent neural architectures. The basis for this approach is the use of mutual information to analyze long distance dependencies (LDDs) within a dataset. We also report a set of experiments that demonstrate how using this approach, we obtain state-of-the-art results for DilatedRNNs across a range of benchmark datasets.",2020-12-08T18:52:01Z,2020-12-08T18:52:01Z,http://arxiv.org/abs/2012.04632v1,http://arxiv.org/pdf/2012.04632v1,"cs.LG, cs.IT, math.IT"
A New Neural Network Architecture Invariant to the Action of Symmetry   Subgroups,"Piotr Kicki, Mete Ozay, Piotr Skrzypczyński","We propose a computationally efficient $G$-invariant neural network that approximates functions invariant to the action of a given permutation subgroup $G \leq S_n$ of the symmetric group on input data. The key element of the proposed network architecture is a new $G$-invariant transformation module, which produces a $G$-invariant latent representation of the input data. Theoretical considerations are supported by numerical experiments, which demonstrate the effectiveness and strong generalization properties of the proposed method in comparison to other $G$-invariant neural networks.",2020-12-11T16:19:46Z,2020-12-11T16:19:46Z,http://arxiv.org/abs/2012.06452v1,http://arxiv.org/pdf/2012.06452v1,"cs.LG, cs.AI, I.2.6"
Generative Adversarial Network for Image Synthesis,"Yang Lei, Richard L. J. Qiu, Tonghe Wang, Walter J. Curran, Tian Liu, Xiaofeng Yang","This chapter reviews recent developments of generative adversarial networks (GAN)-based methods for medical and biomedical image synthesis tasks. These methods are classified into conditional GAN and Cycle-GAN according to the network architecture designs. For each category, a literature survey is given, which covers discussions of the network architecture designs, highlights important contributions and identifies specific challenges.",2020-12-31T04:38:46Z,2020-12-31T04:38:46Z,http://arxiv.org/abs/2012.15446v1,http://arxiv.org/pdf/2012.15446v1,"physics.med-ph, eess.IV"
All-Optical Image Identification with Programmable Matrix Transformation,"Shikang Li, Baohua Ni, Xue Feng, Kaiyu Cui, Fang Liu, Wei Zhang, Yidong Huang","An optical neural network is proposed and demonstrated with programmable matrix transformation and nonlinear activation function of photodetection (square-law detection). Based on discrete phase-coherent spatial modes, the dimensionality of programmable optical matrix operations is 30~37, which is implemented by spatial light modulators. With this architecture, all-optical classification tasks of handwritten digits, objects and depth images are performed on the same platform with high accuracy. Due to the parallel nature of matrix multiplication, the processing speed of our proposed architecture is potentially as high as7.4T~74T FLOPs per second (with 10~100GHz detector)",2021-04-01T03:07:32Z,2021-04-01T03:07:32Z,http://arxiv.org/abs/2104.02474v1,http://arxiv.org/pdf/2104.02474v1,"physics.optics, eess.IV"
MobileStyleGAN: A Lightweight Convolutional Neural Network for   High-Fidelity Image Synthesis,Sergei Belousov,"In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality.",2021-04-10T13:46:49Z,2021-09-09T21:03:58Z,http://arxiv.org/abs/2104.04767v2,http://arxiv.org/pdf/2104.04767v2,"cs.CV, eess.IV"
Latency Analysis of Consortium Blockchained Federated Learning,"Pengcheng Ren, Tongjiang Yan","A decentralized federated learning architecture is proposed to apply to the Businesses-to-Businesses scenarios by introducing the consortium blockchain in this paper. We introduce a model verification mechanism to ensure the quality of local models trained by participators. To analyze the latency of the system, a latency model is constructed by considering the work flow of the architecture. Finally the experiment results show that our latency model does well in quantifying the actual delays.",2021-05-10T03:14:52Z,2021-05-10T03:14:52Z,http://arxiv.org/abs/2105.04087v1,http://arxiv.org/pdf/2105.04087v1,"stat.ML, cs.DC, cs.LG"
Towards a Predictive Processing Implementation of the Common Model of   Cognition,"Alexander Ororbia, M. A. Kelly","In this article, we present a cognitive architecture that is built from powerful yet simple neural models. Specifically, we describe an implementation of the common model of cognition grounded in neural generative coding and holographic associative memory. The proposed system creates the groundwork for developing agents that learn continually from diverse tasks as well as model human performance at larger scales than what is possible with existant cognitive architectures.",2021-05-15T22:55:23Z,2021-05-18T21:14:26Z,http://arxiv.org/abs/2105.07308v2,http://arxiv.org/pdf/2105.07308v2,"cs.AI, cs.LG, q-bio.NC"
A Reconfigurable Relay for Polarization Encoded QKD Networks,"Jing Wang, Bernardo A. Huberman","We propose a method for reconfiguring a relay node for polarization encoded quantum key distribution (QKD) networks. The relay can be switched between trusted and untrusted modes to adapt to different network conditions, relay distances, and security requirements. This not only extends the distance over which a QKD network operates but also enables point-to-multipoint (P2MP) network topologies. The proposed architecture centralizes the expensive and delicate single-photon detectors (SPDs) at the relay node with eased maintenance and cooling while simplifying each user node so that it only needs commercially available devices for low-cost qubit preparation.",2021-06-02T21:24:47Z,2021-06-02T21:24:47Z,http://arxiv.org/abs/2106.01475v1,http://arxiv.org/pdf/2106.01475v1,"quant-ph, cs.NI, physics.app-ph"
Long short-term relevance learning,"Bram van de Weg, Lars Greve, Bojana Rosic","To incorporate prior knowledge as well as measurement uncertainties in the traditional long short term memory (LSTM) neural networks, an efficient sparse Bayesian training algorithm is introduced to the network architecture. The proposed scheme automatically determines relevant neural connections and adapts accordingly, in contrast to the classical LSTM solution. Due to its flexibility, the new LSTM scheme is less prone to overfitting, and hence can approximate time dependent solutions by use of a smaller data set. On a structural nonlinear finite element application we show that the self-regulating framework does not require prior knowledge of a suitable network architecture and size, while ensuring satisfying accuracy at reasonable computational cost.",2021-06-21T09:07:17Z,2021-06-21T09:07:17Z,http://arxiv.org/abs/2106.12694v1,http://arxiv.org/pdf/2106.12694v1,"cs.LG, stat.ML"
Cybersecurity Challenges in Distributed Control,Md Amimul Ehsan,"Cyber-physical systems are becoming core of the most modern systems consisting control, data sharing and real-time monitoring. While centralized control technique has been implemented in the past, recent innovation in distributed control schemes makes it attractive due to various reasons. One of them is the use of state-of-the-art communication protocols that makes the system more robust toward extreme conditions and ensures observability. Thus, as an application of cyber-physical systems, distributed control architectures are prone to various cyber-vulnerability which makes cybersecurity research critical in this application domain. This paper reviews recent researches of distributed control architectures, their cyber-vulnerabilities, and reported mitigation schemes. Finally, some research needs are addressed.",2021-06-25T15:50:39Z,2021-06-25T15:50:39Z,http://arxiv.org/abs/2106.13712v1,http://arxiv.org/pdf/2106.13712v1,"eess.SY, cs.SY"
Bag of Tricks for Neural Architecture Search,"Thomas Elsken, Benedikt Staffler, Arber Zela, Jan Hendrik Metzen, Frank Hutter","While neural architecture search methods have been successful in previous years and led to new state-of-the-art performance on various problems, they have also been criticized for being unstable, being highly sensitive with respect to their hyperparameters, and often not performing better than random search. To shed some light on this issue, we discuss some practical considerations that help improve the stability, efficiency and overall performance.",2021-07-08T09:57:39Z,2021-07-08T09:57:39Z,http://arxiv.org/abs/2107.03719v1,http://arxiv.org/pdf/2107.03719v1,"cs.LG, cs.AI, stat.ML"
A Bayesian Approach to Invariant Deep Neural Networks,"Nikolaos Mourdoukoutas, Marco Federici, Georges Pantalos, Mark van der Wilk, Vincent Fortuin","We propose a novel Bayesian neural network architecture that can learn invariances from data alone by inferring a posterior distribution over different weight-sharing schemes. We show that our model outperforms other non-invariant architectures, when trained on datasets that contain specific invariances. The same holds true when no data augmentation is performed.",2021-07-20T07:33:58Z,2021-11-02T18:01:19Z,http://arxiv.org/abs/2107.09301v2,http://arxiv.org/pdf/2107.09301v2,"stat.ML, cs.LG"
Malware Classification Using Transfer Learning,"Hikmat Farhat, Veronica Rammouz","With the rapid growth of the number of devices on the Internet, malware poses a threat not only to the affected devices but also their ability to use said devices to launch attacks on the Internet ecosystem. Rapid malware classification is an important tools to combat that threat. One of the successful approaches to classification is based on malware images and deep learning. While many deep learning architectures are very accurate they usually take a long time to train. In this work we perform experiments on multiple well known, pre-trained, deep network architectures in the context of transfer learning. We show that almost all them classify malware accurately with a very short training period.",2021-07-29T04:34:52Z,2021-07-29T04:34:52Z,http://arxiv.org/abs/2107.13743v1,http://arxiv.org/pdf/2107.13743v1,"cs.CR, cs.LG, I.2.10"
SCIMAT: Science and Mathematics Dataset,"Neeraj Kollepara, Snehith Kumar Chatakonda, Pawan Kumar","In this work, we announce a comprehensive well curated and opensource dataset with millions of samples for pre-college and college level problems in mathematicsand science. A preliminary set of results using transformer architecture with character to character encoding is shown. The dataset identifies some challenging problem and invites research on better architecture search",2021-09-30T11:01:11Z,2021-09-30T11:01:11Z,http://arxiv.org/abs/2109.15005v1,http://arxiv.org/pdf/2109.15005v1,"math.NA, cs.IR, cs.LG, cs.NA"
Neural Synthesis of Footsteps Sound Effects with Generative Adversarial   Networks,"Marco Comunità, Huy Phan, Joshua D. Reiss","Footsteps are among the most ubiquitous sound effects in multimedia applications. There is substantial research into understanding the acoustic features and developing synthesis models for footstep sound effects. In this paper, we present a first attempt at adopting neural synthesis for this task. We implemented two GAN-based architectures and compared the results with real recordings as well as six traditional sound synthesis methods. Our architectures reached realism scores as high as recorded samples, showing encouraging results for the task at hand.",2021-10-18T20:04:46Z,2021-12-10T12:11:13Z,http://arxiv.org/abs/2110.09605v2,http://arxiv.org/pdf/2110.09605v2,"cs.SD, cs.AI, cs.LG, eess.AS"
"Development of Quantum Circuits for Perceptron Neural Network Training,   Based on the Principles of Grover's Algorithm","Cesar Borisovich Pronin, Andrey Vladimirovich Ostroukh",This paper highlights a practical research of the possibility of forming quantum circuits for training neural networks. The demonstrated quantum circuits were based on the principles of Grover's Search Algorithm. The perceptron was chosen as the architecture for the example neural network. The multilayer perceptron is a popular neural network architecture due to its scalability and applicability for solving a wide range of problems.,2021-10-15T13:07:18Z,2021-10-15T13:07:18Z,http://arxiv.org/abs/2110.09891v1,http://arxiv.org/pdf/2110.09891v1,"quant-ph, cs.ET"
Brain Tumor Detection using Convolutional Neural Networks with Skip   Connections,"Aupam Hamran, Marzieh Vaeztourshizi, Amirhossein Esmaili, Massoud Pedram","In this paper, we present different architectures of Convolutional Neural Networks (CNN) to analyze and classify the brain tumors into benign and malignant types using the Magnetic Resonance Imaging (MRI) technique. Different CNN architecture optimization techniques such as widening and deepening of the network and adding skip connections are applied to improve the accuracy of the network. Results show that a subset of these techniques can judiciously be used to outperform a baseline CNN model used for the same purpose.",2023-07-14T17:52:15Z,2023-07-14T17:52:15Z,http://arxiv.org/abs/2307.07503v1,http://arxiv.org/pdf/2307.07503v1,"eess.IV, cs.CV, cs.LG"
Ultrafast laser architectures for quantum control of nuclear fusion,Jake Levitt,"Quantum control of nuclear fusion involves engineering quantum coherences in a nuclear wavepacket to accelerate tunneling through the Coulomb barrier and modifying the analytic structure of the $\textit{S}$-matrix to facilitate long-range reactive capture. We present a three-body fusion reaction which is amenable to quantum control. The main result of the present inquiry is the discovery of an embodied class of ultrafast laser architectures [Levitt $\textit{et al.}$, U.S. Patent Application No. 17/855,476 (2022)] which realize solutions to the Schr\""odinger equation in a logically consistent manner as to that presented in [Saha $\textit{et al.}$, Mol. Phys. $\textbf{110}$, 9-10 (2012)]. Further, we provide some necessary (but not necessarily sufficient) conditions for net electrical power production using the optical designs here.",2023-08-14T19:11:59Z,2023-08-14T19:11:59Z,http://arxiv.org/abs/2308.07417v1,http://arxiv.org/pdf/2308.07417v1,"physics.optics, physics.plasm-ph, quant-ph"
Auto-calibrating Universal Programmable Photonic Circuits: Hardware   Error-Correction and Defect Resilience,"Matthew Markowitz, Kevin Zelaya, Mohammad-Ali Miri","It is recently shown that discrete $N\times N$ linear unitary operators can be represented by interlacing $N+1$ phase shift layers with a fixed intervening operator such as Discrete Fractional Fourier Transform (DFrFT). Here, we show that introducing perturbations to the intervening operations does not compromise the universality of this architecture. Furthermore, we show that this architecture is resilient to defects in the phase shifters as long as no more than one faulty phase shifter is present in each layer. These properties enable post-fabrication auto-calibration of such universal photonic circuits, effectively compensating for fabrication errors and defects in phase components.",2023-08-17T18:45:32Z,2023-08-17T18:45:32Z,http://arxiv.org/abs/2308.09151v1,http://arxiv.org/pdf/2308.09151v1,"cs.ET, physics.optics, quant-ph"
Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting,"Riccardo Miccini, Alaa Zniber, Clément Laroche, Tobias Piechowiak, Martin Schoeberl, Luca Pezzarossa, Ouassim Karrakchou, Jens Sparsø, Mounir Ghogho","Although deep learning has made strides in the field of deep noise suppression, leveraging deep architectures on resource-constrained devices still proved challenging. Therefore, we present an early-exiting model based on nsNet2 that provides several levels of accuracy and resource savings by halting computations at different stages. Moreover, we adapt the original architecture by splitting the information flow to take into account the injected dynamism. We show the trade-offs between performance and computational complexity based on established metrics.",2023-08-31T12:29:24Z,2023-08-31T12:29:24Z,http://arxiv.org/abs/2308.16678v1,http://arxiv.org/pdf/2308.16678v1,"cs.SD, cs.LG, eess.AS"
Evaluating the Risk of Changes in a Microservices Architecture,"Matteo Collina, Luca Maraschi, Tommaso Pirini 1. Platformatic Inc","In a microservices-based system, reliability and availability are key components to guarantee the best-in-class experience for the consumers. One of the key advantages of microservices architecture is the ability to independently deploy services, providing maximum change flexibility. However, this introduces an extra complexity in managing the risk associated with every change: any mutation of a service might cause the whole system to fail. In this research, we would propose an algorithm to enable development teams to determine the risk associated with each change to any of the microservices in the system.",2023-09-12T13:54:28Z,2023-09-12T13:54:28Z,http://arxiv.org/abs/2309.06238v1,http://arxiv.org/pdf/2309.06238v1,"cs.SE, D.2.4; K.6.3; C.2.4"
Heterogeneous Rank Beamforming for Industrial Communications,"Andrea Bedin, Akshay Jain, Andrea Zanella, Karthik Upadhya","This paper proposes a novel hardware beamforming architecture, which is capable of utilizing a different number of Radio Frequency (RF) chains in different parts of the bandwidth. It also shows that a proportional fairness scheduler will effectively utilize the high rank part of the bandwidth in a multi-user setting, thus operating more efficiently and effectively than classical beamforming schemes.",2023-09-22T06:09:01Z,2023-09-22T06:09:01Z,http://arxiv.org/abs/2309.12636v1,http://arxiv.org/pdf/2309.12636v1,"cs.IT, cs.NI, math.IT"
Nek5000/RS Performance on Advanced GPU Architectures,"Misun Min, Yu-Hsiang Lan, Paul Fischer, Thilina Rathnayake, John Holmen","We demonstrate NekRS performance results on various advanced GPU architectures. NekRS is a GPU-accelerated version of Nek5000 that targets high performance on exascale platforms. It is being developed in DOE's Center of Efficient Exascale Discretizations, which is one of the co-design centers under the Exascale Computing Project. In this paper, we consider Frontier, Crusher, Spock, Polaris, Perlmutter, ThetaGPU, and Summit. Simulations are performed with 17x17 rod-bundle geometries from small modular reactor applications. We discuss strong-scaling performance and analysis.",2023-09-28T12:29:02Z,2023-09-28T12:29:02Z,http://arxiv.org/abs/2309.16381v1,http://arxiv.org/pdf/2309.16381v1,"cs.DC, cs.PF, 35-04, D.0; F.2; G.2; G.4; I.6; J.2"
Position Sensing Errors in Synchronous Motor Drives,Prerit Pramod,"Non-ideal position estimation results in degraded performance of synchronous motor drive systems due to reduction of the average capability of the drive as well as torque harmonics of different orders. The signature and extent of the performance degradation is further dependent, quite significantly, on the current control architecture, i.e., feedforward or feedback control, employed. This paper presents a comprehensive analysis of non-idealities or errors in position estimation and their effects on the control performance of synchronous motor drives. Analytical models capturing the error in various signals caused by position sensing errors in the drive system for different control architectures are presented and are validated with simulation and experimental results on a prototype permanent magnet synchronous motor drive.",2023-10-02T08:38:42Z,2023-10-10T09:49:33Z,http://arxiv.org/abs/2310.00977v2,http://arxiv.org/pdf/2310.00977v2,"eess.SY, cs.SY"
Cortical Functional architectures as contact and sub-Riemannian geometry,"Giovanna Citti, Alessandro Sarti","In a joint paper, Jean Petitot together with the authors of the present paper described the functional geometry of the visual cortex as the symplectization of a contact form to describe the family of cells sensitive to position, orientation and scale. In the present paper, as a ""homage"" to the enormous contribution of Jean Petitot to neurogeometry, we will extend this approach to more complex functional architectures built as a sequence of contactization or a symplectization process, able to extend the dimension of the space. We will also outline a few examples where a sub-Riemannian lifting is needed.",2023-10-16T03:09:05Z,2023-10-16T03:09:05Z,http://arxiv.org/abs/2310.16056v1,http://arxiv.org/pdf/2310.16056v1,"q-bio.NC, math.DG"
On the Behavior of Audio-Visual Fusion Architectures in Identity   Verification Tasks,"Daniel Claborne, Eric Slyman, Karl Pazdernik","We train an identity verification architecture and evaluate modifications to the part of the model that combines audio and visual representations, including in scenarios where one input is missing in either of two examples to be compared. We report results on the Voxceleb1-E test set that suggest averaging the output embeddings improves error rate in the full-modality setting and when a single modality is missing, and makes more complete use of the embedding space than systems which use shared layers and discuss possible reasons for this behavior.",2023-11-09T00:09:18Z,2023-11-09T00:09:18Z,http://arxiv.org/abs/2311.05071v1,http://arxiv.org/pdf/2311.05071v1,"cs.LG, cs.CV, cs.SD, eess.AS, I.4.0; I.2.10; I.5.0"
Alternatives to the Scaled Dot Product for Attention in the Transformer   Neural Network Architecture,James Bernhard,"The transformer neural network architecture uses a form of attention in which the dot product of query and key is divided by the square root of the key dimension before applying softmax. This scaling of the dot product is designed to avoid the absolute value of the dot products becoming so large that applying softmax leads to vanishing gradients. In this paper, we propose some alternative scalings, including dividing the dot product instead by the sum of the key lengths before applying softmax. We use simulated keys and queries to show that in many situations this appears to be more effective at avoiding regions where applying softmax leads to vanishing gradients.",2023-11-15T22:10:42Z,2023-11-15T22:10:42Z,http://arxiv.org/abs/2311.09406v1,http://arxiv.org/pdf/2311.09406v1,"cs.LG, cs.CL, cs.NE, I.2.0; I.2.7"
TransOpt: Transformer-based Representation Learning for Optimization   Problem Classification,"Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov","We propose a representation of optimization problem instances using a transformer-based neural network architecture trained for the task of problem classification of the 24 problem classes from the Black-box Optimization Benchmarking (BBOB) benchmark. We show that transformer-based methods can be trained to recognize problem classes with accuracies in the range of 70\%-80\% for different problem dimensions, suggesting the possible application of transformer architectures in acquiring representations for black-box optimization problems.",2023-11-29T19:20:47Z,2023-11-29T19:20:47Z,http://arxiv.org/abs/2311.18035v1,http://arxiv.org/pdf/2311.18035v1,"cs.LG, math.OC"
Energy Sustainability in Dense Radio Access Networks via High Altitude   Platform Stations,"Maryam Salamatmoghadasi, Amir Mehrabian, Halim Yanikomeroglu","The growing demand for radio access networks (RANs) driven by advanced wireless technology and the everincreasing mobile traffic, faces significant energy consumption challenges that threaten sustainability. To address this, an architecture referring to the vertical heterogeneous network (vHetNet) has recently been proposed. Our study seeks to enhance network operations in terms of energy efficiency and sustainability by examining a vHetNet configuration, comprising a high altitude platform station (HAPS) acting as a super macro base station (SMBS), along with a macro base station (MBS) and a set of small base stations (SBSs) in a densely populated area.",2023-12-15T18:47:39Z,2023-12-15T18:47:39Z,http://arxiv.org/abs/2312.10027v1,http://arxiv.org/pdf/2312.10027v1,"cs.NI, cs.SY, eess.SY"
A Mathematical Guide to Operator Learning,"Nicolas Boullé, Alex Townsend","Operator learning aims to discover properties of an underlying dynamical system or partial differential equation (PDE) from data. Here, we present a step-by-step guide to operator learning. We explain the types of problems and PDEs amenable to operator learning, discuss various neural network architectures, and explain how to employ numerical PDE solvers effectively. We also give advice on how to create and manage training data and conduct optimization. We offer intuition behind the various neural network architectures employed in operator learning by motivating them from the point-of-view of numerical linear algebra.",2023-12-22T13:43:57Z,2023-12-22T13:43:57Z,http://arxiv.org/abs/2312.14688v1,http://arxiv.org/pdf/2312.14688v1,"math.NA, cs.AI, cs.LG, cs.NA"
Transformer-based approach for Ethereum Price Prediction Using   Crosscurrency correlation and Sentiment Analysis,"Shubham Singh, Mayur Bhat","The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments.",2024-01-16T03:03:39Z,2024-01-16T03:03:39Z,http://arxiv.org/abs/2401.08077v1,http://arxiv.org/pdf/2401.08077v1,"cs.LG, cs.AI, q-fin.PR"
QuantumReservoirPy: A Software Package for Time Series Prediction,"Stanley Miao, Ola Tangen Kulseng, Alexander Stasik, Franz G. Fuchs","In recent times, quantum reservoir computing has emerged as a potential resource for time series prediction. Hence, there is a need for a flexible framework to test quantum circuits as nonlinear dynamical systems. We have developed a software package to allow for quantum reservoirs to fit a common structure, similar to that of reservoirpy which is advertised as ""a python tool designed to easily define, train and use (classical) reservoir computing architectures"". Our package results in simplified development and logical methods of comparison between quantum reservoir architectures. Examples are provided to demonstrate the resulting simplicity of executing quantum reservoir computing using our software package.",2024-01-19T13:31:29Z,2024-01-19T13:31:29Z,http://arxiv.org/abs/2401.10683v1,http://arxiv.org/pdf/2401.10683v1,"quant-ph, cs.SE"
Statistical Guarantees for Link Prediction using Graph Neural Networks,"Alan Chung, Amin Saberi, Morgane Austern","This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense graphs. Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets.",2024-02-05T03:03:00Z,2024-02-07T16:16:08Z,http://arxiv.org/abs/2402.02692v2,http://arxiv.org/pdf/2402.02692v2,"cs.LG, cs.SI, math.ST, stat.ML, stat.TH"
Outer Code Designs for Augmented and Local-Global Polar Code   Architectures,"Ziyuan Zhu, Paul H. Siegel","In this paper, we introduce two novel methods to design outer polar codes for two previously proposed concatenated polar code architectures: augmented polar codes and local-global polar codes. These methods include a stopping set (SS) construction and a nonstationary density evolution (NDE) construction. Simulation results demonstrate the advantage of these methods over previously proposed constructions based on density evolution (DE) and LLR evolution.",2024-02-07T00:23:52Z,2024-05-13T19:39:27Z,http://arxiv.org/abs/2402.04486v2,http://arxiv.org/pdf/2402.04486v2,"cs.IT, math.IT"
Quantum Embedding with Transformer for High-dimensional Data,"Hao-Yuan Chen, Yen-Jui Chang, Shih-Wei Liao, Ching-Ray Chang","Quantum embedding with transformers is a novel and promising architecture for quantum machine learning to deliver exceptional capability on near-term devices or simulators. The research incorporated a vision transformer (ViT) to advance quantum significantly embedding ability and results for a single qubit classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a challenging high-dimensional dataset. The study showcases and analyzes empirical evidence that our transformer-based architecture is a highly versatile and practical approach to modern quantum machine learning problems.",2024-02-20T04:06:28Z,2024-02-20T04:06:28Z,http://arxiv.org/abs/2402.12704v1,http://arxiv.org/pdf/2402.12704v1,"quant-ph, cs.LG"
Data-driven architecture to encode information in the kinematics of   robots and artificial avatars,"Francesco De Lellis, Marco Coraggio, Nathan C. Foster, Riccardo Villa, Cristina Becchio, Mario di Bernardo",We present a data-driven control architecture for modifying the kinematics of robots and artificial avatars to encode specific information such as the presence or not of an emotion in the movements of an avatar or robot driven by a human operator. We validate our approach on an experimental dataset obtained during the reach-to-grasp phase of a pick-and-place task.,2024-03-11T10:00:26Z,2024-03-11T10:00:26Z,http://arxiv.org/abs/2403.06557v1,http://arxiv.org/pdf/2403.06557v1,"eess.SY, cs.LG, cs.RO, cs.SY"
Nonlinear model reduction for operator learning,"Hamidreza Eivazi, Stefan Wittek, Andreas Rausch","Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.",2024-03-27T16:24:26Z,2024-03-27T16:24:26Z,http://arxiv.org/abs/2403.18735v1,http://arxiv.org/pdf/2403.18735v1,"cs.LG, cs.NA, math.NA"
Clustering and spatial distribution of mitochondria in dendritic trees,"Mario Hidalgo-Soria, Elena F. Koslover","Neuronal dendrites form densely branched tree architectures through which mitochondria must be distributed to supply the cell's energetic needs. Dendritic mitochondria circulate through the tree, undergoing fusion and fission to form clusters of varying sizes. We present a mathematical model for the distribution of such actively-driven particles in a branched geometry. Our model demonstrates that `balanced' trees (wherein cross-sectional area is conserved across junctions and thicker branches support more bushy subtrees) enable symmetric yet distally enriched particle distributions and promote dispersion into smaller clusters. These results highlight the importance of tree architecture and radius-dependent fusion in governing the distribution of neuronal mitochondria.",2024-05-07T21:54:24Z,2024-05-07T21:54:24Z,http://arxiv.org/abs/2405.04684v1,http://arxiv.org/pdf/2405.04684v1,"physics.bio-ph, q-bio.CB"
On the Computation of 2-Dimensional Recurrence Equations,Giuseppe Natale,"The paper demonstrates how a 2-dimensional recurrence problem can be reduced to a mono-dimensional recurrence problem where the Kogge and Stone algorithm is applicable, with the computation time - excluding the reduction step - becoming proportional to $log_2(2n-1)$.",2024-06-04T08:05:59Z,2024-06-04T08:05:59Z,http://arxiv.org/abs/2406.02082v1,http://arxiv.org/pdf/2406.02082v1,"cs.DS, cs.CC, cs.DC, G.1.0, F.1.2; F.2.1"
Quantum states from normalizing flows,"Scott Lawrence, Arlee Shelby, Yukari Yamauchi","We introduce an architecture for neural quantum states for many-body quantum-mechanical systems, based on normalizing flows. The use of normalizing flows enables efficient uncorrelated sampling of configurations from the probability distribution defined by the wavefunction, mitigating a major cost of using neural states in simulation. We demonstrate the use of this architecture for both ground-state preparation (for self-interacting particles in a harmonic trap) and real-time evolution (for one-dimensional tunneling). Finally, we detail a procedure for obtaining rigorous estimates of the systematic error when using neural states to approximate quantum evolution.",2024-06-04T16:16:58Z,2024-06-04T16:16:58Z,http://arxiv.org/abs/2406.02451v1,http://arxiv.org/pdf/2406.02451v1,"quant-ph, nucl-th, physics.comp-ph"
Symplectic Methods in Deep Learning,"Sofya Maslovskaya, Sina Ober-Blöbaum","Deep learning is widely used in tasks including image recognition and generation, in learning dynamical systems from data and many more. It is important to construct learning architectures with theoretical guarantees to permit safety in the applications. There has been considerable progress in this direction lately. In particular, symplectic networks were shown to have the non vanishing gradient property, essential for numerical stability. On the other hand, architectures based on higher order numerical methods were shown to be efficient in many tasks where the learned function has an underlying dynamical structure. In this work we construct symplectic networks based on higher order explicit methods with non vanishing gradient property and test their efficiency on various examples.",2024-06-06T14:20:55Z,2024-06-06T14:20:55Z,http://arxiv.org/abs/2406.04104v1,http://arxiv.org/pdf/2406.04104v1,"math.NA, cs.NA, math.OC"
From Cognition to Computation: A Comparative Review of Human Attention   and Transformer Architectures,"Minglu Zhao, Dehong Xu, Tao Gao","Attention is a cornerstone of human cognition that facilitates the efficient extraction of information in everyday life. Recent developments in artificial intelligence like the Transformer architecture also incorporate the idea of attention in model designs. However, despite the shared fundamental principle of selectively attending to information, human attention and the Transformer model display notable differences, particularly in their capacity constraints, attention pathways, and intentional mechanisms. Our review aims to provide a comparative analysis of these mechanisms from a cognitive-functional perspective, thereby shedding light on several open research questions. The exploration encourages interdisciplinary efforts to derive insights from human attention mechanisms in the pursuit of developing more generalized artificial intelligence.",2024-04-25T05:13:38Z,2024-04-25T05:13:38Z,http://arxiv.org/abs/2407.01548v1,http://arxiv.org/pdf/2407.01548v1,"q-bio.OT, cs.AI, cs.LG"
Targeted Augmented Data for Audio Deepfake Detection,"Marcella Astrid, Enjie Ghorbel, Djamila Aouada","The availability of highly convincing audio deepfake generators highlights the need for designing robust audio deepfake detectors. Existing works often rely solely on real and fake data available in the training set, which may lead to overfitting, thereby reducing the robustness to unseen manipulations. To enhance the generalization capabilities of audio deepfake detectors, we propose a novel augmentation method for generating audio pseudo-fakes targeting the decision boundary of the model. Inspired by adversarial attacks, we perturb original real data to synthesize pseudo-fakes with ambiguous prediction probabilities. Comprehensive experiments on two well-known architectures demonstrate that the proposed augmentation contributes to improving the generalization capabilities of these architectures.",2024-07-10T12:31:53Z,2024-07-10T12:31:53Z,http://arxiv.org/abs/2407.07598v1,http://arxiv.org/pdf/2407.07598v1,"cs.SD, cs.LG, eess.AS"
Power System Architecture and Control for Green Hydrogen Production via   Power Converter-less Photovoltaic-Electrolyser Integration,"Aymeric Fabre, Glen Farivar, Andre Chambers","This paper proposes a power system architecture and control for efficient and low-cost green hydrogen production. The proposed system integrates photovoltaic (PV) sources directly with an electrolyser stack, thereby eliminating the need for traditional power converters. With the removal of traditional power converters, maximum power point tracking is achieved through dynamic switching of electrolyser cells in the stack, enabling load variation to maintain optimal voltage for maximum power output. The demonstration methodology involves comprehensive MATLAB Simulink analysis of the integrated system performance through controlled PV-electrolyser interactions.",2024-07-14T04:40:20Z,2024-07-14T04:40:20Z,http://arxiv.org/abs/2407.10075v1,http://arxiv.org/pdf/2407.10075v1,"eess.SY, cs.SY"
A Framework for AI assisted Musical Devices,"Miguel Civit, Luis Munoz Saavedra, Francisco Jose Cuadrado, Charles Tijus, Maria J. Escalona","In this paper we present a novel framework for the study and design of AI assisted musical devices (AIMEs). Initially, we present a taxonomy of these devices and illustrate it with a set of scenarios and personas. Later, we propose a generic architecture for the implementation of AIMEs and present some examples from the scenarios. We show that the proposed framework and architecture are a valid tool for the study of intelligent musical devices.",2024-07-03T17:52:25Z,2024-07-28T19:59:21Z,http://arxiv.org/abs/2407.16899v2,http://arxiv.org/pdf/2407.16899v2,"cs.HC, cs.SD, eess.AS"
Thermodynamics-Consistent Graph Neural Networks,"Jan G. Rittig, Alexander Mitsos","We propose excess Gibbs free energy graph neural networks (GE-GNNs) for predicting composition-dependent activity coefficients of binary mixtures. The GE-GNN architecture ensures thermodynamic consistency by predicting the molar excess Gibbs free energy and using thermodynamic relations to obtain activity coefficients. As these are differential, automatic differentiation is applied to learn the activity coefficients in an end-to-end manner. Since the architecture is based on fundamental thermodynamics, we do not require additional loss terms to learn thermodynamic consistency. As the output is a fundamental property, we neither impose thermodynamic modeling limitations and assumptions. We demonstrate high accuracy and thermodynamic consistency of the activity coefficient predictions.",2024-07-08T06:58:56Z,2024-07-08T06:58:56Z,http://arxiv.org/abs/2407.18372v1,http://arxiv.org/pdf/2407.18372v1,"cond-mat.dis-nn, cs.LG, physics.chem-ph"
Deep Learning for Speaker Identification: Architectural Insights from   AB-1 Corpus Analysis and Performance Evaluation,Matthias Bartolo,"In the fields of security systems, forensic investigations, and personalized services, the importance of speech as a fundamental human input outweighs text-based interactions. This research delves deeply into the complex field of Speaker Identification (SID), examining its essential components and emphasising Mel Spectrogram and Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. Moreover, this study evaluates six slightly distinct model architectures using extensive analysis to evaluate their performance, with hyperparameter tuning applied to the best-performing model. This work performs a linguistic analysis to verify accent and gender accuracy, in addition to bias evaluation within the AB-1 Corpus dataset.",2024-08-13T10:46:50Z,2024-08-13T10:46:50Z,http://arxiv.org/abs/2408.06804v1,http://arxiv.org/pdf/2408.06804v1,"cs.SD, cs.AI, eess.AS"
Hardware Implementation of Projection-Aggregation Decoders for   Reed-Muller Codes,"Marzieh Hashemipour-Nazari, Andrea Nardi-Dei, Kees Goossens, Alexios Balatsoukas-Stimming","This paper presents the hardware implementation of two variants of projection-aggregation-based decoding of Reed-Muller (RM) codes, namely unique projection aggregation (UPA) and collapsed projection aggregation (CPA). Our study focuses on introducing hardware architectures for both UPA and CPA. Through thorough analysis and experimentation, we observe that the hardware implementation of UPA exhibits superior resource usage and reduced energy consumption compared to CPA for the vanilla IPA decoder. This finding underscores a critical insight: software optimizations, in isolation, may not necessarily translate into hardware cost-effectiveness.",2024-08-20T13:44:41Z,2024-08-20T13:44:41Z,http://arxiv.org/abs/2408.10850v1,http://arxiv.org/pdf/2408.10850v1,"cs.AR, cs.IT, eess.SP, math.IT"
Toward Scalable and Efficient Visual Data Transmission in 6G Networks,"Junhao Cai, Taegun An, Changhee Joo","6G network technology will emerge in a landscape where visual data transmissions dominate global mobile traffic and are expected to grow continuously, driven by the increasing demand for AI-based computer vision applications. This will make already challenging task of visual data transmission even more difficult. In this work, we review effective techniques for visual data transmission, such as content compression and adaptive video streaming, highlighting their advantages and limitations. Further, considering the scalability and cost issues of cloud-based and on-device AI services, we explore distributed in-network computing architecture like fog-computing as a direction of 6G networks, and investigate the necessary technical properties for the timely delivery of visual data.",2024-09-24T10:46:47Z,2024-09-24T10:46:47Z,http://arxiv.org/abs/2409.15961v1,http://arxiv.org/pdf/2409.15961v1,"cs.NI, eess.SP"
The Credibility Transformer,"Ronald Richman, Salvatore Scognamiglio, Mario V. Wüthrich","Inspired by the large success of Transformers in Large Language Models, these architectures are increasingly applied to tabular data. This is achieved by embedding tabular data into low-dimensional Euclidean spaces resulting in similar structures as time-series data. We introduce a novel credibility mechanism to this Transformer architecture. This credibility mechanism is based on a special token that should be seen as an encoder that consists of a credibility weighted average of prior information and observation based information. We demonstrate that this novel credibility mechanism is very beneficial to stabilize training, and our Credibility Transformer leads to predictive models that are superior to state-of-the-art deep learning models.",2024-09-25T06:16:45Z,2024-09-25T06:16:45Z,http://arxiv.org/abs/2409.16653v1,http://arxiv.org/pdf/2409.16653v1,"cs.LG, q-fin.GN"
Hyperspectral Unmixing of Agricultural Images taken from UAV Using   Adapted U-Net Architecture,"Vytautas Paura, Virginijus Marcinkevičius","The hyperspectral unmixing method is an algorithm that extracts material (usually called endmember) data from hyperspectral data cube pixels along with their abundances. Due to a lower spatial resolution of hyperspectral sensors data in each of the pixels may contain mixed information from multiple endmembers. In this paper we create a hyperspectral unmixing dataset, created from blueberry field data gathered by a hyperspectral camera mounted on a UAV. We also propose a hyperspectral unmixing algorithm based on U-Net network architecture to achieve more accurate unmixing results on existing and newly created hyperspectral unmixing datasets.",2024-09-29T13:24:48Z,2024-09-29T13:24:48Z,http://arxiv.org/abs/2409.19701v1,http://arxiv.org/pdf/2409.19701v1,"eess.IV, cs.CV"
Qompose: A Technique to Select Optimal Algorithm- Specific Layout for   Neutral Atom Quantum Architectures,"Daniel Silver, Tirthak Patel, Devesh Tiwari","As quantum computing architecture matures, it is important to investigate new technologies that lend unique advantages. In this work, we propose, Qompose, a neutral atom quantum computing framework for efficiently composing quantum circuits on 2-D topologies of neutral atoms. Qompose selects an efficient topology for any given circuit in order to optimize for length of execution through efficient parallelism and for overall fidelity. our extensive evaluation demonstrates the Qompose is effective for a large collection of randomly-generated quantum circuits and a range of real-world benchmarks including VQE, ISING, and QAOA.",2024-09-29T23:03:08Z,2024-09-29T23:03:08Z,http://arxiv.org/abs/2409.19820v1,http://arxiv.org/pdf/2409.19820v1,"quant-ph, cs.AI"
Ensembles provably learn equivariance through data augmentation,"Oskar Nordenfors, Axel Flinth","Recently, it was proved that group equivariance emerges in ensembles of neural networks as the result of full augmentation in the limit of infinitely wide neural networks (neural tangent kernel limit). In this paper, we extend this result significantly. We provide a proof that this emergence does not depend on the neural tangent kernel limit at all. We also consider stochastic settings, and furthermore general architectures. For the latter, we provide a simple sufficient condition on the relation between the architecture and the action of the group for our results to hold. We validate our findings through simple numeric experiments.",2024-10-02T12:02:43Z,2024-10-02T12:02:43Z,http://arxiv.org/abs/2410.01452v1,http://arxiv.org/pdf/2410.01452v1,"cs.LG, cs.NA, math.NA, 68T07 (primary), 3799, 20C35"
RelUNet: Relative Channel Fusion U-Net for Multichannel Speech   Enhancement,"Ibrahim Aldarmaki, Thamar Solorio, Bhiksha Raj, Hanan Aldarmaki","Neural multi-channel speech enhancement models, in particular those based on the U-Net architecture, demonstrate promising performance and generalization potential. These models typically encode input channels independently, and integrate the channels during later stages of the network. In this paper, we propose a novel modification of these models by incorporating relative information from the outset, where each channel is processed in conjunction with a reference channel through stacking. This input strategy exploits comparative differences to adaptively fuse information between channels, thereby capturing crucial spatial information and enhancing the overall performance. The experiments conducted on the CHiME-3 dataset demonstrate improvements in speech enhancement metrics across various architectures.",2024-10-07T13:19:10Z,2024-10-07T13:19:10Z,http://arxiv.org/abs/2410.05019v1,http://arxiv.org/pdf/2410.05019v1,"cs.SD, cs.LG, eess.AS"
Quantum Operating System Support for Quantum Trusted Execution   Environments,"Theodoros Trochatos, Jakub Szefer","With the growing reliance on cloud-based quantum computing, ensuring the confidentiality and integrity of quantum computations is paramount. Quantum Trusted Execution Environments (QTEEs) have been proposed to protect users' quantum circuits when they are submitted to remote cloud-based quantum computers. However, deployment of QTEEs necessitates a Quantum Operating Systems (QOS) that can support QTEEs hardware and operation. This work introduces the first architecture for a QOS to support and enable essential steps required for secure quantum task execution on cloud platforms.",2024-10-11T03:27:34Z,2024-10-11T03:27:34Z,http://arxiv.org/abs/2410.08486v1,http://arxiv.org/pdf/2410.08486v1,"quant-ph, cs.AR, cs.CR"
Low-Power Encoding for PAM-3 DRAM Bus,"Jonghyeon Nam, Jaeduk Han, Hokeun Kim","The 3-level pulse amplitude modulation (PAM-3) signaling is expected to be widely used in memory interfaces for its greater voltage margins compared to PAM-4. To maximize the benefit of PAM-3, we propose three low-power data encoding algorithms: PAM3-DBI, PAM3-MF, and PAM3-SORT. With the DRAM memory traces from the gem5 computer architecture simulator running benchmarks, we evaluate the energy efficiency of our three PAM-3 encoding techniques. The experimental results show the proposed algorithms can reduce termination power for high-speed memory links significantly by 41% to 90% for benchmark programs.",2024-10-16T19:35:02Z,2024-10-16T19:35:02Z,http://arxiv.org/abs/2410.12990v1,http://arxiv.org/pdf/2410.12990v1,"eess.SY, cs.AR, cs.SY"
Comb Tensor Networks vs. Matrix Product States: Enhanced Efficiency in   High-Dimensional Spaces,"Danylo Kolesnyk, Yelyzaveta Vodovozova","Modern approaches to generative modeling of continuous data using tensor networks incorporate compression layers to capture the most meaningful features of high-dimensional inputs. These methods, however, rely on traditional Matrix Product States (MPS) architectures. Here, we demonstrate that beyond a certain threshold in data and bond dimensions, a comb-shaped tensor network architecture can yield more efficient contractions than a standard MPS. This finding suggests that for continuous and high-dimensional data distributions, transitioning from MPS to a comb tensor network representation can substantially reduce computational overhead while maintaining accuracy.",2024-12-08T20:28:49Z,2024-12-08T20:28:49Z,http://arxiv.org/abs/2412.06857v1,http://arxiv.org/pdf/2412.06857v1,"cs.LG, quant-ph"
Robust and Reconfigurable On-Board Data Handling Subsystem for Present   and Future Brazilian CubeSat Missions,"Victor O. Costa, Mauren D'Ávila, Douglas Arena, Vinicius Schreiner, Renan Menezes, Cleber Hoffmann, Edson Pereira, Lidia Shibuya Sato, Felipe Tavares, Luis Loures, Fernanda L. Kastensmidt","CubeSats require robust OBDH solutions in harsh environments. The Demoiselle OBC, featuring a radiation-tolerant APSoC and layered FSW, supports reuse, in-orbit updates, and secure operations. To be validated through ITASAT2 and SelenITA, it ensures fault tolerance, flexibility, and compatibility with emerging technologies. This architecture establishes a foundation for long-lasting, scalable OBDH systems in future Brazilian CubeSat missions, ensuring long-term reliability and adaptability.",2024-12-23T17:29:57Z,2024-12-23T17:29:57Z,http://arxiv.org/abs/2412.17732v1,http://arxiv.org/pdf/2412.17732v1,"astro-ph.IM, cs.AR"
Protein Structure Prediction in the 3D HP Model Using Deep Reinforcement   Learning,"Giovanny Espitia, Yui Tik Pang, James C. Gumbart","We address protein structure prediction in the 3D Hydrophobic-Polar lattice model through two novel deep learning architectures. For proteins under 36 residues, our hybrid reservoir-based model combines fixed random projections with trainable deep layers, achieving optimal conformations with 25% fewer training episodes. For longer sequences, we employ a long short-term memory network with multi-headed attention, matching best-known energy values. Both architectures leverage a stabilized Deep Q-Learning framework with experience replay and target networks, demonstrating consistent achievement of optimal conformations while significantly improving training efficiency compared to existing methods.",2024-12-29T02:55:54Z,2024-12-29T02:55:54Z,http://arxiv.org/abs/2412.20329v1,http://arxiv.org/pdf/2412.20329v1,"cs.LG, cs.AI, q-bio.BM"
Open-Source Heterogeneous SoCs for AI: The PULP Platform Experience,"Francesco Conti, Angelo Garofalo, Davide Rossi, Giuseppe Tagliavini, Luca Benini","Since 2013, the PULP (Parallel Ultra-Low Power) Platform project has been one of the most active and successful initiatives in designing research IPs and releasing them as open-source. Its portfolio now ranges from processor cores to network-on-chips, peripherals, SoC templates, and full hardware accelerators. In this article, we focus on the PULP experience designing heterogeneous AI acceleration SoCs - an endeavour encompassing SoC architecture definition; development, verification, and integration of acceleration IPs; front- and back-end VLSI design; testing; development of AI deployment software.",2024-12-29T08:04:54Z,2024-12-29T08:04:54Z,http://arxiv.org/abs/2412.20391v1,http://arxiv.org/pdf/2412.20391v1,"cs.AR, cs.NE, eess.SP"
Spatio-Temporal Graph Convolutional Networks: Optimised Temporal   Architecture,Edward Turner,Spatio-Temporal graph convolutional networks were originally introduced with CNNs as temporal blocks for feature extraction. Since then LSTM temporal blocks have been proposed and shown to have promising results. We propose a novel architecture combining both CNN and LSTM temporal blocks and then provide an empirical comparison between our new and the pre-existing models. We provide theoretical arguments for the different temporal blocks and use a multitude of tests across different datasets to assess our hypotheses.,2025-01-14T19:44:15Z,2025-01-14T19:44:15Z,http://arxiv.org/abs/2501.10454v1,http://arxiv.org/pdf/2501.10454v1,"cs.LG, stat.ML"
The Mathematics of Artificial Intelligence,Gabriel Peyré,"This overview article highlights the critical role of mathematics in artificial intelligence (AI), emphasizing that mathematics provides tools to better understand and enhance AI systems. Conversely, AI raises new problems and drives the development of new mathematics at the intersection of various fields. This article focuses on the application of analytical and probabilistic tools to model neural network architectures and better understand their optimization. Statistical questions (particularly the generalization capacity of these networks) are intentionally set aside, though they are of crucial importance. We also shed light on the evolution of ideas that have enabled significant advances in AI through architectures tailored to specific tasks, each echoing distinct mathematical techniques. The goal is to encourage more mathematicians to take an interest in and contribute to this exciting field.",2025-01-15T15:00:23Z,2025-01-15T15:00:23Z,http://arxiv.org/abs/2501.10465v1,http://arxiv.org/pdf/2501.10465v1,"math.OC, cs.AI"
Neural Networks Learn Distance Metrics,Alan Oursland,"Neural networks may naturally favor distance-based representations, where smaller activations indicate closer proximity to learned prototypes. This contrasts with intensity-based approaches, which rely on activation magnitudes. To test this hypothesis, we conducted experiments with six MNIST architectural variants constrained to learn either distance or intensity representations. Our results reveal that the underlying representation affects model performance. We develop a novel geometric framework that explains these findings and introduce OffsetL2, a new architecture based on Mahalanobis distance equations, to further validate this framework. This work highlights the importance of considering distance-based learning in neural network design.",2025-02-04T08:35:57Z,2025-02-04T08:35:57Z,http://arxiv.org/abs/2502.02103v1,http://arxiv.org/pdf/2502.02103v1,"cs.LG, cs.AI, stat.ML, 68T07 (Primary) 62H12 (Secondary), I.5.1; G.3"
"Multi-Agent Stock Prediction Systems: Machine Learning Models,   Simulations, and Real-Time Trading Strategies","Daksh Dave, Gauransh Sawhney, Vikhyat Chauhan","This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.",2025-02-21T06:36:16Z,2025-02-21T06:36:16Z,http://arxiv.org/abs/2502.15853v1,http://arxiv.org/pdf/2502.15853v1,"q-fin.ST, cs.LG"
FDDI: Current Issues and Future Trends,R. Jain,"Key issues in upcoming FDDI standards including low-cost fiber, twisted-pair, SONET mapping, and FDDI follow-on LAN are discussed after a brief introduction to FDDI and FDDI-II",1998-09-24T04:44:01Z,1998-09-24T04:44:01Z,http://arxiv.org/abs/cs/9809086v1,http://arxiv.org/pdf/cs/9809086v1,"cs.NI, C.2.1"
Architecture of monitoring elements for the network element modeling in   a Grid infrastructure,"A. Ciuffoletti, T. Ferrari A. Ghiselli, C. Vistoli","Several tools exist that collect host-to-host connectivity measurements. To improve the usability of such measurements, they should be mapped into a framework consisting of complex subsystems, and the infrastructure that connects them. We introduce one such framework, and analyze the architectural implications on the network structure. In our framework, a complex subsystem consists of several computing facilities and the infrastructure that connects them: we call it a -monitoring domain-. The task of measuring the connectivity between -monitoring domains- is considered distinct from the activity of -storage- and -computing- elements. Therefore we introduce a new element in our topology: we call it -theodolite- element, since its function is similar to that of a transponder. Using these basic concepts, we analyze the architectural implications on the network structure: in a nutshell, if we want that -theodolites- serve as a reference, than the contribution to the relevant network metrics due to the -monitoring domain- infrastructure must be negligible with respect to contributions of the inter-domain infrastructure. In addition all -theodolites- of a -monitoring domain- must give an image of the inter-domain infrastructure that is consistent with that experienced by network applications. We conclude giving a running SQL example of how information about -monitoring domains- and -theodolites- could be organized, and we outline the application of such framework in the GLUE schema activity for the network element",2003-07-10T15:14:56Z,2003-07-10T15:14:56Z,http://arxiv.org/abs/cs/0307024v1,http://arxiv.org/pdf/cs/0307024v1,"cs.NI, C.2.4"
Quantum Computers,Archil Avaliani,"This research paper gives an overview of quantum computers - description of their operation, differences between quantum and silicon computers, major construction problems of a quantum computer and many other basic aspects. No special scientific knowledge is necessary for the reader.",2004-05-03T20:25:00Z,2004-05-03T20:25:00Z,http://arxiv.org/abs/cs/0405004v1,http://arxiv.org/pdf/cs/0405004v1,"cs.AI, cs.AR, B.0; C.0; K.4.0; I.0"
Neural Architectures for Robot Intelligence,"H. Ritter, J. J. Steil, C. Noelker, F. Roethling, P. C. McGuire","We argue that the direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching.   Regarding the issue of learning, we propose to view real-world learning from the perspective of data mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our lab in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems.",2004-10-18T10:50:28Z,2004-10-18T10:50:28Z,http://arxiv.org/abs/cs/0410042v1,http://arxiv.org/pdf/cs/0410042v1,"cs.RO, cs.CV, cs.HC, cs.LG, cs.NE, q-bio.NC, I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4"
Quantum Algorithm Processor For Finding Exact Divisors,John Robert Burger,"Wiring diagrams are given for a quantum algorithm processor in CMOS to compute, in parallel, all divisors of an n-bit integer. Lines required in a wiring diagram are proportional to n. Execution time is proportional to the square of n.",2005-08-04T17:35:38Z,2005-08-04T17:35:38Z,http://arxiv.org/abs/cs/0508038v1,http://arxiv.org/pdf/cs/0508038v1,"cs.AR, B.7.1; C.1.2"
Optimal Causal Inference: Estimating Stored Information and   Approximating Causal Architecture,"Susanne Still, James P. Crutchfield, Christopher J. Ellison","We introduce an approach to inferring the causal architecture of stochastic dynamical systems that extends rate distortion theory to use causal shielding---a natural principle of learning. We study two distinct cases of causal inference: optimal causal filtering and optimal causal estimation.   Filtering corresponds to the ideal case in which the probability distribution of measurement sequences is known, giving a principled method to approximate a system's causal structure at a desired level of representation. We show that, in the limit in which a model complexity constraint is relaxed, filtering finds the exact causal architecture of a stochastic dynamical system, known as the causal-state partition. From this, one can estimate the amount of historical information the process stores. More generally, causal filtering finds a graded model-complexity hierarchy of approximations to the causal architecture. Abrupt changes in the hierarchy, as a function of approximation, capture distinct scales of structural organization.   For nonideal cases with finite data, we show how the correct number of underlying causal states can be found by optimal causal estimation. A previously derived model complexity control term allows us to correct for the effect of statistical fluctuations in probability estimates and thereby avoid over-fitting.",2007-08-11T19:13:29Z,2010-08-19T23:46:24Z,http://arxiv.org/abs/0708.1580v2,http://arxiv.org/pdf/0708.1580v2,"cs.IT, cond-mat.stat-mech, cs.LG, math.IT, math.ST, stat.TH"
Neural network learning of optimal Kalman prediction and control,Ralph Linsker,"Although there are many neural network (NN) algorithms for prediction and for control, and although methods for optimal estimation (including filtering and prediction) and for optimal control in linear systems were provided by Kalman in 1960 (with nonlinear extensions since then), there has been, to my knowledge, no NN algorithm that learns either Kalman prediction or Kalman control (apart from the special case of stationary control). Here we show how optimal Kalman prediction and control (KPC), as well as system identification, can be learned and executed by a recurrent neural network composed of linear-response nodes, using as input only a stream of noisy measurement data.   The requirements of KPC appear to impose significant constraints on the allowed NN circuitry and signal flows. The NN architecture implied by these constraints bears certain resemblances to the local-circuit architecture of mammalian cerebral cortex. We discuss these resemblances, as well as caveats that limit our current ability to draw inferences for biological function. It has been suggested that the local cortical circuit (LCC) architecture may perform core functions (as yet unknown) that underlie sensory, motor,and other cortical processing. It is reasonable to conjecture that such functions may include prediction, the estimation or inference of missing or noisy sensory data, and the goal-driven generation of control signals. The resemblances found between the KPC NN architecture and that of the LCC are consistent with this conjecture.",2008-05-28T01:57:11Z,2008-05-28T01:57:11Z,http://arxiv.org/abs/0805.4247v1,http://arxiv.org/pdf/0805.4247v1,"cs.NE, C.1.3; C.3"
Rank Metric Decoder Architectures for Random Linear Network Coding with   Error Control,"Ning Chen, Zhiyuan Yan, Maximilien Gadouleau, Ying Wang, Bruce W. Suter","While random linear network coding is a powerful tool for disseminating information in communication networks, it is highly susceptible to errors caused by various sources. Due to error propagation, errors greatly deteriorate the throughput of network coding and seriously undermine both reliability and security of data. Hence error control for network coding is vital. Recently, constant-dimension codes (CDCs), especially K\""otter-Kschischang (KK) codes, have been proposed for error control in random linear network coding. KK codes can also be constructed from Gabidulin codes, an important class of rank metric codes. Rank metric decoders have been recently proposed for both Gabidulin and KK codes, but they have high computational complexities. Furthermore, it is not clear whether such decoders are feasible and suitable for hardware implementations. In this paper, we reduce the complexities of rank metric decoders and propose novel decoder architectures for both codes. The synthesis results of our decoder architectures for Gabidulin and KK codes with limited error-correcting capabilities over small fields show that our architectures not only are affordable, but also achieve high throughput.",2009-09-25T04:20:29Z,2011-01-22T06:46:46Z,http://arxiv.org/abs/0909.4601v3,http://arxiv.org/pdf/0909.4601v3,"cs.IT, math.IT"
Statics and Dynamics of the Wormlike Bundle Model,"Claus Heussinger, Felix Schueller, Erwin Frey","Bundles of filamentous polymers are primary structural components of a broad range of cytoskeletal structures, and their mechanical properties play key roles in cellular functions ranging from locomotion to mechanotransduction and fertilization. We give a detailed derivation of a wormlike bundle model as a generic description for the statics and dynamics of polymer bundles consisting of semiflexible polymers interconnected by crosslinking agents. The elastic degrees of freedom include bending as well as twist deformations of the filaments and shear deformation of the crosslinks. We show that a competition between the elastic properties of the filaments and those of the crosslinks leads to renormalized effective bend and twist rigidities that become mode-number dependent. The strength and character of this dependence is found to vary with bundle architecture, such as the arrangement of filaments in the cross section and pretwist. We discuss two paradigmatic cases of bundle architecture, a uniform arrangement of filaments as found in F-actin bundles and a shell-like architecture as characteristic for microtubules. Each architecture is found to have its own universal ratio of maximal to minimal bending rigidity, independent of the specific type of crosslink induced filament coupling; our predictions are in reasonable agreement with available experimental data for microtubules. Moreover, we analyze the predictions of the wormlike bundle model for experimental observables such as the tangent-tangent correlation function and dynamic response and correlation functions. Finally, we analyze the effect of pretwist (helicity) on the mechanical properties of bundles. We predict that microtubules with different number of protofilaments should have distinct variations in their effective bending rigidity.",2009-09-28T10:45:08Z,2009-09-28T10:45:08Z,http://arxiv.org/abs/0909.5059v1,http://arxiv.org/pdf/0909.5059v1,"cond-mat.soft, cond-mat.stat-mech, q-bio.BM"
When Should I Use Network Emulation?,"Emmanuel Lochin, Tanguy Perennou, Laurent Dairaine","The design and development of a complex system requires an adequate methodology and efficient instrumental support in order to early detect and correct anomalies in the functional and non-functional properties of the tested protocols. Among the various tools used to provide experimental support for such developments, network emulation relies on real-time production of impairments on real traffic according to a communication model, either realistically or not.   This paper aims at simply presenting to newcomers in network emulation (students, engineers, ...) basic principles and practices illustrated with a few commonly used tools. The motivation behind is to fill a gap in terms of introductory and pragmatic papers in this domain.   The study particularly considers centralized approaches, allowing cheap and easy implementation in the context of research labs or industrial developments. In addition, an architectural model for emulation systems is proposed, defining three complementary levels, namely hardware, impairment and model levels. With the help of this architectural framework, various existing tools are situated and described. Various approaches for modeling the emulation actions are studied, such as impairment-based scenarios and virtual architectures, real-time discrete simulation and trace-based systems. Those modeling approaches are described and compared in terms of services and we study their ability to respond to various designer needs to assess when emulation is needed.",2010-02-15T08:19:45Z,2011-06-28T06:53:21Z,http://arxiv.org/abs/1002.2827v2,http://arxiv.org/pdf/1002.2827v2,"cs.NI, C.4"
Quantum Complexity: restrictions on algorithms and architectures,Daniel James Shepherd,"A dissertation submitted to the University of Bristol in accordance with the requirements of the degree of Doctor of Philosophy (PhD) in the Faculty of Engineering, Department of Computer Science, July 2009.",2010-05-09T21:10:04Z,2010-05-09T21:10:04Z,http://arxiv.org/abs/1005.1425v1,http://arxiv.org/pdf/1005.1425v1,"cs.CC, quant-ph"
Bidirectional Pipelining for Scalable IP Lookup and Packet   Classification,"Weirong Jiang, Hoang Le, Viktor K. Prasanna","Both IP lookup and packet classification in IP routers can be implemented by some form of tree traversal. SRAM-based Pipelining can improve the throughput dramatically. However, previous pipelining schemes result in unbalanced memory allocation over the pipeline stages. This has been identified as a major challenge for scalable pipelined solutions. This paper proposes a flexible bidirectional linear pipeline architecture based on widely-used dual-port SRAMs. A search tree is partitioned, and then mapped onto pipeline stages by a bidirectional fine-grained mapping scheme. We introduce the notion of inversion factor and several heuristics to invert subtrees for memory balancing. Due to its linear structure, the architecture maintains packet input order, and supports non-blocking route updates. Our experiments show that, the architecture can achieve a perfectly balanced memory distribution over the pipeline stages, for both trie-based IP lookup and tree-based multi-dimensional packet classification. For IP lookup, it can store a full backbone routing table with 154419 entries using 2MB of memory, and sustain a high throughput of 1.87 billion packets per second (GPPS), i.e. 0.6 Tbps for the minimum size (40 bytes) packets. The throughput can be improved further to be 2.4 Tbps, by employing caching to exploit the Internet traffic locality.",2011-07-27T03:01:36Z,2011-07-27T03:01:36Z,http://arxiv.org/abs/1107.5372v1,http://arxiv.org/pdf/1107.5372v1,"cs.NI, cs.DS, 68M10"
Quantifying loopy network architectures,"Eleni Katifori, Marcelo O. Magnasco","Biology presents many examples of planar distribution and structural networks having dense sets of closed loops. An archetype of this form of network organization is the vasculature of dicotyledonous leaves, which showcases a hierarchically-nested architecture containing closed loops at many different levels. Although a number of methods have been proposed to measure aspects of the structure of such networks, a robust metric to quantify their hierarchical organization is still lacking. We present an algorithmic framework, the hierarchical loop decomposition, that allows mapping loopy networks to binary trees, preserving in the connectivity of the trees the architecture of the original graph. We apply this framework to investigate computer generated graphs, such as artificial models and optimal distribution networks, as well as natural graphs extracted from digitized images of dicotyledonous leaves and vasculature of rat cerebral neocortex. We calculate various metrics based on the Asymmetry, the cumulative size distribution and the Strahler bifurcation ratios of the corresponding trees and discuss the relationship of these quantities to the architectural organization of the original graphs. This algorithmic framework decouples the geometric information (exact location of edges and nodes) from the metric topology (connectivity and edge weight) and it ultimately allows us to perform a quantitative statistical comparison between predictions of theoretical models and naturally occurring loopy graphs.",2011-10-06T23:26:57Z,2011-10-06T23:26:57Z,http://arxiv.org/abs/1110.1412v1,http://arxiv.org/pdf/1110.1412v1,"q-bio.QM, cond-mat.stat-mech, nlin.AO"
Disseny d'un prototipus de xarxa MESH sense fils multirádio i   multicanal sobre OLSR modificat amb canal de senyalització dedicat,"M. A. Jaume, J. Paradells",Wireless mesh cubes are used to improve the channel signaling.,2011-12-27T13:35:38Z,2011-12-27T13:35:38Z,http://arxiv.org/abs/1112.5959v1,http://arxiv.org/pdf/1112.5959v1,"cs.NI, 28-00, I.2.4"
Distributed Multiuser Sequential Channel Sensing Schemes in Multichannel   Cognitive Radio Networks,"Hossein Shokri-Ghadikolaei, Fatemeh Sheikholeslami, Masoumeh Nasiri-Kenari",This paper has been withdrawn by the author due to a crucial problem associated with Figs. 2 and 3.,2012-01-05T09:26:40Z,2012-02-07T06:29:58Z,http://arxiv.org/abs/1201.1090v2,http://arxiv.org/pdf/1201.1090v2,"cs.NI, cs.PF, math.PR"
"SLA-Oriented Resource Provisioning for Cloud Computing: Challenges,   Architecture, and Solutions","Rajkumar Buyya, Saurabh Kumar Garg, Rodrigo N. Calheiros","Cloud computing systems promise to offer subscription-oriented, enterprise-quality computing services to users worldwide. With the increased demand for delivering services to a large number of users, they need to offer differentiated services to users and meet their quality expectations. Existing resource management systems in data centers are yet to support Service Level Agreement (SLA)-oriented resource allocation, and thus need to be enhanced to realize cloud computing and utility computing. In addition, no work has been done to collectively incorporate customer-driven service management, computational risk management, and autonomic resource management into a market-based resource management system to target the rapidly changing enterprise requirements of Cloud computing. This paper presents vision, challenges, and architectural elements of SLA-oriented resource management. The proposed architecture supports integration of marketbased provisioning policies and virtualisation technologies for flexible allocation of resources to applications. The performance results obtained from our working prototype system shows the feasibility and effectiveness of SLA-based resource provisioning in Clouds.",2012-01-22T01:41:04Z,2012-01-22T01:41:04Z,http://arxiv.org/abs/1201.4522v1,http://arxiv.org/pdf/1201.4522v1,"cs.DC, cs.NI, C.1.4"
Lensless Compressive Sensing Imaging,"Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford","In this paper, we propose a lensless compressive sensing imaging architecture. The architecture consists of two components, an aperture assembly and a sensor. No lens is used. The aperture assembly consists of a two dimensional array of aperture elements. The transmittance of each aperture element is independently controllable. The sensor is a single detection element, such as a single photo-conductive cell. Each aperture element together with the sensor defines a cone of a bundle of rays, and the cones of the aperture assembly define the pixels of an image. Each pixel value of an image is the integration of the bundle of rays in a cone. The sensor is used for taking compressive measurements. Each measurement is the integration of rays in the cones modulated by the transmittance of the aperture elements. A compressive sensing matrix is implemented by adjusting the transmittance of the individual aperture elements according to the values of the sensing matrix. The proposed architecture is simple and reliable because no lens is used. Furthermore, the sharpness of an image from our device is only limited by the resolution of the aperture assembly, but not affected by blurring due to defocus. The architecture can be used for capturing images of visible lights, and other spectra such as infrared, or millimeter waves. Such devices may be used in surveillance applications for detecting anomalies or extracting features such as speed of moving objects. Multiple sensors may be used with a single aperture assembly to capture multi-view images simultaneously. A prototype was built by using a LCD panel and a photoelectric sensor for capturing images of visible spectrum.",2013-02-07T16:00:35Z,2013-02-07T16:00:35Z,http://arxiv.org/abs/1302.1789v1,http://arxiv.org/pdf/1302.1789v1,"cs.CV, cs.IT, math.IT"
"Principles of Security: Human, Cyber, and Biological","Blake C. Stacey, Yaneer Bar-Yam","Cybersecurity attacks are a major and increasing burden to economic and social systems globally. Here we analyze the principles of security in different domains and demonstrate an architectural flaw in current cybersecurity. Cybersecurity is inherently weak because it is missing the ability to defend the overall system instead of individual computers. The current architecture enables all nodes in the computer network to communicate transparently with one another, so security would require protecting every computer in the network from all possible attacks. In contrast, other systems depend on system-wide protections. In providing conventional security, police patrol neighborhoods and the military secures borders, rather than defending each individual household. Likewise, in biology, the immune system provides security against viruses and bacteria using primarily action at the skin, membranes, and blood, rather than requiring each cell to defend itself. We propose applying these same principles to address the cybersecurity challenge. This will require: (a) Enabling pervasive distribution of self-propagating securityware and creating a developer community for such securityware, and (b) Modifying the protocols of internet routers to accommodate adaptive security software that would regulate internet traffic. The analysis of the immune system architecture provides many other principles that should be applied to cybersecurity. Among these principles is a careful interplay of detection and action that includes evolutionary improvement. However, achieving significant security gains by applying these principles depends strongly on remedying the underlying architectural limitations.",2013-03-11T20:47:40Z,2013-03-11T20:47:40Z,http://arxiv.org/abs/1303.2682v1,http://arxiv.org/pdf/1303.2682v1,"cs.CR, nlin.AO, physics.soc-ph, q-bio.PE"
Scrambling Code Planning in TD-SCDMA Systems,"Zhouyun Wu, Aiping Huang, Hsiao-Hwa Chen",This paper has been withdrawn by the author due to a crucial sign error in equation 2.,2013-03-25T02:31:32Z,2013-03-30T10:05:47Z,http://arxiv.org/abs/1303.6017v2,http://arxiv.org/pdf/1303.6017v2,"cs.IT, cs.NI, math.IT"
An Analysis on the Inter-Cell Station Dependency Probability in an IEEE   802.11 Infrastructure WLANs,"Albert Sunny, Joy Kuri, Anurag Kumar","In this document, we are primarily interested in computing the probabilities of various types of dependencies that can occur in a multi-cell infrastructure network.",2013-04-09T13:13:33Z,2013-04-09T13:13:33Z,http://arxiv.org/abs/1304.2574v1,http://arxiv.org/pdf/1304.2574v1,"cs.NI, cs.IT, math.IT"
A large-scale evaluation framework for EEG deep learning architectures,"Felix A. Heilmeyer, Robin T. Schirrmeister, Lukas D. J. Fiederer, Martin Völker, Joos Behncke, Tonio Ball","EEG is the most common signal source for noninvasive BCI applications. For such applications, the EEG signal needs to be decoded and translated into appropriate actions. A recently emerging EEG decoding approach is deep learning with Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many different architectures already published. Here we present a novel framework for the large-scale evaluation of different deep-learning architectures on different EEG datasets. This framework comprises (i) a collection of EEG datasets currently including 100 examples (recording sessions) from six different classification problems, (ii) a collection of different EEG decoding algorithms, and (iii) a wrapper linking the decoders to the data as well as handling structured documentation of all settings and (hyper-) parameters and statistics, designed to ensure transparency and reproducibility. As an applications example we used our framework by comparing three publicly available CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow ConvNet, and two versions of EEGNet. We also show how our framework can be used to study similarities and differences in the performance of different decoding methods across tasks. We argue that the deep learning EEG framework as described here could help to tap the full potential of deep learning for BCI applications.",2018-06-18T15:49:23Z,2018-07-25T15:25:46Z,http://arxiv.org/abs/1806.07741v2,http://arxiv.org/pdf/1806.07741v2,"eess.SP, cs.LG, cs.NE, q-bio.NC, stat.ML"
MotherNets: Rapid Deep Ensemble Learning,"Abdul Wasay, Brian Hentschel, Yuze Liao, Sanyuan Chen, Stratos Idreos","Ensembles of deep neural networks significantly improve generalization accuracy. However, training neural network ensembles requires a large amount of computational resources and time. State-of-the-art approaches either train all networks from scratch leading to prohibitive training cost that allows only very small ensemble sizes in practice, or generate ensembles by training a monolithic architecture, which results in lower model diversity and decreased prediction accuracy. We propose MotherNets to enable higher accuracy and practical training cost for large and diverse neural network ensembles: A MotherNet captures the structural similarity across some or all members of a deep neural network ensemble which allows us to share data movement and computation costs across these networks. We first train a single or a small set of MotherNets and, subsequently, we generate the target ensemble networks by transferring the function from the trained MotherNet(s). Then, we continue to train these ensemble networks, which now converge drastically faster compared to training from scratch. MotherNets handle ensembles with diverse architectures by clustering ensemble networks of similar architecture and training a separate MotherNet for every cluster. MotherNets also use clustering to control the accuracy vs. training cost tradeoff. We show that compared to state-of-the-art approaches such as Snapshot Ensembles, Knowledge Distillation, and TreeNets, MotherNets provide a new Pareto frontier for the accuracy-training cost tradeoff. Crucially, training cost and accuracy improvements continue to scale as we increase the ensemble size (2 to 3 percent reduced absolute test error rate and up to 35 percent faster training compared to Snapshot Ensembles). We verify these benefits over numerous neural network architectures and large data sets.",2018-09-12T06:36:31Z,2020-03-08T02:53:18Z,http://arxiv.org/abs/1809.04270v2,http://arxiv.org/pdf/1809.04270v2,"cs.LG, stat.ML"
Network Recasting: A Universal Method for Network Architecture   Transformation,"Joonsang Yu, Sungbum Kang, Kiyoung Choi","This paper proposes network recasting as a general method for network architecture transformation. The primary goal of this method is to accelerate the inference process through the transformation, but there can be many other practical applications. The method is based on block-wise recasting; it recasts each source block in a pre-trained teacher network to a target block in a student network. For the recasting, a target block is trained such that its output activation approximates that of the source block. Such a block-by-block recasting in a sequential manner transforms the network architecture while preserving the accuracy. This method can be used to transform an arbitrary teacher network type to an arbitrary student network type. It can even generate a mixed-architecture network that consists of two or more types of block. The network recasting can generate a network with fewer parameters and/or activations, which reduce the inference time significantly. Naturally, it can be used for network compression by recasting a trained network into a smaller network of the same type. Our experiments show that it outperforms previous compression approaches in terms of actual speedup on a GPU.",2018-09-14T05:39:15Z,2019-06-19T09:38:15Z,http://arxiv.org/abs/1809.05262v2,http://arxiv.org/pdf/1809.05262v2,"cs.LG, cs.CV, stat.ML"
A Comprehensive guide to Bayesian Convolutional Neural Network with   Variational Inference,"Kumar Shridhar, Felix Laumann, Marcus Liwicki","Artificial Neural Networks are connectionist systems that perform a given task by learning on examples without having prior knowledge about the task. This is done by finding an optimal point estimate for the weights in every node. Generally, the network using point estimates as weights perform well with large datasets, but they fail to express uncertainty in regions with little or no data, leading to overconfident decisions.   In this paper, Bayesian Convolutional Neural Network (BayesCNN) using Variational Inference is proposed, that introduces probability distribution over the weights. Furthermore, the proposed BayesCNN architecture is applied to tasks like Image Classification, Image Super-Resolution and Generative Adversarial Networks. The results are compared to point-estimates based architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image CLassification task, on BSD300 dataset for Image Super Resolution task and on CIFAR10 dataset again for Generative Adversarial Network task.   BayesCNN is based on Bayes by Backprop which derives a variational approximation to the true posterior. We, therefore, introduce the idea of applying two convolutional operations, one for the mean and one for the variance. Our proposed method not only achieves performances equivalent to frequentist inference in identical architectures but also incorporate a measurement for uncertainties and regularisation. It further eliminates the use of dropout in the model. Moreover, we predict how certain the model prediction is based on the epistemic and aleatoric uncertainties and empirically show how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the training accuracy increases. Finally, we propose ways to prune the Bayesian architecture and to make it more computational and time effective.",2019-01-08T13:03:14Z,2019-01-08T13:03:14Z,http://arxiv.org/abs/1901.02731v1,http://arxiv.org/pdf/1901.02731v1,"cs.LG, stat.ML"
Fast Deep Learning for Automatic Modulation Classification,"Sharan Ramjee, Shengtai Ju, Diyu Yang, Xiaoyu Liu, Aly El Gamal, Yonina C. Eldar","In this work, we investigate the feasibility and effectiveness of employing deep learning algorithms for automatic recognition of the modulation type of received wireless communication signals from subsampled data. Recent work considered a GNU radio-based data set that mimics the imperfections in a real wireless channel and uses 10 different modulation types. A Convolutional Neural Network (CNN) architecture was then developed and shown to achieve performance that exceeds that of expert-based approaches. Here, we continue this line of work and investigate deep neural network architectures that deliver high classification accuracy. We identify three architectures - namely, a Convolutional Long Short-term Deep Neural Network (CLDNN), a Long Short-Term Memory neural network (LSTM), and a deep Residual Network (ResNet) - that lead to typical classification accuracy values around 90% at high SNR. We then study algorithms to reduce the training time by minimizing the size of the training data set, while incurring a minimal loss in classification accuracy. To this end, we demonstrate the performance of Principal Component Analysis in significantly reducing the training time, while maintaining good performance at low SNR. We also investigate subsampling techniques that further reduce the training time, and pave the way for online classification at high SNR. Finally, we identify representative SNR values for training each of the candidate architectures, and consequently, realize drastic reductions of the training time, with negligible loss in classification accuracy.",2019-01-16T01:15:50Z,2019-01-16T01:15:50Z,http://arxiv.org/abs/1901.05850v1,http://arxiv.org/pdf/1901.05850v1,"eess.SP, cs.AI, cs.LG, stat.ML"
Applying Visual Domain Style Transfer and Texture Synthesis Techniques   to Audio - Insights and Challenges,"M. Huzaifah, L. Wyse","Style transfer is a technique for combining two images based on the activations and feature statistics in a deep learning neural network architecture. This paper studies the analogous task in the audio domain and takes a critical look at the problems that arise when adapting the original vision-based framework to handle spectrogram representations. We conclude that CNN architectures with features based on 2D representations and convolutions are better suited for visual images than for time-frequency representations of audio. Despite the awkward fit, experiments show that the Gram matrix determined ""style"" for audio is more closely aligned with timbral signatures without temporal structure whereas network layer activity determining audio ""content"" seems to capture more of the pitch and rhythmic structures. We shed insight on several reasons for the domain differences with illustrative examples. We motivate the use of several types of one-dimensional CNNs that generate results that are better aligned with intuitive notions of audio texture than those based on existing architectures built for images. These ideas also prompt an exploration of audio texture synthesis with architectural variants for extensions to infinite textures, multi-textures, parametric control of receptive fields and the constant-Q transform as an alternative frequency scaling for the spectrogram.",2019-01-29T11:59:47Z,2019-01-29T11:59:47Z,http://arxiv.org/abs/1901.10240v1,http://arxiv.org/pdf/1901.10240v1,"cs.SD, eess.AS"
The Evolved Transformer,"David R. So, Chen Liang, Quoc V. Le","Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original ""big"" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.",2019-01-30T22:03:01Z,2019-05-17T19:47:49Z,http://arxiv.org/abs/1901.11117v4,http://arxiv.org/pdf/1901.11117v4,"cs.LG, cs.CL, cs.NE, stat.ML"
DeepTurbo: Deep Turbo Decoder,"Yihan Jiang, Hyeji Kim, Himanshu Asnani, Sreeram Kannan, Sewoong Oh, Pramod Viswanath","Present-day communication systems routinely use codes that approach the channel capacity when coupled with a computationally efficient decoder. However, the decoder is typically designed for the Gaussian noise channel and is known to be sub-optimal for non-Gaussian noise distribution. Deep learning methods offer a new approach for designing decoders that can be trained and tailored for arbitrary channel statistics. We focus on Turbo codes and propose DeepTurbo, a novel deep learning based architecture for Turbo decoding.   The standard Turbo decoder (Turbo) iteratively applies the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm with an interleaver in the middle. A neural architecture for Turbo decoding termed (NeuralBCJR), was proposed recently. There, the key idea is to create a module that imitates the BCJR algorithm using supervised learning, and to use the interleaver architecture along with this module, which is then fine-tuned using end-to-end training. However, knowledge of the BCJR algorithm is required to design such an architecture, which also constrains the resulting learned decoder. Here we remedy this requirement and propose a fully end-to-end trained neural decoder - Deep Turbo Decoder (DeepTurbo). With novel learnable decoder structure and training methodology, DeepTurbo reveals superior performance under both AWGN and non-AWGN settings as compared to the other two decoders - Turbo and NeuralBCJR. Furthermore, among all the three, DeepTurbo exhibits the lowest error floor.",2019-03-06T10:34:50Z,2019-04-25T00:52:23Z,http://arxiv.org/abs/1903.02295v2,http://arxiv.org/pdf/1903.02295v2,"eess.SP, cs.IT, math.IT"
Energy-Efficient Power Control in Cell-Free and User-Centric Massive   MIMO at Millimeter Wave,"Mario Alonzo, Stefano Buzzi, Alessio Zappone, Ciro D'Elia","In a cell-free massive MIMO architecture a very large number of distributed access points simultaneously and jointly serves a much smaller number of mobile stations; a variant of the cell-free technique is the user-centric approach, wherein each access point just serves a reduced set of mobile stations. This paper introduces and analyzes the cell-free and user-centric architectures at millimeter wave frequencies, considering a training-based channel estimation phase, and the downlink and uplink data transmission phases. First of all, a multiuser clustered millimeter wave channel model is introduced in order to account for the correlation among the channels of nearby users; second, an uplink multiuser channel estimation scheme is described along with low-complexity hybrid analog/digital beamforming architectures. Third, the non-convex problem of power allocation for downlink global energy efficiency maximization is addressed. Interestingly, in the proposed schemes no channel estimation is needed at the mobile stations, and the beamforming schemes used at the mobile stations are channel-independent and have a very simple structure. Numerical results show the benefits granted by the power control procedure, that the considered architectures are effective, and permit assessing the loss incurred by the use of the hybrid beamformers and by the channel estimation errors.",2019-03-27T11:50:16Z,2019-03-27T11:50:16Z,http://arxiv.org/abs/1903.11365v1,http://arxiv.org/pdf/1903.11365v1,"eess.SP, cs.IT, math.IT"
PI-Net: A Deep Learning Approach to Extract Topological Persistence   Images,"Anirudh Som, Hongjun Choi, Karthikeyan Natesan Ramamurthy, Matthew Buman, Pavan Turaga","Topological features such as persistence diagrams and their functional approximations like persistence images (PIs) have been showing substantial promise for machine learning and computer vision applications. This is greatly attributed to the robustness topological representations provide against different types of physical nuisance variables seen in real-world data, such as view-point, illumination, and more. However, key bottlenecks to their large scale adoption are computational expenditure and difficulty incorporating them in a differentiable architecture. We take an important step in this paper to mitigate these bottlenecks by proposing a novel one-step approach to generate PIs directly from the input data. We design two separate convolutional neural network architectures, one designed to take in multi-variate time series signals as input and another that accepts multi-channel images as input. We call these networks Signal PI-Net and Image PI-Net respectively. To the best of our knowledge, we are the first to propose the use of deep learning for computing topological features directly from data. We explore the use of the proposed PI-Net architectures on two applications: human activity recognition using tri-axial accelerometer sensor data and image classification. We demonstrate the ease of fusion of PIs in supervised deep learning architectures and speed up of several orders of magnitude for extracting PIs from data. Our code is available at https://github.com/anirudhsom/PI-Net.",2019-06-05T00:54:06Z,2020-05-23T14:51:14Z,http://arxiv.org/abs/1906.01769v2,http://arxiv.org/pdf/1906.01769v2,"cs.CV, cs.LG, math.AT"
A Sustainable Multi-modal Multi-layer Emotion-aware Service at the Edge,"Long Hu, Wei Li, Jun Yang, Giancarlo Fortino, Min Chen","Limited by the computational capabilities and battery energy of terminal devices and network bandwidth, emotion recognition tasks fail to achieve good interactive experience for users. The intolerable latency for users also seriously restricts the popularization of emotion recognition applications in the edge environments such as fatigue detection in auto-driving. The development of edge computing provides a more sustainable solution for this problem. Based on edge computing, this article proposes a multi-modal multi-layer emotion-aware service (MULTI-EASE) architecture that considers user's facial expression and voice as a multi-modal data source of emotion recognition, and employs the intelligent terminal, edge server and cloud as multi-layer execution environment. By analyzing the average delay of each task and the average energy consumption at the mobile device, we formulate a delay-constrained energy minimization problem and perform a task scheduling policy between multiple layers to reduce the end-to-end delay and energy consumption by using an edge-based approach, further to improve the users' emotion interactive experience and achieve energy saving in edge computing. Finally, a prototype system is also implemented to validate the architecture of MULTI-EASE, the experimental results show that MULTI-EASE is a sustainable and efficient platform for emotion analysis applications, and also provide a valuable reference for dynamic task scheduling under MULTI-EASE architecture.",2019-06-05T03:35:28Z,2019-06-05T03:35:28Z,http://arxiv.org/abs/1906.01810v1,http://arxiv.org/pdf/1906.01810v1,"cs.NI, eess.SP"
Data-Free Quantization Through Weight Equalization and Bias Correction,"Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling","We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.",2019-06-11T17:47:51Z,2019-11-25T15:00:11Z,http://arxiv.org/abs/1906.04721v3,http://arxiv.org/pdf/1906.04721v3,"cs.LG, cs.CV, stat.ML"
Dense Deformation Network for High Resolution Tissue Cleared Image   Registration,"Abdullah Nazib, Clinton Fookes, Dimitri Perrin","The recent application of deep learning in various areas of medical image analysis has brought excellent performance gains. In particular, technologies based on deep learning in medical image registration can outperform traditional optimisation-based registration algorithms both in registration time and accuracy. However, the U-net based architectures used in most of the image registration frameworks downscale the data, which removes global information and affects the deformation. In this paper, we present a densely connected convolutional architecture for deformable image registration. Our proposed dense network downsizes data only in one stage and have dense connections instead of the skip connections in U-net architecture. The training of the network is unsupervised and does not require ground-truth deformation or any synthetic deformation as a label. The proposed architecture is trained and tested on two different versions of tissue-cleared data, at 10\% and 25\% resolution of the original single-cell-resolution dataset. We demonstrate comparable registration performance to state-of-the-art registration methods and superior performance to the deep-learning based VoxelMorph method in terms of accuracy and increased resolution handling ability. In both resolutions, the proposed DenseDeformation network outperforms VoxelMorph in registration accuracy. Importantly, it can register brains in one minute where conventional methods can take hours at 25\% resolution.",2019-06-13T11:54:19Z,2019-09-06T00:56:27Z,http://arxiv.org/abs/1906.06180v2,http://arxiv.org/pdf/1906.06180v2,"eess.IV, cs.CV, cs.LG, stat.ML"
Sample-Efficient Neural Architecture Search by Learning Action Space,"Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian","Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at https://github.com/facebookresearch/LaMCTS.",2019-06-17T03:50:25Z,2021-03-31T19:13:16Z,http://arxiv.org/abs/1906.06832v2,http://arxiv.org/pdf/1906.06832v2,"cs.LG, cs.CV, stat.ML"
"Internet of Autonomous Vehicles: Architecture, Features, and   Socio-Technological Challenges","Furqan Jameel, Zheng Chang, Jun Huang, Tapani Ristaniemi","Mobility is the backbone of urban life and a vital economic factor in the development of the world. Rapid urbanization and the growth of mega-cities is bringing dramatic changes in the capabilities of vehicles. Innovative solutions like autonomy, electrification, and connectivity are on the horizon. How, then, we can provide ubiquitous connectivity to the legacy and autonomous vehicles? This paper seeks to answer this question by combining recent leaps of innovation in network virtualization with remarkable feats of wireless communications. To do so, this paper proposes a novel paradigm called the Internet of autonomous vehicles (IoAV). We begin painting the picture of IoAV by discussing the salient features, and applications of IoAV which is followed by a detailed discussion on the key enabling technologies. Next, we describe the proposed layered architecture of IoAV and uncover some critical functions of each layer. This is followed by the performance evaluation of IoAV which shows the significant advantage of the proposed architecture in terms of transmission time and energy consumption. Finally, to best capture the benefits of IoAV, we enumerate some social and technological challenges and explain how some unresolved issues can disrupt the widespread use of autonomous vehicles in the future.",2019-06-24T13:02:20Z,2019-06-24T13:02:20Z,http://arxiv.org/abs/1906.09918v1,http://arxiv.org/pdf/1906.09918v1,"cs.NI, cs.CY, eess.SP"
On the performance of residual block design alternatives in   convolutional neural networks for end-to-end audio classification,"Javier Naranjo-Alcazar, Sergi Perez-Castanos, Irene Martin-Morato, Pedro Zuccarello, Maximo Cobos","Residual learning is a recently proposed learning framework to facilitate the training of very deep neural networks. Residual blocks or units are made of a set of stacked layers, where the inputs are added back to their outputs with the aim of creating identity mappings. In practice, such identity mappings are accomplished by means of the so-called skip or residual connections. However, multiple implementation alternatives arise with respect to where such skip connections are applied within the set of stacked layers that make up a residual block. While ResNet architectures for image classification using convolutional neural networks (CNNs) have been widely discussed in the literature, few works have adopted ResNet architectures so far for 1D audio classification tasks. Thus, the suitability of different residual block designs for raw audio classification is partly unknown. The purpose of this paper is to analyze and discuss the performance of several residual block implementations within a state-of-the-art CNN-based architecture for end-to-end audio classification using raw audio waveforms. For comparison purposes, we analyze as well the performance of the residual blocks under a similar 2D architecture using a conventional time-frequency audio represen-tation as input. The results show that the achieved accuracy is considerably dependent, not only on the specific residual block implementation, but also on the selected input normalization.",2019-06-26T07:47:54Z,2019-09-26T07:06:48Z,http://arxiv.org/abs/1906.10891v3,http://arxiv.org/pdf/1906.10891v3,"cs.SD, cs.LG, eess.AS"
Report on the performance portability demonstrated for the relevant   Weather & Climate Dwarfs,Carlos Osuna,"This document is one of the deliverable reports created for the ESCAPE project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather Prediction at Exascale. The project develops world-class, extreme-scale computing capabilities for European operational numerical weather prediction and future climate models. This is done by identifying Weather & Climate dwarfs which are key patterns in terms of computation and communication (in the spirit of the Berkeley dwarfs). These dwarfs are then optimised for different hardware architectures (single and multi-node) and alternative algorithms are explored. Performance portability is addressed through the use of domain specific languages.   This deliverable provides an evaluation of the work performed within ESCAPE to port different dwarfs to accelerators, using different programming models. A key metric of the evaluation is the performance portability of the resulting porting efforts. Portability means that a single source code containing the numerical operators can be compiled and run in multiple architectures, while performance portability additionally requires that the single source code runs efficiently in all the different architectures.   As results of other deliverables like D2.1, D2.4 ESCAPE provides a collection of dwarfs ported to different computing architectures like traditional CPUs, Intel XeonPhi and NVIDIA GPUs. Additionally D3.3 went through an optimization process to obtain efficient and energy efficient dwarfs.   In this deliverable we present a review of the different programming models employed and their use to port various dwarfs of ESCAPE. A final evaluation of the different approaches based on different metrics like performance portability, readability of the numerical methods, efforts to port a dwarf and efficiency of the implementation obtained is reported.",2019-08-16T16:50:59Z,2019-08-16T16:50:59Z,http://arxiv.org/abs/1908.06094v1,http://arxiv.org/pdf/1908.06094v1,"cs.DC, D.2.8; G.1.8; G.4"
Integrated Intelligent and Predictive Control: A Multi-Agent Adaptive   Type-2 Fuzzy Control Architecture,"Anahita Jamshidnejad, Emilio Frazzoli, Mohammad J. Mahjoob, Bart De Schutter","We propose a novel two-layer multi-agent architecture aimed at efficient real-time control of large-scale and complex-dynamics systems. The proposed architecture integrates intelligent control approaches (which have a low computation time and fit real-time applications) with model-predictive control (which takes care of the optimality requirements of control). The bottom control layer (intelligent-control module) includes several distributed intelligent-control agents, the design parameters of which are tuned by the top layer (model-predictive control module). The model-predictive control module fulfills two significant roles: looking ahead to the effects of the control decisions, and coordinating the intelligent-control agents of the lower control layer. The resulting multi-agent control system has a very low computation time, and provides adaptivity, control coordination, and aims at excellent performance. Additionally, we give a general treatment of type-2 fuzzy membership functions, and introduce two categories for them: probabilistic-fuzzy (which is a novel concept introduced in this paper) and fuzzy-fuzzy (which is a new treatment of the existing type-2 fuzzy membership functions). The performance of the proposed modeling and control approaches are assessed via a case study involving a simple urban traffic network: the results show that the novel concept of probabilistic-fuzzy membership function outperforms the type-1 and type-2 membership functions that have already been introduced in the literature. Furthermore, the proposed two-layer integrated multi-agent control architecture significantly outperforms a multi-agent decentralized fuzzy control system (without coordination among the agents), while requiring a comparable computation time.",2019-08-28T08:53:44Z,2019-08-28T08:53:44Z,http://arxiv.org/abs/1908.10599v1,http://arxiv.org/pdf/1908.10599v1,"eess.SY, cs.SY, 49-XX"
Scalable Reinforcement-Learning-Based Neural Architecture Search for   Cancer Deep Learning Research,"Prasanna Balaprakash, Romain Egele, Misha Salim, Stefan Wild, Venkatram Vishwanath, Fangfang Xia, Tom Brettin, Rick Stevens","Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.",2019-09-01T02:49:59Z,2019-09-01T02:49:59Z,http://arxiv.org/abs/1909.00311v1,http://arxiv.org/pdf/1909.00311v1,"cs.LG, stat.ML"
"Patient trajectory prediction in the Mimic-III dataset, challenges and   pitfalls","Jose F Rodrigues-Jr, Gabriel Spadon, Bruno Brandoli, Sihem Amer-Yahia","Automated medical prognosis has gained interest as artificial intelligence evolves and the potential for computer-aided medicine becomes evident. Nevertheless, it is challenging to design an effective system that, given a patient's medical history, is able to predict probable future conditions. Previous works, mostly carried out over private datasets, have tackled the problem by using artificial neural network architectures that cannot deal with low-cardinality datasets, or by means of non-generalizable inference approaches. We introduce a Deep Learning architecture whose design results from an intensive experimental process. The final architecture is based on two parallel Minimal Gated Recurrent Unit networks working in bi-directional manner, which was extensively tested with the open-access Mimic-III dataset. Our results demonstrate significant improvements in automated medical prognosis, as measured with Recall@k. We summarize our experience as a set of relevant insights for the design of Deep Learning architectures. Our work improves the performance of computer-aided medicine and can serve as a guide in designing artificial neural networks used in prediction tasks.",2019-09-10T16:30:24Z,2019-11-28T13:07:08Z,http://arxiv.org/abs/1909.04605v4,http://arxiv.org/pdf/1909.04605v4,"cs.LG, cs.NE, stat.ML"
Quantum-Computing Architecture based on Large-Scale Multi-Dimensional   Continuous-Variable Cluster States in a Scalable Photonic Platform,"Bo-Han Wu, Rafael N. Alexander, Shuai Liu, Zheshen Zhang","Quantum computing is a disruptive paradigm widely believed to be capable of solving classically intractable problems. However, the route toward full-scale quantum computers is obstructed by immense challenges associated with the scalability of the platform, the connectivity of qubits, and the required fidelity of various components. One-way quantum computing is an appealing approach that shifts the burden from high-fidelity quantum gates and quantum memories to the generation of high-quality entangled resource states and high fidelity measurements. Cluster states are an important ingredient for one-way quantum computing, and a compact, portable, and mass producible platform for large-scale cluster states will be essential for the widespread deployment of one-way quantum computing. Here, we bridge two distinct fields---Kerr microcombs and continuous-variable (CV) quantum information---to formulate a one-way quantum computing architecture based on programmable large-scale CV cluster states. The architecture can accommodate hundreds of simultaneously addressable entangled optical modes multiplexed in the frequency domain and an unlimited number of sequentially addressable entangled optical modes in time domain. One-dimensional, two-dimensional, and three-dimensional CV cluster states can be deterministically produced. We note cluster states of at least three dimensions are required for fault-tolerant one-way quantum computing with known error-correction strategies. This architecture can be readily implemented with silicon photonics, opening a promising avenue for quantum computing at a large scale.",2019-09-12T05:02:13Z,2019-09-21T04:18:42Z,http://arxiv.org/abs/1909.05455v2,http://arxiv.org/pdf/1909.05455v2,"quant-ph, cond-mat.quant-gas, physics.optics"
Learned-SBL: A Deep Learning Architecture for Sparse Signal Recovery,"Rubin Jose Peter, Chandra R. Murthy","In this paper, we present a computationally efficient sparse signal recovery scheme using Deep Neural Networks (DNN). The architecture of the introduced neural network is inspired from sparse Bayesian learning (SBL) and named as Learned-SBL (L-SBL). We design a common architecture to recover sparse as well as block sparse vectors from single measurement vector (SMV) or multiple measurement vectors (MMV) depending on the nature of the training data. In the MMV model, the L-SBL network can be trained to learn any underlying sparsity pattern among the vectors including joint sparsity, block sparsity, etc. In particular, for block sparse recovery, learned-SBL does not require any prior knowledge of block boundaries. In each layer of the L-SBL, an estimate of the signal covariance matrix is obtained as the output of a neural network. Then a maximum a posteriori (MAP) estimator of the unknown sparse vector is implemented with non-trainable parameters. In many applications, the measurement matrix may be time-varying. The existing DNN based sparse signal recovery schemes demand the retraining of the neural network using current measurement matrix. The architecture of L-SBL allows it to accept the measurement matrix as an input to the network, and thereby avoids the need for retraining. We also evaluate the performance of Learned-SBL in the detection of an extended target using a multiple-input multiple-output (MIMO) radar. Simulation results illustrate that the proposed approach offers superior sparse recovery performance compared to the state-of-the-art methods.",2019-09-17T04:05:30Z,2019-09-17T04:05:30Z,http://arxiv.org/abs/1909.08185v1,http://arxiv.org/pdf/1909.08185v1,"cs.IT, cs.LG, eess.SP, math.IT"
Deep Reinforcement Learning with Modulated Hebbian plus Q Network   Architecture,"Pawel Ladosz, Eseoghene Ben-Iwhiwhu, Jeffery Dick, Yang Hu, Nicholas Ketz, Soheil Kolouri, Jeffrey L. Krichmar, Praveen Pilly, Andrea Soltoggio","This paper presents a new neural architecture that combines a modulated Hebbian network (MOHN) with DQN, which we call modulated Hebbian plus Q network architecture (MOHQA). The hypothesis is that such a combination allows MOHQA to solve difficult partially observable Markov decision process (POMDP) problems which impair temporal difference (TD)-based RL algorithms such as DQN, as the TD error cannot be easily derived from observations. The key idea is to use a Hebbian network with bio-inspired neural traces in order to bridge temporal delays between actions and rewards when confounding observations and sparse rewards result in inaccurate TD errors. In MOHQA, DQN learns low level features and control, while the MOHN contributes to the high-level decisions by associating rewards with past states and actions. Thus the proposed architecture combines two modules with significantly different learning algorithms, a Hebbian associative network and a classical DQN pipeline, exploiting the advantages of both. Simulations on a set of POMDPs and on the MALMO environment show that the proposed algorithm improved DQN's results and even outperformed control tests with A2C, QRDQN+LSTM and REINFORCE algorithms on some POMDPs with confounding stimuli and sparse rewards.",2019-09-21T21:32:47Z,2021-10-14T07:49:39Z,http://arxiv.org/abs/1909.09902v5,http://arxiv.org/pdf/1909.09902v5,"cs.LG, stat.ML"
Hierarchical Neural Architecture Search via Operator Clustering,"Guilin Li, Xing Zhang, Zitong Wang, Matthias Tan, Jiashi Feng, Zhenguo Li, Tong Zhang","Recently, the efficiency of automatic neural architecture design has been significantly improved by gradient-based search methods such as DARTS. However, recent literature has brought doubt to the generalization ability of DARTS, arguing that DARTS performs poorly when the search space is changed, i.e, when different set of candidate operators are used. Regularization techniques such as early stopping have been proposed to partially solve this problem. In this paper, we tackle this problem from a different perspective by identifying two contributing factors to the collapse of DARTS when the search space changes: (1) the correlation of similar operators incurs unfavorable competition among them and makes their relative importance score unreliable and (2) the optimization complexity gap between the proxy search stage and the final training. Based on these findings, we propose a new hierarchical search algorithm. With its operator clustering and optimization complexity match, the algorithm can consistently find high-performance architecture across various search spaces. For all the five variants of the popular cell-based search spaces, the proposed algorithm always obtains state-of-the-art architecture with best accuracy on the CIFAR-10, CIFAR-100 and ImageNet over other well-established DARTS-alike algorithms. Code is available at https://github.com/susan0199/StacNAS.",2019-09-26T06:26:58Z,2021-01-25T10:03:07Z,http://arxiv.org/abs/1909.11926v5,http://arxiv.org/pdf/1909.11926v5,"cs.LG, cs.CV, stat.ML"
Exascale Deep Learning to Accelerate Cancer Research,"Robert M. Patton, J. Travis Johnston, Steven R. Young, Catherine D. Schuman, Thomas E. Potok, Derek C. Rose, Seung-Hwan Lim, Junghoon Chae, Le Hou, Shahira Abousamra, Dimitris Samaras, Joel Saltz","Deep learning, through the use of neural networks, has demonstrated remarkable ability to automate many routine tasks when presented with sufficient data for training. The neural network architecture (e.g. number of layers, types of layers, connections between layers, etc.) plays a critical role in determining what, if anything, the neural network is able to learn from the training data. The trend for neural network architectures, especially those trained on ImageNet, has been to grow ever deeper and more complex. The result has been ever increasing accuracy on benchmark datasets with the cost of increased computational demands. In this paper we demonstrate that neural network architectures can be automatically generated, tailored for a specific application, with dual objectives: accuracy of prediction and speed of prediction. Using MENNDL--an HPC-enabled software stack for neural architecture search--we generate a neural network with comparable accuracy to state-of-the-art networks on a cancer pathology dataset that is also $16\times$ faster at inference. The speedup in inference is necessary because of the volume and velocity of cancer pathology data; specifically, the previous state-of-the-art networks are too slow for individual researchers without access to HPC systems to keep pace with the rate of data generation. Our new model enables researchers with modest computational resources to analyze newly generated data faster than it is collected.",2019-09-26T17:53:26Z,2019-09-26T17:53:26Z,http://arxiv.org/abs/1909.12291v1,http://arxiv.org/pdf/1909.12291v1,"cs.LG, cs.DC, stat.ML"
Automated design of error-resilient and hardware-efficient deep neural   networks,"Christoph Schorn, Thomas Elsken, Sebastian Vogel, Armin Runge, Andre Guntoro, Gerd Ascheid","Applying deep neural networks (DNNs) in mobile and safety-critical systems, such as autonomous vehicles, demands a reliable and efficient execution on hardware. Optimized dedicated hardware accelerators are being developed to achieve this. However, the design of efficient and reliable hardware has become increasingly difficult, due to the increased complexity of modern integrated circuit technology and its sensitivity against hardware faults, such as random bit-flips. It is thus desirable to exploit optimization potential for error resilience and efficiency also at the algorithmic side, e.g., by optimizing the architecture of the DNN. Since there are numerous design choices for the architecture of DNNs, with partially opposing effects on the preferred characteristics (such as small error rates at low latency), multi-objective optimization strategies are necessary. In this paper, we develop an evolutionary optimization technique for the automated design of hardware-optimized DNN architectures. For this purpose, we derive a set of easily computable objective functions, which enable the fast evaluation of DNN architectures with respect to their hardware efficiency and error resilience solely based on the network topology. We observe a strong correlation between predicted error resilience and actual measurements obtained from fault injection simulations. Furthermore, we analyze two different quantization schemes for efficient DNN computation and find significant differences regarding their effect on error resilience.",2019-09-30T17:08:22Z,2019-09-30T17:08:22Z,http://arxiv.org/abs/1909.13844v1,http://arxiv.org/pdf/1909.13844v1,"cs.LG, stat.ML"
Rethinking the Value of Network Pruning,"Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell","Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned ""important"" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ""important"" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the ""Lottery Ticket Hypothesis"" (Frankle & Carbin 2019), and find that with optimal learning rate, the ""winning ticket"" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.",2018-10-11T22:15:28Z,2019-03-05T05:58:11Z,http://arxiv.org/abs/1810.05270v2,http://arxiv.org/pdf/1810.05270v2,"cs.LG, cs.CV, stat.ML"
Deep learning long-range information in undirected graphs with wave   networks,"Matthew K. Matlock, Arghya Datta, Na Le Dang, Kevin Jiang, S. Joshua Swamidass","Graph algorithms are key tools in many fields of science and technology. Some of these algorithms depend on propagating information between distant nodes in a graph. Recently, there have been a number of deep learning architectures proposed to learn on undirected graphs. However, most of these architectures aggregate information in the local neighborhood of a node, and therefore they may not be capable of efficiently propagating long-range information. To solve this problem we examine a recently proposed architecture, wave, which propagates information back and forth across an undirected graph in waves of nonlinear computation. We compare wave to graph convolution, an architecture based on local aggregation, and find that wave learns three different graph-based tasks with greater efficiency and accuracy. These three tasks include (1) labeling a path connecting two nodes in a graph, (2) solving a maze presented as an image, and (3) computing voltages in a circuit. These tasks range from trivial to very difficult, but wave can extrapolate from small training examples to much larger testing examples. These results show that wave may be able to efficiently solve a wide range of problems that require long-range information propagation across undirected graphs. An implementation of the wave network, and example code for the maze problem are included in the tflon deep learning toolkit (https://bitbucket.org/mkmatlock/tflon).",2018-10-29T14:35:40Z,2018-10-29T14:35:40Z,http://arxiv.org/abs/1810.12153v1,http://arxiv.org/pdf/1810.12153v1,"cs.LG, stat.ML"
Adaptive Extreme Learning Machine for Recurrent Beta-basis Function   Neural Network Training,"Naima Chouikhi, Adel M. Alimi","Beta Basis Function Neural Network (BBFNN) is a special kind of kernel basis neural networks. It is a feedforward network typified by the use of beta function as a hidden activation function. Beta is a flexible transfer function representing richer forms than the common existing functions. As in every network, the architecture setting as well as the learning method are two main gauntlets faced by BBFNN. In this paper, new architecture and training algorithm are proposed for the BBFNN. An Extreme Learning Machine (ELM) is used as a training approach of BBFNN with the aim of quickening the training process. The peculiarity of ELM is permitting a certain decrement of the computing time and complexity regarding the already used BBFNN learning algorithms such as backpropagation, OLS, etc. For the architectural design, a recurrent structure is added to the common BBFNN architecture in order to make it more able to deal with complex, non linear and time varying problems. Throughout this paper, the conceived recurrent ELM-trained BBFNN is tested on a number of tasks related to time series prediction, classification and regression. Experimental results show noticeable achievements of the proposed network compared to common feedforward and recurrent networks trained by ELM and using hyperbolic tangent as activation function. These achievements are in terms of accuracy and robustness against data breakdowns such as noise signals.",2018-10-31T07:31:08Z,2018-10-31T07:31:08Z,http://arxiv.org/abs/1810.13135v1,http://arxiv.org/pdf/1810.13135v1,"cs.LG, cs.AI, stat.ML"
Deep Demosaicing for Edge Implementation,"Ramchalam Kinattinkara Ramakrishnan, Shangling Jui, Vahid Patrovi Nia","Most digital cameras use sensors coated with a Color Filter Array (CFA) to capture channel components at every pixel location, resulting in a mosaic image that does not contain pixel values in all channels. Current research on reconstructing these missing channels, also known as demosaicing, introduces many artifacts, such as zipper effect and false color. Many deep learning demosaicing techniques outperform other classical techniques in reducing the impact of artifacts. However, most of these models tend to be over-parametrized. Consequently, edge implementation of the state-of-the-art deep learning-based demosaicing algorithms on low-end edge devices is a major challenge. We provide an exhaustive search of deep neural network architectures and obtain a pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the performance criterion versus the number of parameters as the model complexity that beats the state-of-the-art. Architectures on the pareto front can then be used to choose the best architecture for a variety of resource constraints. Simple architecture search methods such as exhaustive search and grid search require some conditions of the loss function to converge to the optimum. We clarify these conditions in a brief theoretical study.",2019-03-26T15:04:17Z,2019-05-23T15:20:54Z,http://arxiv.org/abs/1904.00775v3,http://arxiv.org/pdf/1904.00775v3,"cs.CV, cs.LG, stat.ML"
A Self-Healing Hardware Architecture for Safety-Critical Digital   Embedded Devices,Shawkat Sabah Khairullah,"Digital Embedded Devices of next-generation safety-critical industrial automation systems require high levels of survivability and resilience against the hardware and software failure. One of the concepts for achieving this requirement is the design of resilient and survivable digital embedded systems. In the last two decades, development of self-healing digital systems based on molecular and cellular biology have received attention for the design of robust digital systems. However, many of these approaches have not been architected from the outset with safety in mind, nor have they been targeted for the applications of automation community where a significant need exists. This paper presents a new self-healing hardware architecture, inspired from the way nature responds, defends and heals: the stem cells in the immune system of living organisms, the life cycle of the living cell, and the pathway from Deoxyribonucleic acid (DNA) to protein. The proposed architecture is integrating cellular-based biological concepts, traditional fault tolerance techniques, and operational schematics for the international standard IEC 61131-3 to facilitate adoption in the automation industry and safety-critical applications. To date, two industrial applications have been mapped on the proposed architecture, which are capable of tolerating a significant number of faults that can stem from harsh environmental changes and external disturbances and we believe the nexus of its concepts can positively impact the next generation of critical systems in the automation industry",2019-09-30T19:32:17Z,2019-09-30T19:32:17Z,http://arxiv.org/abs/1910.00064v1,http://arxiv.org/pdf/1910.00064v1,"cs.OH, eess.SP"
Pure and Spurious Critical Points: a Geometric Study of Linear Networks,"Matthew Trager, Kathlén Kohn, Joan Bruna","The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network's weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of ""bad"" local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (""filling architectures"") but it holds only for the quadratic loss when the functional space is a determinantal variety (""non-filling architectures""). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.",2019-10-03T18:22:30Z,2020-04-03T02:46:46Z,http://arxiv.org/abs/1910.01671v2,http://arxiv.org/pdf/1910.01671v2,"cs.LG, math.AG, stat.ML"
Multiplierless and Sparse Machine Learning based on Margin Propagation   Networks,"Nazreen P. M., Shantanu Chakrabartty, Chetan Singh Thakur","The new generation of machine learning processors have evolved from multi-core and parallel architectures that were designed to efficiently implement matrix-vector-multiplications (MVMs). This is because at the fundamental level, neural network and machine learning operations extensively use MVM operations and hardware compilers exploit the inherent parallelism in MVM operations to achieve hardware acceleration on GPUs and FPGAs. However, many IoT and edge computing platforms require embedded ML devices close to the network in order to compensate for communication cost and latency. Hence a natural question to ask is whether MVM operations are even necessary to implement ML algorithms and whether simpler hardware primitives can be used to implement an ultra-energy-efficient ML processor/architecture. In this paper we propose an alternate hardware-software codesign of ML and neural network architectures where instead of using MVM operations and non-linear activation functions, the architecture only uses simple addition and thresholding operations to implement inference and learning. At the core of the proposed approach is margin-propagation (MP) based computation that maps multiplications into additions and additions into a dynamic rectifying-linear-unit (ReLU) operations. This mapping results in significant improvement in computational and hence energy cost. In this paper, we show how the MP network formulation can be applied for designing linear classifiers, shallow multi-layer perceptrons and support vector networks suitable fot IoT platforms and tiny ML applications. We show that these MP based classifiers give comparable results to that of their traditional counterparts for benchmark UCI datasets, with the added advantage of reduction in computational complexity enabling an improvement in energy efficiency.",2019-10-05T18:09:57Z,2020-11-05T12:43:48Z,http://arxiv.org/abs/1910.02304v2,http://arxiv.org/pdf/1910.02304v2,"cs.LG, stat.ML"
Stabilizing DARTS with Amended Gradient Estimation on Architectural   Parameters,"Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian","DARTS is a popular algorithm for neural architecture search (NAS). Despite its great advantage in search efficiency, DARTS often suffers weak stability, which reflects in the large variation among individual trials as well as the sensitivity to the hyper-parameters of the search process. This paper owes such instability to an optimization gap between the super-network and its sub-networks, namely, improving the validation accuracy of the super-network does not necessarily lead to a higher expectation on the performance of the sampled sub-networks. Then, we point out that the gap is due to the inaccurate estimation of the architectural gradients, based on which we propose an amended estimation method. Mathematically, our method guarantees a bounded error from the true gradients while the original estimation does not. Our approach bridges the gap from two aspects, namely, amending the estimation on the architectural gradients, and unifying the hyper-parameter settings in the search and re-training stages. Experiments on CIFAR10 and ImageNet demonstrate that our approach largely improves search stability and, more importantly, enables DARTS-based approaches to explore much larger search spaces that have not been investigated before.",2019-10-25T16:31:25Z,2020-05-04T10:19:18Z,http://arxiv.org/abs/1910.11831v5,http://arxiv.org/pdf/1910.11831v5,"cs.LG, cs.CV, stat.ML"
PAPR-Limited Precoding in Massive MIMO Systems with Reflect- and   Transmit-Array Antennas,"Ali Bereyhi, Vahid Jamali, Ralf R. Müller, Georg Fischer, Robert Schober, Antonia M. Tulino","Conventional hybrid analog-digital architectures for millimeter-wave massive multiple-input multiple-output (MIMO) systems suffer from poor scalability and high implementational costs. The former is caused by the high power loss in the analog network, and the latter is due to the fact that classic MIMO transmission techniques require power amplifiers with high back-offs.   This paper proposes a novel hybrid analog-digital architecture which addresses both of these challenges. This architecture implements the analog front-end via a passive reflect- or transmit-array to resolve the scalability issue. To keep the system cost-efficient, a digital precoder is designed whose peak-to-average power ratio (PAPR) on each active antenna is tunable. Using the approximate message passing algorithm, this precoder is implemented with tractable computational complexity. The proposed architecture allows for the use of power amplifiers with low back-offs which reduces the overall radio frequency cost of the system. Numerical results demonstrate that for low PAPRs, significant performance enhancements are achieved compared to the state of the art.",2019-12-01T19:26:14Z,2019-12-01T19:26:14Z,http://arxiv.org/abs/1912.00485v1,http://arxiv.org/pdf/1912.00485v1,"cs.IT, eess.SP, math.IT"
Integration of Neural Network-Based Symbolic Regression in Deep Learning   for Scientific Discovery,"Samuel Kim, Peter Y. Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir Čeperić, Marin Soljačić","Symbolic regression is a powerful technique that can discover analytical equations that describe data, which can lead to explainable models and generalizability outside of the training data set. In contrast, neural networks have achieved amazing levels of accuracy on image recognition and natural language processing tasks, but are often seen as black-box models that are difficult to interpret and typically extrapolate poorly. Here we use a neural network-based architecture for symbolic regression called the Equation Learner (EQL) network and integrate it with other deep learning architectures such that the whole system can be trained end-to-end through backpropagation. To demonstrate the power of such systems, we study their performance on several substantially different tasks. First, we show that the neural network can perform symbolic regression and learn the form of several functions. Next, we present an MNIST arithmetic task where a separate part of the neural network extracts the digits. Finally, we demonstrate prediction of dynamical systems where an unknown parameter is extracted through an encoder. We find that the EQL-based architecture can extrapolate quite well outside of the training data set compared to a standard neural network-based architecture, paving the way for deep learning to be applied in scientific exploration and discovery.",2019-12-10T17:07:52Z,2020-08-13T18:40:43Z,http://arxiv.org/abs/1912.04825v2,http://arxiv.org/pdf/1912.04825v2,"cs.LG, cs.NE, physics.data-an, stat.ML"
Integration and Evaluation of Quantum Accelerators for Data-Driven User   Functions,"Thomas Hubregtsen, Christoph Segler, Josef Pichlmeier, Aritra Sarkar, Thomas Gabor, Koen Bertels","Quantum computers hold great promise for accelerating computationally challenging algorithms on noisy intermediate-scale quantum (NISQ) devices in the upcoming years. Much attention of the current research is directed to algorithmic research on artificial data that is disconnected from live systems, such as optimization of systems or training of learning algorithms. In this paper we investigate the integration of quantum systems into industry-grade system architectures. In this work we propose a system architecture for the integration of quantum accelerators. In order to evaluate our proposed system architecture we implemented various algorithms including a classical system, a gate-based quantum accelerator and a quantum annealer. This algorithm automates user habits using data-driven functions trained on real-world data. This also includes an evaluation of the quantum enhanced kernel, that previously was only evaluated on artificial data. In our evaluation, we showed that the quantum-enhanced kernel performs at least equally well to a classical state-of-the-art kernel. We also showed a low reduction in accuracy and latency numbers within acceptable bounds when running on the gate-based IBM quantum accelerator. We, therefore, conclude it is feasible to integrate NISQ-era devices in industry-grade system architecture in preparation for future hardware improvements.",2019-12-12T15:30:21Z,2020-01-25T16:27:44Z,http://arxiv.org/abs/1912.06032v2,http://arxiv.org/pdf/1912.06032v2,"quant-ph, cs.LG"
Deep learning architectures for nonlinear operator functions and   nonlinear inverse problems,"Maarten V. de Hoop, Matti Lassas, Christopher A. Wong","We develop a theoretical analysis for special neural network architectures, termed operator recurrent neural networks, for approximating nonlinear functions whose inputs are linear operators. Such functions commonly arise in solution algorithms for inverse boundary value problems. Traditional neural networks treat input data as vectors, and thus they do not effectively capture the multiplicative structure associated with the linear operators that correspond to the data in such inverse problems. We therefore introduce a new family that resembles a standard neural network architecture, but where the input data acts multiplicatively on vectors. Motivated by compact operators appearing in boundary control and the analysis of inverse boundary value problems for the wave equation, we promote structure and sparsity in selected weight matrices in the network. After describing this architecture, we study its representation properties as well as its approximation properties. We furthermore show that an explicit regularization can be introduced that can be derived from the mathematical analysis of the mentioned inverse problems, and which leads to certain guarantees on the generalization properties. We observe that the sparsity of the weight matrices improves the generalization estimates. Lastly, we discuss how operator recurrent networks can be viewed as a deep learning analogue to deterministic algorithms such as boundary control for reconstructing the unknown wavespeed in the acoustic wave equation from boundary measurements.",2019-12-23T20:05:47Z,2022-01-03T20:38:34Z,http://arxiv.org/abs/1912.11090v3,http://arxiv.org/pdf/1912.11090v3,"math.OC, cs.LG, 68T05, 35R30, 62M45"
An Analysis of the Expressiveness of Deep Neural Network Architectures   Based on Their Lipschitz Constants,"SiQi Zhou, Angela P. Schoellig","Deep neural networks (DNNs) have emerged as a popular mathematical tool for function approximation due to their capability of modelling highly nonlinear functions. Their applications range from image classification and natural language processing to learning-based control. Despite their empirical successes, there is still a lack of theoretical understanding of the representative power of such deep architectures. In this work, we provide a theoretical analysis of the expressiveness of fully-connected, feedforward DNNs with 1-Lipschitz activation functions. In particular, we characterize the expressiveness of a DNN by its Lipchitz constant. By leveraging random matrix theory, we show that, given sufficiently large and randomly distributed weights, the expected upper and lower bounds of the Lipschitz constant of a DNN and hence their expressiveness increase exponentially with depth and polynomially with width, which gives rise to the benefit of the depth of DNN architectures for efficient function approximation. This observation is consistent with established results based on alternative expressiveness measures of DNNs. In contrast to most of the existing work, our analysis based on the Lipschitz properties of DNNs is applicable to a wider range of activation nonlinearities and potentially allows us to make sensible comparisons between the complexity of a DNN and the function to be approximated by the DNN. We consider this work to be a step towards understanding the expressive power of DNNs and towards designing appropriate deep architectures for practical applications such as system control.",2019-12-24T20:00:26Z,2019-12-24T20:00:26Z,http://arxiv.org/abs/1912.11511v1,http://arxiv.org/pdf/1912.11511v1,"cs.LG, cs.RO, cs.SY, eess.SY, stat.ML"
Fault Diagnosis of the 10MW Floating Offshore Wind Turbine Benchmark: a   Mixed Model and Signal-based Approach,"Yichao Liu, Riccardo Ferrari, Ping Wu, Xiaoli Jiang, Sunwei Li, Jan-Willem van Wingerden","Floating Offshore Wind Turbines (FOWTs) operate in the harsh marine environment with limited accessibility and maintainability. Not only failures are more likely to occur than in land-based turbines, but also corrective maintenance is more expensive. In the present study, a mixed model and signal-based Fault Diagnosis (FD) architecture is developed to detect and isolate critical faults in FOWTs. More specifically, a model-based scheme is developed to detect and isolate the faults associated with the turbine system. It is based on a fault detection and approximation estimator and fault isolation estimators, with time-varying adaptive thresholds to guarantee against false-alarms. In addition, a signal-based scheme is established, within the proposed architecture, for detecting and isolating two representative mooring lines faults. For the purpose of verification, a 10MW FOWT benchmark is developed and its operating conditions, which contains predefined faults, are simulated by extending the high-fidelity simulator. Based on it, the effectiveness of the proposed architecture is illustrated. In addition, the advantages and limitations are discussed by comparing its fault detection to the results delivered by other approaches. Results show that the proposed architecture has the best performance in detecting and isolating the critical faults in FOWTs under diverse operating conditions.",2020-07-03T14:23:01Z,2020-07-03T14:23:01Z,http://arxiv.org/abs/2007.01708v1,http://arxiv.org/pdf/2007.01708v1,"eess.SY, cs.SY"
SOTERIA: In Search of Efficient Neural Networks for Private Inference,"Anshul Aggarwal, Trevor E. Carlson, Reza Shokri, Shruti Tople","ML-as-a-service is gaining popularity where a cloud server hosts a trained model and offers prediction (inference) service to users. In this setting, our objective is to protect the confidentiality of both the users' input queries as well as the model parameters at the server, with modest computation and communication overhead. Prior solutions primarily propose fine-tuning cryptographic methods to make them efficient for known fixed model architectures. The drawback with this line of approach is that the model itself is never designed to operate with existing efficient cryptographic computations. We observe that the network architecture, internal functions, and parameters of a model, which are all chosen during training, significantly influence the computation and communication overhead of a cryptographic method, during inference. Based on this observation, we propose SOTERIA -- a training method to construct model architectures that are by-design efficient for private inference. We use neural architecture search algorithms with the dual objective of optimizing the accuracy of the model and the overhead of using cryptographic primitives for secure inference. Given the flexibility of modifying a model during training, we find accurate models that are also efficient for private computation. We select garbled circuits as our underlying cryptographic primitive, due to their expressiveness and efficiency, but this approach can be extended to hybrid multi-party computation settings. We empirically evaluate SOTERIA on MNIST and CIFAR10 datasets, to compare with the prior work. Our results confirm that SOTERIA is indeed effective in balancing performance and accuracy.",2020-07-25T13:53:02Z,2020-07-25T13:53:02Z,http://arxiv.org/abs/2007.12934v1,http://arxiv.org/pdf/2007.12934v1,"cs.CR, cs.LG, stat.ML"
Weakly supervised one-stage vision and language disease detection using   large scale pneumonia and pneumothorax studies,"Leo K. Tam, Xiaosong Wang, Evrim Turkbey, Kevin Lu, Yuhong Wen, Daguang Xu","Detecting clinically relevant objects in medical images is a challenge despite large datasets due to the lack of detailed labels. To address the label issue, we utilize the scene-level labels with a detection architecture that incorporates natural language information. We present a challenging new set of radiologist paired bounding box and natural language annotations on the publicly available MIMIC-CXR dataset especially focussed on pneumonia and pneumothorax. Along with the dataset, we present a joint vision language weakly supervised transformer layer-selected one-stage dual head detection architecture (LITERATI) alongside strong baseline comparisons with class activation mapping (CAM), gradient CAM, and relevant implementations on the NIH ChestXray-14 and MIMIC-CXR dataset. Borrowing from advances in vision language architectures, the LITERATI method demonstrates joint image and referring expression (objects localized in the image using natural language) input for detection that scales in a purely weakly supervised fashion. The architectural modifications address three obstacles -- implementing a supervised vision and language detection method in a weakly supervised fashion, incorporating clinical referring expression natural language information, and generating high fidelity detections with map probabilities. Nevertheless, the challenging clinical nature of the radiologist annotations including subtle references, multi-instance specifications, and relatively verbose underlying medical reports, ensures the vision language detection task at scale remains stimulating for future investigation.",2020-07-31T00:04:14Z,2020-07-31T00:04:14Z,http://arxiv.org/abs/2007.15778v1,http://arxiv.org/pdf/2007.15778v1,"cs.CV, cs.LG, eess.IV"
Coordination Among Neural Modules Through a Shared Global Workspace,"Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, Yoshua Bengio","Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.",2021-03-01T18:43:48Z,2022-03-22T21:31:37Z,http://arxiv.org/abs/2103.01197v2,http://arxiv.org/pdf/2103.01197v2,"cs.LG, cs.AI, stat.ML"
The whole brain architecture approach: Accelerating the development of   artificial general intelligence by referring to the brain,Hiroshi Yamakawa,"The vastness of the design space created by the combination of a large number of computational mechanisms, including machine learning, is an obstacle to creating an artificial general intelligence (AGI). Brain-inspired AGI development, in other words, cutting down the design space to look more like a biological brain, which is an existing model of a general intelligence, is a promising plan for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain because the neuroscientific data required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA) -- the flow of information and the diagram of corresponding components -- and the task of developing each component using the BRA. This is called BRA-driven development. Another difficulty lies in the extraction of the operating principles necessary for reproducing the cognitive-behavioral function of the brain from neuroscience data. Therefore, this study proposes the Structure-constrained Interface Decomposition (SCID) method, which is a hypothesis-building method for creating a hypothetical component diagram consistent with neuroscientific findings. The application of this approach has begun for building various regions of the brain. Moving forward, we will examine methods of evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be merged, associated with the same regions of the brain.",2021-03-06T04:58:12Z,2021-03-06T04:58:12Z,http://arxiv.org/abs/2103.06123v1,http://arxiv.org/pdf/2103.06123v1,"cs.AI, cs.LG, cs.NE, q-bio.NC, I.2.0"
LSTMs and Deep Residual Networks for Carbohydrate and Bolus   Recommendations in Type 1 Diabetes Management,"Jeremy Beauchamp, Razvan Bunescu, Cindy Marling, Zhongen Li, Chang Liu","To avoid serious diabetic complications, people with type 1 diabetes must keep their blood glucose levels (BGLs) as close to normal as possible. Insulin dosages and carbohydrate consumption are important considerations in managing BGLs. Since the 1960s, models have been developed to forecast blood glucose levels based on the history of BGLs, insulin dosages, carbohydrate intake, and other physiological and lifestyle factors. Such predictions can be used to alert people of impending unsafe BGLs or to control insulin flow in an artificial pancreas. In past work, we have introduced an LSTM-based approach to blood glucose level prediction aimed at ""what if"" scenarios, in which people could enter foods they might eat or insulin amounts they might take and then see the effect on future BGLs. In this work, we invert the ""what-if"" scenario and introduce a similar architecture based on chaining two LSTMs that can be trained to make either insulin or carbohydrate recommendations aimed at reaching a desired BG level in the future. Leveraging a recent state-of-the-art model for time series forecasting, we then derive a novel architecture for the same recommendation task, in which the two LSTM chain is used as a repeating block inside a deep residual architecture. Experimental evaluations using real patient data from the OhioT1DM dataset show that the new integrated architecture compares favorably with the previous LSTM-based approach, substantially outperforming the baselines. The promising results suggest that this novel approach could potentially be of practical use to people with type 1 diabetes for self-management of BGLs.",2021-03-06T19:06:14Z,2021-03-06T19:06:14Z,http://arxiv.org/abs/2103.06708v1,http://arxiv.org/pdf/2103.06708v1,"cs.LG, I.2.1; J.3"
Free-form Design of Discrete Architectural Surfaces by use of Circle   Packing,"Shizuo Kaji, Jingyao Zhang","This paper presents an efficient approach for the conceptual design of architectural surfaces which are composed of triangular panels. In the free-form design of discrete architectural surfaces, the Gaussian curvature plays an important role not only aesthetically but also in terms of stiffness and constructability. However, designing a surface manually with specific Gaussian curvatures can be a time-consuming task. We propose a method to find a triangulated surface with user-specified Gaussian curvatures (not limited to constant Gaussian curvatures) and boundary vertex positions. In addition, the conformal class of the final design can be specified; that is, the user has control over the shape (the corner angles) of each triangular panel. The panels could be encouraged to form a regular tessellation or kept close to those of the initial design. The controllability of the conformal class suppresses possible distortion of the panels, resulting in higher structural performance and aesthetics. Our method relies on the idea in computational conformal geometry called circle packing. In this line of research, the discrete Ricci flow has been widely used for surface modelling. However, it is not trivial to incorporate constraints such as boundary locations and convexity of the spanned surface, which are essential to architectural applications. We propose a perturbation of the discrete Ricci energy and develop a least-squares-based optimisation scheme to address these problems with an open-source implementation available online.",2021-03-13T00:31:50Z,2022-05-11T08:01:41Z,http://arxiv.org/abs/2103.07584v2,http://arxiv.org/pdf/2103.07584v2,"cs.CG, I.3.5; J.6"
Generalization capabilities of translationally equivariant neural   networks,"Srinath Bulusu, Matteo Favoni, Andreas Ipp, David I. Müller, Daniel Schuh","The rising adoption of machine learning in high energy physics and lattice field theory necessitates the re-evaluation of common methods that are widely used in computer vision, which, when applied to problems in physics, can lead to significant drawbacks in terms of performance and generalizability. One particular example for this is the use of neural network architectures that do not reflect the underlying symmetries of the given physical problem. In this work, we focus on complex scalar field theory on a two-dimensional lattice and investigate the benefits of using group equivariant convolutional neural network architectures based on the translation group. For a meaningful comparison, we conduct a systematic search for equivariant and non-equivariant neural network architectures and apply them to various regression and classification tasks. We demonstrate that in most of these tasks our best equivariant architectures can perform and generalize significantly better than their non-equivariant counterparts, which applies not only to physical parameters beyond those represented in the training set, but also to different lattice sizes.",2021-03-26T18:53:36Z,2021-10-11T14:38:26Z,http://arxiv.org/abs/2103.14686v3,http://arxiv.org/pdf/2103.14686v3,"hep-lat, cs.LG, hep-ph, stat.ML"
Coded Caching with Heterogenous Cache Sizes,"Sinong Wang, Wenxin Li, Xiaohua Tian, Hui Liu",We investigate the coded caching scheme under heterogenous cache sizes.,2015-04-05T13:53:57Z,2015-08-29T04:09:16Z,http://arxiv.org/abs/1504.01123v3,http://arxiv.org/pdf/1504.01123v3,"cs.IT, cs.NI, math.IT"
Mixed-ADC Massive MIMO,"Ning Liang, Wenyi Zhang","Motivated by the demand for energy-efficient communication solutions in the next generation cellular network, a mixed-ADC architecture for massive multiple input multiple output (MIMO) systems is proposed, which differs from previous works in that herein one-bit analog-to-digital converters (ADCs) partially replace the conventionally assumed high-resolution ADCs. The information-theoretic tool of generalized mutual information (GMI) is exploited to analyze the achievable data rates of the proposed system architecture and an array of analytical results of engineering interest are obtained. For fixed single input multiple output (SIMO) channels, a closed-form expression of the GMI is derived, based on which the linear combiner is optimized. The analysis is then extended to ergodic fading channels, for which tight lower and upper bounds of the GMI are obtained. Impacts of dithering and imperfect channel state information (CSI) are also investigated, and it is shown that dithering can remarkably improve the system performance while imperfect CSI only introduces a marginal rate loss. Finally, the analytical framework is applied to the multi-user access scenario. Numerical results demonstrate that the mixed-ADC architecture with a relatively small number of high-resolution ADCs is able to achieve a large fraction of the channel capacity of conventional architecture, while reduce the energy consumption considerably even compared with antenna selection, for both single-user and multi-user scenarios.",2015-04-14T12:30:24Z,2015-12-23T08:12:10Z,http://arxiv.org/abs/1504.03516v2,http://arxiv.org/pdf/1504.03516v2,"cs.IT, math.IT"
RWTH ASR Systems for LibriSpeech: Hybrid vs Attention -- w/o Data   Augmentation,"Christoph Lüscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel, Albert Zeyer, Ralf Schlüter, Hermann Ney","We present state-of-the-art automatic speech recognition (ASR) systems employing a standard hybrid DNN/HMM architecture compared to an attention-based encoder-decoder design for the LibriSpeech task. Detailed descriptions of the system development, including model design, pretraining schemes, training schedules, and optimization approaches are provided for both system architectures. Both hybrid DNN/HMM and attention-based systems employ bi-directional LSTMs for acoustic modeling/encoding. For language modeling, we employ both LSTM and Transformer based architectures. All our systems are built using RWTHs open-source toolkits RASR and RETURNN. To the best knowledge of the authors, the results obtained when training on the full LibriSpeech training set, are the best published currently, both for the hybrid DNN/HMM and the attention-based systems. Our single hybrid system even outperforms previous results obtained from combining eight single systems. Our comparison shows that on the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the attention-based system by 15% relative on the clean and 40% relative on the other test sets in terms of word error rate. Moreover, experiments on a reduced 100h-subset of the LibriSpeech training corpus even show a more pronounced margin between the hybrid DNN/HMM and attention-based architectures.",2019-05-08T13:57:28Z,2019-07-25T15:49:22Z,http://arxiv.org/abs/1905.03072v3,http://arxiv.org/pdf/1905.03072v3,"cs.CL, cs.SD, eess.AS"
Benchmarking Deep Learning Architectures for Predicting Readmission to   the ICU and Describing Patients-at-Risk,"Sebastiano Barbieri, James Kemp, Oscar Perez-Concha, Sradha Kotwal, Martin Gallagher, Angus Ritchie, Louisa Jorm","Objective: To compare different deep learning architectures for predicting the risk of readmission within 30 days of discharge from the intensive care unit (ICU). The interpretability of attention-based models is leveraged to describe patients-at-risk. Methods: Several deep learning architectures making use of attention mechanisms, recurrent layers, neural ordinary differential equations (ODEs), and medical concept embeddings with time-aware attention were trained using publicly available electronic medical record data (MIMIC-III) associated with 45,298 ICU stays for 33,150 patients. Bayesian inference was used to compute the posterior over weights of an attention-based model. Odds ratios associated with an increased risk of readmission were computed for static variables. Diagnoses, procedures, medications, and vital signs were ranked according to the associated risk of readmission. Results: A recurrent neural network, with time dynamics of code embeddings computed by neural ODEs, achieved the highest average precision of 0.331 (AUROC: 0.739, F1-Score: 0.372). Predictive accuracy was comparable across neural network architectures. Groups of patients at risk included those suffering from infectious complications, with chronic or progressive conditions, and for whom standard medical care was not suitable. Conclusions: Attention-based networks may be preferable to recurrent networks if an interpretable model is required, at only marginal cost in predictive accuracy.",2019-05-21T11:08:31Z,2020-01-07T00:12:36Z,http://arxiv.org/abs/1905.08547v3,http://arxiv.org/pdf/1905.08547v3,"cs.LG, stat.ML"
N-BEATS: Neural basis expansion analysis for interpretable time series   forecasting,"Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio","We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.",2019-05-24T20:28:57Z,2020-02-20T21:08:57Z,http://arxiv.org/abs/1905.10437v4,http://arxiv.org/pdf/1905.10437v4,"cs.LG, stat.ML"
Let's Agree to Agree: Neural Networks Share Classification Order on Real   Datasets,"Guy Hacohen, Leshem Choshen, Daphna Weinshall","We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries -- models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Thus, when fixing the architecture, we show synthetic datasets where this pattern ceases to exist. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.",2019-05-26T18:51:22Z,2020-07-20T11:28:16Z,http://arxiv.org/abs/1905.10854v7,http://arxiv.org/pdf/1905.10854v7,"cs.LG, stat.ML"
A Competitive Edge: Can FPGAs Beat GPUs at DCNN Inference Acceleration   in Resource-Limited Edge Computing Applications?,"Ian Colbert, Jake Daly, Ken Kreutz-Delgado, Srinjoy Das","When trained as generative models, Deep Learning algorithms have shown exceptional performance on tasks involving high dimensional data such as image denoising and super-resolution. In an increasingly connected world dominated by mobile and edge devices, there is surging demand for these algorithms to run locally on embedded platforms. FPGAs, by virtue of their reprogrammability and low-power characteristics, are ideal candidates for these edge computing applications. As such, we design a spatio-temporally parallelized hardware architecture capable of accelerating a deconvolution algorithm optimized for power-efficient inference on a resource-limited FPGA. We propose this FPGA-based accelerator to be used for Deconvolutional Neural Network (DCNN) inference in low-power edge computing applications. To this end, we develop methods that systematically exploit micro-architectural innovations, design space exploration, and statistical analysis. Using a Xilinx PYNQ-Z2 FPGA, we leverage our architecture to accelerate inference for two DCNNs trained on the MNIST and CelebA datasets using the Wasserstein GAN framework. On these networks, our FPGA design achieves a higher throughput to power ratio with lower run-to-run variation when compared to the NVIDIA Jetson TX1 edge computing GPU.",2021-01-30T19:20:14Z,2021-03-09T05:19:30Z,http://arxiv.org/abs/2102.00294v2,http://arxiv.org/pdf/2102.00294v2,"cs.DC, cs.AR, eess.IV, eess.SP"
Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition,"Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, Rong Jin","Accuracy predictor is a key component in Neural Architecture Search (NAS) for ranking architectures. Building a high-quality accuracy predictor usually costs enormous computation. To address this issue, instead of using an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of Zen-Score only takes a few forward inferences through a randomly initialized network, without training network parameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network under given inference budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multiple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Our source code and pre-trained models are released on https://github.com/idstcv/ZenNAS.",2021-02-01T18:53:40Z,2021-08-23T02:33:29Z,http://arxiv.org/abs/2102.01063v4,http://arxiv.org/pdf/2102.01063v4,"cs.CV, cs.LG, 68T07, 65D19, I.2.10; J.6; I.4.0; I.5.2"
LoRD-Net: Unfolded Deep Detection Network with Low-Resolution Receivers,"Shahin Khobahi, Nir Shlezinger, Mojtaba Soltanalian, Yonina C. Eldar","The need to recover high-dimensional signals from their noisy low-resolution quantized measurements is widely encountered in communications and sensing. In this paper, we focus on the extreme case of one-bit quantizers, and propose a deep detector entitled LoRD-Net for recovering information symbols from one-bit measurements. Our method is a model-aware data-driven architecture based on deep unfolding of first-order optimization iterations. LoRD-Net has a task-based architecture dedicated to recovering the underlying signal of interest from the one-bit noisy measurements without requiring prior knowledge of the channel matrix through which the one-bit measurements are obtained. The proposed deep detector has much fewer parameters compared to black-box deep networks due to the incorporation of domain-knowledge in the design of its architecture, allowing it to operate in a data-driven fashion while benefiting from the flexibility, versatility, and reliability of model-based optimization methods. LoRD-Net operates in a blind fashion, which requires addressing both the non-linear nature of the data-acquisition system as well as identifying a proper optimization objective for signal recovery. Accordingly, we propose a two-stage training method for LoRD-Net, in which the first stage is dedicated to identifying the proper form of the optimization process to unfold, while the latter trains the resulting model in an end-to-end manner. We numerically evaluate the proposed receiver architecture for one-bit signal recovery in wireless communications and demonstrate that the proposed hybrid methodology outperforms both data-driven and model-based state-of-the-art methods, while utilizing small datasets, on the order of merely $\sim 500$ samples, for training.",2021-02-05T04:26:05Z,2021-02-05T04:26:05Z,http://arxiv.org/abs/2102.02993v1,http://arxiv.org/pdf/2102.02993v1,"eess.SP, cs.LG, math.OC"
Safe-visor Architecture for Sandboxing (AI-based) Unverified Controllers   in Stochastic Cyber-Physical Systems,"Bingzhuo Zhong, Abolfazl Lavaei, Hongpeng Cao, Majid Zamani, Marco Caccamo","High performance but unverified controllers, e.g., artificial intelligence-based (a.k.a. AI-based) controllers, are widely employed in cyber-physical systems (CPSs) to accomplish complex control missions. However, guaranteeing the safety and reliability of CPSs with this kind of controllers is currently very challenging, which is of vital importance in many real-life safety-critical applications. To cope with this difficulty, we propose in this work a Safe-visor architecture for sandboxing unverified controllers in CPSs operating in noisy environments (a.k.a. stochastic CPSs). The proposed architecture contains a history-based supervisor, which checks inputs from the unverified controller and makes a compromise between functionality and safety of the system, and a safety advisor that provides fallback when the unverified controller endangers the safety of the system. Both the history-based supervisor and the safety advisor are designed based on an approximate probabilistic relation between the original system and its finite abstraction. By employing this architecture, we provide formal probabilistic guarantees on preserving the safety specifications expressed by accepting languages of deterministic finite automata (DFA). Meanwhile, the unverified controllers can still be employed in the control loop even though they are not reliable. We demonstrate the effectiveness of our proposed results by applying them to two (physical) case studies.",2021-02-10T15:29:26Z,2021-08-19T20:21:05Z,http://arxiv.org/abs/2102.05490v2,http://arxiv.org/pdf/2102.05490v2,"eess.SY, cs.SY"
Searching for Fast Model Families on Datacenter Accelerators,"Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun Cheng, Quoc Le, Norman P. Jouppi","Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. However, as neither NAS nor model scaling considers sufficient hardware architecture details, they do not take full advantage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efficient inference on DC accelerators. We first analyze DC accelerators and find that existing CNNs suffer from insufficient operational intensity, parallelism, and execution efficiency. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise convolutions, and block-wise activation functions. On top of our DC accelerator optimized neural architecture search space, we further propose a latency-aware compound scaling (LACS), the first multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and network width, which is quite different from previous compound scaling results. With the new search space and LACS, our search and scaling on datacenter accelerators results in a new model series named EfficientNet-X. EfficientNet-X is up to more than 2X faster than EfficientNet (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet and ResNeSt on TPUv3 and GPUv100.",2021-02-10T18:15:40Z,2021-02-10T18:15:40Z,http://arxiv.org/abs/2102.05610v1,http://arxiv.org/pdf/2102.05610v1,"cs.CV, eess.IV"
Orchestrated Trios: Compiling for Efficient Communication in Quantum   Programs with 3-Qubit Gates,"Casey Duckering, Jonathan M. Baker, Andrew Litteken, Frederic T. Chong","Current quantum computers are especially error prone and require high levels of optimization to reduce operation counts and maximize the probability the compiled program will succeed. These computers only support operations decomposed into one- and two-qubit gates and only two-qubit gates between physically connected pairs of qubits. Typical compilers first decompose operations, then route data to connected qubits. We propose a new compiler structure, Orchestrated Trios, that first decomposes to the three-qubit Toffoli, routes the inputs of the higher-level Toffoli operations to groups of nearby qubits, then finishes decomposition to hardware-supported gates.   This significantly reduces communication overhead by giving the routing pass access to the higher-level structure of the circuit instead of discarding it. A second benefit is the ability to now select an architecture-tuned Toffoli decomposition such as the 8-CNOT Toffoli for the specific hardware qubits now known after the routing pass. We perform real experiments on IBM Johannesburg showing an average 35% decrease in two-qubit gate count and 23% increase in success rate of a single Toffoli over Qiskit. We additionally compile many near-term benchmark algorithms showing an average 344% increase in (or 4.44x) simulated success rate on the Johannesburg architecture and compare with other architecture types.",2021-02-16T21:06:58Z,2021-02-16T21:06:58Z,http://arxiv.org/abs/2102.08451v1,http://arxiv.org/pdf/2102.08451v1,"quant-ph, cs.ET"
Learning with invariances in random features and kernel models,"Song Mei, Theodor Misiakiewicz, Andrea Montanari","A number of machine learning tasks entail a high degree of invariance: the data distribution does not change if we act on the data with a certain group of transformations. For instance, labels of images are invariant under translations of the images. Certain neural network architectures -- for instance, convolutional networks -- are believed to owe their success to the fact that they exploit such invariance properties. With the objective of quantifying the gain achieved by invariant architectures, we introduce two classes of models: invariant random features and invariant kernel methods. The latter includes, as a special case, the neural tangent kernel for convolutional networks with global average pooling. We consider uniform covariates distributions on the sphere and hypercube and a general invariant target function. We characterize the test error of invariant methods in a high-dimensional regime in which the sample size and number of hidden units scale as polynomials in the dimension, for a class of groups that we call `degeneracy $\alpha$', with $\alpha \leq 1$. We show that exploiting invariance in the architecture saves a $d^\alpha$ factor ($d$ stands for the dimension) in sample size and number of hidden units to achieve the same test error as for unstructured architectures.   Finally, we show that output symmetrization of an unstructured kernel estimator does not give a significant statistical improvement; on the other hand, data augmentation with an unstructured kernel estimator is equivalent to an invariant kernel estimator and enjoys the same improvement in statistical efficiency.",2021-02-25T23:06:21Z,2021-02-25T23:06:21Z,http://arxiv.org/abs/2102.13219v1,http://arxiv.org/pdf/2102.13219v1,"stat.ML, cs.LG, math.ST, stat.TH, 62J99 (Primary)"
Design and simulation of a sigma delta ADC,Moslem Rashidi,In this report we describe the design and simulation of a Sigma Delta ADC in Matlan/Simulink,2010-11-17T23:56:51Z,2012-07-24T00:58:43Z,http://arxiv.org/abs/1011.4109v2,http://arxiv.org/pdf/1011.4109v2,"cs.IT, cs.AR, math.IT"
Resource Allocation in NOMA based Fog Radio Access Networks,"H. Zhang, Y. Qiu, K. Long, G. K. Karagiannidis, X. Wang, A. Nallanathan","In the wake of growth in intelligent mobile devices and wide usage of bandwidth-hungry applications of mobile Internet, the demand of wireless data traffic and ubiquitous mobile broadband is rapidly increasing. On account of these developments, the research on fifth generation (5G) networks presents an accelerative tendency on a global scale. Edge computing draw lots of attention for reducing the time delay and improving the Quality of Service for the networks. While, fog radio access networks (F-RANs) is an emergent architecture, which takes full use of edge computing and distributed storing capabilities in edge devices. In this article, we propose an architecture of non-orthogonal multiple access (NOMA) based F-RANs, which has a strong capability of edge computing and can meet the heterogeneous requirements in 5G systems. NOMA with successive interference cancellation (SIC) is regarded as a critical multi-user access technology. In NOMA, more than one user can access the same time, code domain, and frequency resources. With assigning different power levels to multi-user and implementing SIC, multiple users detection can be achieved. In this article, we provide a description of the NOMA based F-RANs architecture, and discuss the resource allocation in that. We will focus on the power and subchannel allocation in consideration of using NOMA and the edge caching. Simulation results show that the proposed NOMA baesd F-RANs architecture and the resource management mechanisms can achieve the high net utility for the RANs.",2018-03-15T08:54:34Z,2018-03-15T08:54:34Z,http://arxiv.org/abs/1803.05641v1,http://arxiv.org/pdf/1803.05641v1,"cs.IT, math.IT"
Bayesian Optimal Data Detector for Hybrid mmWave MIMO-OFDM Systems with   Low-Resolution ADCs,"Hengtao He, Chao-Kai Wen, Shi Jin","Hybrid analog-digital precoding architectures and low-resolution analog-to-digital converter (ADC) receivers are two solutions to reduce hardware cost and power consumption for millimeter wave (mmWave) multiple-input multiple-output (MIMO) communication systems with large antenna arrays. In this study, we consider a mmWave MIMO-OFDM receiver with a generalized hybrid architecture in which a small number of radio-frequency (RF) chains and low-resolution ADCs are employed simultaneously. Owing to the strong nonlinearity introduced by low-resolution ADCs, the task of data detection is challenging, particularly achieving a Bayesian optimal data detector. This study aims to fill this gap. By using generalized expectation consistent signal recovery technique, we propose a computationally efficient data detection algorithm that provides a minimum mean-square error estimate on data symbols and is extended to a mixed-ADC architecture. Considering particular structure of MIMO-OFDM channel matirx, we provide a lowcomplexity realization in which only FFT operation and matrixvector multiplications are required. Furthermore, we present an analytical framework to study the theoretical performance of the detector in the large-system limit, which can precisely evaluate the performance expressions such as mean-square error and symbol error rate. Based on this optimal detector, the potential of adding a few low-resolution RF chains and high-resolution ADCs for mixed-ADC architecture is investigated. Simulation results confirm the accuracy of our theoretical analysis and can be used for system design rapidly. The results reveal that adding a few low-resolution RF chains to original unquantized systems can obtain significant gains.",2018-03-25T01:55:13Z,2018-03-25T01:55:13Z,http://arxiv.org/abs/1803.09190v1,http://arxiv.org/pdf/1803.09190v1,"cs.IT, math.IT"
Brainstorming Generative Adversarial Networks (BGANs): Towards   Multi-Agent Generative Models with Distributed Private Datasets,"Aidin Ferdowsi, Walid Saad","To achieve a high learning accuracy, generative adversarial networks (GANs) must be fed by large datasets that adequately represent the data space. However, in many scenarios, the available datasets may be limited and distributed across multiple agents, each of which is seeking to learn the distribution of the data on its own. In such scenarios, the agents often do not wish to share their local data as it can cause communication overhead for large datasets. In this paper, to address this multi-agent GAN problem, a novel brainstorming GAN (BGAN) architecture is proposed using which multiple agents can generate real-like data samples while operating in a fully distributed manner. BGAN allows the agents to gain information from other agents without sharing their real datasets but by ``brainstorming'' via the sharing of their generated data samples. In contrast to existing distributed GAN solutions, the proposed BGAN architecture is designed to be fully distributed, and it does not need any centralized controller. Moreover, BGANs are shown to be scalable and not dependent on the hyperparameters of the agents' deep neural networks (DNNs) thus enabling the agents to have different DNN architectures. Theoretically, the interactions between BGAN agents are analyzed as a game whose unique Nash equilibrium is derived. Experimental results show that BGAN can generate real-like data samples with higher quality and lower Jensen-Shannon divergence (JSD) and Fr\`echet Inception distance (FID) compared to other distributed GAN architectures.",2020-02-02T02:58:32Z,2023-09-25T01:47:15Z,http://arxiv.org/abs/2002.00306v3,http://arxiv.org/pdf/2002.00306v3,"cs.LG, cs.DC, stat.ML"
Co-Exploration of Neural Architectures and Heterogeneous ASIC   Accelerator Designs Targeting Multiple Tasks,"Lei Yang, Zheyu Yan, Meng Li, Hyoukjun Kwon, Liangzhen Lai, Tushar Krishna, Vikas Chandra, Weiwen Jiang, Yiyu Shi","Neural Architecture Search (NAS) has demonstrated its power on various AI accelerating platforms such as Field Programmable Gate Arrays (FPGAs) and Graphic Processing Units (GPUs). However, it remains an open problem, how to integrate NAS with Application-Specific Integrated Circuits (ASICs), despite them being the most powerful AI accelerating platforms. The major bottleneck comes from the large design freedom associated with ASIC designs. Moreover, with the consideration that multiple DNNs will run in parallel for different workloads with diverse layer operations and sizes, integrating heterogeneous ASIC sub-accelerators for distinct DNNs in one design can significantly boost performance, and at the same time further complicate the design space. To address these challenges, in this paper we build ASIC template set based on existing successful designs, described by their unique dataflows, so that the design space is significantly reduced. Based on the templates, we further propose a framework, namely NASAIC, which can simultaneously identify multiple DNN architectures and the associated heterogeneous ASIC accelerator design, such that the design specifications (specs) can be satisfied, while the accuracy can be maximized. Experimental results show that compared with successive NAS and ASIC design optimizations which lead to design spec violations, NASAIC can guarantee the results to meet the design specs with 17.77%, 2.49x, and 2.32x reductions on latency, energy, and area and with 0.76% accuracy loss. To the best of the authors' knowledge, this is the first work on neural architecture and ASIC accelerator design co-exploration.",2020-02-10T22:22:19Z,2020-02-10T22:22:19Z,http://arxiv.org/abs/2002.04116v1,http://arxiv.org/pdf/2002.04116v1,"cs.LG, eess.SP, stat.ML"
An Inductive Bias for Distances: Neural Nets that Respect the Triangle   Inequality,"Silviu Pitis, Harris Chan, Kiarash Jamali, Jimmy Ba","Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically--to prove convergence and optimality guarantees--and empirically--as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to model two broad classes of subadditive distances, common in graphs and reinforcement learning: asymmetric metrics, and metrics that cannot be embedded into Euclidean space. To address these problems, we introduce novel architectures that are guaranteed to satisfy the triangle inequality. We prove our architectures universally approximate norm-induced metrics on $\mathbb{R}^n$, and present a similar result for modified Input Convex Neural Networks. We show that our architectures outperform existing metric approaches when modeling graph distances and have a better inductive bias than non-metric approaches when training data is limited in the multi-goal reinforcement learning setting.",2020-02-14T00:47:31Z,2020-07-06T20:06:56Z,http://arxiv.org/abs/2002.05825v3,http://arxiv.org/pdf/2002.05825v3,"cs.LG, stat.ML"
Identifying Critical Neurons in ANN Architectures using Mixed Integer   Programming,"Mostafa ElAraby, Guy Wolf, Margarida Carvalho","We introduce a mixed integer program (MIP) for assigning importance scores to each neuron in deep neural network architectures which is guided by the impact of their simultaneous pruning on the main learning task of the network. By carefully devising the objective function of the MIP, we drive the solver to minimize the number of critical neurons (i.e., with high importance score) that need to be kept for maintaining the overall accuracy of the trained neural network. Further, the proposed formulation generalizes the recently considered lottery ticket optimization by identifying multiple ""lucky"" sub-networks resulting in optimized architecture that not only performs well on a single dataset, but also generalizes across multiple ones upon retraining of network weights. Finally, we present a scalable implementation of our method by decoupling the importance scores across layers using auxiliary networks. We demonstrate the ability of our formulation to prune neural networks with marginal loss in accuracy and generalizability on popular datasets and architectures.",2020-02-17T21:32:47Z,2020-09-07T16:39:50Z,http://arxiv.org/abs/2002.07259v4,http://arxiv.org/pdf/2002.07259v4,"cs.LG, cs.NE, math.OC, stat.ML"
Intelligent and Reconfigurable Architecture for KL Divergence Based   Online Machine Learning Algorithm,"S. V. Sai Santosh, Sumit J. Darak","Online machine learning (OML) algorithms do not need any training phase and can be deployed directly in an unknown environment. OML includes multi-armed bandit (MAB) algorithms that can identify the best arm among several arms by achieving a balance between exploration of all arms and exploitation of optimal arm. The Kullback-Leibler divergence based upper confidence bound (KLUCB) is the state-of-the-art MAB algorithm that optimizes exploration-exploitation trade-off but it is complex due to underlining optimization routine. This limits its usefulness for robotics and radio applications which demand integration of KLUCB with the PHY on the system on chip (SoC). In this paper, we efficiently map the KLUCB algorithm on SoC by realizing optimization routine via alternative synthesizable computation without compromising on the performance. The proposed architecture is dynamically reconfigurable such that the number of arms, as well as type of algorithm, can be changed on-the-fly. Specifically, after initial learning, on-the-fly switch to light-weight UCB offers around 10-factor improvement in latency and throughput. Since learning duration depends on the unknown arm statistics, we offer intelligence embedded in architecture to decide the switching instant. We validate the functional correctness and usefulness of the proposed architecture via a realistic wireless application and detailed complexity analysis demonstrates its feasibility in realizing intelligent radios.",2020-02-18T16:39:57Z,2020-02-18T16:39:57Z,http://arxiv.org/abs/2002.07713v1,http://arxiv.org/pdf/2002.07713v1,"eess.SP, cs.LG"
Honing and proofing Astrophysical codes on the road to Exascale.   Experiences from code modernization on many-core systems,"Salvatore Cielo, Luigi Iapichino, Fabio Baruffa, Matteo Bugli, Christoph Federrath","The complexity of modern and upcoming computing architectures poses severe challenges for code developers and application specialists, and forces them to expose the highest possible degree of parallelism, in order to make the best use of the available hardware. The Intel$^{(R)}$ Xeon Phi$^{(TM)}$ of second generation (code-named Knights Landing, henceforth KNL) is the latest many-core system, which implements several interesting hardware features like for example a large number of cores per node (up to 72), the 512 bits-wide vector registers and the high-bandwidth memory. The unique features of KNL make this platform a powerful testbed for modern HPC applications. The performance of codes on KNL is therefore a useful proxy of their readiness for future architectures. In this work we describe the lessons learnt during the optimisation of the widely used codes for computational astrophysics P-Gadget-3, Flash and Echo. Moreover, we present results for the visualisation and analysis tools VisIt and yt. These examples show that modern architectures benefit from code optimisation at different levels, even more than traditional multi-core systems. However, the level of modernisation of typical community codes still needs improvements, for them to fully utilise resources of novel architectures.",2020-02-19T13:18:01Z,2020-02-19T13:18:01Z,http://arxiv.org/abs/2002.08161v1,http://arxiv.org/pdf/2002.08161v1,"cs.DC, astro-ph.IM, cs.PF, physics.comp-ph, C.0; D.4.8; J.0; J.2"
Investigating the interaction between gradient-only line searches and   different activation functions,"D. Kafka, Daniel. N. Wilke","Gradient-only line searches (GOLS) adaptively determine step sizes along search directions for discontinuous loss functions resulting from dynamic mini-batch sub-sampling in neural network training. Step sizes in GOLS are determined by localizing Stochastic Non-Negative Associated Gradient Projection Points (SNN-GPPs) along descent directions. These are identified by a sign change in the directional derivative from negative to positive along a descent direction. Activation functions are a significant component of neural network architectures as they introduce non-linearities essential for complex function approximations. The smoothness and continuity characteristics of the activation functions directly affect the gradient characteristics of the loss function to be optimized. Therefore, it is of interest to investigate the relationship between activation functions and different neural network architectures in the context of GOLS. We find that GOLS are robust for a range of activation functions, but sensitive to the Rectified Linear Unit (ReLU) activation function in standard feedforward architectures. The zero-derivative in ReLU's negative input domain can lead to the gradient-vector becoming sparse, which severely affects training. We show that implementing architectural features such as batch normalization and skip connections can alleviate these difficulties and benefit training with GOLS for all activation functions considered.",2020-02-23T12:28:27Z,2020-02-23T12:28:27Z,http://arxiv.org/abs/2002.09889v1,http://arxiv.org/pdf/2002.09889v1,"stat.ML, cs.LG, math.OC, 90C15, 90C59, 90C30, 90C26, 90C56, I.2.6"
A Hardware Architecture for Reconfigurable Intelligent Surfaces with   Minimal Active Elements for Explicit Channel Estimation,"George C. Alexandropoulos, Evangelos Vlachos","Intelligent surfaces comprising of cost effective, nearly passive, and reconfigurable unit elements are lately gaining increasing interest due to their potential in enabling fully programmable wireless environments. They are envisioned to offer environmental intelligence for diverse communication objectives, when coated on various objects of the deployment area of interest. To achieve this overarching goal, the channels where the Reconfigurable Intelligent Surfaces (RISs) are involved need to be in principle estimated. However, this is a challenging task with the currently available hardware RIS architectures requiring lengthy training periods among the network nodes utilizing RIS-assisted wireless communication. In this paper, we present a novel RIS architecture comprising of any number of passive reflecting elements, a simple controller for their adjustable configuration, and a single Radio Frequency (RF) chain for baseband measurements. Capitalizing on this architecture and assuming sparse wireless channels in the beamspace domain, we present an alternating optimization approach for explicit estimation of the channel gains at the RIS elements attached to the single RF chain. Representative simulation results demonstrate the channel estimation accuracy and achievable end-to-end performance for various training lengths and numbers of reflecting unit elements.",2020-02-24T16:55:59Z,2022-05-25T15:55:12Z,http://arxiv.org/abs/2002.10371v2,http://arxiv.org/pdf/2002.10371v2,"cs.IT, eess.SP, math.IT"
Full Duplex Hybrid A/D Beamforming with Reduced Complexity Multi-Tap   Analog Cancellation,"George C. Alexandropoulos, Md Atiqul Islam, Besma Smida","Although the hardware complexity of the analog self-interference canceller in full duplex Multiple Input Multiple Output (MIMO) designs does not necessarily scale with the number of transceiver antennas, exploiting the benefits of analog cancellation in massive MIMO systems with hundreds of antenna elements is still quite impractical. Hybrid Analog and Digital (A/D) beamforming architectures have been lately considered as a candidate technology for realizing massive MIMO transceivers with very large number of antenna elements, but with much fewer numbers of Radio Frequency (RF) chains. In this paper, we present a novel architecture for full duplex hybrid A/D beamforming transceivers including multi-tap analog cancellation with reduced number of taps and simple multiplexers for efficient signal routing among the transceiver RF chains. Capitalizing on the proposed transceiver architecture, we present a joint design of analog cancellation and A/D beamforming with the objective to maximize the achievable full duplex rate performance. Representative millimeter wave simulation results demonstrate the effectiveness of the proposed architecture and algorithmic framework for enabling simultaneous uplink and downlink communications with reduced complexity analog self-interference cancellation.",2020-02-26T23:12:29Z,2020-05-04T05:27:57Z,http://arxiv.org/abs/2002.11837v2,http://arxiv.org/pdf/2002.11837v2,"cs.IT, eess.SP, math.IT"
Deep Randomized Neural Networks,"Claudio Gallicchio, Simone Scardapane","Randomized Neural Networks explore the behavior of neural systems where the majority of connections are fixed, either in a stochastic or a deterministic fashion. Typical examples of such systems consist of multi-layered neural network architectures where the connections to the hidden layer(s) are left untrained after initialization. Limiting the training algorithms to operate on a reduced set of weights inherently characterizes the class of Randomized Neural Networks with a number of intriguing features. Among them, the extreme efficiency of the resulting learning processes is undoubtedly a striking advantage with respect to fully trained architectures. Besides, despite the involved simplifications, randomized neural systems possess remarkable properties both in practice, achieving state-of-the-art results in multiple domains, and theoretically, allowing to analyze intrinsic properties of neural architectures (e.g. before training of the hidden layers' connections). In recent years, the study of Randomized Neural Networks has been extended towards deep architectures, opening new research directions to the design of effective yet extremely efficient deep learning models in vectorial as well as in more complex data domains. This chapter surveys all the major aspects regarding the design and analysis of Randomized Neural Networks, and some of the key results with respect to their approximation capabilities. In particular, we first introduce the fundamentals of randomized neural models in the context of feed-forward networks (i.e., Random Vector Functional Link and equivalent models) and convolutional filters, before moving to the case of recurrent systems (i.e., Reservoir Computing networks). For both, we focus specifically on recent results in the domain of deep randomized systems, and (for recurrent models) their application to structured domains.",2020-02-27T17:57:58Z,2021-02-02T15:19:10Z,http://arxiv.org/abs/2002.12287v2,http://arxiv.org/pdf/2002.12287v2,"cs.LG, cs.NE, stat.ML"
SNR-Based Features and Diverse Training Data for Robust DNN-Based Speech   Enhancement,"Robert Rehr, Timo Gerkmann","In this paper, we address the generalization of deep neural network (DNN) based speech enhancement to unseen noise conditions for the case that training data is limited in size and diversity. To gain more insights, we analyze the generalization with respect to (1) the size and diversity of the training data, (2) different network architectures, and (3) the chosen features. To address (1), we train networks on the Hu noise corpus (limited size), the CHiME 3 noise corpus (limited diversity) and also propose a large and diverse dataset collected based on freely available sounds. To address (2), we compare a fully-connected feed-forward and a long short-term memory (LSTM) architecture. To address (3), we compare three input features, namely logarithmized noisy periodograms, noise aware training (NAT) and the proposed signal-to-noise ratio (SNR) based noise aware training (SNR-NAT). We confirm that rich training data and improved network architectures help DNNs to generalize. Furthermore, we show via experimental results and an analysis using t-distributed stochastic neighbor embedding (t-SNE) that the proposed SNR-NAT features yield robust and level independent results in unseen noise even with simple network architectures and when trained on only small datasets, which is the key contribution of this paper.",2020-04-07T16:09:54Z,2021-05-15T14:04:40Z,http://arxiv.org/abs/2004.03512v2,http://arxiv.org/pdf/2004.03512v2,"eess.AS, cs.LG, cs.SD"
Systematically designing better instance counting models on cell images   with Neural Arithmetic Logic Units,"Ashish Rana, Taranveer Singh, Harpreet Singh, Neeraj Kumar, Prashant Singh Rana","The big problem for neural network models which are trained to count instances is that whenever test range goes high training range generalization error increases i.e. they are not good generalizers outside training range. Consider the case of automating cell counting process where more dense images with higher cell counts are commonly encountered as compared to images used in training data. By making better predictions for higher ranges of cell count we are aiming to create better generalization systems for cell counting. With architecture proposal of neural arithmetic logic units (NALU) for arithmetic operations, task of counting has become feasible for higher numeric ranges which were not included in training data with better accuracy. As a part of our study we used these units and different other activation functions for learning cell counting task with two different architectures namely Fully Convolutional Regression Network and U-Net. These numerically biased units are added in the form of residual concatenated layers to original architectures and a comparative experimental study is done with these newly proposed changes. This comparative study is described in terms of optimizing regression loss problem from these models trained with extensive data augmentation techniques. We were able to achieve better results in our experiments of cell counting tasks with introduction of these numerically biased units to already existing architectures in the form of residual layer concatenation connections. Our results confirm that above stated numerically biased units does help models to learn numeric quantities for better generalization results.",2020-04-14T17:23:37Z,2020-06-15T07:44:46Z,http://arxiv.org/abs/2004.06674v2,http://arxiv.org/pdf/2004.06674v2,"cs.LG, cs.NE, stat.ML"
Geometry-Aware Gradient Algorithms for Neural Architecture Search,"Liam Li, Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar","Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.",2020-04-16T17:46:39Z,2021-03-18T17:47:28Z,http://arxiv.org/abs/2004.07802v5,http://arxiv.org/pdf/2004.07802v5,"cs.LG, cs.CV, cs.NE, math.OC, stat.ML"
Towards Ubiquitous AI in 6G with Federated Learning,"Yong Xiao, Guangming Shi, Marwan Krunz","With 5G cellular systems being actively deployed worldwide, the research community has started to explore novel technological advances for the subsequent generation, i.e., 6G. It is commonly believed that 6G will be built on a new vision of ubiquitous AI, an hyper-flexible architecture that brings human-like intelligence into every aspect of networking systems. Despite its great promise, there are several novel challenges expected to arise in ubiquitous AI-based 6G. Although numerous attempts have been made to apply AI to wireless networks, these attempts have not yet seen any large-scale implementation in practical systems. One of the key challenges is the difficulty to implement distributed AI across a massive number of heterogeneous devices. Federated learning (FL) is an emerging distributed AI solution that enables data-driven AI solutions in heterogeneous and potentially massive-scale networks. Although it still in an early stage of development, FL-inspired architecture has been recognized as one of the most promising solutions to fulfill ubiquitous AI in 6G. In this article, we identify the requirements that will drive convergence between 6G and AI. We propose an FL-based network architecture and discuss its potential for addressing some of the novel challenges expected in 6G. Future trends and key research problems for FL-enabled 6G are also discussed.",2020-04-26T13:05:29Z,2020-04-26T13:05:29Z,http://arxiv.org/abs/2004.13563v1,http://arxiv.org/pdf/2004.13563v1,"eess.SP, cs.LG, cs.NI"
A novel Region of Interest Extraction Layer for Instance Segmentation,"Leonardo Rossi, Akbar Karimi, Andrea Prati","Given the wide diffusion of deep neural network architectures for computer vision tasks, several new applications are nowadays more and more feasible. Among them, a particular attention has been recently given to instance segmentation, by exploiting the results achievable by two-stage networks (such as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex architectures, a crucial role is played by the Region of Interest (RoI) extraction layer, devoted to extracting a coherent subset of features from a single Feature Pyramid Network (FPN) layer attached on top of a backbone.   This paper is motivated by the need to overcome the limitations of existing RoI extractors which select only one (the best) layer from FPN. Our intuition is that all the layers of FPN retain useful information. Therefore, the proposed layer (called Generic RoI Extractor - GRoIE) introduces non-local building blocks and attention mechanisms to boost the performance.   A comprehensive ablation study at component level is conducted to find the best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can be integrated seamlessly with every two-stage architecture for both object detection and instance segmentation tasks. Therefore, the improvements brought about by the use of GRoIE in different state-of-the-art architectures are also evaluated. The proposed layer leads up to gain a 1.1% AP improvement on bounding box detection and 1.7% AP improvement on instance segmentation.   The code is publicly available on GitHub repository at https://github.com/IMPLabUniPr/mmdetection/tree/groie_dev",2020-04-28T17:07:32Z,2020-10-01T14:12:03Z,http://arxiv.org/abs/2004.13665v2,http://arxiv.org/pdf/2004.13665v2,"cs.CV, I.4.6; I.5.1"
Systematic two-scale image analysis of extreme deformations in soft   architectured sheets,"Filippo Agnelli, Pierre Margerit, Paolo Celli, Chiara Daraio, Andrei Constantinescu","The multi-scale nature of architectured materials raises the need for advanced experimental methods suitable for the identification of their effective properties, especially when their size is finite and they undergo extreme deformations. The present work demonstrates that state-of-the art image processing methods combined with numerical and analytical models provide a comprehensive quantitative description of these solids and their global behaviour, including the influence of the boundary conditions, of the manufacturing process, and of geometric and constitutive non-linearities. To this end, an adapted multi-scale digital image correlation analysis is used to track both elongations and rotations of particular features of the unit cell at the local and global (homogenized) scale of the material. This permits to observe with unprecedented clarity the strains for various unit cells in the structure and to detect global deformation patterns and heterogeneities of the homogenized strain distribution. This method is here demonstrated on elastic sheets undergoing extreme longitudinal and shear deformations. These experimental results are compared to non-linear finite element simulations, which are also used to evaluate the effects of manufacturing imperfections on the response. A skeletal representation of the architectured solid is then extracted from the experiments and used to create a purely-kinematic truss-hinge model that can accurately capture its behaviour. The analysis proposed in this work can be extended to guide the design of two-dimensional architectured solids featuring other regular, quasi-regular or graded patterns, and subjected to other types of loads.",2020-04-30T14:28:07Z,2020-08-03T12:54:36Z,http://arxiv.org/abs/2004.14822v2,http://arxiv.org/pdf/2004.14822v2,"physics.app-ph, cond-mat.mtrl-sci"
Hierarchical Attention Transformer Architecture For Syntactic Spell   Correction,"Abhishek Niranjan, M Ali Basha Shaik, Kushal Verma","The attention mechanisms are playing a boosting role in advancements in sequence-to-sequence problems. Transformer architecture achieved new state of the art results in machine translation, and it's variants are since being introduced in several other sequence-to-sequence problems. Problems which involve a shared vocabulary, can benefit from the similar semantic and syntactic structure in the source and target sentences. With the motivation of building a reliable and fast post-processing textual module to assist all the text-related use cases in mobile phones, we take on the popular spell correction problem. In this paper, we propose multi encoder-single decoder variation of conventional transformer. Outputs from the three encoders with character level 1-gram, 2-grams and 3-grams inputs are attended in hierarchical fashion in the decoder. The context vectors from the encoders clubbed with self-attention amplify the n-gram properties at the character level and helps in accurate decoding. We demonstrate our model on spell correction dataset from Samsung Research, and report significant improvement of 0.11\%, 0.32\% and 0.69\% in character (CER), word (WER) and sentence (SER) error rates from existing state-of-the-art machine-translation architectures. Our architecture is also trains ~7.8 times faster, and is only about 1/3 in size from the next most accurate model.",2020-05-11T06:19:01Z,2020-05-11T06:19:01Z,http://arxiv.org/abs/2005.04876v1,http://arxiv.org/pdf/2005.04876v1,"cs.LG, stat.ML"
Neural Architecture Search for Gliomas Segmentation on Multimodal   Magnetic Resonance Imaging,Feifan Wang,"Past few years have witnessed the artificial intelligence inspired evolution in various medical fields. The diagnosis and treatment of gliomas -- one of the most commonly seen brain tumors with low survival rate -- rely heavily on the computer assisted segmentation process undertaken on the magnetic resonance imaging (MRI) scans. Although the encoder-decoder shaped deep learning networks have been the de facto standard style for semantic segmentation tasks in medical imaging analysis, enormous effort is still required to be spent on designing the detailed architecture of the down-sampling and up-sampling blocks. In this work, we propose a neural architecture search (NAS) based solution to brain tumor segmentation tasks on multimodal volumetric MRI scans. Three sets of candidate operations are composed respectively for three kinds of basic building blocks in which each operation is assigned with a specific probabilistic parameter to be learned. Through alternately updating the weights of operations and the other parameters in the network, the searching mechanism ends up with two optimal structures for the upward and downward blocks. Moreover, the developed solution also integrates normalization and patching strategies tailored for brain MRI processing. Extensive comparative experiments on the BraTS 2019 dataset demonstrate that the proposed algorithm not only could relieve the pressure of fabricating block architectures but also possesses competitive feasibility and scalability.",2020-05-13T14:32:00Z,2020-05-20T06:00:43Z,http://arxiv.org/abs/2005.06338v2,http://arxiv.org/pdf/2005.06338v2,"eess.IV, cs.CV, stat.ML"
A Survey on Large Scale Metadata Server for Big Data Storage,"Ripon Patgiri, Sabuzima Nayak","Big Data is defined as high volume of variety of data with an exponential data growth rate. Data are amalgamated to generate revenue, which results a large data silo. Data are the oils of modern IT industries. Therefore, the data are growing at an exponential pace. The access mechanism of these data silos are defined by metadata. The metadata are decoupled from data server for various beneficial reasons. For instance, ease of maintenance. The metadata are stored in metadata server (MDS). Therefore, the study on the MDS is mandatory in designing of a large scale storage system. The MDS requires many parameters to augment with its architecture. The architecture of MDS depends on the demand of the storage system's requirements. Thus, MDS is categorized in various ways depending on the underlying architecture and design methodology. The article surveys on the various kinds of MDS architecture, designs, and methodologies. This article emphasizes on clustered MDS (cMDS) and the reports are prepared based on a) Bloom filter$-$based MDS, b) Client$-$funded MDS, c) Geo$-$aware MDS, d) Cache$-$aware MDS, e) Load$-$aware MDS, f) Hash$-$based MDS, and g) Tree$-$based MDS. Additionally, the article presents the issues and challenges of MDS for mammoth sized data.",2020-04-11T20:49:58Z,2020-04-11T20:49:58Z,http://arxiv.org/abs/2005.06963v1,http://arxiv.org/pdf/2005.06963v1,"cs.DC, cs.IR, 68-02, 68M14, 68W10, 68W15, D.4; H.3"
Stacked Bidirectional and Unidirectional LSTM Recurrent Neural Network   for Forecasting Network-wide Traffic State with Missing Values,"Zhiyong Cui, Ruimin Ke, Ziyuan Pu, Yinhai Wang","Short-term traffic forecasting based on deep learning methods, especially recurrent neural networks (RNN), has received much attention in recent years. However, the potential of RNN-based models in traffic forecasting has not yet been fully exploited in terms of the predictive power of spatial-temporal data and the capability of handling missing data. In this paper, we focus on RNN-based models and attempt to reformulate the way to incorporate RNN and its variants into traffic prediction models. A stacked bidirectional and unidirectional LSTM network architecture (SBU-LSTM) is proposed to assist the design of neural network structures for traffic state forecasting. As a key component of the architecture, the bidirectional LSTM (BDLSM) is exploited to capture the forward and backward temporal dependencies in spatiotemporal data. To deal with missing values in spatial-temporal data, we also propose a data imputation mechanism in the LSTM structure (LSTM-I) by designing an imputation unit to infer missing values and assist traffic prediction. The bidirectional version of LSTM-I is incorporated in the SBU-LSTM architecture. Two real-world network-wide traffic state datasets are used to conduct experiments and published to facilitate further traffic prediction research. The prediction performance of multiple types of multi-layer LSTM or BDLSTM models is evaluated. Experimental results indicate that the proposed SBU-LSTM architecture, especially the two-layer BDLSTM network, can achieve superior performance for the network-wide traffic prediction in both accuracy and robustness. Further, comprehensive comparison results show that the proposed data imputation mechanism in the RNN-based models can achieve outstanding prediction performance when the model's input data contains different patterns of missing values.",2020-05-24T00:17:15Z,2020-05-24T00:17:15Z,http://arxiv.org/abs/2005.11627v1,http://arxiv.org/pdf/2005.11627v1,"cs.LG, eess.SP, stat.ML"
Streaming Language Identification using Combination of Acoustic   Representations and ASR Hypotheses,"Chander Chandak, Zeynab Raeesy, Ariya Rastrow, Yuzong Liu, Xiangyang Huang, Siyu Wang, Dong Kwon Joo, Roland Maas","This paper presents our modeling and architecture approaches for building a highly accurate low-latency language identification system to support multilingual spoken queries for voice assistants. A common approach to solve multilingual speech recognition is to run multiple monolingual ASR systems in parallel and rely on a language identification (LID) component that detects the input language. Conventionally, LID relies on acoustic only information to detect input language. We propose an approach that learns and combines acoustic level representations with embeddings estimated on ASR hypotheses resulting in up to 50% relative reduction of identification error rate, compared to a model that uses acoustic only features. Furthermore, to reduce the processing cost and latency, we exploit a streaming architecture to identify the spoken language early when the system reaches a predetermined confidence level, alleviating the need to run multiple ASR systems until the end of input query. The combined acoustic and text LID, coupled with our proposed streaming runtime architecture, results in an average of 1500ms early identification for more than 50% of utterances, with almost no degradation in accuracy. We also show improved results by adopting a semi-supervised learning (SSL) technique using the newly proposed model architecture as a teacher model.",2020-06-01T04:08:55Z,2020-06-01T04:08:55Z,http://arxiv.org/abs/2006.00703v1,http://arxiv.org/pdf/2006.00703v1,"eess.AS, cs.CL, cs.SD"
Hyperparameter optimization with REINFORCE and Transformers,"Chepuri Shri Krishna, Ashish Gupta, Swarnim Narayan, Himanshu Rai, Diksha Manchanda","Reinforcement Learning has yielded promising results for Neural Architecture Search (NAS). In this paper, we demonstrate how its performance can be improved by using a simplified Transformer block to model the policy network. The simplified Transformer uses a 2-stream attention-based mechanism to model hyper-parameter dependencies while avoiding layer normalization and position encoding. We posit that this parsimonious design balances model complexity against expressiveness, making it suitable for discovering optimal architectures in high-dimensional search spaces with limited exploration budgets. We demonstrate how the algorithm's performance can be further improved by a) using an actor-critic style algorithm instead of plain vanilla policy gradient and b) ensembling Transformer blocks with shared parameters, each block conditioned on a different auto-regressive factorization order. Our algorithm works well as both a NAS and generic hyper-parameter optimization (HPO) algorithm: it outperformed most algorithms on NAS-Bench-101, a public data-set for benchmarking NAS algorithms. In particular, it outperformed RL based methods that use alternate architectures to model the policy network, underlining the value of using attention-based networks in this setting. As a generic HPO algorithm, it outperformed Random Search in discovering more accurate multi-layer perceptron model architectures across 2 regression tasks. We have adhered to guidelines listed in Lindauer and Hutter while designing experiments and reporting results.",2020-06-01T13:35:48Z,2020-11-05T04:55:03Z,http://arxiv.org/abs/2006.00939v4,http://arxiv.org/pdf/2006.00939v4,"cs.LG, cs.NE, stat.ML"
Reusing Trained Layers of Convolutional Neural Networks to Shorten   Hyperparameters Tuning Time,"Roberto L. Castro, Diego Andrade, Basilio Fraguela","Hyperparameters tuning is a time-consuming approach, particularly when the architecture of the neural network is decided as part of this process. For instance, in convolutional neural networks (CNNs), the selection of the number and the characteristics of the hidden (convolutional) layers may be decided. This implies that the search process involves the training of all these candidate network architectures.   This paper describes a proposal to reuse the weights of hidden (convolutional) layers among different trainings to shorten this process. The rationale is that if a set of convolutional layers have been trained to solve a given problem, the weights calculated in this training may be useful when a new convolutional layer is added to the network architecture.   This idea has been tested using the CIFAR-10 dataset, testing different CNNs architectures with up to 3 convolutional layers and up to 3 fully connected layers. The experiments compare the training time and the validation loss when reusing and not reusing convolutional layers. They confirm that this strategy reduces the training time while it even increases the accuracy of the resulting neural network. This finding opens up the future possibility of integrating this strategy in existing AutoML methods with the purpose of reducing the total search time.",2020-06-16T11:39:39Z,2020-07-30T15:30:27Z,http://arxiv.org/abs/2006.09083v2,http://arxiv.org/pdf/2006.09083v2,"cs.LG, cs.DC, stat.ML"
From Discrete to Continuous Convolution Layers,"Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani","A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.",2020-06-19T13:16:06Z,2020-06-19T13:16:06Z,http://arxiv.org/abs/2006.11120v1,http://arxiv.org/pdf/2006.11120v1,"cs.LG, cs.CV, stat.ML"
Fully-parallel Convolutional Neural Network Hardware,"Christiam F. Frasser, Pablo Linares-Serrano, V. Canals, Miquel Roca, T. Serrano-Gotarredona, Josep L. Rossello","A new trans-disciplinary knowledge area, Edge Artificial Intelligence or Edge Intelligence, is beginning to receive a tremendous amount of interest from the machine learning community due to the ever increasing popularization of the Internet of Things (IoT). Unfortunately, the incorporation of AI characteristics to edge computing devices presents the drawbacks of being power and area hungry for typical machine learning techniques such as Convolutional Neural Networks (CNN). In this work, we propose a new power-and-area-efficient architecture for implementing Articial Neural Networks (ANNs) in hardware, based on the exploitation of correlation phenomenon in Stochastic Computing (SC) systems. The architecture purposed can solve the difficult implementation challenges that SC presents for CNN applications, such as the high resources used in binary-tostochastic conversion, the inaccuracy produced by undesired correlation between signals, and the stochastic maximum function implementation. Compared with traditional binary logic implementations, experimental results showed an improvement of 19.6x and 6.3x in terms of speed performance and energy efficiency, for the FPGA implementation. We have also realized a full VLSI implementation of the proposed SC-CNN architecture demonstrating that our optimization achieve a 18x area reduction over previous SC-DNN architecture VLSI implementation in a comparable technological node. For the first time, a fully-parallel CNN as LENET-5 is embedded and tested in a single FPGA, showing the benefits of using stochastic computing for embedded applications, in contrast to traditional binary logic implementations.",2020-06-22T17:19:09Z,2020-06-22T17:19:09Z,http://arxiv.org/abs/2006.12439v1,http://arxiv.org/pdf/2006.12439v1,"cs.NE, I.2.m, B.7.1"
Stochastic gene transcription with non-competitive transcription   regulatory architecture,Amit Kumar Das,"The transcription factors, such as activators and repressors, can interact with the promoter of gene either in a competitive or non-competitive way. In this paper, we construct a stochastic model with non-competitive transcriptional regulatory architecture and develop an analytical theory that re-establishes the experimental results with an improved data fitting. The analytical expressions in the theory allow us to study the nature of the system corresponding to any of its parameters, and hence enable us to find out the factors that govern the regulation of gene expression for that architecture. We notice that, along with transcriptional reinitiation and repressors, there are other parameters that can control the noisiness of this network. We also observe that, the Fano factor (at mRNA level) varies from sub-Poissonian regime to superPoissonian regime. In addition to the aforementioned properties, we observe some anomalous characteristics of the Fano factor (at mRNA level) and that of the variance of protein at lower activator concentrations in presence of repressor molecules. This model is useful to understand the architecture of interactions which may buffer the stochasticity inherent to gene transcription.",2021-08-03T16:48:04Z,2022-05-26T15:14:38Z,http://arxiv.org/abs/2108.01620v4,http://arxiv.org/pdf/2108.01620v4,"q-bio.MN, cond-mat.soft, cond-mat.stat-mech, physics.bio-ph"
Model architecture can transform catastrophic forgetting into positive   transfer,Miguel Ruiz-Garcia,"The work of McCloskey and Cohen popularized the concept of catastrophic interference. They used a neural network that tried to learn addition using two groups of examples as two different tasks. In their case, learning the second task rapidly deteriorated the acquired knowledge about the previous one. We hypothesize that this could be a symptom of a fundamental problem: addition is an algorithmic task that should not be learned through pattern recognition. Therefore, other model architectures better suited for this task would avoid catastrophic forgetting. We use a neural network with a different architecture that can be trained to recover the correct algorithm for the addition of binary numbers. This neural network includes conditional clauses that are naturally treated within the back-propagation algorithm. We test it in the setting proposed by McCloskey and Cohen and training on random additions one by one. The neural network not only does not suffer from catastrophic forgetting but it improves its predictive power on unseen pairs of numbers as training progresses. We also show that this is a robust effect, also present when averaging many simulations. This work emphasizes the importance that neural network architecture has for the emergence of catastrophic forgetting and introduces a neural network that is able to learn an algorithm.",2021-08-09T11:12:43Z,2022-04-28T17:22:52Z,http://arxiv.org/abs/2108.03940v3,http://arxiv.org/pdf/2108.03940v3,"cs.LG, cond-mat.soft, nlin.AO"
Bi-Temporal Semantic Reasoning for the Semantic Change Detection in HR   Remote Sensing Images,"Lei Ding, Haitao Guo, Sicong Liu, Lichao Mou, Jing Zhang, Lorenzo Bruzzone","Semantic change detection (SCD) extends the multi-class change detection (MCD) task to provide not only the change locations but also the detailed land-cover/land-use (LCLU) categories before and after the observation intervals. This fine-grained semantic change information is very useful in many applications. Recent studies indicate that the SCD can be modeled through a triple-branch Convolutional Neural Network (CNN), which contains two temporal branches and a change branch. However, in this architecture, the communications between the temporal branches and the change branch are insufficient. To overcome the limitations in existing methods, we propose a novel CNN architecture for the SCD, where the semantic temporal features are merged in a deep CD unit. Furthermore, we elaborate on this architecture to reason the bi-temporal semantic correlations. The resulting Bi-temporal Semantic Reasoning Network (Bi-SRNet) contains two types of semantic reasoning blocks to reason both single-temporal and cross-temporal semantic correlations, as well as a novel loss function to improve the semantic consistency of change detection results. Experimental results on a benchmark dataset show that the proposed architecture obtains significant accuracy improvements over the existing approaches, while the added designs in the Bi-SRNet further improves the segmentation of both semantic categories and the changed areas. The codes in this paper are accessible at: github.com/ggsDing/Bi-SRNet.",2021-08-13T07:28:09Z,2022-01-05T14:30:44Z,http://arxiv.org/abs/2108.06103v4,http://arxiv.org/pdf/2108.06103v4,"cs.CV, eess.IV"
High-Throughput VLSI Architecture for GRAND Markov Order,"Syed Mohsin Abbas, Marwan Jalaleddine, Warren J. Gross","Guessing Random Additive Noise Decoding (GRAND) is a recently proposed Maximum Likelihood (ML) decoding technique. Irrespective of the structure of the error correcting code, GRAND tries to guess the noise that corrupted the codeword in order to decode any linear error-correcting block code. GRAND Markov Order (GRAND-MO) is a variant of GRAND that is useful to decode error correcting code transmitted over communication channels with memory which are vulnerable to burst noise. Usually, interleavers and de-interleavers are used in communication systems to mitigate the effects of channel memory. Interleaving and de-interleaving introduce undesirable latency, which increases with channel memory. To prevent this added latency penalty, GRAND-MO can be directly used on the hard demodulated channel signals. This work reports the first GRAND-MO hardware architecture which achieves an average throughput of up to $52$ Gbps and $64$ Gbps for a code length of $128$ and $79$ respectively. Compared to the GRANDAB, hard-input variant of GRAND, the proposed architecture achieves $3$ dB gain in decoding performance for a target FER of $10^{-5}$. Similarly, comparing the GRAND-MO decoder with a decoder tailored for a $(79,64)$ BCH code showed that the proposed architecture achieves 33$\%$ higher worst case throughput and $2$ dB gain in decoding performance.",2021-08-28T03:39:34Z,2021-08-28T03:39:34Z,http://arxiv.org/abs/2108.12563v1,http://arxiv.org/pdf/2108.12563v1,"cs.IT, math.IT"
KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal   Action Localization,"Kalana Abeywardena, Shechem Sumanthiran, Sakuna Jayasundara, Sachira Karunasena, Ranga Rodrigo, Peshala Jayasekara","Real-time and online action localization in a video is a critical yet highly challenging problem. Accurate action localization requires the utilization of both temporal and spatial information. Recent attempts achieve this by using computationally intensive 3D CNN architectures or highly redundant two-stream architectures with optical flow, making them both unsuitable for real-time, online applications. To accomplish activity localization under highly challenging real-time constraints, we propose utilizing fast and efficient key-point based bounding box prediction to spatially localize actions. We then introduce a tube-linking algorithm that maintains the continuity of action tubes temporally in the presence of occlusions. Further, we eliminate the need for a two-stream architecture by combining temporal and spatial information into a cascaded input to a single network, allowing the network to learn from both types of information. Temporal information is efficiently extracted using a structural similarity index map as opposed to computationally intensive optical flow. Despite the simplicity of our approach, our lightweight end-to-end architecture achieves state-of-the-art frame-mAP of 74.7% on the challenging UCF101-24 dataset, demonstrating a performance gain of 6.4% over the previous best online methods. We also achieve state-of-the-art video-mAP results compared to both online and offline methods. Moreover, our model achieves a frame rate of 41.8 FPS, which is a 10.7% improvement over contemporary real-time methods.",2021-11-05T08:39:36Z,2021-11-05T08:39:36Z,http://arxiv.org/abs/2111.03319v1,http://arxiv.org/pdf/2111.03319v1,"cs.CV, I.4.8"
Keys to Accurate Feature Extraction Using Residual Spiking Neural   Networks,"Alex Vicente-Sola, Davide L. Manna, Paul Kirkland, Gaetano Di Caterina, Trevor Bihl","Spiking neural networks (SNNs) have become an interesting alternative to conventional artificial neural networks (ANN) thanks to their temporal processing capabilities and energy efficient implementations in neuromorphic hardware. However the challenges involved in training SNNs have limited their performance in terms of accuracy and thus their applications. Improving learning algorithms and neural architectures for a more accurate feature extraction is therefore one of the current priorities in SNN research. In this paper we present a study on the key components of modern spiking architectures. We design a spiking version of the successful residual network architecture and provide an in-depth study on the possible implementations of spiking residual connections. This study shows how, depending on the use case, the optimal residual connection implementation may vary. Additionally, we empirically compare different techniques in image classification datasets taken from the best performing networks. Our results provide a state of the art guide to SNN design, which allows to make informed choices when trying to build the optimal visual feature extractor. Finally, our network outperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100 (74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%), with less parameters than the previous state of the art and without the need for ANN-SNN conversion. Code available at https://github.com/VicenteAlex/Spiking_ResNet",2021-11-10T21:29:19Z,2022-06-23T15:01:24Z,http://arxiv.org/abs/2111.05955v4,http://arxiv.org/pdf/2111.05955v4,"cs.LG, cs.CV, I.2.6; I.2.10; I.4.8; I.5.2; D.2.13"
Exploiting Long-Distance Interactions and Tolerating Atom Loss in   Neutral Atom Quantum Architectures,"Jonathan M. Baker, Andrew Litteken, Casey Duckering, Henry Hoffman, Hannes Bernien, Frederic T. Chong","Quantum technologies currently struggle to scale beyond moderate scale prototypes and are unable to execute even reasonably sized programs due to prohibitive gate error rates or coherence times. Many software approaches rely on heavy compiler optimization to squeeze extra value from noisy machines but are fundamentally limited by hardware. Alone, these software approaches help to maximize the use of available hardware but cannot overcome the inherent limitations posed by the underlying technology. An alternative approach is to explore the use of new, though potentially less developed, technology as a path towards scalability. In this work we evaluate the advantages and disadvantages of a Neutral Atom (NA) architecture. NA systems offer several promising advantages such as long range interactions and native multiqubit gates which reduce communication overhead, overall gate count, and depth for compiled programs. Long range interactions, however, impede parallelism with restriction zones surrounding interacting qubit pairs. We extend current compiler methods to maximize the benefit of these advantages and minimize the cost. Furthermore, atoms in an NA device have the possibility to randomly be lost over the course of program execution which is extremely detrimental to total program execution time as atom arrays are slow to load. When the compiled program is no longer compatible with the underlying topology, we need a fast and efficient coping mechanism. We propose hardware and compiler methods to increase system resilience to atom loss dramatically reducing total computation time by circumventing complete reloads or full recompilation every cycle.",2021-11-11T21:34:31Z,2021-11-11T21:34:31Z,http://arxiv.org/abs/2111.06469v1,http://arxiv.org/pdf/2111.06469v1,"quant-ph, cs.AR, cs.ET"
ContourletNet: A Generalized Rain Removal Architecture Using   Multi-Direction Hierarchical Representation,"Wei-Ting Chen, Cheng-Che Tsai, Hao-Yu Fang, I-Hsiang Chen, Jian-Jiun Ding, Sy-Yen Kuo","Images acquired from rainy scenes usually suffer from bad visibility which may damage the performance of computer vision applications. The rainy scenarios can be categorized into two classes: moderate rain and heavy rain scenes. Moderate rain scene mainly consists of rain streaks while heavy rain scene contains both rain streaks and the veiling effect (similar to haze). Although existing methods have achieved excellent performance on these two cases individually, it still lacks a general architecture to address both heavy rain and moderate rain scenarios effectively. In this paper, we construct a hierarchical multi-direction representation network by using the contourlet transform (CT) to address both moderate rain and heavy rain scenarios. The CT divides the image into the multi-direction subbands (MS) and the semantic subband (SS). First, the rain streak information is retrieved to the MS based on the multi-orientation property of the CT. Second, a hierarchical architecture is proposed to reconstruct the background information including damaged semantic information and the veiling effect in the SS. Last, the multi-level subband discriminator with the feedback error map is proposed. By this module, all subbands can be well optimized. This is the first architecture that can address both of the two scenarios effectively. The code is available in https://github.com/cctakaet/ContourletNet-BMVC2021.",2021-11-25T05:55:30Z,2021-11-25T05:55:30Z,http://arxiv.org/abs/2111.12925v1,http://arxiv.org/pdf/2111.12925v1,"cs.CV, cs.AI, cs.LG, eess.IV, eess.SP"
MAPLE: Microprocessor A Priori for Latency Estimation,"Saad Abbasi, Alexander Wong, Mohammad Javad Shafiee","Modern deep neural networks must demonstrate state-of-the-art accuracy while exhibiting low latency and energy consumption. As such, neural architecture search (NAS) algorithms take these two constraints into account when generating a new architecture. However, efficiency metrics such as latency are typically hardware dependent requiring the NAS algorithm to either measure or predict the architecture latency. Measuring the latency of every evaluated architecture adds a significant amount of time to the NAS process. Here we propose Microprocessor A Priori for Latency Estimation MAPLE that does not rely on transfer learning or domain adaptation but instead generalizes to new hardware by incorporating a prior hardware characteristics during training. MAPLE takes advantage of a novel quantitative strategy to characterize the underlying microprocessor by measuring relevant hardware performance metrics, yielding a fine-grained and expressive hardware descriptor. Moreover, the proposed MAPLE benefits from the tightly coupled I/O between the CPU and GPU and their dependency to predict DNN latency on GPUs while measuring microprocessor performance hardware counters from the CPU feeding the GPU hardware. Through this quantitative strategy as the hardware descriptor, MAPLE can generalize to new hardware via a few shot adaptation strategy where with as few as 3 samples it exhibits a 6% improvement over state-of-the-art methods requiring as much as 10 samples. Experimental results showed that, increasing the few shot adaptation samples to 10 improves the accuracy significantly over the state-of-the-art methods by 12%. Furthermore, it was demonstrated that MAPLE exhibiting 8-10% better accuracy, on average, compared to relevant baselines at any number of adaptation samples.",2021-11-30T03:52:15Z,2022-05-25T10:40:27Z,http://arxiv.org/abs/2111.15106v2,http://arxiv.org/pdf/2111.15106v2,"cs.LG, cs.AI, I.2.0; I.5.0"
Serializable HTAP with Abort-/Wait-free Snapshot Read,"Takamitsu Shioi, Takashi Kambayashi, Suguru Arakawa, Ryoji Kurosawa, Satoshi Hikida, Haruo Yokota","Concurrency Control (CC) ensuring consistency of updated data is an essential element of OLTP systems. Recently, hybrid transactional/analytical processing (HTAP) systems developed for executing OLTP and OLAP have attracted much attention. The OLAP side CC domain has been isolated from OLTP's CC and in many cases has been achieved by snapshot isolation (SI) to establish HTAP systems. Although higher isolation level is ideal, considering OLAP read-only transactions in the context of OLTP scheduling achieving serializability forces aborts/waits and would be a potential performance problem. Furthermore, executing OLAP without affecting OLTP as much as possible is needed for HTAP systems.   The aim of this study was serializability without additional aborts/waits. We propose read safe snapshot (RSS) using multiversion CC (MVCC) theory and introduce the RSS construction algorithm utilizing serializable snapshot isolation (SSI). For serializability of HTAP systems, our model makes use of multiversion and allows more schedules with read operations whose corresponding write operations do not participate in the dependency cycles. Furthermore, we implemented the algorithm practically in an open-source database system that offers SSI. Our algorithm was integrated into two types of architecture as HTAP systems called as unified (single-node) or decoupled (multinode) storage architecture. We evaluate the performance and abort rate of the single-node architecture where SSI is applicable. The multi-node architecture was investigated for examining the performance overhead applying our algorithm.",2022-01-20T04:18:07Z,2022-04-21T10:01:29Z,http://arxiv.org/abs/2201.07993v2,http://arxiv.org/pdf/2201.07993v2,"cs.DB, H.2.4"
A Critical Review of Baseband Architectures for CubeSats Communication   Systems,"Amr Zeedan, Tamer Khattab","Small satellite communications recently entered a period of massive interest driven by the uprising space applications. CubeSats are particularly attractive due to their low development costs which makes them very promising in playing a central role in the global wireless communication sector with numerous applications. Moreover, constellations of CubeSats in low-earth orbits can meet the increasing demands of global-coverage flexible low-cost high-speed connectivity. However, this requires innovative solutions to overcome the significant challenges that face high-data-rate low-power space communications. This paper provides a comprehensive and critical review of the design and architecture of recent CubeSat communication systems with a particular focus on their baseband architectures. The literature is surveyed in detail to identify all baseband design, testing, and demonstration stages as well as accurately describe the systems architecture and communication protocols. The reliability, performance, data rate, and power consumption of the reviewed systems are critically evaluated to understand the limitations of current CubeSat systems and identify directions of future developments. It is concluded that CubeSat communication systems still face many challenges, namely the development of energy-efficient high-speed modems that satisfy CubeSats requirements. Nevertheless, there are several promising directions for improvements such as the use of improved coding algorithms, use of Field Programmable Gate Arrays, multiple access techniques, beamforming, advanced antennas, and transition to higher frequency bands. By providing a concrete summary of current CubeSat communication systems and by critically evaluating their features, limitations, and offering insights about potential improvements, the review should aid CubeSat developers to develop more efficient and high data rate systems.",2022-01-24T15:27:32Z,2022-01-24T15:27:32Z,http://arxiv.org/abs/2201.09748v1,http://arxiv.org/pdf/2201.09748v1,"eess.SP, cs.SY, eess.SY"
Automatic Classification of Alzheimer's Disease using brain MRI data and   deep Convolutional Neural Networks,"Zahraa Sh. Aaraji, Hawraa H. Abbas","Alzheimer's disease (AD) is one of the most common public health issues the world is facing today. This disease has a high prevalence primarily in the elderly accompanying memory loss and cognitive decline. AD detection is a challenging task which many authors have developed numerous computerized automatic diagnosis systems utilizing neuroimaging and other clinical data. MRI scans provide high-intensity visible features, making these scans the most widely used brain imaging technique. In recent years deep learning has achieved leading success in medical image analysis. But a relatively little investigation has been done to apply deep learning techniques for the brain MRI classification. This paper explores the construction of several deep learning architectures evaluated on brain MRI images and segmented images. The idea behind segmented images investigates the influence of image segmentation step on deep learning classification. The image processing presented a pipeline consisting of pre-processing to enhance the MRI scans and post-processing consisting of a segmentation method for segmenting the brain tissues. The results show that the processed images achieved a better accuracy in the binary classification of AD vs. CN (Cognitively Normal) across four different architectures. ResNet architecture resulted in the highest prediction accuracy amongst the other architectures (90.83% for the original brain images and 93.50% for the processed images).",2022-03-31T20:15:51Z,2022-03-31T20:15:51Z,http://arxiv.org/abs/2204.00068v1,http://arxiv.org/pdf/2204.00068v1,"eess.IV, cs.CV, physics.med-ph"
Lyapunov-Guided Representation of Recurrent Neural Network Performance,"Ryan Vogt, Yang Zheng, Eli Shlizerman","Recurrent Neural Networks (RNN) are ubiquitous computing systems for sequences and multivariate time series data. While several robust architectures of RNN are known, it is unclear how to relate RNN initialization, architecture, and other hyperparameters with accuracy for a given task. In this work, we propose to treat RNN as dynamical systems and to correlate hyperparameters with accuracy through Lyapunov spectral analysis, a methodology specifically designed for nonlinear dynamical systems. To address the fact that RNN features go beyond the existing Lyapunov spectral analysis, we propose to infer relevant features from the Lyapunov spectrum with an Autoencoder and an embedding of its latent representation (AeLLE). Our studies of various RNN architectures show that AeLLE successfully correlates RNN Lyapunov spectrum with accuracy. Furthermore, the latent representation learned by AeLLE is generalizable to novel inputs from the same task and is formed early in the process of RNN training. The latter property allows for the prediction of the accuracy to which RNN would converge when training is complete. We conclude that representation of RNN through Lyapunov spectrum along with AeLLE provides a novel method for organization and interpretation of variants of RNN architectures.",2022-04-11T05:38:38Z,2023-12-27T05:19:29Z,http://arxiv.org/abs/2204.04876v2,http://arxiv.org/pdf/2204.04876v2,"cs.LG, math.DS, nlin.CD, stat.ML"
Comparison Analysis of Traditional Machine Learning and Deep Learning   Techniques for Data and Image Classification,"Efstathios Karypidis, Stylianos G. Mouslech, Kassiani Skoulariki, Alexandros Gazis","The purpose of the study is to analyse and compare the most common machine learning and deep learning techniques used for computer vision 2D object classification tasks. Firstly, we will present the theoretical background of the Bag of Visual words model and Deep Convolutional Neural Networks (DCNN). Secondly, we will implement a Bag of Visual Words model, the VGG16 CNN Architecture. Thirdly, we will present our custom and novice DCNN in which we test the aforementioned implementations on a modified version of the Belgium Traffic Sign dataset. Our results showcase the effects of hyperparameters on traditional machine learning and the advantage in terms of accuracy of DCNNs compared to classical machine learning methods. As our tests indicate, our proposed solution can achieve similar - and in some cases better - results than existing DCNNs architectures. Finally, the technical merit of this article lies in the presented computationally simpler DCNN architecture, which we believe can pave the way towards using more efficient architectures for basic tasks.",2022-04-11T11:34:43Z,2022-04-11T11:34:43Z,http://arxiv.org/abs/2204.05983v1,http://arxiv.org/pdf/2204.05983v1,"cs.CV, cs.LG, K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0"
Build a Robust QA System with Transformer-based Mixture of Experts,"Yu Qing Zhou, Xixuan Julie Liu, Yuanzhe Dong","In this paper, we aim to build a robust question answering system that can adapt to out-of-domain datasets. A single network may overfit to the superficial correlation in the training distribution, but with a meaningful number of expert sub-networks, a gating network that selects a sparse combination of experts for each input, and careful balance on the importance of expert sub-networks, the Mixture-of-Experts (MoE) model allows us to train a multi-task learner that can be generalized to out-of-domain datasets. We also explore the possibility of bringing the MoE layers up to the middle of the DistilBERT and replacing the dense feed-forward network with a sparsely-activated switch FFN layers, similar to the Switch Transformer architecture, which simplifies the MoE routing algorithm with reduced communication and computational costs. In addition to model architectures, we explore techniques of data augmentation including Easy Data Augmentation (EDA) and back translation, to create more meaningful variance among the small out-of-domain training data, therefore boosting the performance and robustness of our models. In this paper, we show that our combination of best architecture and data augmentation techniques achieves a 53.477 F1 score in the out-of-domain evaluation, which is a 9.52% performance gain over the baseline. On the final test set, we reported a higher 59.506 F1 and 41.651 EM. We successfully demonstrate the effectiveness of Mixture-of-Expert architecture in a Robust QA task.",2022-03-20T02:38:29Z,2022-03-20T02:38:29Z,http://arxiv.org/abs/2204.09598v1,http://arxiv.org/pdf/2204.09598v1,"cs.CL, cs.AI, I.2.7"
Numerical Computation of Partial Differential Equations by Hidden-Layer   Concatenated Extreme Learning Machine,"Naxian Ni, Suchuan Dong","The extreme learning machine (ELM) method can yield highly accurate solutions to linear/nonlinear partial differential equations (PDEs), but requires the last hidden layer of the neural network to be wide to achieve a high accuracy. If the last hidden layer is narrow, the accuracy of the existing ELM method will be poor, irrespective of the rest of the network configuration. In this paper we present a modified ELM method, termed HLConcELM (hidden-layer concatenated ELM), to overcome the above drawback of the conventional ELM method. The HLConcELM method can produce highly accurate solutions to linear/nonlinear PDEs when the last hidden layer of the network is narrow and when it is wide. The new method is based on a type of modified feedforward neural networks (FNN), termed HLConcFNN (hidden-layer concatenated FNN), which incorporates a logical concatenation of the hidden layers in the network and exposes all the hidden nodes to the output-layer nodes. HLConcFNNs have the interesting property that, given a network architecture, when additional hidden layers are appended to the network or when extra nodes are added to the existing hidden layers the representation capacity of the HLConcFNN associated with the new architecture is guaranteed to be not smaller than that of the original network architecture. Here representation capacity refers to the set of all functions that can be exactly represented by the neural network of a given architecture. We present ample benchmark tests with linear/nonlinear PDEs to demonstrate the computational accuracy and performance of the HLConcELM method and the superiority of this method to the conventional ELM from previous works.",2022-04-24T22:39:06Z,2022-05-15T21:04:57Z,http://arxiv.org/abs/2204.11375v2,http://arxiv.org/pdf/2204.11375v2,"math.NA, cs.LG, cs.NA, physics.comp-ph, physics.flu-dyn"
System Architecture and Key Technologies for 5G Heterogeneous Cloud   Radio Access Networks,"Mugen Peng, Yong Li, Zhongyuan Zhao, Chonggang Wang","Compared with the fourth generation (4G) cellular systems, the fifth generation wireless communication systems (5G) are anticipated to provide spectral and energy efficiency growth by a factor of at least 10, and the area throughput growth by a factor of at least 25. To achieve these goals, a heterogeneous cloud radio access network (H-CRAN) is presented in this article as the advanced wireless access network paradigm, where cloud computing is used to fulfill the centralized large-scale cooperative processing for suppressing co-channel interferences. The state-of-the-art research achievements in aspects of system architecture and key technologies for H-CRANs are surveyed. Particularly, Node C as a new communication entity is defined to converge the existing ancestral base stations and act as the base band unit (BBU) pool to manage all accessed remote radio heads (RRHs), and the software-defined H-CRAN system architecture is presented to be compatible with software-defined networks (SDN). The principles, performance gains and open issues of key technologies including adaptive large-scale cooperative spatial signal processing, cooperative radio resource management, network function virtualization, and self-organization are summarized. The major challenges in terms of fronthaul constrained resource allocation optimization and energy harvesting that may affect the promotion of H-CRANs are discussed as well.",2014-12-20T18:44:24Z,2014-12-20T18:44:24Z,http://arxiv.org/abs/1412.6677v1,http://arxiv.org/pdf/1412.6677v1,"cs.IT, cs.NI, math.IT"
Hybrid Architectures with Few-Bit ADC Receivers: Achievable Rates and   Energy-Rate Tradeoffs,"Jianhua Mo, Ahmed Alkhateeb, Shadi Abu-Surra, Robert W. Heath Jr","Hybrid analog/digital architectures and receivers with low-resolution analog-to-digital converters (ADCs) are two low power solutions for wireless systems with large antenna arrays, such as millimeter wave and massive MIMO systems. Most prior work represents two extreme cases in which either a small number of RF chains with full-resolution ADCs, or low resolution ADC with a number of RF chains equal to the number of antennas is assumed. In this paper, a generalized hybrid architecture with a small number of RF chains and finite number of ADC bits is proposed. For this architecture, achievable rates with channel inversion and SVD based transmission methods are derived. Results show that the achievable rate is comparable to that obtained by full-precision ADC receivers at low and medium SNRs. A trade-off between the achievable rate and power consumption for different numbers of bits and RF chains is devised. This enables us to draw some conclusions on the number of ADC bits needed to maximize the system energy efficiency. Numerical simulations show that coarse ADC quantization is optimal under various system configurations. This means that hybrid combining with coarse quantization achieves better energy-rate trade-off compared to both hybrid combining with full-resolutions ADCs and 1-bit ADC combining.",2016-05-02T20:11:59Z,2016-11-04T17:14:32Z,http://arxiv.org/abs/1605.00668v2,http://arxiv.org/pdf/1605.00668v2,"cs.IT, math.IT"
Single photon in hierarchical architecture for physical reinforcement   learning: Photon intelligence,"Makoto Naruse, Martin Berthel, Aurélien Drezet, Serge Huant, Hirokazu Hori, Song-Ju Kim","Understanding and using natural processes for intelligent functionalities, referred to as natural intelligence, has recently attracted interest from a variety of fields, including post-silicon computing for artificial intelligence and decision making in the behavioural sciences. In a past study, we successfully used the wave-particle duality of single photons to solve the two-armed bandit problem, which constitutes the foundation of reinforcement learning and decision making. In this study, we propose and confirm a hierarchical architecture for single-photon-based reinforcement learning and decision making that verifies the scalability of the principle. Specifically, the four-armed bandit problem is solved given zero prior knowledge in a two-layer hierarchical architecture, where polarization is autonomously adapted in order to effect adequate decision making using single-photon measurements. In the hierarchical structure, the notion of layer-dependent decisions emerges. The optimal solutions in the coarse layer and in the fine layer, however, conflict with each other in some contradictive problems. We show that while what we call a tournament strategy resolves such contradictions, the probabilistic nature of single photons allows for the direct location of the optimal solution even for contradictive problems, hence manifesting the exploration ability of single photons. This study provides insights into photon intelligence in hierarchical architectures for future artificial intelligence as well as the potential of natural processes for intelligent functionalities.",2016-09-01T09:32:29Z,2016-09-01T09:32:29Z,http://arxiv.org/abs/1609.00686v1,http://arxiv.org/pdf/1609.00686v1,"cs.LG, physics.optics, quant-ph"
A GPU-Outperforming FPGA Accelerator Architecture for Binary   Convolutional Neural Networks,"Yixing Li, Zichuan Liu, Kai Xu, Hao Yu, Fengbo Ren","FPGA-based hardware accelerators for convolutional neural networks (CNNs) have obtained great attentions due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based solutions to achieve a higher throughput than GPU counterparts. In this paper, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized FPGA accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. A key advantage of the FPGA accelerator is that its performance is insensitive to data batch size, while the performance of GPU acceleration varies largely depending on the batch size of the data. Experiment results show that the proposed accelerator architecture for binary CNNs running on a Virtex-7 FPGA is 8.3x faster and 75x more energy-efficient than a Titan X GPU for processing online individual requests in small batch sizes. For processing static data in large batch sizes, the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5x higher energy efficiency.",2017-02-20T05:21:34Z,2017-06-08T16:09:55Z,http://arxiv.org/abs/1702.06392v2,http://arxiv.org/pdf/1702.06392v2,"cs.DC, cs.AR, cs.CV, cs.LG, C.3"
On architectural choices in deep learning: From network structure to   gradient convergence and parameter estimation,"Vamsi K Ithapu, Sathya N Ravi, Vikas Singh","We study mechanisms to characterize how the asymptotic convergence of backpropagation in deep architectures, in general, is related to the network structure, and how it may be influenced by other design choices including activation type, denoising and dropout rate. We seek to analyze whether network architecture and input data statistics may guide the choices of learning parameters and vice versa. Given the broad applicability of deep architectures, this issue is interesting both from theoretical and a practical standpoint. Using properties of general nonconvex objectives (with first-order information), we first build the association between structural, distributional and learnability aspects of the network vis-\`a-vis their interaction with parameter convergence rates. We identify a nice relationship between feature denoising and dropout, and construct families of networks that achieve the same level of convergence. We then derive a workflow that provides systematic guidance regarding the choice of network sizes and learning parameters often mediated4 by input statistics. Our technical results are corroborated by an extensive set of evaluations, presented in this paper as well as independent empirical observations reported by other groups. We also perform experiments showing the practical implications of our framework for choosing the best fully-connected design for a given problem.",2017-02-28T07:05:27Z,2017-02-28T07:05:27Z,http://arxiv.org/abs/1702.08670v1,http://arxiv.org/pdf/1702.08670v1,"cs.LG, math.OC, stat.ML"
A Unified Deep Learning Architecture for Abuse Detection,"Antigoni-Maria Founta, Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Athena Vakali, Ilias Leontiadis","Hate speech, offensive language, sexism, racism and other types of abusive behavior have become a common phenomenon in many online social media platforms. In recent years, such diverse abusive behaviors have been manifesting with increased frequency and levels of intensity. This is due to the openness and willingness of popular media platforms, such as Twitter and Facebook, to host content of sensitive or controversial topics. However, these platforms have not adequately addressed the problem of online abusive behavior, and their responsiveness to the effective detection and blocking of such inappropriate behavior remains limited.   In the present paper, we study this complex problem by following a more holistic approach, which considers the various aspects of abusive behavior. To make the approach tangible, we focus on Twitter data and analyze user and textual properties from different angles of abusive posting behavior. We propose a deep learning architecture, which utilizes a wide variety of available metadata, and combines it with automatically-extracted hidden patterns within the text of the tweets, to detect multiple abusive behavioral norms which are highly inter-related. We apply this unified architecture in a seamless, transparent fashion to detect different types of abusive behavior (hate speech, sexism vs. racism, bullying, sarcasm, etc.) without the need for any tuning of the model architecture for each task. We test the proposed approach with multiple datasets addressing different and multiple abusive behaviors on Twitter. Our results demonstrate that it largely outperforms the state-of-art methods (between 21 and 45\% improvement in AUC, depending on the dataset).",2018-02-01T16:48:39Z,2018-02-21T14:19:57Z,http://arxiv.org/abs/1802.00385v2,http://arxiv.org/pdf/1802.00385v2,"cs.CL, cs.SI, 68T06, K.4.2"
Regularized Evolution for Image Classifier Architecture Search,"Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le","The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.",2018-02-05T18:20:52Z,2019-02-16T23:28:16Z,http://arxiv.org/abs/1802.01548v7,http://arxiv.org/pdf/1802.01548v7,"cs.NE, cs.AI, cs.CV, cs.DC, I.2.6; I.5.1; I.5.2"
Residual-Based Detections and Unified Architecture for Massive MIMO   Uplink,"Chuan Zhang, Yufeng Yang, Shunqing Zhang, Zaichen Zhang, Xiaohu You","Massive multiple-input multiple-output (M-MIMO) technique brings better energy efficiency and coverage but higher computational complexity than small-scale MIMO. For linear detections such as minimum mean square error (MMSE), prohibitive complexity lies in solving large-scale linear equations. For a better trade-off between bit-error-rate (BER) performance and computational complexity, iterative linear algorithms like conjugate gradient (CG) have been applied and have shown their feasibility in recent years. In this paper, residual-based detection (RBD) algorithms are proposed for M-MIMO detection, including minimal residual (MINRES) algorithm, generalized minimal residual (GMRES) algorithm, and conjugate residual (CR) algorithm. RBD algorithms focus on the minimization of residual norm per iteration, whereas most existing algorithms focus on the approximation of exact signal. Numerical results have shown that, for $64$-QAM $128\times 8$ MIMO, RBD algorithms are only $0.13$ dB away from the exact matrix inversion method when BER$=10^{-4}$. Stability of RBD algorithms has also been verified in various correlation conditions. Complexity comparison has shown that, CR algorithm require $87\%$ less complexity than the traditional method for $128\times 60$ MIMO. The unified hardware architecture is proposed with flexibility, which guarantees a low-complexity implementation for a family of RBD M-MIMO detectors.",2018-02-15T10:54:31Z,2018-02-15T10:54:31Z,http://arxiv.org/abs/1802.05982v1,http://arxiv.org/pdf/1802.05982v1,"eess.SP, cs.AR, cs.CE, cs.NA"
Closing the loop on multisensory interactions: A neural architecture for   multisensory causal inference and recalibration,"Jonathan Tong, German I. Parisi, Stefan Wermter, Brigitte Röder","When the brain receives input from multiple sensory systems, it is faced with the question of whether it is appropriate to process the inputs in combination, as if they originated from the same event, or separately, as if they originated from distinct events. Furthermore, it must also have a mechanism through which it can keep sensory inputs calibrated to maintain the accuracy of its internal representations. We have developed a neural network architecture capable of i) approximating optimal multisensory spatial integration, based on Bayesian causal inference, and ii) recalibrating the spatial encoding of sensory systems. The architecture is based on features of the dorsal processing hierarchy, including the spatial tuning properties of unisensory neurons and the convergence of different sensory inputs onto multisensory neurons. Furthermore, we propose that these unisensory and multisensory neurons play dual roles in i) encoding spatial location as separate or integrated estimates and ii) accumulating evidence for the independence or relatedness of multisensory stimuli. We further propose that top-down feedback connections spanning the dorsal pathway play key a role in recalibrating spatial encoding at the level of early unisensory cortices. Our proposed architecture provides possible explanations for a number of human electrophysiological and neuroimaging results and generates testable predictions linking neurophysiology with behaviour.",2018-02-19T11:31:02Z,2018-03-04T14:59:39Z,http://arxiv.org/abs/1802.06591v3,http://arxiv.org/pdf/1802.06591v3,"cs.NE, q-bio.NC"
Neural Architecture Search with Bayesian Optimisation and Optimal   Transport,"Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, Eric Xing","Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function $f$ which is only accessible via point evaluations. It is typically used in settings where $f$ is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network \emph{architectures}. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.",2018-02-11T17:45:44Z,2019-03-15T18:23:39Z,http://arxiv.org/abs/1802.07191v3,http://arxiv.org/pdf/1802.07191v3,"cs.LG, stat.ML"
Synergy: A HW/SW Framework for High Throughput CNNs on Embedded   Heterogeneous SoC,"Guanwen Zhong, Akshat Dubey, Tan Cheng, Tulika Mitra","Convolutional Neural Networks (CNN) have been widely deployed in diverse application domains. There has been significant progress in accelerating both their training and inference using high-performance GPUs, FPGAs, and custom ASICs for datacenter-scale environments. The recent proliferation of mobile and IoT devices have necessitated real-time, energy-efficient deep neural network inference on embedded-class, resource-constrained platforms. In this context, we present {\em Synergy}, an automated, hardware-software co-designed, pipelined, high-throughput CNN inference framework on embedded heterogeneous system-on-chip (SoC) architectures (Xilinx Zynq). {\em Synergy} leverages, through multi-threading, all the available on-chip resources, which includes the dual-core ARM processor along with the FPGA and the NEON SIMD engines as accelerators. Moreover, {\em Synergy} provides a unified abstraction of the heterogeneous accelerators (FPGA and NEON) and can adapt to different network configurations at runtime without changing the underlying hardware accelerator architecture by balancing workload across accelerators through work-stealing. {\em Synergy} achieves 7.3X speedup, averaged across seven CNN models, over a well-optimized software-only solution. {\em Synergy} demonstrates substantially better throughput and energy-efficiency compared to the contemporary CNN implementations on the same SoC architecture.",2018-03-28T16:02:45Z,2018-03-28T16:02:45Z,http://arxiv.org/abs/1804.00706v1,http://arxiv.org/pdf/1804.00706v1,"cs.DC, cs.AR, cs.LG, C.1.3"
Anytime Neural Prediction via Slicing Networks Vertically,"Hankook Lee, Jinwoo Shin","The pioneer deep neural networks (DNNs) have emerged to be deeper or wider for improving their accuracy in various applications of artificial intelligence. However, DNNs are often too heavy to deploy in practice, and it is often required to control their architectures dynamically given computing resource budget, i.e., anytime prediction. While most existing approaches have focused on training multiple shallow sub-networks jointly, we study training thin sub-networks instead. To this end, we first build many inclusive thin sub-networks (of the same depth) under a minor modification of existing multi-branch DNNs, and found that they can significantly outperform the state-of-art dense architecture for anytime prediction. This is remarkable due to their simplicity and effectiveness, but training many thin sub-networks jointly faces a new challenge on training complexity. To address the issue, we also propose a novel DNN architecture by forcing a certain sparsity pattern on multi-branch network parameters, making them train efficiently for the purpose of anytime prediction. In our experiments on the ImageNet dataset, its sub-networks have up to $43.3\%$ smaller sizes (FLOPs) compared to those of the state-of-art anytime model with respect to the same accuracy. Finally, we also propose an alternative task under the proposed architecture using a hierarchical taxonomy, which brings a new angle for anytime prediction.",2018-07-07T03:55:26Z,2018-07-07T03:55:26Z,http://arxiv.org/abs/1807.02609v1,http://arxiv.org/pdf/1807.02609v1,"cs.LG, stat.ML"
Massively Parallel Simulations of Binary Black Hole   Intermediate-Mass-Ratio Inspirals,"Milinda Fernando, David Neilsen, Hyun Lim, Eric Hirschmann, Hari Sundar","We present a highly-scalable framework that targets problems of interest to the numerical relativity and broader astrophysics communities. This framework combines a parallel octree-refined adaptive mesh with a wavelet adaptive multiresolution and a physics module to solve the Einstein equations of general relativity in the BSSN formulation. The goal of this work is to perform advanced, massively parallel numerical simulations of Intermediate Mass Ratio Inspirals (IMRIs) of binary black holes with mass ratios on the order of 100:1. These studies will be used to generate waveforms as used in LIGO data analysis and to calibrate semi-analytical approximate methods. Our framework consists of a distributed memory octree-based adaptive meshing framework in conjunction with a node-local code generator. The code generator makes our code portable across different architectures. The equations corresponding to the target application are written in symbolic notation and generators for different architectures can be added independent of the application. Additionally, this symbolic interface also makes our code extensible, and as such has been designed to easily accommodate many existing algorithms in astrophysics for plasma dynamics and radiation hydrodynamics. Our adaptive meshing algorithms and data-structures have been optimized for modern architectures with deep memory hierarchies. This enables our framework to have achieve excellent performance and scalability on modern leadership architectures. We demonstrate excellent weak scalability up to 131K cores on ORNL's Titan for binary mergers for mass ratios up to 100.",2018-07-16T21:54:42Z,2019-01-19T19:05:19Z,http://arxiv.org/abs/1807.06128v2,http://arxiv.org/pdf/1807.06128v2,"gr-qc, astro-ph.HE"
Stock Chart Pattern recognition with Deep Learning,"Marc Velay, Fabrice Daniel","This study evaluates the performances of CNN and LSTM for recognizing common charts patterns in a stock historical data. It presents two common patterns, the method used to build the training set, the neural networks architectures and the accuracies obtained.",2018-08-01T17:00:43Z,2018-08-01T17:00:43Z,http://arxiv.org/abs/1808.00418v1,http://arxiv.org/pdf/1808.00418v1,"cs.LG, stat.ML"
On Deep Neural Networks for Detecting Heart Disease,"Nathalie-Sofia Tomov, Stanimire Tomov","Heart disease is the leading cause of death, and experts estimate that approximately half of all heart attacks and strokes occur in people who have not been flagged as ""at risk."" Thus, there is an urgent need to improve the accuracy of heart disease diagnosis. To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. Our main contribution is the design, evaluation, and optimization of DNN architectures of increasing depth for heart disease diagnosis. This work led to the discovery of a novel five layer DNN architecture - named Heart Evaluation for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields best prediction accuracy. HEARO-5's design employs regularization optimization and automatically deals with missing data and/or data outliers. To evaluate and tune the architectures we use k-way cross-validation as well as Matthews correlation coefficient (MCC) to measure the quality of our classifications. The study is performed on the publicly available Cleveland dataset of medical information, and we are making our developments open source, to further facilitate openness and research on the use of DNNs in medicine. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.",2018-08-22T00:51:57Z,2018-08-22T00:51:57Z,http://arxiv.org/abs/1808.07168v1,http://arxiv.org/pdf/1808.07168v1,"cs.LG, cs.AI, cs.PF, stat.ML"
Searching Toward Pareto-Optimal Device-Aware Neural Architectures,"An-Chieh Cheng, Jin-Dong Dong, Chi-Hung Hsu, Shu-Huan Chang, Min Sun, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan","Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performance in many tasks such as image classification and language understanding. However, most existing works only optimize for model accuracy and largely ignore other important factors imposed by the underlying hardware and devices, such as latency and energy, when making inference. In this paper, we first introduce the problem of NAS and provide a survey on recent works. Then we deep dive into two recent advancements on extending NAS into multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net are capable of optimizing accuracy and other objectives imposed by devices, searching for neural architectures that can be best deployed on a wide spectrum of devices: from embedded systems and mobile devices to workstations. Experimental results are poised to show that architectures found by MONAS and DPP-Net achieves Pareto optimality w.r.t the given objectives for various devices.",2018-08-29T13:52:40Z,2018-08-30T00:35:52Z,http://arxiv.org/abs/1808.09830v2,http://arxiv.org/pdf/1808.09830v2,"cs.LG, stat.ML"
A Two-layer Decentralized Control Architecture for DER Coordination,"Thomas Navidi, Abbas El Gamal, Ram Rajagopal","This paper presents a two-layer distributed energy resource (DER) coordination architecture that allows for separate ownership of data, operates with data subjected to a large buffering delay, and employs a new measure of power quality. The two-layer architecture comprises a centralized model predictive controller (MPC) and several decentralized MPCs each operating independently with no direct communication between them and with infrequent communication with the centralized controller. The goal is to minimize a combination of total energy cost and a measure of power quality while obeying cyber-physical constraints. The global controller utilizes a fast AC optimal power flow (OPF) solver and extensive parallelization to scale the solution to large networks. Each local controller attempts to maximize arbitrage profit while following the load profile and constraints dictated by the global controller. Extensive simulations are performed for two distribution networks under a wide variety of possible storage and solar penetrations enabled by the controller speed. The simulations show that (i) the two-layer architecture can achieve tenfold improvement in power quality relative to no coordination, while capturing nearly all of the available arbitrage profit for a moderate amount of storage penetration, and (ii) both power quality and arbitrage profits are optimized when the solar and storage are distributed more widely over the network, hence it is more effective to install storage closer to the consumer.",2018-11-01T04:43:49Z,2018-11-01T04:43:49Z,http://arxiv.org/abs/1811.00224v1,http://arxiv.org/pdf/1811.00224v1,"eess.SP, cs.SY"
Gravitational octree code performance evaluation on Volta GPU,Yohei Miki,"In this study, the gravitational octree code originally optimized for the Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta architecture. The Volta architecture introduces independent thread scheduling requiring either the insertion of the explicit synchronizations at appropriate locations or the enforcement of the same implicit synchronizations as do the Pascal or earlier architectures by specifying \texttt{-gencode arch=compute\_60,code=sm\_70}. The performance measurements on Tesla V100, the current flagship GPU by NVIDIA, revealed that the $N$-body simulations of the Andromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \times 10^{-2}$~s or $3.3 \times 10^{-2}$~s per step for each case. Tesla V100 achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the flagship GPU in the previous generation. The observed speed-up of 2.2 is greater than 1.5, which is the ratio of the theoretical peak performance of the two GPUs. The independence of the units for integer operations from those for floating-point number operations enables the overlapped execution of integer and floating-point number operations. It hides the execution time of the integer operations leading to the speed-up rate above the theoretical peak performance ratio. Tesla V100 can execute $N$-body simulation with up to $25 \times 2^{20} = 26214400$ particles, and it took $2.0 \times 10^{-1}$~s per step. It corresponds to $3.5$~TFlop/s, which is 22\% of the single-precision theoretical peak performance.",2018-11-07T05:00:23Z,2018-11-07T05:00:23Z,http://arxiv.org/abs/1811.02761v1,http://arxiv.org/pdf/1811.02761v1,"cs.MS, astro-ph.IM, cs.PF, physics.comp-ph"
Spatially-Coupled Neural Network Architectures,"Arman Hasanzadeh, Nagaraj T. Janakiraman, Vamsi K. Amalladinne, Krishna R. Narayanan","In this work, we leverage advances in sparse coding techniques to reduce the number of trainable parameters in a fully connected neural network. While most of the works in literature impose $\ell_1$ regularization, DropOut or DropConnect techniques to induce sparsity, our scheme considers feature importance as a criterion to allocate the trainable parameters (resources) efficiently in the network. Even though sparsity is ensured, $\ell_1$ regularization requires training on all the resources in a deep neural network. The DropOut/DropConnect techniques reduce the number of trainable parameters in the training stage by dropping a random collection of neurons/edges in the hidden layers. However, both these techniques do not pay heed to the underlying structure in the data when dropping the neurons/edges. Moreover, these frameworks require a storage space equivalent to the number of parameters in a fully connected neural network. We address the above issues with a more structured architecture inspired from spatially-coupled sparse constructions. The proposed architecture is shown to have a performance akin to a conventional fully connected neural network with dropouts, and yet achieving a $94\%$ reduction in the training parameters. Extensive simulations are presented and the performance of the proposed scheme is compared against traditional neural network architectures.",2019-07-03T17:37:35Z,2019-07-03T17:37:35Z,http://arxiv.org/abs/1907.02051v1,http://arxiv.org/pdf/1907.02051v1,"cs.LG, cs.IT, math.IT, stat.ML"
Color Cerberus,"A. ~Savchik, E. ~Ershov, S. ~Karpenko","Simple convolutional neural network was able to win ISISPA color constancy competition. Partial reimplementation of (Bianco, 2017) neural architecture would have shown even better results in this setup.",2019-07-15T13:04:31Z,2019-07-15T13:04:31Z,http://arxiv.org/abs/1907.06483v1,http://arxiv.org/pdf/1907.06483v1,"cs.CV, cs.LG, eess.IV"
Batch-Shaping for Learning Conditional Channel Gated Networks,"Babak Ehteshami Bejnordi, Tijmen Blankevoort, Max Welling","We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool $batch$-$shaping$ that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.",2019-07-15T17:58:04Z,2020-04-03T08:42:24Z,http://arxiv.org/abs/1907.06627v4,http://arxiv.org/pdf/1907.06627v4,"cs.LG, cs.CV, stat.ML"
OmniNet: A unified architecture for multi-modal multi-task learning,"Subhojeet Pramanik, Priyanka Agrawal, Aman Hussain","Transformer is a popularly used neural network architecture, especially for language understanding. We introduce an extended and unified architecture that can be used for tasks involving a variety of modalities like image, text, videos, etc. We propose a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning, thus we refer to it as OmniNet. For example, a single instance of OmniNet can concurrently learn to perform the tasks of part-of-speech tagging, image captioning, visual question answering and video activity recognition. We demonstrate that training these four tasks together results in about three times compressed model while retaining the performance in comparison to training them individually. We also show that using this neural network pre-trained on some modalities assists in learning unseen tasks such as video captioning and video question answering. This illustrates the generalization capacity of the self-attention mechanism on the spatio-temporal cache present in OmniNet.",2019-07-17T22:59:56Z,2020-07-03T09:59:06Z,http://arxiv.org/abs/1907.07804v2,http://arxiv.org/pdf/1907.07804v2,"cs.LG, cs.CL, cs.CV, cs.NE, stat.ML"
Self-Adaptive 2D-3D Ensemble of Fully Convolutional Networks for Medical   Image Segmentation,"Maria G. Baldeon Calisto, Susana K. Lai-Yuen","Segmentation is a critical step in medical image analysis. Fully Convolutional Networks (FCNs) have emerged as powerful segmentation models achieving state-of-the-art results in various medical image datasets. Network architectures are usually designed manually for a specific segmentation task so applying them to other medical datasets requires extensive experience and time. Moreover, the segmentation requires handling large volumetric data that results in big and complex architectures. Recently, methods that automatically design neural networks for medical image segmentation have been presented; however, most approaches either do not fully consider volumetric information or do not optimize the size of the network. In this paper, we propose a novel self-adaptive 2D-3D ensemble of FCNs for medical image segmentation that incorporates volumetric information and optimizes both the model's performance and size. The model is composed of an ensemble of a 2D FCN that extracts intra-slice information, and a 3D FCN that exploits inter-slice information. The architectures of the 2D and 3D FCNs are automatically adapted to a medical image dataset using a multiobjective evolutionary based algorithm that minimizes both the segmentation error and number of parameters in the network. The proposed 2D-3D FCN ensemble was tested on the task of prostate segmentation on the image dataset from the PROMISE12 Grand Challenge. The resulting network is ranked in the top 10 submissions, surpassing the performance of other automatically-designed architectures while being considerably smaller in size.",2019-07-26T14:14:53Z,2019-07-26T14:14:53Z,http://arxiv.org/abs/1907.11587v1,http://arxiv.org/pdf/1907.11587v1,"eess.IV, cs.CV"
Review: Ordinary Differential Equations For Deep Learning,Xinshi Chen,"To better understand and improve the behavior of neural networks, a recent line of works bridged the connection between ordinary differential equations (ODEs) and deep neural networks (DNNs). The connections are made in two folds: (1) View DNN as ODE discretization; (2) View the training of DNN as solving an optimal control problem. The former connection motivates people either to design neural architectures based on ODE discretization schemes or to replace DNN by a continuous model characterized by ODEs. Several works demonstrated distinct advantages of using a continuous model instead of traditional DNN in some specific applications. The latter connection is inspiring. Based on Pontryagin's maximum principle, which is popular in the optimal control literature, some developed new optimization methods for training neural networks and some developed algorithms to train the infinite-deep continuous model with low memory-cost. This paper is organized as follows: In Section 2, the relation between neural architecture and ODE discretization is introduced. Some architectures are not motivated by ODE, but they are later found to be associated with some specific discretization schemes. Some architectures are designed based on ODE discretization and expected to achieve some special properties. Section 3 formulates the optimization problem where a traditional neural network is replaced by a continuous model (ODE). The formulated optimization problem is an optimal control problem. Therefore, two different types of controls will also be discussed in this section. In Section 4, we will discuss how we can utilize the optimization methods that are popular in optimal control literature to help the training of machine learning problems. Finally, two applications of using a continuous model will be shown in Section 5 and 6 to demonstrate some of its advantages over traditional neural networks.",2019-11-01T04:26:22Z,2019-11-01T04:26:22Z,http://arxiv.org/abs/1911.00502v1,http://arxiv.org/pdf/1911.00502v1,"cs.LG, stat.ML"
LGN-CNN: a biologically inspired CNN architecture,"Federico Bertoni, Giovanna Citti, Alessandro Sarti","In this paper we introduce a biologically inspired Convolutional Neural Network (CNN) architecture called LGN-CNN that has a first convolutional layer composed by a single filter that mimics the role of the Lateral Geniculate Nucleus (LGN). The first layer of the neural network shows a rotational symmetric pattern justified by the structure of the net itself that turns up to be an approximation of a Laplacian of Gaussian (LoG). The latter function is in turn a good approximation of the receptive field profiles (RFPs) of the cells in the LGN. The analogy with the visual system is established, emerging directly from the architecture of the neural network. A proof of rotation invariance of the first layer is given on a fixed LGN-CNN architecture and the computational results are shown. Thus, contrast invariance capability of the LGN-CNN is investigated and a comparison between the Retinex effects of the first layer of LGN-CNN and the Retinex effects of a LoG is provided on different images. A statistical study is done on the filters of the second convolutional layer with respect to biological data. In conclusion, the model we have introduced approximates well the RFPs of both LGN and V1 attaining similar behavior as regards long range connections of LGN cells that show Retinex effects.",2019-11-14T18:00:14Z,2021-10-28T15:10:00Z,http://arxiv.org/abs/1911.06276v3,http://arxiv.org/pdf/1911.06276v3,"cs.NE, q-bio.NC"
A Scalable Decoder Micro-architecture for Fault-Tolerant Quantum   Computing,"Poulami Das, Christopher A. Pattison, Srilatha Manne, Douglas Carmean, Krysta Svore, Moinuddin Qureshi, Nicolas Delfosse","Quantum computation promises significant computational advantages over classical computation for some problems. However, quantum hardware suffers from much higher error rates than in classical hardware. As a result, extensive quantum error correction is required to execute a useful quantum algorithm. The decoder is a key component of the error correction scheme whose role is to identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical applications. In this work, we consider surface code error correction, which is the most popular family of error correcting codes for quantum computing, and we design a decoder micro-architecture for the Union-Find decoding algorithm. We propose a three-stage fully pipelined hardware implementation of the decoder that significantly speeds up the decoder. Then, we optimize the amount of decoding hardware required to perform error correction simultaneously over all the logical qubits of the quantum computer. By sharing resources between logical qubits, we obtain a 67% reduction of the number of hardware units and the memory capacity is reduced by 70%. Moreover, we reduce the bandwidth required for the decoding process by a factor at least 30x using low-overhead compression algorithms. Finally, we provide numerical evidence that our optimized micro-architecture can be executed fast enough to correct errors in a quantum computer.",2020-01-18T04:44:52Z,2020-01-18T04:44:52Z,http://arxiv.org/abs/2001.06598v1,http://arxiv.org/pdf/2001.06598v1,"quant-ph, cs.AR"
MEMO: A Deep Network for Flexible Combination of Episodic Memories,"Andrea Banino, Adrià Puigdomènech Badia, Raphael Köster, Martin J. Chadwick, Vinicius Zambaldi, Demis Hassabis, Caswell Barry, Matthew Botvinick, Dharshan Kumaran, Charles Blundell","Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the memory-based reasoning neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed MEMO, an architecture endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories (facts) stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of ""memory hops"" before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as match state of the art results in bAbI.",2020-01-29T15:56:16Z,2020-01-29T15:56:16Z,http://arxiv.org/abs/2001.10913v1,http://arxiv.org/pdf/2001.10913v1,"cs.LG, cs.AI, I.2.6"
LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree   Implementations,"SeyedRamin Rasoulinezhad, Siddhartha, Hao Zhou, Lingli Wang, David Boland, Philip H. W. Leong","We propose two tiers of modifications to FPGA logic cell architecture to deliver a variety of performance and utilization benefits with only minor area overheads. In the irst tier, we augment existing commercial logic cell datapaths with a 6-input XOR gate in order to improve the expressiveness of each element, while maintaining backward compatibility. This new architecture is vendor-agnostic, and we refer to it as LUXOR. We also consider a secondary tier of vendor-speciic modifications to both Xilinx and Intel FPGAs, which we refer to as X-LUXOR+ and I-LUXOR+ respectively. We demonstrate that compressor tree synthesis using generalized parallel counters (GPCs) is further improved with the proposed modifications. Using both the Intel adaptive logic module and the Xilinx slice at the 65nm technology node for a comparative study, it is shown that the silicon area overhead is less than 0.5% for LUXOR and 5-6% for LUXOR+, while the delay increments are 1-6% and 3-9% respectively. We demonstrate that LUXOR can deliver an average reduction of 13-19% in logic utilization on micro-benchmarks from a variety of domains.BNN benchmarks benefit the most with an average reduction of 37-47% in logic utilization, which is due to the highly-efficient mapping of the XnorPopcount operation on our proposed LUXOR+ logic cells.",2020-03-06T05:54:11Z,2020-03-06T05:54:11Z,http://arxiv.org/abs/2003.03043v1,http://arxiv.org/pdf/2003.03043v1,"cs.AR, cs.DC, B.2.1; C.0"
Enabling automated driving by ICT infrastructure: A reference   architecture,"Michael Buchholz, Jan Strohbeck, Anna-Maria Adaktylos, Friedrich Vogl, Gottfried Allmer, Sergio Cabrero Barros, Yassine Lassoued, Markus Wimmer, Birger Hätty, Guillemette Massot, Christophe Ponchel, Maxime Bretin, Vasilis Sourlas, Angelos Amditis","Information and communication technology (ICT) is an enabler for establishing automated vehicles (AVs) in today's traffic systems. By providing complementary and/or redundant information via radio communication to the AV's perception by on-board sensors, higher levels of automated driving become more comfortable, safer, or even possible without interaction by the driver, especially in complex scenarios. Additionally, communication between vehicles and/or a central service can improve the efficiency of traffic flow. This paper presents a reference architecture for such an infrastructure-based support of AVs. The architecture combines innovative concepts and technologies from different technological fields like communication, IT environment and data flows, and cyber-security and privacy. Being the basis for the EU-funded project ICT4CART, exemplary implementations of this architecture will show its power for a variety of use cases on highways and in urban areas in test sites in Austria, Germany, and Italy, including cross-border interoperability.",2020-03-11T11:45:10Z,2020-03-11T11:45:10Z,http://arxiv.org/abs/2003.05229v1,http://arxiv.org/pdf/2003.05229v1,"eess.SY, cs.SY"
Tensor Graph Convolutional Networks for Multi-relational and Robust   Learning,"Vassilis N. Ioannidis, Antonio G. Marques, Georgios B. Giannakis","The era of ""data deluge"" has sparked renewed interest in graph-based learning methods and their widespread applications ranging from sociology and biology to transportation and communications. In this context of graph-aware methods, the present paper introduces a tensor-graph convolutional network (TGCN) for scalable semi-supervised learning (SSL) from data associated with a collection of graphs, that are represented by a tensor. Key aspects of the novel TGCN architecture are the dynamic adaptation to different relations in the tensor graph via learnable weights, and the consideration of graph-based regularizers to promote smoothness and alleviate over-parameterization. The ultimate goal is to design a powerful learning architecture able to: discover complex and highly nonlinear data associations, combine (and select) multiple types of relations, scale gracefully with the graph size, and remain robust to perturbations on the graph edges. The proposed architecture is relevant not only in applications where the nodes are naturally involved in different relations (e.g., a multi-relational graph capturing family, friendship and work relations in a social network), but also in robust learning setups where the graph entails a certain level of uncertainty, and the different tensor slabs correspond to different versions (realizations) of the nominal graph. Numerical tests showcase that the proposed architecture achieves markedly improved performance relative to standard GCNs, copes with state-of-the-art adversarial attacks, and leads to remarkable SSL performance over protein-to-protein interaction networks.",2020-03-15T02:33:21Z,2020-03-15T02:33:21Z,http://arxiv.org/abs/2003.07729v1,http://arxiv.org/pdf/2003.07729v1,"cs.LG, eess.SP, stat.ML"
Capsule GAN Using Capsule Network for Generator Architecture,"Kanako Marusaki, Hiroshi Watanabe","This paper presents Capsule GAN, a Generative adversarial network using Capsule Network not only in the discriminator but also in the generator. Recently, Generative adversarial networks (GANs) has been intensively studied. However, generating images by GANs is difficult. Therefore, GANs sometimes generate poor quality images. These GANs use convolutional neural networks (CNNs). However, CNNs have the defect that the relational information between features of the image may be lost. Capsule Network, proposed by Hinton in 2017, overcomes the defect of CNNs. Capsule GAN reported previously uses Capsule Network in the discriminator. However, instead of using Capsule Network, Capsule GAN reported in previous studies uses CNNs in generator architecture like DCGAN. This paper introduces two approaches to use Capsule Network in the generator. One is to use DigitCaps layer from the discriminator as the input to the generator. DigitCaps layer is the output layer of Capsule Network. It has the features of the input images of the discriminator. The other is to use the reverse operation of recognition process in Capsule Network in the generator. We compare Capsule GAN proposed in this paper with conventional GAN using CNN and Capsule GAN which uses Capsule Network in the discriminator only. The datasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN outperforms the GAN using CNN and the GAN using Capsule Network in the discriminator only. The architecture of Capsule GAN proposed in this paper is a basic architecture using Capsule Network. Therefore, we can apply the existing improvement techniques for GANs to Capsule GAN.",2020-03-18T05:14:51Z,2020-03-18T05:14:51Z,http://arxiv.org/abs/2003.08047v1,http://arxiv.org/pdf/2003.08047v1,"cs.CV, cs.LG, eess.IV, 68T05"
On Infinite-Width Hypernetworks,"Etai Littwin, Tomer Galanti, Lior Wolf, Greg Yang","{\em Hypernetworks} are architectures that produce the weights of a task-specific {\em primary network}. A notable application of hypernetworks in the recent literature involves learning to output functional representations. In these scenarios, the hypernetwork learns a representation corresponding to the weights of a shallow MLP, which typically encodes shape or image information. While such representations have seen considerable success in practice, they remain lacking in the theoretical guarantees in the wide regime of the standard architectures. In this work, we study wide over-parameterized hypernetworks. We show that unlike typical architectures, infinitely wide hypernetworks do not guarantee convergence to a global minima under gradient descent. We further show that convexity can be achieved by increasing the dimensionality of the hypernetwork's output, to represent wide MLPs. In the dually infinite-width regime, we identify the functional priors of these architectures by deriving their corresponding GP and NTK kernels, the latter of which we refer to as the {\em hyperkernel}. As part of this study, we make a mathematical contribution by deriving tight bounds on high order Taylor expansion terms of standard fully connected ReLU networks.",2020-03-27T00:50:29Z,2021-02-22T23:10:56Z,http://arxiv.org/abs/2003.12193v7,http://arxiv.org/pdf/2003.12193v7,"cs.LG, stat.ML"
Combinatorial Geometry of Threshold-Linear Networks,"Carina Curto, Christopher Langdon, Katherine Morrison","The architecture of a neural network constrains the potential dynamics that can emerge. Some architectures may only allow for a single dynamic regime, while others display a great deal of flexibility with qualitatively different dynamics that can be reached by modulating connection strengths. In this work, we develop novel mathematical techniques to study the dynamic constraints imposed by different network architectures in the context of competitive threshold-linear networks (TLNs). Any given TLN is naturally characterized by a hyperplane arrangement in $\mathbb{R}^n$, and the combinatorial properties of this arrangement determine the pattern of fixed points of the dynamics. This observation enables us to recast the question of network flexibility in the language of oriented matroids, allowing us to employ tools and results from this theory in order to characterize the different dynamic regimes a given architecture can support. In particular, fixed points of a TLN correspond to cocircuits of an associated oriented matroid; and mutations of the matroid correspond to bifurcations in the collection of fixed points. As an application, we provide a complete characterization of all possible sets of fixed points that can arise in networks through size $n=3$, together with descriptions of how to modulate synaptic strengths of the network in order to access the different dynamic regimes. These results provide a framework for studying the possible computational roles of various motifs observed in real neural networks.",2020-08-03T17:17:35Z,2020-08-03T17:17:35Z,http://arxiv.org/abs/2008.01032v1,http://arxiv.org/pdf/2008.01032v1,"math.CO, q-bio.NC"
Towards a Reference Architecture Model for Industrial Edge Computing,"Alexander Willner, Varun Gowtham","In the context of the digital transformation of the industry, whole value chains get connected across various application domains; as long as economic, ecologic, or social benefits arise to do so. Under the umbrella of the Industrial Internet of Things (IIoT), traditional Operational Technology (OT) approaches are replaced or at least augmented by Information and Communication Technology (ICT) systems to facilitate this development. To meet industrial requirements, for example, related to privacy, determinism, latency, or autonomy, established Cloud Computing mechanisms are being moved closer to data sources and actuators. Depending on the context, this distributed Cloud Computing paradigm is named Edge Computing or Fog Computing and various challenges have been subject to several publications. However, a proper reference model that describes the multi-dimensional problem space which is being spanned by this paradigm, seems still to be undefined. Such a model should provide orientation, put work in relation and support the identification of current and future research issues. This paper aims to fill this gap with a focus on industrial automation and follows analog models that have been developed for specific domains such as the Smart Grid Architecture Model (SGAM) and the Reference Architecture Model Industrie 4.0 (RAMI4.0). The proposed Reference Architecture Model Edge Computing (RAMEC) identifies 210 views on the Edge Computing paradigm in the manufacturing domain. Future iterations of this model might be used for the classification of relevant research, standardization, and development activities.",2020-08-10T14:45:06Z,2020-08-10T14:45:06Z,http://arxiv.org/abs/2008.04164v1,http://arxiv.org/pdf/2008.04164v1,"cs.DC, Computer systems organization~Cloud computing, C.2.4"
End-to-End Neural Transformer Based Spoken Language Understanding,"Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann","Spoken language understanding (SLU) refers to the process of inferring the semantic information from audio signals. While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing (NLP), their merits in a closely related field, i.e., spoken language understanding (SLU) have not beed investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end transformer SLU predicts the domains, intents and slots in the Fluent Speech Commands dataset with accuracy equal to 98.1 \%, 99.6 \%, and 99.6 \%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4 \% while the size of our model is 25\% smaller than that of these architectures. Additionally, due to independent sub-space projections in the self-attention layer, the model is highly parallelizable which makes it a good candidate for on-device SLU.",2020-08-12T22:58:20Z,2020-08-12T22:58:20Z,http://arxiv.org/abs/2008.10984v1,http://arxiv.org/pdf/2008.10984v1,"cs.CL, cs.SD, eess.AS"
A comparative study of convolutional neural network models for wind   field downscaling,"Kevin Höhlein, Michael Kern, Timothy Hewson, Rüdiger Westermann","We analyze the applicability of convolutional neural network (CNN) architectures for downscaling of short-range forecasts of near-surface winds on extended spatial domains. Short-range wind field forecasts (at the 100 m level) from ECMWF ERA5 reanalysis initial conditions at 31 km horizontal resolution are downscaled to mimic HRES (deterministic) short-range forecasts at 9 km resolution. We evaluate the downscaling quality of four exemplary model architectures and compare these against a multi-linear regression model. We conduct a qualitative and quantitative comparison of model predictions and examine whether the predictive skill of CNNs can be enhanced by incorporating additional atmospheric variables, such as geopotential height and forecast surface roughness, or static high-resolution fields, like land-sea mask and topography. We further propose DeepRU, a novel U-Net-based CNN architecture, which is able to infer situation-dependent wind structures that cannot be reconstructed by other models. Inferring a target 9 km resolution wind field from the low-resolution input fields over the Alpine area takes less than 10 milliseconds on our GPU target architecture, which compares favorably to an overhead in simulation time of minutes or hours between low- and high-resolution forecast simulations.",2020-08-18T09:29:13Z,2020-12-21T12:28:29Z,http://arxiv.org/abs/2008.12257v2,http://arxiv.org/pdf/2008.12257v2,"physics.ao-ph, physics.data-an, stat.AP"
LAVARNET: Neural Network Modeling of Causal Variable Relationships for   Multivariate Time Series Forecasting,"Christos Koutlis, Symeon Papadopoulos, Manos Schinas, Ioannis Kompatsiaris","Multivariate time series forecasting is of great importance to many scientific disciplines and industrial sectors. The evolution of a multivariate time series depends on the dynamics of its variables and the connectivity network of causal interrelationships among them. Most of the existing time series models do not account for the causal effects among the system's variables and even if they do they rely just on determining the between-variables causality network. Knowing the structure of such a complex network and even more specifically knowing the exact lagged variables that contribute to the underlying process is crucial for the task of multivariate time series forecasting. The latter is a rather unexplored source of information to leverage. In this direction, here a novel neural network-based architecture is proposed, termed LAgged VAriable Representation NETwork (LAVARNET), which intrinsically estimates the importance of lagged variables and combines high dimensional latent representations of them to predict future values of time series. Our model is compared with other baseline and state of the art neural network architectures on one simulated data set and four real data sets from meteorology, music, solar activity, and finance areas. The proposed architecture outperforms the competitive architectures in most of the experiments.",2020-09-02T10:57:28Z,2020-09-02T10:57:28Z,http://arxiv.org/abs/2009.00945v1,http://arxiv.org/pdf/2009.00945v1,"cs.LG, cs.NE, stat.ML"
An Efficient Quantitative Approach for Optimizing Convolutional Neural   Networks,"Yuke Wang, Boyuan Feng, Xueqiao Peng, Yufei Ding","With the increasing popularity of deep learning, Convolutional Neural Networks (CNNs) have been widely applied in various domains, such as image classification and object detection, and achieve stunning success in terms of their high accuracy over the traditional statistical methods. To exploit the potential of CNN models, a huge amount of research and industry efforts have been devoted to optimizing CNNs. Among these endeavors, CNN architecture design has attracted tremendous attention because of its great potential of improving model accuracy or reducing model complexity. However, existing work either introduces repeated training overhead in the search process or lacks an interpretable metric to guide the design. To clear these hurdles, we propose 3D-Receptive Field (3DRF), an explainable and easy-to-compute metric, to estimate the quality of a CNN architecture and guide the search process of designs. To validate the effectiveness of 3DRF, we build a static optimizer to improve the CNN architectures at both the stage level and the kernel level. Our optimizer not only provides a clear and reproducible procedure but also mitigates unnecessary training efforts in the architecture search process. Extensive experiments and studies show that the models generated by our optimizer can achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction, compared with state-of-the-art CNN structures like MobileNet and ResNet.",2020-09-11T05:14:34Z,2021-09-15T16:59:45Z,http://arxiv.org/abs/2009.05236v4,http://arxiv.org/pdf/2009.05236v4,"cs.CV, cs.LG, eess.IV"
BET: A Backtranslation Approach for Easy Data Augmentation in   Transformer-based Paraphrase Identification Context,"Jean-Philippe Corbeil, Hadi Abdi Ghadivel","Newly-introduced deep learning architectures, namely BERT, XLNet, RoBERTa and ALBERT, have been proved to be robust on several NLP tasks. However, the datasets trained on these architectures are fixed in terms of size and generalizability. To relieve this issue, we apply one of the most inexpensive solutions to update these datasets. We call this approach BET by which we analyze the backtranslation data augmentation on the transformer-based architectures. Using the Google Translate API with ten intermediary languages from ten different language families, we externally evaluate the results in the context of automatic paraphrase identification in a transformer-based framework. Our findings suggest that BET improves the paraphrase identification performance on the Microsoft Research Paraphrase Corpus (MRPC) to more than 3% on both accuracy and F1 score. We also analyze the augmentation in the low-data regime with downsampled versions of MRPC, Twitter Paraphrase Corpus (TPC) and Quora Question Pairs. In many low-data cases, we observe a switch from a failing model on the test set to reasonable performances. The results demonstrate that BET is a highly promising data augmentation technique: to push the current state-of-the-art of existing datasets and to bootstrap the utilization of deep learning architectures in the low-data regime of a hundred samples.",2020-09-25T22:06:06Z,2020-09-25T22:06:06Z,http://arxiv.org/abs/2009.12452v1,http://arxiv.org/pdf/2009.12452v1,"cs.CL, 68T50"
Breaking the Memory Wall for AI Chip with a New Dimension,"Eugene Tam, Shenfei Jiang, Paul Duan, Shawn Meng, Yue Pang, Cayden Huang, Yi Han, Jacke Xie, Yuanjun Cui, Jinsong Yu, Minggui Lu","Recent advancements in deep learning have led to the widespread adoption of artificial intelligence (AI) in applications such as computer vision and natural language processing. As neural networks become deeper and larger, AI modeling demands outstrip the capabilities of conventional chip architectures. Memory bandwidth falls behind processing power. Energy consumption comes to dominate the total cost of ownership. Currently, memory capacity is insufficient to support the most advanced NLP models. In this work, we present a 3D AI chip, called Sunrise, with near-memory computing architecture to address these three challenges. This distributed, near-memory computing architecture allows us to tear down the performance-limiting memory wall with an abundance of data bandwidth. We achieve the same level of energy efficiency on 40nm technology as competing chips on 7nm technology. By moving to similar technologies as other AI chips, we project to achieve more than ten times the energy efficiency, seven times the performance of the current state-of-the-art chips, and twenty times of memory capacity as compared with the best chip in each benchmark.",2020-09-28T22:34:10Z,2020-09-28T22:34:10Z,http://arxiv.org/abs/2009.13664v1,http://arxiv.org/pdf/2009.13664v1,"cs.AR, cs.AI, cs.CV, cs.LG, eess.IV"
Cascaded WLAN-FWA Networking and Computing Architecture for Pervasive   In-Home Healthcare,"Sergio Martiradonna, Giulia Cisotto, Gennaro Boggia, Giuseppe Piro, Lorenzo Vangelista, Stefano Tomasin","Pervasive healthcare is a promising assisted-living solution for chronic patients. However, current cutting-edge communication technologies are not able to strictly meet the requirements of these applications, especially in the case of life-threatening events. To bridge this gap, this paper proposes a new architecture to support indoor healthcare monitoring, with a focus on epileptic patients. Several novel elements are introduced. The first element is the cascading of a WLAN and a cellular network, where IEEE 802.11ax is used for the wireless local area network to collect physiological and environmental data in-home and 5G-enabled Fixed Wireless Access links transfer them to a remote hospital. The second element is the extension of the network slicing concept to the WLAN, and the introduction of two new slice types to support both regular monitoring and emergency handling. Moreover, the inclusion of local computing capabilities at the WLAN router, together with a mobile edge computing resource, represents a further architectural enhancement. Local computation is required to trigger not only health-related alarms, but also the network slicing change in case of emergency: in fact, proper radio resource scheduling is necessary for the cascaded networks to handle healthcare traffic together with other promiscuous everyday communication services. Numerical results demonstrate the effectiveness of the proposed approach while highlighting the performance gain achieved with respect to baseline solutions.",2020-10-08T07:16:00Z,2020-10-08T07:16:00Z,http://arxiv.org/abs/2010.03805v1,http://arxiv.org/pdf/2010.03805v1,"cs.NI, cs.CY, cs.DC, eess.SP"
Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel   Segmentation Using a Genetic Algorithm,"Jiahong Wei, Zhun Fan","Recently, many methods based on hand-designed convolutional neural networks (CNNs) have achieved promising results in automatic retinal vessel segmentation. However, these CNNs remain constrained in capturing retinal vessels in complex fundus images. To improve their segmentation performance, these CNNs tend to have many parameters, which may lead to overfitting and high computational complexity. Moreover, the manual design of competitive CNNs is time-consuming and requires extensive empirical knowledge. Herein, a novel automated design method, called Genetic U-Net, is proposed to generate a U-shaped CNN that can achieve better retinal vessel segmentation but with fewer architecture-based parameters, thereby addressing the above issues. First, we devised a condensed but flexible search space based on a U-shaped encoder-decoder. Then, we used an improved genetic algorithm to identify better-performing architectures in the search space and investigated the possibility of finding a superior network architecture with fewer parameters. The experimental results show that the architecture obtained using the proposed method offered a superior performance with less than 1% of the number of the original U-Net parameters in particular and with significantly fewer parameters than other state-of-the-art models. Furthermore, through in-depth investigation of the experimental results, several effective operations and patterns of networks to generate superior retinal vessel segmentations were identified.",2020-10-29T13:31:36Z,2021-06-11T09:58:57Z,http://arxiv.org/abs/2010.15560v4,http://arxiv.org/pdf/2010.15560v4,"eess.IV, cs.CV"
Correlator Convolutional Neural Networks: An Interpretable Architecture   for Image-like Quantum Matter Data,"Cole Miles, Annabelle Bohrdt, Ruihan Wu, Christie Chiu, Muqing Xu, Geoffrey Ji, Markus Greiner, Kilian Q. Weinberger, Eugene Demler, Eun-Ah Kim","Machine learning models are a powerful theoretical tool for analyzing data from quantum simulators, in which results of experiments are sets of snapshots of many-body states. Recently, they have been successfully applied to distinguish between snapshots that can not be identified using traditional one and two point correlation functions. Thus far, the complexity of these models has inhibited new physical insights from this approach. Here, using a novel set of nonlinearities we develop a network architecture that discovers features in the data which are directly interpretable in terms of physical observables. In particular, our network can be understood as uncovering high-order correlators which significantly differ between the data studied. We demonstrate this new architecture on sets of simulated snapshots produced by two candidate theories approximating the doped Fermi-Hubbard model, which is realized in state-of-the art quantum gas microscopy experiments. From the trained networks, we uncover that the key distinguishing features are fourth-order spin-charge correlators, providing a means to compare experimental data to theoretical predictions. Our approach lends itself well to the construction of simple, end-to-end interpretable architectures and is applicable to arbitrary lattice data, thus paving the way for new physical insights from machine learning studies of experimental as well as numerical data.",2020-11-06T17:04:10Z,2020-11-06T17:04:10Z,http://arxiv.org/abs/2011.03474v1,http://arxiv.org/pdf/2011.03474v1,"cond-mat.str-el, cond-mat.dis-nn, cond-mat.quant-gas, cs.LG, physics.comp-ph"
REAL-X -- Robot open-Ended Autonomous Learning Architectures: Achieving   Truly End-to-End Sensorimotor Autonomous Learning Systems,"Emilio Cartoni, Davide Montella, Jochen Triesch, Gianluca Baldassarre","Open-ended learning is a core research field of developmental robotics and AI aiming to build learning machines and robots that can autonomously acquire knowledge and skills incrementally as infants and children. The first contribution of this work is to study the challenges posed by the previously proposed benchmark `REAL competition' aiming to foster the development of truly open-ended learning robot architectures. The competition involves a simulated camera-arm robot that: (a) in a first `intrinsic phase' acquires sensorimotor competence by autonomously interacting with objects; (b) in a second `extrinsic phase' is tested with tasks unknown in the intrinsic phase to measure the quality of knowledge previously acquired. This benchmark requires the solution of multiple challenges usually tackled in isolation, in particular exploration, sparse-rewards, object learning, generalisation, task/goal self-generation, and autonomous skill learning. As a second contribution, we present a set of `REAL-X' robot architectures that are able to solve different versions of the benchmark, where we progressively release initial simplifications. The architectures are based on a planning approach that dynamically increases abstraction, and intrinsic motivations to foster exploration. REAL-X achieves a good performance level in very demanding conditions. We argue that the REAL benchmark represents a valuable tool for studying open-ended learning in its hardest form.",2020-11-27T18:12:06Z,2022-03-02T11:37:18Z,http://arxiv.org/abs/2011.13880v2,http://arxiv.org/pdf/2011.13880v2,"cs.RO, cs.AI, I.2.9"
Multimodal Transfer Learning-based Approaches for Retinal Vascular   Segmentation,"José Morano, Álvaro S. Hervella, Noelia Barreira, Jorge Novo, José Rouco","In ophthalmology, the study of the retinal microcirculation is a key issue in the analysis of many ocular and systemic diseases, like hypertension or diabetes. This motivates the research on improving the retinal vasculature segmentation. Nowadays, Fully Convolutional Neural Networks (FCNs) usually represent the most successful approach to image segmentation. However, the success of these models is conditioned by an adequate selection and adaptation of the architectures and techniques used, as well as the availability of a large amount of annotated data. These two issues become specially relevant when applying FCNs to medical image segmentation as, first, the existent models are usually adjusted from broad domain applications over photographic images, and second, the amount of annotated data is usually scarcer. In this work, we present multimodal transfer learning-based approaches for retinal vascular segmentation, performing a comparative study of recent FCN architectures. In particular, to overcome the annotated data scarcity, we propose the novel application of self-supervised network pretraining that takes advantage of existent unlabelled multimodal data. The results demonstrate that the self-supervised pretrained networks obtain significantly better vascular masks with less training in the target task, independently of the network architecture, and that some FCN architecture advances motivated for broad domain applications do not translate into significant improvements over the vasculature segmentation task.",2020-12-18T10:38:35Z,2020-12-18T10:38:35Z,http://arxiv.org/abs/2012.10160v1,http://arxiv.org/pdf/2012.10160v1,"eess.IV, cs.CV"
RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs   using a Novel Multi-scale Generative Adversarial Network,"Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod, Kenton M. Sanders, Salah A. Baker","High fidelity segmentation of both macro and microvascular structure of the retina plays a pivotal role in determining degenerative retinal diseases, yet it is a difficult problem. Due to successive resolution loss in the encoding phase combined with the inability to recover this lost information in the decoding phase, autoencoding based segmentation approaches are limited in their ability to extract retinal microvascular structure. We propose RV-GAN, a new multi-scale generative architecture for accurate retinal vessel segmentation to alleviate this. The proposed architecture uses two generators and two multi-scale autoencoding discriminators for better microvessel localization and segmentation. In order to avoid the loss of fidelity suffered by traditional GAN-based segmentation systems, we introduce a novel weighted feature matching loss. This new loss incorporates and prioritizes features from the discriminator's decoder over the encoder. Doing so combined with the fact that the discriminator's decoder attempts to determine real or fake images at the pixel level better preserves macro and microvascular structure. By combining reconstruction and weighted feature matching loss, the proposed architecture achieves an area under the curve (AUC) of 0.9887, 0.9914, and 0.9887 in pixel-wise segmentation of retinal vasculature from three publicly available datasets, namely DRIVE, CHASE-DB1, and STARE, respectively. Additionally, RV-GAN outperforms other architectures in two additional relevant metrics, mean intersection-over-union (Mean-IOU) and structural similarity measure (SSIM).",2021-01-03T01:04:49Z,2021-05-14T08:22:07Z,http://arxiv.org/abs/2101.00535v2,http://arxiv.org/pdf/2101.00535v2,"eess.IV, cs.CV"
A review paper of bio-inspired environmental adaptive and precisely   maneuverable soft robots,Mengqi Shen,"This paper summarizes the most recent research in soft robotic field from the factors of material, actuation, mechanicsproperty, dimension & scale and architecture, and then presents the relations among the functionalities, manufacturing process and the factors mentioned above.",2021-01-10T21:26:21Z,2021-01-10T21:26:21Z,http://arxiv.org/abs/2101.03171v1,http://arxiv.org/pdf/2101.03171v1,"cs.RO, physics.bio-ph"
Trilevel Neural Architecture Search for Efficient Single Image   Super-Resolution,"Yan Wu, Zhiwu Huang, Suryansh Kumar, Rhea Sanjay Sukthanker, Radu Timofte, Luc Van Gool","Modern solutions to the single image super-resolution (SISR) problem using deep neural networks aim not only at better performance accuracy but also at a lighter and computationally efficient model. To that end, recently, neural architecture search (NAS) approaches have shown some tremendous potential. Following the same underlying, in this paper, we suggest a novel trilevel NAS method that provides a better balance between different efficiency metrics and performance to solve SISR. Unlike available NAS, our search is more complete, and therefore it leads to an efficient, optimized, and compressed architecture. We innovatively introduce a trilevel search space modeling, i.e., hierarchical modeling on network-, cell-, and kernel-level structures. To make the search on trilevel spaces differentiable and efficient, we exploit a new sparsestmax technique that is excellent at generating sparse distributions of individual neural architecture candidates so that they can be better disentangled for the final selection from the enlarged search space. We further introduce the sorting technique to the sparsestmax relaxation for better network-level compression. The proposed NAS optimization additionally facilitates simultaneous search and training in a single phase, reducing search time and train time. Comprehensive evaluations on the benchmark datasets show our method's clear superiority over the state-of-the-art NAS in terms of a good trade-off between model size, performance, and efficiency.",2021-01-17T12:19:49Z,2021-04-23T15:50:09Z,http://arxiv.org/abs/2101.06658v2,http://arxiv.org/pdf/2101.06658v2,"cs.CV, cs.LG, eess.IV"
"Hybrid Beamforming for Terahertz Wireless Communications: Challenges,   Architectures, and Open Problems","Chong Han, Longfei Yan, Jinhong Yuan","Terahertz (THz) communications are regarded as a pillar technology for the sixth generation (6G) wireless systems, by offering multi-ten-GHz bandwidth. To overcome the short transmission distance and huge propagation loss, ultra-massive (UM) MIMO systems that employ sub-millimeter wavelength antennas array are proposed to enable an enticingly high array gain. In the UM-MIMO systems, hybrid beamforming stands out for its great potential in promisingly high data rate and reduced power consumption. In this paper, challenges and features of the THz hybrid beamforming design are investigated, in light of the distinctive THz peculiarities. Specifically, we demonstrate that the spatial degree-of-freedom (SDoF) is less than 5, which is caused by the extreme sparsity of the THz channel. The blockage problem caused by the huge reflection and scattering losses, as high as 15 dB or over, is studied. Moreover, we analyze the challenges led by the array containing 1024 or more antennas, including the requirement for intelligent subarray architecture, strict energy efficiency, and propagation characterization based on spherical-wave propagation mechanisms. Owning up to hundreds of GHz bandwidth, beam squint effect could cause over 5~dB array gain loss, when the fractional bandwidth exceeds 10%. Inspired by these facts, three novel THz-specific hybrid beamforming architectures are presented, including widely-spaced multi-subarray, dynamic array-of-subarrays, and true-time-delay-based architectures. We also demonstrate the potential data rate, power consumption, and array gain capabilities for THz communications. As a roadmap of THz hybrid beamforming design, multiple open problems and potential research directions are elaborated.",2021-01-21T07:11:59Z,2021-01-21T07:11:59Z,http://arxiv.org/abs/2101.08469v1,http://arxiv.org/pdf/2101.08469v1,"cs.IT, eess.SP, math.IT"
Automatic Liver Segmentation from CT Images Using Deep Learning   Algorithms: A Comparative Study,"K. E. Sengun, Y. T. Cetin, M. S Guzel, S. Can, E. Bostanci","Medical imaging has been employed to support medical diagnosis and treatment. It may also provide crucial information to surgeons to facilitate optimal surgical preplanning and perioperative management. Essentially, semi-automatic organ and tumor segmentation has been studied by many researchers. Recently, with the development of Deep Learning (DL) algorithms, automatic organ segmentation has been gathered lots of attention from the researchers. This paper addresses to propose the most efficient DL architectures for Liver segmentation by adapting and comparing state-of-the-art DL frameworks, studied in different disciplines. These frameworks are implemented and adapted into a Commercial software, 'LiverVision'. It is aimed to reveal the most effective and accurate DL architecture for fully automatic liver segmentation. Equal conditions were provided to all architectures in the experiments so as to measure the effectiveness of algorithms accuracy, and Dice coefficient metrics were also employed to support comparative analysis. Experimental results prove that 'U-Net' and 'SegNet' have been superior in line with the experiments conducted considering the concepts of time, cost, and effectiveness. Considering both architectures, 'SegNet' was observed to be more successful in eliminating false-positive values. Besides, it was seen that the accuracy metric used to measure effectiveness in image segmentation alone was not enough. Results reveal that DL algorithms are able to automate organ segmentation from DICOM images with high accuracy. This contribution is critical for surgical preplanning and motivates author to apply this approach to the different organs and field of medicine.",2021-01-25T10:05:46Z,2021-01-25T10:05:46Z,http://arxiv.org/abs/2101.09987v1,http://arxiv.org/pdf/2101.09987v1,"eess.IV, cs.CV, cs.LG"
Evolutionary Multi-objective Architecture Search Framework: Application   to COVID-19 3D CT Classification,"Xin He, Guohao Ying, Jiyong Zhang, Xiaowen Chu","The COVID-19 pandemic has threatened global health. Many studies have applied deep convolutional neural networks (CNN) to recognize COVID-19 based on chest 3D computed tomography (CT). Recent works show that no model generalizes well across CT datasets from different countries, and manually designing models for specific datasets requires expertise; thus, neural architecture search (NAS) that aims to search models automatically has become an attractive solution. To reduce the search cost on large 3D CT datasets, most NAS-based works use the weight-sharing (WS) strategy to make all models share weights within a supernet; however, WS inevitably incurs search instability, leading to inaccurate model estimation. In this work, we propose an efficient Evolutionary Multi-objective ARchitecture Search (EMARS) framework. We propose a new objective, namely potential, which can help exploit promising models to indirectly reduce the number of models involved in weights training, thus alleviating search instability. We demonstrate that under objectives of accuracy and potential, EMARS can balance exploitation and exploration, i.e., reducing search time and finding better models. Our searched models are small and perform better than prior works on three public COVID-19 3D CT datasets.",2021-01-26T09:52:42Z,2022-07-08T06:28:40Z,http://arxiv.org/abs/2101.10667v2,http://arxiv.org/pdf/2101.10667v2,"eess.IV, cs.CV"
Resource-aware Time Series Imaging Classification for Wireless Link   Layer Anomalies,"Blaž Bertalanič, Marko Meža, Carolina Fortuna","The number of end devices that use the last mile wireless connectivity is dramatically increasing with the rise of smart infrastructures and require reliable functioning to support smooth and efficient business processes. To efficiently manage such massive wireless networks, more advanced and accurate network monitoring and malfunction detection solutions are required. In this paper, we perform a first time analysis of image-based representation techniques for wireless anomaly detection using recurrence plots and Gramian angular fields and propose a new deep learning architecture enabling accurate anomaly detection. We elaborate on the design considerations for developing a resource aware architecture and propose a new model using time-series to image transformation using recurrence plots. We show that the proposed model a) outperforms the one based on Grammian angular fields by up to 14 percentage points, b) outperforms classical ML models using dynamic time warping by up to 24 percentage points, c) outperforms or performs on par with mainstream architectures such as AlexNet and VGG11 while having <10 times their weights and up to $\approx$8\% of their computational complexity and d) outperforms the state of the art in the respective application area by up to 55 percentage points. Finally, we also explain on randomly chosen examples how the classifier takes decisions.",2021-04-02T10:23:06Z,2021-11-23T11:32:01Z,http://arxiv.org/abs/2104.00972v2,http://arxiv.org/pdf/2104.00972v2,"cs.LG, cs.NI, eess.SP"
MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for   Discriminative Music Modeling on Raw Waveforms,"Kai Middlebrook, Shyam Sudhakaran, David Guy Brizan","In this work, we aim to improve the expressive capacity of waveform-based discriminative music networks by modeling both sequential (temporal) and hierarchical information in an efficient end-to-end architecture. We present MuSLCAT, or Multi-scale and Multi-level Convolutional Attention Transformer, a novel architecture for learning robust representations of complex music tags directly from raw waveform recordings. We also introduce a lightweight variant of MuSLCAT called MuSLCAN, short for Multi-scale and Multi-level Convolutional Attention Network. Both MuSLCAT and MuSLCAN model features from multiple scales and levels by integrating a frontend-backend architecture. The frontend targets different frequency ranges while modeling long-range dependencies and multi-level interactions by using two convolutional attention networks with attention-augmented convolution (AAC) blocks. The backend dynamically recalibrates multi-scale and level features extracted from the frontend by incorporating self-attention. The difference between MuSLCAT and MuSLCAN is their backend components. MuSLCAT's backend is a modified version of BERT. While MuSLCAN's is a simple AAC block. We validate the proposed MuSLCAT and MuSLCAN architectures by comparing them to state-of-the-art networks on four benchmark datasets for music tagging and genre recognition. Our experiments show that MuSLCAT and MuSLCAN consistently yield competitive results when compared to state-of-the-art waveform-based models yet require considerably fewer parameters.",2021-04-06T06:17:22Z,2021-04-06T06:17:22Z,http://arxiv.org/abs/2104.02309v1,http://arxiv.org/pdf/2104.02309v1,"cs.SD, cs.LG, eess.AS"
Fast Design Space Exploration of Nonlinear Systems: Part II,"Prerit Terway, Kenza Hamidouche, Niraj K. Jha","Nonlinear system design is often a multi-objective optimization problem involving search for a design that satisfies a number of predefined constraints. The design space is typically very large since it includes all possible system architectures with different combinations of components composing each architecture. In this article, we address nonlinear system design space exploration through a two-step approach encapsulated in a framework called Fast Design Space Exploration of Nonlinear Systems (ASSENT). In the first step, we use a genetic algorithm to search for system architectures that allow discrete choices for component values or else only component values for a fixed architecture. This step yields a coarse design since the system may or may not meet the target specifications. In the second step, we use an inverse design to search over a continuous space and fine-tune the component values with the goal of improving the value of the objective function. We use a neural network to model the system response. The neural network is converted into a mixed-integer linear program for active learning to sample component values efficiently. We illustrate the efficacy of ASSENT on problems ranging from nonlinear system design to design of electrical circuits. Experimental results show that ASSENT achieves the same or better value of the objective function compared to various other optimization techniques for nonlinear system design by up to 54%. We improve sample efficiency by 6-10x compared to reinforcement learning based synthesis of electrical circuits.",2021-04-05T16:11:50Z,2021-04-08T19:35:15Z,http://arxiv.org/abs/2104.02464v2,http://arxiv.org/pdf/2104.02464v2,"eess.SY, cs.AI, cs.LG, cs.NE, cs.SY"
Certified Control: An Architecture for Verifiable Safety of Autonomous   Vehicles,"Daniel Jackson, Valerie Richmond, Mike Wang, Jeff Chow, Uriel Guajardo, Soonho Kong, Sergio Campos, Geoffrey Litt, Nikos Arechiga","Widespread adoption of autonomous cars will require greater confidence in their safety than is currently possible. Certified control is a new safety architecture whose goal is two-fold: to achieve a very high level of safety, and to provide a framework for justifiable confidence in that safety. The key idea is a runtime monitor that acts, along with sensor hardware and low-level control and actuators, as a small trusted base, ensuring the safety of the system as a whole.   Unfortunately, in current systems complex perception makes the verification even of a runtime monitor challenging. Unlike traditional runtime monitoring, therefore, a certified control monitor does not perform perception and analysis itself. Instead, the main controller assembles evidence that the proposed action is safe into a certificate that is then checked independently by the monitor. This exploits the classic gap between the costs of finding and checking. The controller is assigned the task of finding the certificate, and can thus use the most sophisticated algorithms available (including learning-enabled software); the monitor is assigned only the task of checking, and can thus run quickly and be smaller and formally verifiable.   This paper explains the key ideas of certified control and illustrates them with a certificate for LiDAR data and its formal verification. It shows how the architecture dramatically reduces the amount of code to be verified, providing an end-to-end safety analysis that would likely not be achievable in a traditional architecture.",2021-03-29T01:12:15Z,2021-03-29T01:12:15Z,http://arxiv.org/abs/2104.06178v1,http://arxiv.org/pdf/2104.06178v1,"cs.RO, cs.LO, cs.SE, cs.SY, eess.SY"
Analytical Methods for High-Rate Global Quantum Networks,"Cillian Harney, Stefano Pirandola","The development of a future, global quantum communication network (or quantum internet) will enable high rate private communication and entanglement distribution over very long distances. However, the large-scale performance of ground-based quantum networks (which employ photons as information carriers through optical-fibres) is fundamentally limited by fibre quality and link length, with the latter being a primary design factor for practical network architectures. While these fundamental limits are well established for arbitrary network topologies, the question of how to best design global architectures remains open. In this work, we introduce a large-scale quantum network model called weakly-regular architectures. Such networks are capable of idealising network connectivity, provide freedom to capture a broad class of spatial topologies and remain analytically treatable. This allows us to investigate the effectiveness of large-scale networks with consistent connective properties, and unveil critical conditions under which end-to-end rates remain optimal. Furthermore, through a strict performance comparison of ideal, ground-based quantum networks with that of realistic satellite quantum communication protocols, we establish conditions for which satellites can be used to outperform fibre-based quantum infrastructure; {rigorously proving the efficacy of satellite-based technologies for global quantum communications.",2021-04-21T18:01:43Z,2022-01-24T10:48:16Z,http://arxiv.org/abs/2104.10701v2,http://arxiv.org/pdf/2104.10701v2,"quant-ph, cs.NI"
Inductive biases and Self Supervised Learning in modelling a physical   heating system,Cristian Vicas,"Model Predictive Controllers (MPC) require a good model for the controlled process. In this paper I infer inductive biases about a physical system. I use these biases to derive a new neural network architecture that can model this real system that has noise and inertia. The main inductive biases exploited here are: the delayed impact of some inputs on the system and the separability between the temporal component and how the inputs interact to produce the output of a system. The inputs are independently delayed using shifted convolutional kernels. Feature interactions are modelled using a fully connected network that does not have access to temporal information. The available data and the problem setup allow the usage of Self Supervised Learning in order to train the models. The baseline architecture is an Attention based Reccurent network adapted to work with MPC like inputs. The proposed networks are faster, better at exploiting larger data volumes and are almost as good as baseline networks in terms of prediction performance. The proposed architecture family called Delay can be used in a real scenario to control systems with delayed responses with respect to its controls or inputs. Ablation studies show that the presence of delay kernels are vital to obtain any learning in proposed architecture. Code and some experimental data are available online.",2021-04-23T08:50:41Z,2021-04-23T08:50:41Z,http://arxiv.org/abs/2104.11478v1,http://arxiv.org/pdf/2104.11478v1,"cs.LG, cs.SY, eess.SY, I.2.8"
"Transformers: ""The End of History"" for NLP?","Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov","Recent advances in neural architectures, such as the Transformer, coupled with the emergence of large-scale pre-trained models such as BERT, have revolutionized the field of Natural Language Processing (NLP), pushing the state of the art for a number of NLP tasks. A rich family of variations of these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but fundamentally, they all remain limited in their ability to model certain kinds of information, and they cannot cope with certain information sources, which was easy for pre-existing models. Thus, here we aim to shed light on some important theoretical limitations of pre-trained BERT-style models that are inherent in the general Transformer architecture. First, we demonstrate in practice on two general types of tasks -- segmentation and segment labeling -- and on four datasets that these limitations are indeed harmful and that addressing them, even in some very simple and naive ways, can yield sizable improvements over vanilla RoBERTa and XLNet models. Then, we offer a more general discussion on desiderata for future additions to the Transformer architecture that would increase its expressiveness, which we hope could help in the design of the next generation of deep NLP architectures.",2021-04-09T08:29:42Z,2021-09-23T06:00:54Z,http://arxiv.org/abs/2105.00813v2,http://arxiv.org/pdf/2105.00813v2,"cs.CL, cs.IR, cs.LG, 68T50, F.2.2; I.2.7"
Hierarchical Graph Neural Networks,Stanislav Sobolevsky,"Over the recent years, Graph Neural Networks have become increasingly popular in network analytic and beyond. With that, their architecture noticeable diverges from the classical multi-layered hierarchical organization of the traditional neural networks. At the same time, many conventional approaches in network science efficiently utilize the hierarchical approaches to account for the hierarchical organization of the networks, and recent works emphasize their critical importance. This paper aims to connect the dots between the traditional Neural Network and the Graph Neural Network architectures as well as the network science approaches, harnessing the power of the hierarchical network organization. A Hierarchical Graph Neural Network architecture is proposed, supplementing the original input network layer with the hierarchy of auxiliary network layers and organizing the computational scheme updating the node features through both - horizontal network connections within each layer as well as the vertical connection between the layers. It enables simultaneous learning of the individual node features along with the aggregated network features at variable resolution and uses them to improve the convergence and stability of the individual node feature learning. The proposed Hierarchical Graph Neural network architecture is successfully evaluated on the network embedding and modeling as well as network classification, node labeling, and community tasks and demonstrates increased efficiency in those.",2021-05-07T16:47:18Z,2021-05-14T14:35:38Z,http://arxiv.org/abs/2105.03388v2,http://arxiv.org/pdf/2105.03388v2,"cs.LG, cs.AI, math.CO, physics.data-an, 68T07, 05C85"
Architectural bone parameters and the relationship to titanium lattice   design for powder bed fusion additive manufacturing,"Martine McGregor, Sagar Patel, Stewart McLachlin, Mihaela Vlasea","Additive manufacturing (AM) of titanium (Ti) and Ti-6Al-4V lattices has been proposed for bone implants and augmentation devices. Ti and Ti-6Al-4V have favourable biocompatibility, corrosion resistance and fatigue strength for bone applications; yet, the optimal parameters for Ti-6Al-4V lattice designs corresponding to the natural micro- and meso-scale architecture of human trabecular and cortical bone are not well understood. A comprehensive review was completed to compare the natural lattice architecture properties in human bone to Ti and Ti-6Al-4V lattice structures for bone replacement and repair. Ti and Ti-6Al-4V lattice porosity has varied from 15% to 97% with most studies reporting a porosity between 50-70%. Cortical bone is roughly 5-15% porous and lattices with 50-70% porosity are able to achieve comparable stiffness, compressive strength, and yield strength. Trabecular bone has a reported porosity range from 70-90%, with trabecular thickness varying from 120-200 {\mu}m. Existing powder bed fusion technologies have produced strut and wall thicknesses ranging from 200-1669 {\mu}m. This suggests limited overlap between current AM of Ti and Ti-6Al-4V lattice structures and trabecular bone architecture, indicating that replicating natural trabecular bone parameters with latticing is prohibitively challenging. This review contributes to the body of knowledge by identifying the correspondence of Ti and Ti-6Al-4V lattices to the natural parameters of bone microarchitectures, and provides further guidance on the design and AM recommendations towards addressing recognized performance gaps with powder bed fusion technologies.",2021-05-17T15:33:47Z,2021-07-02T17:13:08Z,http://arxiv.org/abs/2105.07945v2,http://arxiv.org/pdf/2105.07945v2,"cond-mat.mtrl-sci, physics.app-ph"
Cross-Platform Simulation Architecture with application to truck   platooning impact assessment,"Andres Ladino, Lin Xiao, Kingsley Adjenugwhure, Nicolás Deschle, Gerdien Klunder","Simulation-based traffic impact assessment studies of advanced technologies such as truck platooning need to be carried out to ascertain their benefits for traffic efficiency, safety and environment. To reduce uncertainty in the results of such simulation-based studies, the same simulation studies can be performed in different simulation software. Many traffic simulation software packages (Aimsun, SymuVia, Vissim, SUMO) are currently available for traffic impact assessment of new technologies such as truckplatooning. However, to fully model and simulate the functionalities of such advanced technologies in different simulation environments, several extensions need to be made to the simulation platforms. In most cases, these extensions have to be programmed in different programming languages (C++, Python) and each simulator has its own simulator specific API. This makes it difficult to reuse software written for a specific functionality in one simulation platform in a different simulation platform. To overcome this issue, this paper presents a novel architecture for cross-platform simulation. The architecture is designed such that a specific functionality such as truck-platooning or any other functionality is made platform independent. We designed a cross-platform architecture for simulating a truck-platooning functionality using Vissim and SymuVia simulation software to determine the traffic flow effects of multi-brand truck platooning in the context of the EU project ENSEMBLE. In this draft paper, we present the structure of the framework as well as some preliminary results from a simple simulation performed with the cross-platform simulator.",2021-05-19T08:32:07Z,2021-05-19T08:32:07Z,http://arxiv.org/abs/2105.08987v1,http://arxiv.org/pdf/2105.08987v1,"eess.SY, cs.SY"
Quantifying the Similarity of Planetary System Architectures,"Dolev Bashi, Shay Zucker","The planetary systems detected so far already exhibit a wide diversity of architectures, and various methods are proposed to study quantitatively this diversity. Straightforward ways to quantify the difference between two systems and more generally, two sets of multiplanetary systems, are useful tools in the study of this diversity. In this work we present a novel approach, using a Weighted extension of the Energy Distance (WED) metric, to quantify the difference between planetary systems on the logarithmic period-radius plane. We demonstrate the use of this metric and its relation to previously introduced descriptive measures to characterise the arrangements of Kepler planetary systems. By applying exploratory machine learning tools, we attempt to find whether there is some order that can be ascribed to the set of Kepler multiplanet system architectures. Based on WED, the 'Sequencer', which is such an automatic tool, identifies a progression from small and compact planetary systems to systems with distant giant planets. It is reassuring to see that a WED-based tool indeed identifies this progression. Next, we extend WED to define the Inter-Catalogue Energy Distance (ICED) - a distance metric between sets of multiplanetary systems. We have made the specific implementation presented in the paper available to the community through a public repository. We suggest to use these metrics as complementary tools in attempting to compare between architectures of planetary system, and in general, catalogues of planetary systems.",2021-06-01T18:00:03Z,2021-06-01T18:00:03Z,http://arxiv.org/abs/2106.00688v1,http://arxiv.org/pdf/2106.00688v1,"astro-ph.EP, astro-ph.IM, stat.AP"
DeepCompress: Efficient Point Cloud Geometry Compression,"Ryan Killea, Yun Li, Saeed Bastani, Paul McLachlan","Point clouds are a basic data type that is increasingly of interest as 3D content becomes more ubiquitous. Applications using point clouds include virtual, augmented, and mixed reality and autonomous driving. We propose a more efficient deep learning-based encoder architecture for point clouds compression that incorporates principles from established 3D object detection and image compression architectures. Through an ablation study, we show that incorporating the learned activation function from Computational Efficient Neural Image Compression (CENIC) and designing more parameter-efficient convolutional blocks yields dramatic gains in efficiency and performance. Our proposed architecture incorporates Generalized Divisive Normalization activations and propose a spatially separable InceptionV4-inspired block. We then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized Full Bodies dataset to evaluate our model's performance. Our proposed modifications outperform the baseline approaches by a small margin in terms of Bjontegard delta rate and PSNR values, yet reduces necessary encoder convolution operations by 8 percent and reduces total encoder parameters by 20 percent. Our proposed architecture, when considered on its own, has a small penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit rate in Point to Plane Distance for the same peak signal-to-noise ratio.",2021-06-02T23:18:11Z,2021-06-02T23:18:11Z,http://arxiv.org/abs/2106.01504v1,http://arxiv.org/pdf/2106.01504v1,"cs.CV, cs.GR, cs.LG, eess.IV"
Redefining Neural Architecture Search of Heterogeneous Multi-Network   Models by Characterizing Variation Operators and Model Components,"Unai Garciarena, Roberto Santana, Alexander Mendiburu","With neural architecture search methods gaining ground on manually designed deep neural networks -even more rapidly as model sophistication escalates-, the research trend shifts towards arranging different and often increasingly complex neural architecture search spaces. In this conjuncture, delineating algorithms which can efficiently explore these search spaces can result in a significant improvement over currently used methods, which, in general, randomly select the structural variation operator, hoping for a performance gain. In this paper, we investigate the effect of different variation operators in a complex domain, that of multi-network heterogeneous neural models. These models have an extensive and complex search space of structures as they require multiple sub-networks within the general model in order to answer to different output types. From that investigation, we extract a set of general guidelines, whose application is not limited to that particular type of model, and are useful to determine the direction in which an architecture optimization method could find the largest improvement. To deduce the set of guidelines, we characterize both the variation operators, according to their effect on the complexity and performance of the model; and the models, relying on diverse metrics which estimate the quality of the different parts composing it.",2021-06-16T17:12:26Z,2022-08-17T17:38:04Z,http://arxiv.org/abs/2106.08972v2,http://arxiv.org/pdf/2106.08972v2,"cs.NE, 68T07, I.2.6"
An Edge Computing Paradigm for Massive IoT Connectivity over   High-Altitude Platform Networks,"Malong Ke, Zhen Gao, Yang Huang, Guoru Ding, Derrick Wing Kwan Ng, Qihui Wu, Jun Zhang","With the advent of the Internet-of-Things (IoT) era, the ever-increasing number of devices and emerging applications have triggered the need for ubiquitous connectivity and more efficient computing paradigms. These stringent demands have posed significant challenges to the current wireless networks and their computing architectures. In this article, we propose a high-altitude platform (HAP) network-enabled edge computing paradigm to tackle the key issues of massive IoT connectivity. Specifically, we first provide a comprehensive overview of the recent advances in non-terrestrial network-based edge computing architectures. Then, the limitations of the existing solutions are further summarized from the perspectives of the network architecture, random access procedure, and multiple access techniques. To overcome the limitations, we propose a HAP-enabled aerial cell-free massive multiple-input multiple-output network to realize the edge computing paradigm, where multiple HAPs cooperate via the edge servers to serve IoT devices. For the case of a massive number of devices, we further adopt a grant-free massive access scheme to guarantee low-latency and high-efficiency massive IoT connectivity to the network. Besides, a case study is provided to demonstrate the effectiveness of the proposed solution. Finally, to shed light on the future research directions of HAP network-enabled edge computing paradigms, the key challenges and open issues are discussed.",2021-06-25T07:41:16Z,2021-06-25T07:41:16Z,http://arxiv.org/abs/2106.13476v1,http://arxiv.org/pdf/2106.13476v1,"cs.IT, cs.NI, eess.SP, math.IT"
Managing Knowledge in Energy Data Spaces,"Valentina Janev, Maria-Esther Vidal, Kemele Endris, Dea Pujic","Data in the energy domain grows at unprecedented rates and is usually generated by heterogeneous energy systems. Despite the great potential that big data-driven technologies can bring to the energy sector, general adoption is still lagging. Several challenges related to controlled data exchange and data integration are still not wholly achieved. As a result, fragmented applications are developed against energy data silos, and data exchange is limited to few applications. In this paper, we analyze the challenges and requirements related to energy-related data applications. We also evaluate the use of Energy Data Ecosystems (EDEs) as data-driven infrastructures to overcome the current limitations of fragmented energy applications. EDEs are inspired by the International Data Space (IDS) initiative launched in Germany at the end of 2014 with an overall objective to take both the development and use of the IDS reference architecture model to a European/global level. The reference architecture model consists of four architectures related to business, security, data and service, and software aspects. This paper illustrates the applicability of EDEs and IDS reference architecture in real-world scenarios from the energy sector. The analyzed scenario is positioned in the context of the EU-funded H2020 project PLATOON.",2021-07-05T12:15:58Z,2021-07-05T12:15:58Z,http://arxiv.org/abs/2107.01965v1,http://arxiv.org/pdf/2107.01965v1,"cs.DB, cs.SE, H.2.5; H.2.8"
QoE Evaluation for Adaptive Video Streaming: Enhanced MDT with Deep   Learning,"Hakan Gokcesu, Ozgur Ercetin, Gokhan Kalem, Salih Ergut","The network performance is usually assessed by drive tests, where teams of people with specially equipped vehicles physically drive out to test various locations throughout a radio network. However, intelligent and autonomous troubleshooting is considered a crucial enabler for 5G- and 6G-networks. In this paper, we propose an architecture for performing virtual drive tests by facilitating radio-quality data from the user equipment. Our architecture comprises three main components: i) a pattern recognizer that learns a typical pattern for the application from application Key Performance Indicators (KPI); ii) a predictor for mapping network KPI with the application KPI; iii) an anomaly detector that compares the predicted application performance with that of the typical application pattern. In this work, we use a commercial state-of-the-art network optimization tool to collect network and application KPI at different geographical locations and at various times of the day for training an initial learning model. We perform extensive numerical analysis to demonstrate key parameters impacting correct video quality prediction and anomaly detection. We show that the playback time is the single most important parameter affecting the video quality, since video packets are usually buffered ahead of time during the playback. However, radio frequency (RF) performance indicators characterizing the quality of the cellular connection improve the QoE estimation in exceptional cases. We demonstrate the efficacy of our approach by showing that the mean maximum F1-score of our method is 77%. Finally, the proposed architecture is flexible and autonomous, and it can operate with different user applications as long as the relevant user-based traces are available.",2021-07-26T09:48:46Z,2022-01-14T13:33:19Z,http://arxiv.org/abs/2107.12068v2,http://arxiv.org/pdf/2107.12068v2,"cs.NI, eess.SP"
Multi-Task Learning in Utterance-Level and Segmental-Level Spoof   Detection,"Lin Zhang, Xin Wang, Erica Cooper, Junichi Yamagishi","In this paper, we provide a series of multi-tasking benchmarks for simultaneously detecting spoofing at the segmental and utterance levels in the PartialSpoof database. First, we propose the SELCNN network, which inserts squeeze-and-excitation (SE) blocks into a light convolutional neural network (LCNN) to enhance the capacity of hidden feature selection. Then, we implement multi-task learning (MTL) frameworks with SELCNN followed by bidirectional long short-term memory (Bi-LSTM) as the basic model. We discuss MTL in PartialSpoof in terms of architecture (uni-branch/multi-branch) and training strategies (from-scratch/warm-up) step-by-step. Experiments show that the multi-task model performs relatively better than single-task models. Also, in MTL, a binary-branch architecture more adequately utilizes information from two levels than a uni-branch model. For the binary-branch architecture, fine-tuning a warm-up model works better than training from scratch. Models can handle both segment-level and utterance-level predictions simultaneously overall under a binary-branch multi-task architecture. Furthermore, the multi-task model trained by fine-tuning a segmental warm-up model performs relatively better at both levels except on the evaluation set for segmental detection. Segmental detection should be explored further.",2021-07-29T16:04:25Z,2021-08-31T16:02:58Z,http://arxiv.org/abs/2107.14132v2,http://arxiv.org/pdf/2107.14132v2,"cs.SD, eess.AS"
Efficient conformer: Progressive downsampling and grouped attention for   automatic speech recognition,"Maxime Burchi, Valentin Vielzeuf","The recently proposed Conformer architecture has shown state-of-the-art performances in Automatic Speech Recognition by combining convolution with attention to model both local and global dependencies. In this paper, we study how to reduce the Conformer architecture complexity with a limited computing budget, leading to a more efficient architecture design that we call Efficient Conformer. We introduce progressive downsampling to the Conformer encoder and propose a novel attention mechanism named grouped attention, allowing us to reduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence length $n$, hidden dimension $d$ and group size parameter $g$. We also experiment the use of strided multi-head self-attention as a global downsampling operation. Our experiments are performed on the LibriSpeech dataset with CTC and RNN-Transducer losses. We show that within the same computing budget, the proposed architecture achieves better performances with faster training and decoding compared to the Conformer. Our 13M parameters CTC model achieves competitive WERs of 3.6%/9.0% without using a language model and 2.7%/6.7% with an external n-gram language model on the test-clean/test-other sets while being 29% faster than our CTC Conformer baseline at inference and 36% faster to train.",2021-08-31T07:48:06Z,2021-09-08T11:47:52Z,http://arxiv.org/abs/2109.01163v2,http://arxiv.org/pdf/2109.01163v2,"eess.AS, cs.AI, cs.CL, cs.SD"
BioNetExplorer: Architecture-Space Exploration of Bio-Signal Processing   Deep Neural Networks for Wearables,"Bharath Srinivas Prabakaran, Asima Akhtar, Semeen Rehman, Osman Hasan, Muhammad Shafique","In this work, we propose the BioNetExplorer framework to systematically generate and explore multiple DNN architectures for bio-signal processing in wearables. Our framework adapts key neural architecture parameters to search for an embedded DNN with a low hardware overhead, which can be deployed in wearable edge devices to analyse the bio-signal data and to extract the relevant information, such as arrhythmia and seizure. Our framework also enables hardware-aware DNN architecture search using genetic algorithms by imposing user requirements and hardware constraints (storage, FLOPs, etc.) during the exploration stage, thereby limiting the number of networks explored. Moreover, BioNetExplorer can also be used to search for DNNs based on the user-required output classes; for instance, a user might require a specific output class due to genetic predisposition or a pre-existing heart condition. The use of genetic algorithms reduces the exploration time, on average, by 9x, compared to exhaustive exploration. We are successful in identifying Pareto-optimal designs, which can reduce the storage overhead of the DNN by ~30MB for a quality loss of less than 0.5%. To enable low-cost embedded DNNs, BioNetExplorer also employs different model compression techniques to further reduce the storage overhead of the network by up to 53x for a quality loss of <0.2%.",2021-09-07T07:37:00Z,2021-09-07T07:37:00Z,http://arxiv.org/abs/2109.02909v1,http://arxiv.org/pdf/2109.02909v1,"eess.SP, cs.LG"
Anomalous Self-assembly of Architecturally Semiflexible Block Copolymers,"Shifeng Nian, Zhouhao Fan, Guillaume Freychet, Mikhail Zhernenkov, Stefanie Redemann, Li-Heng Cai","Block copolymer self-assembly is a fundamental process in which incompatible blocks spontaneously form organized microstructures with broad practical applications. Classical understanding is that the domain spacing is limited by the contour length of the polymer backbone. Here, using a combination of molecular design, chemical synthesis, small/wide-angle X-ray scattering, transmission electron microscopy, and electron tomography, we discover that this molecular picture does not hold for architecturally semiflexible block copolymers. For strongly segregated linear-semiflexible bottlebrush-linear triblock copolymers, the size of the bottlebrush domain can be twice the bottlebrush backbone contour length. The mechanism of such anomalous self-assembly likely is that the interfacial repulsion between the incompatible blocks is large enough to pull a part of the linear end-blocks into the bottlebrush domain. This effectively increases the bottlebrush domain size. Moreover, the semiflexible bottlebrush widens the regime for cylinder morphology that is associated with the volume fraction of the end blocks $f_C^{SFB}\in(0.10,>0.40)$. This window is much wider than that for flexible linear block copolymers, $f_C^{F} \in(0.14,0.35)$, and that predicted by recent self-consistent field theory for linear-bottlebrush block copolymers of the same chemistry and molecular architecture. Our experimental findings reveal previously unrecognized mechanisms for the self-assembly of architecturally complex block copolymers.",2021-09-12T14:00:46Z,2021-09-12T14:00:46Z,http://arxiv.org/abs/2109.05520v1,http://arxiv.org/pdf/2109.05520v1,"cond-mat.soft, physics.chem-ph"
Design and Model Predictive Control of Mars Coaxial Quadrotor,"Akash Patel, Avijit Banerjee, Bjorn Lindqvist, Christoforos Kanellakis, George Nikolakopoulos","Mars has been a prime candidate for planetary exploration of the solar system because of the science discoveries that support chances of future habitation on this planet. Martian caves and lava tubes like terrains, which consists of uneven ground, poor visibility and confined space, makes it impossible for wheel based rovers to navigate through these areas. In order to address these limitations and advance the exploration capability in a Martian terrain, this article presents the design and control of a novel coaxial quadrotor Micro Aerial Vehicle (MAV). As it will be presented, the key contributions on the design and control architecture of the proposed Mars coaxial quadrotor, are introducing an alternative and more enhanced, from a control point of view concept, when compared in terms of autonomy to Ingenuity. Based on the presented design, the article will introduce the mathematical modelling and automatic control framework of the vehicle that will consist of a linearised model of a co-axial quadrotor and a corresponding Model Predictive Controller (MPC) for the trajectory tracking. Among the many models, proposed for the aerial flight on Mars, a reliable control architecture lacks in the related state of the art. The MPC based closed loop responses of the proposed MAV will be verified in different conditions during the flight with additional disturbances, induced to replicate a real flight scenario. In order to further validate the proposed control architecture and prove the efficacy of the suggested design, the introduced Mars coaxial quadrotor and the MPC scheme will be compared to a PID-type controller, similar to the Ingenuity helicopter's control architecture for the position and the heading.",2021-09-14T16:45:10Z,2021-10-01T11:01:58Z,http://arxiv.org/abs/2109.06810v2,http://arxiv.org/pdf/2109.06810v2,"cs.RO, I.2.9"
Reinshard: An optimally sharded dual-blockchain for concurrency   resolution,"Vishal Sharma, Zengpeng Li, Pawel Szalachowski, Teik Guan Tan, Jianying Zhou","Decentralized control, low-complexity, flexible and efficient communications are the requirements of an architecture that aims to scale blockchains beyond the current state. Such properties are attainable by reducing ledger size and providing parallel operations in the blockchain. Sharding is one of the approaches that lower the burden of the nodes and enhance performance. However, the current solutions lack the features for resolving concurrency during cross-shard communications. With multiple participants belonging to different shards, handling concurrent operations is essential for optimal sharding. This issue becomes prominent due to the lack of architectural support and requires additional consensus for cross-shard communications. Inspired by hybrid Proof-of-Work/Proof-of-Stake (PoW/PoS), like Ethereum, hybrid consensus and 2-hop blockchain, we propose Reinshard, a new blockchain that inherits the properties of hybrid consensus for optimal sharding. Reinshard uses PoW and PoS chain-pairs with PoS sub-chains for all the valid chain-pairs where the hybrid consensus is attained through Verifiable Delay Function (VDF). Our architecture provides a secure method of arranging nodes in shards and resolves concurrency conflicts using the delay factor of VDF. The applicability of Reinshard is demonstrated through security and experimental evaluations. A practical concurrency problem is considered to show the efficacy of Reinshard in providing optimal sharding.",2021-09-15T14:18:18Z,2021-09-15T14:18:18Z,http://arxiv.org/abs/2109.07316v1,http://arxiv.org/pdf/2109.07316v1,"cs.NI, cs.CR, cs.DC, stat.AP"
Learning through structure: towards deep neuromorphic knowledge graph   embeddings,"Victor Caceres Chian, Marcel Hildebrandt, Thomas Runkler, Dominik Dold","Computing latent representations for graph-structured data is an ubiquitous learning task in many industrial and academic applications ranging from molecule synthetization to social network analysis and recommender systems. Knowledge graphs are among the most popular and widely used data representations related to the Semantic Web. Next to structuring factual knowledge in a machine-readable format, knowledge graphs serve as the backbone of many artificial intelligence applications and allow the ingestion of context information into various learning algorithms. Graph neural networks attempt to encode graph structures in low-dimensional vector spaces via a message passing heuristic between neighboring nodes. Over the recent years, a multitude of different graph neural network architectures demonstrated ground-breaking performances in many learning tasks. In this work, we propose a strategy to map deep graph learning architectures for knowledge graph reasoning to neuromorphic architectures. Based on the insight that randomly initialized and untrained (i.e., frozen) graph neural networks are able to preserve local graph structures, we compose a frozen neural network with shallow knowledge graph embedding models. We experimentally show that already on conventional computing hardware, this leads to a significant speedup and memory reduction while maintaining a competitive performance level. Moreover, we extend the frozen architecture to spiking neural networks, introducing a novel, event-based and highly sparse knowledge graph embedding algorithm that is suitable for implementation in neuromorphic hardware.",2021-09-21T18:01:04Z,2021-09-21T18:01:04Z,http://arxiv.org/abs/2109.10376v1,http://arxiv.org/pdf/2109.10376v1,"cs.NE, cs.AI, cs.LG, q-bio.NC"
Exploring Architectural Ingredients of Adversarially Robust Deep Neural   Networks,"Hanxun Huang, Yisen Wang, Sarah Monazam Erfani, Quanquan Gu, James Bailey, Xingjun Ma","Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs. Code is available at \url{https://github.com/HanxunH/RobustWRN}.",2021-10-07T23:13:33Z,2022-01-23T03:21:33Z,http://arxiv.org/abs/2110.03825v5,http://arxiv.org/pdf/2110.03825v5,"cs.LG, cs.CV, stat.ML"
A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection,"Yongqi Dong, Sandeep Patil, Bart van Arem, Haneen Farah","Accurate and reliable lane detection is vital for the safe performance of lane-keeping assistance and lane departure warning systems. However, under certain challenging circumstances, it is difficult to get satisfactory performance in accurately detecting the lanes from one single image as mostly done in current literature. Since lane markings are continuous lines, the lanes that are difficult to be accurately detected in the current single image can potentially be better deduced if information from previous frames is incorporated. This study proposes a novel hybrid spatial-temporal (ST) sequence-to-one deep learning architecture. This architecture makes full use of the ST information in multiple continuous image frames to detect the lane markings in the very last frame. Specifically, the hybrid model integrates the following aspects: (a) the single image feature extraction module equipped with the spatial convolutional neural network; (b) the ST feature integration module constructed by ST recurrent neural network; (c) the encoder-decoder structure, which makes this image segmentation problem work in an end-to-end supervised learning format. Extensive experiments reveal that the proposed model architecture can effectively handle challenging driving scenes and outperforms available state-of-the-art methods.",2021-10-05T15:47:45Z,2022-07-20T01:54:05Z,http://arxiv.org/abs/2110.04079v5,http://arxiv.org/pdf/2110.04079v5,"cs.CV, cs.AI, cs.LG, eess.IV"
Energy-cost aware off-grid base stations with IoT devices for developing   a green heterogeneous network,"Khondoker Ziaul Islam, MD. Sanwar Hossain, B. M. Ruhul Amin, Ferdous Sohel","Heterogeneous network (HetNet) is a specified cellular platform to tackle the rapidly growing anticipated data traffic. From communications perspective, data loads can be mapped to energy loads that are generally placed on the operator networks. Meanwhile, renewable energy aided networks offer to curtail fossil fuel consumption, so to reduce environmental pollution. This paper proposes a renewable energy based power supply architecture for off-grid HetNet using a novel energy sharing model. Solar photovoltaic (PV) along with sufficient energy storage devices are used for each macro, micro, pico, or femto base station (BS). Additionally, biomass generator (BG) is used for macro and micro BSs. The collocated macro and micro BSs are connected through end-to-end resistive lines. A novel weighted proportional-fair resource-scheduling algorithm with sleep mechanisms is proposed for non-real time (NRT) applications by trading-off the power consumption and communication delays. Furthermore, the proposed algorithm with extended discontinuous reception (eDRX) and power saving mode (PSM) for narrowband internet of things (IoT) applications extends battery lifetime for IoT devices. HOMER optimization software is used to perform optimal system architecture, economic, and carbon footprint analyses while Monte-Carlo simulation tool is used for evaluating the throughput and energy efficiency performances. The proposed algorithms are valid for the practical data of the rural areas. We demonstrate the proposed power supply architecture is energy-efficient, cost-effective, reliable, and eco-friendly.",2021-10-12T11:34:02Z,2021-10-12T11:34:02Z,http://arxiv.org/abs/2110.05906v1,http://arxiv.org/pdf/2110.05906v1,"cs.NI, eess.SP"
MEDUSA: Multi-scale Encoder-Decoder Self-Attention Deep Neural Network   Architecture for Medical Image Analysis,"Hossein Aboutalebi, Maya Pavlova, Hayden Gunraj, Mohammad Javad Shafiee, Ali Sabri, Amer Alaref, Alexander Wong","Medical image analysis continues to hold interesting challenges given the subtle characteristics of certain diseases and the significant overlap in appearance between diseases. In this work, we explore the concept of self-attention for tackling such subtleties in and between diseases. To this end, we introduce MEDUSA, a multi-scale encoder-decoder self-attention mechanism tailored for medical image analysis. While self-attention deep convolutional neural network architectures in existing literature center around the notion of multiple isolated lightweight attention mechanisms with limited individual capacities being incorporated at different points in the network architecture, MEDUSA takes a significant departure from this notion by possessing a single, unified self-attention mechanism with significantly higher capacity with multiple attention heads feeding into different scales in the network architecture. To the best of the authors' knowledge, this is the first ""single body, multi-scale heads"" realization of self-attention and enables explicit global context amongst selective attention at different levels of representational abstractions while still enabling differing local attention context at individual levels of abstractions. With MEDUSA, we obtain state-of-the-art performance on multiple challenging medical image analysis benchmarks including COVIDx, RSNA RICORD, and RSNA Pneumonia Challenge when compared to previous work. Our MEDUSA model is publicly available.",2021-10-12T15:05:15Z,2021-10-12T15:05:15Z,http://arxiv.org/abs/2110.06063v1,http://arxiv.org/pdf/2110.06063v1,"eess.IV, cs.CV"
Impact of self-association on the architectural properties of bacterial   nucleoid proteins,Marc Joyeux,"The chromosomal DNA of bacteria is folded into a compact body called the nucleoid, which is composed essentially of DNA (80%), RNA (10%), and a number of different proteins (10%). These nucleoid proteins act as regulators of gene expression and influence the organization of the nucleoid by bridging, bending, or wrapping the DNA. These so-called architectural properties of nucleoid proteins are still poorly understood. For example, the reason why certain proteins compact the DNA coil in certain environments but make instead the DNA more rigid in other environments is the matter of ongoing debates. Here, we address the question of the impact of the self-association of nucleoid proteins on their architectural properties and try to determine whether differences in self-association are sufficient to induce large changes in the organization of the DNA coil. More specifically, we developed two coarse-grained models of proteins, which interact identically with the DNA but self-associate differently by forming either clusters or filaments in the absence of the DNA. We showed through Brownian dynamics simulations that self-association of the proteins increases dramatically their ability to shape the DNA coil. Moreover, we observed that cluster-forming proteins compact significantly the DNA coil (similar to the DNA-bridging mode of H-NS proteins), whereas filament-forming proteins increase instead significantly the stiffness of the DNA chain (similar to the DNA-stiffening mode of H-NS proteins). This work consequently suggests that the knowledge of the DNA-binding properties of the proteins is in itself not sufficient to understand their architectural properties. Rather, their self-association properties must also be investigated in detail, because they might actually drive the formation of different DNA/protein complexes.",2021-10-14T07:39:40Z,2021-10-14T07:39:40Z,http://arxiv.org/abs/2110.07193v1,http://arxiv.org/pdf/2110.07193v1,"physics.bio-ph, q-bio.BM"
Fully Distributed Actor-Critic Architecture for Multitask Deep   Reinforcement Learning,"Sergio Valcarcel Macua, Ian Davies, Aleksi Tukiainen, Enrique Munoz de Cote","We propose a fully distributed actor-critic architecture, named Diff-DAC, with application to multitask reinforcement learning (MRL). During the learning process, agents communicate their value and policy parameters to their neighbours, diffusing the information across a network of agents with no need for a central station. Each agent can only access data from its local task, but aims to learn a common policy that performs well for the whole set of tasks. The architecture is scalable, since the computational and communication cost per agent depends on the number of neighbours rather than the overall number of agents. We derive Diff-DAC from duality theory and provide novel insights into the actor-critic framework, showing that it is actually an instance of the dual ascent method. We prove almost sure convergence of Diff-DAC to a common policy under general assumptions that hold even for deep-neural network approximations. For more restrictive assumptions, we also prove that this common policy is a stationary point of an approximation of the original problem. Numerical results on multitask extensions of common continuous control benchmarks demonstrate that Diff-DAC stabilises learning and has a regularising effect that induces higher performance and better generalisation properties than previous architectures.",2021-10-23T21:57:43Z,2021-10-23T21:57:43Z,http://arxiv.org/abs/2110.12306v1,http://arxiv.org/pdf/2110.12306v1,"cs.LG, cs.DC, cs.NE, cs.SY, eess.SY"
BINAS: Bilinear Interpretable Neural Architecture Search,"Niv Nayman, Yonathan Aflalo, Asaf Noy, Rong Jin, Lihi Zelnik-Manor","Practical use of neural networks often involves requirements on latency, energy and memory among others. A popular approach to find networks under such requirements is through constrained Neural Architecture Search (NAS). However, previous methods use complicated predictors for the accuracy of the network. Those predictors are hard to interpret and sensitive to many hyperparameters to be tuned, hence, the resulting accuracy of the generated models is often harmed. In this work we resolve this by introducing Bilinear Interpretable Neural Architecture Search (BINAS), that is based on an accurate and simple bilinear formulation of both an accuracy estimator and the expected resource requirement, together with a scalable search method with theoretical guarantees. The simplicity of our proposed estimator together with the intuitive way it is constructed bring interpretability through many insights about the contribution of different design choices. For example, we find that in the examined search space, adding depth and width is more effective at deeper stages of the network and at the beginning of each resolution stage. Our experiments show that BINAS generates comparable to or better architectures than other state-of-the-art NAS methods within a reduced marginal search cost, while strictly satisfying the resource constraints.",2021-10-24T09:45:00Z,2022-04-27T12:55:23Z,http://arxiv.org/abs/2110.12399v3,http://arxiv.org/pdf/2110.12399v3,"cs.LG, cs.AI, cs.CV, math.OC, stat.ML, 68T09, 68T45, G.1.6; G.3; I.2.8; I.2.10; I.5.1"
End-to-End Speech Emotion Recognition: Challenges of Real-Life Emergency   Call Centers Data Recordings,"Théo Deschamps-Berger, Lori Lamel, Laurence Devillers","Recognizing a speaker's emotion from their speech can be a key element in emergency call centers. End-to-end deep learning systems for speech emotion recognition now achieve equivalent or even better results than conventional machine learning approaches. In this paper, in order to validate the performance of our neural network architecture for emotion recognition from speech, we first trained and tested it on the widely used corpus accessible by the community, IEMOCAP. We then used the same architecture as the real life corpus, CEMO, composed of 440 dialogs (2h16m) from 485 speakers. The most frequent emotions expressed by callers in these real life emergency dialogues are fear, anger and positive emotions such as relief. In the IEMOCAP general topic conversations, the most frequent emotions are sadness, anger and happiness. Using the same end-to-end deep learning architecture, an Unweighted Accuracy Recall (UA) of 63% is obtained on IEMOCAP and a UA of 45.6% on CEMO, each with 4 classes. Using only 2 classes (Anger, Neutral), the results for CEMO are 76.9% UA compared to 81.1% UA for IEMOCAP. We expect that these encouraging results with CEMO can be improved by combining the audio channel with the linguistic channel. Real-life emotions are clearly more complex than acted ones, mainly due to the large diversity of emotional expressions of speakers. Index Terms-emotion detection, end-to-end deep learning architecture, call center, real-life database, complex emotions.",2021-10-28T08:56:57Z,2021-10-28T08:56:57Z,http://arxiv.org/abs/2110.14957v1,http://arxiv.org/pdf/2110.14957v1,"cs.AI, cs.CL, cs.SD, eess.AS, stat.ML"
CondenseNeXt: An Ultra-Efficient Deep Neural Network for Embedded   Systems,"Priyank Kalgaonkar, Mohamed El-Sharkawy","Due to the advent of modern embedded systems and mobile devices with constrained resources, there is a great demand for incredibly efficient deep neural networks for machine learning purposes. There is also a growing concern of privacy and confidentiality of user data within the general public when their data is processed and stored in an external server which has further fueled the need for developing such efficient neural networks for real-time inference on local embedded systems. The scope of our work presented in this paper is limited to image classification using a convolutional neural network. A Convolutional Neural Network (CNN) is a class of Deep Neural Network (DNN) widely used in the analysis of visual images captured by an image sensor, designed to extract information and convert it into meaningful representations for real-time inference of the input data. In this paper, we propose a neoteric variant of deep convolutional neural network architecture to ameliorate the performance of existing CNN architectures for real-time inference on embedded systems. We show that this architecture, dubbed CondenseNeXt, is remarkably efficient in comparison to the baseline neural network architecture, CondenseNet, by reducing trainable parameters and FLOPs required to train the network whilst maintaining a balance between the trained model size of less than 3.0 MB and accuracy trade-off resulting in an unprecedented computational efficiency.",2021-12-01T18:20:52Z,2021-12-01T18:20:52Z,http://arxiv.org/abs/2112.00698v1,http://arxiv.org/pdf/2112.00698v1,"cs.CV, cs.LG, eess.IV"
RSBNet: One-Shot Neural Architecture Search for A Backbone Network in   Remote Sensing Image Recognition,"Cheng Peng, Yangyang Li, Ronghua Shang, Licheng Jiao","Recently, a massive number of deep learning based approaches have been successfully applied to various remote sensing image (RSI) recognition tasks. However, most existing advances of deep learning methods in the RSI field heavily rely on the features extracted by the manually designed backbone network, which severely hinders the potential of deep learning models due the complexity of RSI and the limitation of prior knowledge. In this paper, we research a new design paradigm for the backbone architecture in RSI recognition tasks, including scene classification, land-cover classification and object detection. A novel one-shot architecture search framework based on weight-sharing strategy and evolutionary algorithm is proposed, called RSBNet, which consists of three stages: Firstly, a supernet constructed in a layer-wise search space is pretrained on a self-assembled large-scale RSI dataset based on an ensemble single-path training strategy. Next, the pre-trained supernet is equipped with different recognition heads through the switchable recognition module and respectively fine-tuned on the target dataset to obtain task-specific supernet. Finally, we search the optimal backbone architecture for different recognition tasks based on the evolutionary algorithm without any network training. Extensive experiments have been conducted on five benchmark datasets for different recognition tasks, the results show the effectiveness of the proposed search paradigm and demonstrate that the searched backbone is able to flexibly adapt different RSI recognition tasks and achieve impressive performance.",2021-12-07T02:44:16Z,2021-12-07T02:44:16Z,http://arxiv.org/abs/2112.03456v1,http://arxiv.org/pdf/2112.03456v1,"eess.IV, cs.AI, cs.CV"
Towards automated optimisation of residual convolutional neural networks   for electrocardiogram classification,"Zeineb Fki, Boudour Ammar, Mounir Ben Ayed","The interpretation of the electrocardiogram (ECG) gives clinical information and helps in assessing heart function. There are distinct ECG patterns associated with a specific class of arrythmia. The convolutional neural network is currently one of the most commonly employed deep learning algorithms for ECG processing. However, deep learning models require many hyperparameters to tune. Selecting an optimal or best hyperparameter for the convolutional neural network algorithm is a highly challenging task. Often, we end up tuning the model manually with different possible ranges of values until a best fit model is obtained. Automatic hyperparameters tuning using Bayesian optimisation (BO) and evolutionary algorithms can provide an effective solution to current labour-intensive manual configuration approaches. In this paper, we propose to optimise the Residual one Dimensional Convolutional Neural Network model (R-1D-CNN) at two levels. At the first level, a residual convolutional layer and one-dimensional convolutional neural layers are trained to learn patient-specific ECG features over which multilayer perceptron layers can learn to produce the final class vectors of each input. This level is manual and aims to lower the search space. The second level is automatic and based on our proposed BO-based algorithm. Our proposed optimised R-1D-CNN architecture is evaluated on two publicly available ECG Datasets. Comparative experimental results demonstrate that our BO-based algorithm achieves an optimal rate of 99.95%, while the baseline model achieves 99.70% for the MIT-BIH database. Moreover, experiments demonstrate that the proposed architecture fine-tuned with BO achieves a higher accuracy than the other proposed architectures. Our optimised architecture achieves excellent results compared to previous works on benchmark datasets.",2021-12-11T16:52:23Z,2022-01-28T11:01:56Z,http://arxiv.org/abs/2112.06024v2,http://arxiv.org/pdf/2112.06024v2,"eess.SP, cs.LG, 92D25, I.5.1"
Heterogeneous Transformer: A Scale Adaptable Neural Network Architecture   for Device Activity Detection,"Yang Li, Zhilin Chen, Yunqi Wang, Chenyang Yang, Yik-Chung Wu","To support the modern machine-type communications, a crucial task during the random access phase is device activity detection, which is to detect the active devices from a large number of potential devices based on the received signal at the access point. By utilizing the statistical properties of the channel, state-of-the-art covariance based methods have been demonstrated to achieve better activity detection performance than compressed sensing based methods. However, covariance based methods require to solve a high dimensional nonconvex optimization problem by updating the estimate of the activity status of each device sequentially. Since the number of updates is proportional to the device number, the computational complexity and delay make the iterative updates difficult for real-time implementation especially when the device number scales up. Inspired by the success of deep learning for real-time inference, this paper proposes a learning based method with a customized heterogeneous transformer architecture for device activity detection. By adopting an attention mechanism in the architecture design, the proposed method is able to extract the relevance between device pilots and received signal, is permutation equivariant with respect to devices, and is scale adaptable to different numbers of devices. Simulation results demonstrate that the proposed method achieves better activity detection performance with much shorter computation time than state-of-the-art covariance approach, and generalizes well to different numbers of devices, BS-antennas, and different signal-to-noise ratios.",2021-12-19T08:21:31Z,2021-12-19T08:21:31Z,http://arxiv.org/abs/2112.10086v1,http://arxiv.org/pdf/2112.10086v1,"cs.IT, cs.NI, eess.SP, math.IT"
Distance-Aware Precoding for Near-Field Capacity Improvement,"Zidong Wu, Mingyao Cui, Zijian Zhang, Linglong Dai","Extremely large-scale MIMO (XL-MIMO) is a promising technology to improve the capacity for future 6G networks. With a very large number of antennas, the near-field property of XL-MIMO systems becomes dominant. Unlike the classical far-field line-of-sight (LoS) channel with only one available data stream, the significantly increased degrees of freedom (DoFs) are available in the near-field LoS channel. However, limited by the small number of radio frequency (RF) chains, the existing hybrid precoding architecture widely used for 5G is not able to fully exploit the extra DoFs in the near-field region. In this paper, the available DoFs and the capacity of the near-field LoS channel are theoretically analyzed at first. Then, to exploit the near-field effect as a new possibility for capacity improvement, the distance-aware precoding (DAP) scheme is proposed. We develop the DAP architecture, where a dedicated selection circuit is inserted to connect phase shifters and RF chains. Moreover, each RF chain can be flexibly configured to active or inactive according to the distance-related DoFs in the proposed DAP architecture. Based on the developed DAP architecture, a DAP algorithm is proposed to optimize the number of activated RF chains and precoding matrices to match the increased DoFs in the near-field region. Finally, simulation results verify that, the proposed DAP scheme can efficiently utilize the extra DoFs in the near-field region to improve the spectrum efficiency and the energy efficiency as well.",2021-12-29T15:16:27Z,2022-03-29T14:54:39Z,http://arxiv.org/abs/2112.14598v2,http://arxiv.org/pdf/2112.14598v2,"cs.IT, eess.SP, math.IT"
"BP-Net: Cuff-less, Calibration-free, and Non-invasive Blood Pressure   Estimation via a Generic Deep Convolutional Architecture","Soheil Zabihi, Elahe Rahimian, Fatemeh Marefat, Amir Asif, Pedram Mohseni, Arash Mohammadi","Objective: The paper focuses on development of robust and accurate processing solutions for continuous and cuff-less blood pressure (BP) monitoring. In this regard, a robust deep learning-based framework is proposed for computation of low latency, continuous, and calibration-free upper and lower bounds on the systolic and diastolic BP. Method: Referred to as the BP-Net, the proposed framework is a novel convolutional architecture that provides longer effective memory while achieving superior performance due to incorporation of casual dialated convolutions and residual connections. To utilize the real potential of deep learning in extraction of intrinsic features (deep features) and enhance the long-term robustness, the BP-Net uses raw Electrocardiograph (ECG) and Photoplethysmograph (PPG) signals without extraction of any form of hand-crafted features as it is common in existing solutions. Results: By capitalizing on the fact that datasets used in recent literature are not unified and properly defined, a benchmark dataset is constructed from the MIMIC-I and MIMIC-III databases obtained from PhysioNet. The proposed BP-Net is evaluated based on this benchmark dataset demonstrating promising performance and shows superior generalizable capacity. Conclusion: The proposed BP-Net architecture is more accurate than canonical recurrent networks and enhances the long-term robustness of the BP estimation task. Significance: The proposed BP-Net architecture addresses key drawbacks of existing BP estimation solutions, i.e., relying heavily on extraction of hand-crafted features, such as pulse arrival time (PAT), and; Lack of robustness. Finally, the constructed BP-Net dataset provides a unified base for evaluation and comparison of deep learning-based BP estimation algorithms.",2021-12-31T02:34:39Z,2021-12-31T02:34:39Z,http://arxiv.org/abs/2112.15271v1,http://arxiv.org/pdf/2112.15271v1,"cs.LG, eess.SP"
Blind Image Deconvolution Using Variational Deep Image Prior,"Dong Huo, Abbas Masoumzadeh, Rafsanjany Kushol, Yee-Hong Yang","Conventional deconvolution methods utilize hand-crafted image priors to constrain the optimization. While deep-learning-based methods have simplified the optimization by end-to-end training, they fail to generalize well to blurs unseen in the training dataset. Thus, training image-specific models is important for higher generalization. Deep image prior (DIP) provides an approach to optimize the weights of a randomly initialized network with a single degraded image by maximum a posteriori (MAP), which shows that the architecture of a network can serve as the hand-crafted image prior. Different from the conventional hand-crafted image priors that are statistically obtained, it is hard to find a proper network architecture because the relationship between images and their corresponding network architectures is unclear. As a result, the network architecture cannot provide enough constraint for the latent sharp image. This paper proposes a new variational deep image prior (VDIP) for blind image deconvolution, which exploits additive hand-crafted image priors on latent sharp images and approximates a distribution for each pixel to avoid suboptimal solutions. Our mathematical analysis shows that the proposed method can better constrain the optimization. The experimental results further demonstrate that the generated images have better quality than that of the original DIP on benchmark datasets. The source code of our VDIP is available at https://github.com/Dong-Huo/VDIP-Deconvolution.",2022-02-01T01:33:58Z,2023-06-06T02:16:47Z,http://arxiv.org/abs/2202.00179v3,http://arxiv.org/pdf/2202.00179v3,"eess.IV, cs.CV"
A Nano-Architecture for Fertility Monitoring via Intra-body   Communication,"Shama Siddiqui, Anwar Ahmed Khan, Qammer H. Abbasi, Muhammad Ali Imran, Indrakshi Dey","Fertility monitoring in humans for either natural conception or artificial insemination and fertilization has been a critical challenge both for the treating physician and the treated patients. Eggs in human female can be fertilized when they reach the Fallopian tube from the upper parts of the reproductive system. However, no technology, till date, on its own could detect the presence of eggs in the Fallopian tube and communicate their presence to the consulting physician or nurse and the patient, so that the next step can be initiated in a timely fashion. In this paper, we propose a conceptual architecture from a communications engineering point of view. The architecture combines intra-body nano-sensor network for detecting Fallopian tube activity, with body-area network for receiving information from the intra-body network and communicating the same over-the-air to the involved personnel (physician/nurse/patient). Preliminary simulations have been conducted using particle based stochastic simulator to investigate the relationship between achievable information rates, signal to noise ratio (SNR) and distance. It has been found that data rate as high as 300 Mbps is achievable at SNR 45. Hence, the proposed architecture ensures transfer of information with near-zero latency and minimum energy along with high throughput, so that necessary action can be taken within the short time-window of the Fallopian tube activity.",2022-02-04T05:09:51Z,2022-02-04T05:09:51Z,http://arxiv.org/abs/2202.01974v1,http://arxiv.org/pdf/2202.01974v1,"eess.SP, cs.SY, eess.SY"
Relaxed virtual memory in Armv8-A (extended version),"Ben Simner, Alasdair Armstrong, Jean Pichon-Pharabod, Christopher Pulte, Richard Grisenthwaite, Peter Sewell","Virtual memory is an essential mechanism for enforcing security boundaries, but its relaxed-memory concurrency semantics has not previously been investigated in detail. The concurrent systems code managing virtual memory has been left on an entirely informal basis, and OS and hypervisor verification has had to make major simplifying assumptions.   We explore the design space for relaxed virtual memory semantics in the Armv8-A architecture, to support future system-software verification. We identify many design questions, in discussion with Arm; develop a test suite, including use cases from the pKVM production hypervisor under development by Google; delimit the design space with axiomatic-style concurrency models; prove that under simple stable configurations our architectural model collapses to previous ""user"" models; develop tooling to compute allowed behaviours in the model integrated with the full Armv8-A ISA semantics; and develop a hardware test harness.   This lays out some of the main issues in relaxed virtual memory bringing these security-critical systems phenomena into the domain of programming-language semantics and verification with foundational architecture semantics.   This document is an extended version of a paper in ESOP 2022, with additional explanation and examples in the main body, and appendices detailing our litmus tests, models, proofs, and test results.",2022-03-01T17:34:36Z,2022-03-01T17:34:36Z,http://arxiv.org/abs/2203.00642v1,http://arxiv.org/pdf/2203.00642v1,"cs.AR, cs.OS, cs.PL, C.1.2; D.3.1; F.3.2"
Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained   Language Models,"Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen","Recently, Mixture-of-Experts (short as MoE) architecture has achieved remarkable success in increasing the model capacity of large-scale language models. However, MoE requires incorporating significantly more parameters than the base model being extended. In this paper, we propose building a parameter-efficient MoE architecture by sharing information among experts. We adopt the matrix product operator (MPO, a tensor decomposition from quantum many-body physics) to reconstruct the parameter matrix in the expert layer and increase model capacity for pre-trained language models by sharing parameters of the central tensor (containing the core information) among different experts while enabling the specificity through the auxiliary tensors (complementing the central tensor) of different experts. To address the unbalanced optimization issue, we further design the gradient mask strategy for the MPO-based MoE architecture. Extensive experiments based on T5 and GPT-2 show improved performance and efficiency of the pre-trained language model (27.2x reduction in total parameters for the superior model performance, compared with the Switch Transformers). Our code is publicly available at https://github.com/RUCAIBox/MPOE.",2022-03-02T13:44:49Z,2022-10-10T06:41:52Z,http://arxiv.org/abs/2203.01104v4,http://arxiv.org/pdf/2203.01104v4,"cs.CL, cs.AI, cs.LG, quant-ph"
Multi-Scale Adaptive Network for Single Image Denoising,"Yuanbiao Gou, Peng Hu, Jiancheng Lv, Joey Tianyi Zhou, Xi Peng","Multi-scale architectures have shown effectiveness in a variety of tasks thanks to appealing cross-scale complementarity. However, existing architectures treat different scale features equally without considering the scale-specific characteristics, \textit{i.e.}, the within-scale characteristics are ignored in the architecture design. In this paper, we reveal this missing piece for multi-scale architecture design and accordingly propose a novel Multi-Scale Adaptive Network (MSANet) for single image denoising. Specifically, MSANet simultaneously embraces the within-scale characteristics and the cross-scale complementarity thanks to three novel neural blocks, \textit{i.e.}, adaptive feature block (AFeB), adaptive multi-scale block (AMB), and adaptive fusion block (AFuB). In brief, AFeB is designed to adaptively preserve image details and filter noises, which is highly expected for the features with mixed details and noises. AMB could enlarge the receptive field and aggregate the multi-scale information, which meets the need of contextually informative features. AFuB devotes to adaptively sampling and transferring the features from one scale to another scale, which fuses the multi-scale features with varying characteristics from coarse to fine. Extensive experiments on both three real and six synthetic noisy image datasets show the superiority of MSANet compared with 12 methods. The code could be accessed from https://github.com/XLearning-SCU/2022-NeurIPS-MSANet.",2022-03-08T15:13:20Z,2022-10-29T07:57:34Z,http://arxiv.org/abs/2203.04313v2,http://arxiv.org/pdf/2203.04313v2,"eess.IV, cs.CV"
Meta-Learning of NAS for Few-shot Learning in Medical Image Applications,"Viet-Khoa Vo-Ho, Kashu Yamazaki, Hieu Hoang, Minh-Triet Tran, Ngan Le","Deep learning methods have been successful in solving tasks in machine learning and have made breakthroughs in many sectors owing to their ability to automatically extract features from unstructured data. However, their performance relies on manual trial-and-error processes for selecting an appropriate network architecture, hyperparameters for training, and pre-/post-procedures. Even though it has been shown that network architecture plays a critical role in learning feature representation feature from data and the final performance, searching for the best network architecture is computationally intensive and heavily relies on researchers' experience. Automated machine learning (AutoML) and its advanced techniques i.e. Neural Architecture Search (NAS) have been promoted to address those limitations. Not only in general computer vision tasks, but NAS has also motivated various applications in multiple areas including medical imaging. In medical imaging, NAS has significant progress in improving the accuracy of image classification, segmentation, reconstruction, and more. However, NAS requires the availability of large annotated data, considerable computation resources, and pre-defined tasks. To address such limitations, meta-learning has been adopted in the scenarios of few-shot learning and multiple tasks. In this book chapter, we first present a brief review of NAS by discussing well-known approaches in search space, search strategy, and evaluation strategy. We then introduce various NAS approaches in medical imaging with different applications such as classification, segmentation, detection, reconstruction, etc. Meta-learning in NAS for few-shot learning and multiple tasks is then explained. Finally, we describe several open problems in NAS.",2022-03-16T21:21:51Z,2022-03-16T21:21:51Z,http://arxiv.org/abs/2203.08951v1,http://arxiv.org/pdf/2203.08951v1,"cs.LG, cs.CV, eess.IV"
Practical Blind Image Denoising via Swin-Conv-UNet and Data Synthesis,"Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, Luc Van Gool","While recent years have witnessed a dramatic upsurge of exploiting deep neural networks toward solving image denoising, existing methods mostly rely on simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG compression noise and camera sensor noise, and a general-purpose blind denoising method for real images remains unsolved. In this paper, we attempt to solve this problem from the perspective of network architecture design and training data synthesis. Specifically, for the network architecture design, we propose a swin-conv block to incorporate the local modeling ability of residual convolutional layer and non-local modeling ability of swin transformer block, and then plug it as the main building block into the widely-used image-to-image translation UNet architecture. For the training data synthesis, we design a practical noise degradation model which takes into consideration different kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and processed camera sensor noises) and resizing, and also involves a random shuffle strategy and a double degradation strategy. Extensive experiments on AGWN removal and real image denoising demonstrate that the new network architecture design achieves state-of-the-art performance and the new degradation model can help to significantly improve the practicability. We believe our work can provide useful insights into current denoising research.",2022-03-24T18:11:31Z,2023-12-01T15:17:38Z,http://arxiv.org/abs/2203.13278v4,http://arxiv.org/pdf/2203.13278v4,"cs.CV, cs.GR, eess.IV"
"Post-Disaster Communications: Enabling Technologies, Architectures, and   Open Challenges","Maurilio Matracia, Nasir Saeed, Mustafa A. Kishk, Mohamed-Slim Alouini","The number of disasters has increased over the past decade where these calamities significantly affect the functionality of communication networks. In the context of 6G, airborne and spaceborne networks offer hope in disaster recovery to serve the underserved and to be resilient in calamities. Therefore, this paper surveys the state-of-the-art literature on post-disaster wireless communication networks and provides insights for the future establishment of such networks. In particular, we first give an overview of the works investigating the general procedures and strategies for counteracting any large-scale disasters. Then, we present the possible technological solutions for post-disaster communications, such as the recovery of the terrestrial infrastructure, installing aerial networks, and using spaceborne networks. Afterward, we shed light on the technological aspects of post-disaster networks, primarily the physical and networking issues. We present the literature on channel modeling, coverage and capacity, radio resource management, localization, and energy efficiency in the physical layer and discuss the integrated space-air-ground architectures, routing, delay-tolerant/software-defined networks, and edge computing in the networking layer. This paper also presents interesting simulation results which can provide practical guidelines about the deployment of ad hoc network architectures in emergency scenarios. Finally, we present several promising research directions, namely backhauling, placement optimization of aerial base stations, and the mobility-related aspects that come into play when deploying aerial networks, such as planning their trajectories and the consequent handovers.",2022-03-25T12:41:53Z,2022-07-19T18:18:50Z,http://arxiv.org/abs/2203.13621v2,http://arxiv.org/pdf/2203.13621v2,"cs.NI, eess.SP"
Dynamic-subarray with Fixed Phase Shifters for Energy-efficient   Terahertz Hybrid Beamforming under Partial CSI,"Longfei Yan, Chong Han, Nan Yang, Jinhong Yuan","Terahertz (THz) communications are regarded as a pillar technology for the 6G systems, by offering multi-ten-GHz bandwidth. To overcome the huge propagation loss while reducing the hardware complexity, THz ultra-massive (UM) MIMO systems with hybrid beamforming are proposed to offer high array gain. Notably, the adjustable-phase-shifters considered in most existing hybrid beamforming studies are power-hungry and difficult to realize in the THz band. Moreover, due to the ultra-massive antennas, full channel-state-information (CSI) is challenging to obtain. To address these practical concerns, in this paper, an energy-efficient dynamic-subarray with fixed-phase-shifters (DS-FPS) architecture is proposed for THz hybrid beamforming. To compensate for the spectral efficiency loss caused by the fixed-phase of FPS, a switch network is inserted to enable dynamic connections. In addition, by considering the partial CSI, we propose a row-successive-decomposition (RSD) algorithm to design the hybrid beamforming matrices for DS-FPS. A row-by-row (RBR) algorithm is further proposed to reduce computational complexity. Extensive simulation results show that, the proposed DS-FPS architecture with the RSD and RBR algorithms achieves much higher energy efficiency than the existing architectures. Moreover, the DS-FPS architecture with partial CSI achieves 97% spectral efficiency of that with full CSI.",2022-03-29T08:28:24Z,2022-03-29T08:28:24Z,http://arxiv.org/abs/2203.15338v1,http://arxiv.org/pdf/2203.15338v1,"cs.IT, eess.SP, math.IT"
Quasi-orthogonality and intrinsic dimensions as measures of learning and   generalisation,"Qinghua Zhou, Alexander N. Gorban, Evgeny M. Mirkes, Jonathan Bac, Andrei Zinovyev, Ivan Y. Tyukin","Finding best architectures of learning machines, such as deep neural networks, is a well-known technical and theoretical challenge. Recent work by Mellor et al (2021) showed that there may exist correlations between the accuracies of trained networks and the values of some easily computable measures defined on randomly initialised networks which may enable to search tens of thousands of neural architectures without training. Mellor et al used the Hamming distance evaluated over all ReLU neurons as such a measure. Motivated by these findings, in our work, we ask the question of the existence of other and perhaps more principled measures which could be used as determinants of success of a given neural architecture. In particular, we examine, if the dimensionality and quasi-orthogonality of neural networks' feature space could be correlated with the network's performance after training. We showed, using the setup as in Mellor et al, that dimensionality and quasi-orthogonality may jointly serve as network's performance discriminants. In addition to offering new opportunities to accelerate neural architecture search, our findings suggest important relationships between the networks' final performance and properties of their randomly initialised feature spaces: data dimension and quasi-orthogonality.",2022-03-30T21:47:32Z,2022-03-30T21:47:32Z,http://arxiv.org/abs/2203.16687v1,http://arxiv.org/pdf/2203.16687v1,"cs.LG, 68T05, 68Q32"
Understanding Transfer Learning for Chest Radiograph Clinical Report   Generation with Modified Transformer Architectures,"Edward Vendrow, Ethan Schonfeld","The image captioning task is increasingly prevalent in artificial intelligence applications for medicine. One important application is clinical report generation from chest radiographs. The clinical writing of unstructured reports is time consuming and error-prone. An automated system would improve standardization, error reduction, time consumption, and medical accessibility. In this paper we demonstrate the importance of domain specific pre-training and propose a modified transformer architecture for the medical image captioning task. To accomplish this, we train a series of modified transformers to generate clinical reports from chest radiograph image input. These modified transformers include: a meshed-memory augmented transformer architecture with visual extractor using ImageNet pre-trained weights, a meshed-memory augmented transformer architecture with visual extractor using CheXpert pre-trained weights, and a meshed-memory augmented transformer whose encoder is passed the concatenated embeddings using both ImageNet pre-trained weights and CheXpert pre-trained weights. We use BLEU(1-4), ROUGE-L, CIDEr, and the clinical CheXbert F1 scores to validate our models and demonstrate competitive scores with state of the art models. We provide evidence that ImageNet pre-training is ill-suited for the medical image captioning task, especially for less frequent conditions (eg: enlarged cardiomediastinum, lung lesion, pneumothorax). Furthermore, we demonstrate that the double feature model improves performance for specific medical conditions (edema, consolidation, pneumothorax, support devices) and overall CheXbert F1 score, and should be further developed in future work. Such a double feature model, including both ImageNet pre-training as well as domain specific pre-training, could be used in a wide range of image captioning models in medicine.",2022-05-05T03:08:05Z,2022-05-05T03:08:05Z,http://arxiv.org/abs/2205.02841v1,http://arxiv.org/pdf/2205.02841v1,"eess.IV, cs.AI, cs.CV, cs.LG"
Physics-inspired Ising Computing with Ring Oscillator Activated p-bits,"Navid Anjum Aadit, Andrea Grimaldi, Giovanni Finocchio, Kerem Y. Camsari","The nearing end of Moore's Law has been driving the development of domain-specific hardware tailored to solve a special set of problems. Along these lines, probabilistic computing with inherently stochastic building blocks (p-bits) have shown significant promise, particularly in the context of hard optimization and statistical sampling problems. p-bits have been proposed and demonstrated in different hardware substrates ranging from small-scale stochastic magnetic tunnel junctions (sMTJs) in asynchronous architectures to large-scale CMOS in synchronous architectures. Here, we design and implement a truly asynchronous and medium-scale p-computer (with $\approx$ 800 p-bits) that closely emulates the asynchronous dynamics of sMTJs in Field Programmable Gate Arrays (FPGAs). Using hard instances of the planted Ising glass problem on the Chimera lattice, we evaluate the performance of the asynchronous architecture against an ideal, synchronous design that performs parallelized (chromatic) exact Gibbs sampling. We find that despite the lack of any careful synchronization, the asynchronous design achieves parallelism with comparable algorithmic scaling in the ideal, carefully tuned and parallelized synchronous design. Our results highlight the promise of massively scaled p-computers with millions of free-running p-bits made out of nanoscale building blocks such as stochastic magnetic tunnel junctions.",2022-05-15T23:46:58Z,2022-05-15T23:46:58Z,http://arxiv.org/abs/2205.07402v1,http://arxiv.org/pdf/2205.07402v1,"cs.AR, cs.DC, cs.ET, cs.NE, physics.comp-ph"
Parallel bandit architecture based on laser chaos for reinforcement   learning,"Takashi Urushibara, Nicolas Chauvet, Satoshi Kochi, Satoshi Sunada, Kazutaka Kanno, Atsushi Uchida, Ryoichi Horisaki, Makoto Naruse","Accelerating artificial intelligence by photonics is an active field of study aiming to exploit the unique properties of photons. Reinforcement learning is an important branch of machine learning, and photonic decision-making principles have been demonstrated with respect to the multi-armed bandit problems. However, reinforcement learning could involve a massive number of states, unlike previously demonstrated bandit problems where the number of states is only one. Q-learning is a well-known approach in reinforcement learning that can deal with many states. The architecture of Q-learning, however, does not fit well photonic implementations due to its separation of update rule and the action selection. In this study, we organize a new architecture for multi-state reinforcement learning as a parallel array of bandit problems in order to benefit from photonic decision-makers, which we call parallel bandit architecture for reinforcement learning or PBRL in short. Taking a cart-pole balancing problem as an instance, we demonstrate that PBRL adapts to the environment in fewer time steps than Q-learning. Furthermore, PBRL yields faster adaptation when operated with a chaotic laser time series than the case with uniformly distributed pseudorandom numbers where the autocorrelation inherent in the laser chaos provides a positive effect. We also find that the variety of states that the system undergoes during the learning phase exhibits completely different properties between PBRL and Q-learning. The insights obtained through the present study are also beneficial for existing computing platforms, not just photonic realizations, in accelerating performances by the PBRL algorithms and correlated random sequences.",2022-05-19T13:12:21Z,2022-05-19T13:12:21Z,http://arxiv.org/abs/2205.09543v1,http://arxiv.org/pdf/2205.09543v1,"cs.ET, cs.LG, physics.app-ph"
ALPINE: Analog In-Memory Acceleration with Tight Processor Integration   for Deep Learning,"Joshua Klein, Irem Boybat, Yasir Qureshi, Martino Dazzi, Alexandre Levisse, Giovanni Ansaloni, Marina Zapater, Abu Sebastian, David Atienza","Analog in-memory computing (AIMC) cores offers significant performance and energy benefits for neural network inference with respect to digital logic (e.g., CPUs). AIMCs accelerate matrix-vector multiplications, which dominate these applications' run-time. However, AIMC-centric platforms lack the flexibility of general-purpose systems, as they often have hard-coded data flows and can only support a limited set of processing functions. With the goal of bridging this gap in flexibility, we present a novel system architecture that tightly integrates analog in-memory computing accelerators into multi-core CPUs in general-purpose systems. We developed a powerful gem5-based full system-level simulation framework into the gem5-X simulator, ALPINE, which enables an in-depth characterization of the proposed architecture. ALPINE allows the simulation of the entire computer architecture stack from major hardware components to their interactions with the Linux OS. Within ALPINE, we have defined a custom ISA extension and a software library to facilitate the deployment of inference models. We showcase and analyze a variety of mappings of different neural network types, and demonstrate up to 20.5x/20.8x performance/energy gains with respect to a SIMD-enabled ARM CPU implementation for convolutional neural networks, multi-layer perceptrons, and recurrent neural networks.",2022-05-20T09:24:55Z,2022-12-13T15:28:21Z,http://arxiv.org/abs/2205.10042v2,http://arxiv.org/pdf/2205.10042v2,"cs.AR, C.4; I.6.0"
Scaling and performance portability of the particle-in-cell scheme for   plasma physics applications through mini-apps targeting exascale   architectures,"Sriramkrishnan Muralikrishnan, Matthias Frey, Alessandro Vinciguerra, Michael Ligotino, Antoine J. Cerfon, Miroslav Stoyanov, Rahulkumar Gayatri, Andreas Adelmann","We perform a scaling and performance portability study of the particle-in-cell scheme for plasma physics applications through a set of mini-apps we name ""Alpine"", which can make use of exascale computing capabilities. The mini-apps are based on Independent Parallel Particle Layer, a framework that is designed around performance portable and dimension independent particles and fields.   We benchmark the simulations with varying parameters such as grid resolutions ($512^3$ to $2048^3$) and number of simulation particles ($10^9$ to $10^{11}$) with the following mini-apps: weak and strong Landau damping, bump-on-tail and two-stream instabilities, and the dynamics of an electron bunch in a charge-neutral Penning trap. We show strong and weak scaling and analyze the performance of different components on several pre-exascale architectures such as Piz-Daint, Cori, Summit and Perlmutter. While the scaling and portability study helps identify the performance critical components of the particle-in-cell scheme in the current state-of-the-art computing architectures, the mini-apps by themselves can be used to develop new algorithms and optimize their high performance implementations targeting exascale architectures.",2022-05-23T05:35:13Z,2022-11-02T06:25:44Z,http://arxiv.org/abs/2205.11052v2,http://arxiv.org/pdf/2205.11052v2,"physics.comp-ph, cs.DC, cs.NA, math.NA, physics.plasm-ph"
3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual   Architecture,"Jevgenija Rudzusika, Buda Bajić, Thomas Koehler, Ozan Öktem","Deep learning based computed tomography (CT) reconstruction has demonstrated outstanding performance on simulated 2D low-dose CT data. This applies in particular to domain adapted neural networks, which incorporate a handcrafted physics model for CT imaging. Empirical evidence shows that employing such architectures reduces the demand for training data and improves upon generalisation. However, their training requires large computational resources that quickly become prohibitive in 3D helical CT, which is the most common acquisition geometry used for medical imaging. Furthermore, clinical data also comes with other challenges not accounted for in simulations, like errors in flux measurement, resolution mismatch and, most importantly, the absence of the real ground truth. The necessity to have a computationally feasible training combined with the need to address these issues has made it difficult to evaluate deep learning based reconstruction on clinical 3D helical CT. This paper modifies a domain adapted neural network architecture, the Learned Primal-Dual (LPD), so that it can be trained and applied to reconstruction in this setting. We achieve this by splitting the helical trajectory into sections and applying the unrolled LPD iterations to those sections sequentially. To the best of our knowledge, this work is the first to apply an unrolled deep learning architecture for reconstruction on full-sized clinical data, like those in the Low dose CT image and projection data set (LDCT). Moreover, training and testing is done on a single GPU card with 24GB of memory.",2022-05-24T10:32:32Z,2023-11-28T20:13:20Z,http://arxiv.org/abs/2205.11952v3,http://arxiv.org/pdf/2205.11952v3,"eess.IV, cs.AI, cs.CV, cs.LG"
D$^\text{2}$UF: Deep Coded Aperture Design and Unrolling Algorithm for   Compressive Spectral Image Fusion,"Roman Jacome, Jorge Bacca, Henry Arguello","Compressive spectral imaging (CSI) has attracted significant attention since it employs synthetic apertures to codify spatial and spectral information, sensing only 2D projections of the 3D spectral image. However, these optical architectures suffer from a trade-off between the spatial and spectral resolution of the reconstructed image due to technology limitations. To overcome this issue, compressive spectral image fusion (CSIF) employs the projected measurements of two CSI architectures with different resolutions to estimate a high-spatial high-spectral resolution. This work presents the fusion of the compressive measurements of a low-spatial high-spectral resolution coded aperture snapshot spectral imager (CASSI) architecture and a high-spatial low-spectral resolution multispectral color filter array (MCFA) system. Unlike previous CSIF works, this paper proposes joint optimization of the sensing architectures and a reconstruction network in an end-to-end (E2E) manner. The trainable optical parameters are the coded aperture (CA) in the CASSI and the colored coded aperture in the MCFA system, employing a sigmoid activation function and regularization function to encourage binary values on the trainable variables for an implementation purpose. Additionally, an unrolling-based network inspired by the alternating direction method of multipliers (ADMM) optimization is formulated to address the reconstruction step and the acquisition systems design jointly. Finally, a spatial-spectral inspired loss function is employed at the end of each unrolling layer to increase the convergence of the unrolling network. The proposed method outperforms previous CSIF methods, and experimental results validate the method with real measurements.",2022-05-24T15:39:34Z,2022-05-24T15:39:34Z,http://arxiv.org/abs/2205.12158v1,http://arxiv.org/pdf/2205.12158v1,"eess.IV, cs.LG, math.OC"
RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search   for 3D Cardiac Cine MRI Segmentation,"Qing Lu, Xiaowei Xu, Shunjie Dong, Cong Hao, Lei Yang, Cheng Zhuo, Yiyu Shi","Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, while existing literature have demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, they are mostly guided by accuracy, sometimes with computation complexity, and the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and are thus not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that directly handles real-time constraints in a differentiable NAS framework named RT-DNAS. Experiments on extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art manually and automatically designed architectures, RT-DNAS is able to identify ones with better accuracy while satisfying the real-time constraints.",2022-06-08T23:14:08Z,2022-06-13T09:53:33Z,http://arxiv.org/abs/2206.04682v2,http://arxiv.org/pdf/2206.04682v2,"eess.IV, cs.CV, cs.LG"
RAN Functional Splits in NTN: Architectures and Challenges,"Riccardo Campana, Carla Amatetti, Alessandro Vanelli-Coralli","While 5G networks are already being deployed for commercial applications, Academia and industry are focusing their effort on the development and standardization of the next generations of mobile networks, i.e., 5G-Advance and 6G. Beyond 5G networks will revolutionize communications systems providing seamless connectivity, both in time and in space, to a unique ecosystem consisting of the convergence of the digital, physical, and human domains. In this scenario, NonTerrestrial Networks (NTN) will play a crucial role by providing ubiquitous, secure, and resilient infrastructure fully integrated into the overall system. The additional network complexity introduced by the third dimension of the architecture requires the interoperability of different network elements, enabled by the disaggregation and virtualization of network components, their interconnection by standard interfaces and orchestration by data-driven network artificial intelligence. The disaggregation paradigm foresees the division of the radio access network in different virtualized block of functions, introducing the concept of functional split. Wisely selecting the RAN functional split is possible to better exploit the system resources, obtaining costs saving, and to increase the system performances. In this paper, we firstly provide a discussion of the current 6G NTN development in terms of architectural solutions and then, we thoroughly analyze the impact of the typical NTN channel impairments on the available functional splits. Finally, the benefits of introducing the dynamic optimization of the functional split in NTN are analyzed, together with the foreseen challenges.",2023-09-26T10:16:41Z,2023-09-26T10:16:41Z,http://arxiv.org/abs/2309.14810v1,http://arxiv.org/pdf/2309.14810v1,"cs.NI, cs.SY, eess.SY"
Multi-split configuration design for fluid-based thermal management   systems,"Saeid Bayat, Nastaran Shahmansouri, Satya RT Peddada, Alexander Tessier, Adrian Butscher, James T Allison","High power density systems require efficient cooling to maintain their thermal performance. Despite this, as systems get larger and more complex, human practice and insight may not suffice to determine the desired thermal management system designs. To this end, a framework for automatic architecture exploration is presented in this article for a class of single-phase, multi-split cooling systems. For this class of systems, heat generation devices are clustered based on their spatial information, and flow-split are added only when required and at the location of heat devices. To generate different architectures, candidate architectures are represented as graphs. From these graphs, dynamic physics models are created automatically using a graph-based thermal modeling framework. Then, an optimal fluid flow distribution problem is solved by addressing temperature constraints in the presence of exogenous heat loads to achieve optimal performance. The focus in this work is on the design of general multi-split heat management systems. The architectures discussed here can be used for various applications in the domain of configuration design. The multi-split algorithm can produce configurations where splitting can occur at any of the vertices. The results presented include 3 categories of cases and are discussed in detail.",2023-10-24T04:03:51Z,2023-10-24T04:03:51Z,http://arxiv.org/abs/2310.15500v1,http://arxiv.org/pdf/2310.15500v1,"eess.SY, cs.SY"
Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis   of Opposites,"Augusto Seben da Rosa, Frederico Santos de Oliveira, Anderson da Silva Soares, Arnaldo Candido Junior","Computer vision in general presented several advances such as training optimizations, new architectures (pure attention, efficient block, vision language models, generative models, among others). This have improved performance in several tasks such as classification, and others. However, the majority of these models focus on modifications that are taking distance from realistic neuroscientific approaches related to the brain. In this work, we adopt a more bio-inspired approach and present the Yin Yang Convolutional Network, an architecture that extracts visual manifold, its blocks are intended to separate analysis of colors and forms at its initial layers, simulating occipital lobe's operations. Our results shows that our architecture provides State-of-the-Art efficiency among low parameter architectures in the dataset CIFAR-10. Our first model reached 93.32\% test accuracy, 0.8\% more than the older SOTA in this category, while having 150k less parameters (726k in total). Our second model uses 52k parameters, losing only 3.86\% test accuracy. We also performed an analysis on ImageNet, where we reached 66.49\% validation accuracy with 1.6M parameters. We make the code publicly available at: https://github.com/NoSavedDATA/YinYang_CNN.",2023-10-24T19:48:07Z,2023-10-24T19:48:07Z,http://arxiv.org/abs/2310.16148v1,http://arxiv.org/pdf/2310.16148v1,"cs.CV, cs.AI, I.2.10"
RAN Functional Split Options for Integrated Terrestrial and   Non-Terrestrial 6G Networks,"Mohamed Rihan, Tim Due, MohammadAmin Vakilifard, Dirk Wubben, Armin Dekorsy","Leveraging non-terrestrial platforms in 6G networks holds immense significance as it opens up opportunities to expand network coverage, enhance connectivity, and support a wide range of innovative applications, including global-scale Internet of Things and ultra-high-definition content delivery. To accomplish the seamless integration between terrestrial and non-terrestrial networks, substantial changes in radio access network (RAN) architecture are required. These changes involve the development of new RAN solutions that can efficiently manage the diverse characteristics of both terrestrial and non-terrestrial components, ensuring smooth handovers, resource allocation, and quality of service across the integrated network ecosystem. Additionally, the establishment of robust interconnection and communication protocols between terrestrial and non-terrestrial elements will be pivotal to utilize the full potential of 6G technology. Additionally, innovative approaches have been introduced to split the functionalities within the RAN into centralized and distributed domains. These novel paradigms are designed to enhance RAN's flexibility while simultaneously lowering the costs associated with infrastructure deployment, all while ensuring that the quality of service for end-users remains unaffected. In this work, we provide an extensive examination of various Non-Terrestrial Networks (NTN) architectures and the necessary adaptations required on the existing 5G RAN architecture to align with the distinct attributes of NTN. Of particular significance, we emphasize the crucial RAN functional split choices essential for the seamless integration of terrestrial and non-terrestrial components within advanced 6G networks.",2023-10-26T11:27:39Z,2023-10-26T11:27:39Z,http://arxiv.org/abs/2310.17317v1,http://arxiv.org/pdf/2310.17317v1,"cs.NI, cs.SY, eess.SY"
Med-DANet V2: A Flexible Dynamic Architecture for Efficient Medical   Volumetric Segmentation,"Haoran Shen, Yifu Zhang, Wenxuan Wang, Chen Chen, Jing Liu, Shanshan Song, Jiangyun Li","Recent works have shown that the computational efficiency of 3D medical image (e.g. CT and MRI) segmentation can be impressively improved by dynamic inference based on slice-wise complexity. As a pioneering work, a dynamic architecture network for medical volumetric segmentation (i.e. Med-DANet) has achieved a favorable accuracy and efficiency trade-off by dynamically selecting a suitable 2D candidate model from the pre-defined model bank for different slices. However, the issues of incomplete data analysis, high training costs, and the two-stage pipeline in Med-DANet require further improvement. To this end, this paper further explores a unified formulation of the dynamic inference framework from the perspective of both the data itself and the model structure. For each slice of the input volume, our proposed method dynamically selects an important foreground region for segmentation based on the policy generated by our Decision Network and Crop Position Network. Besides, we propose to insert a stage-wise quantization selector to the employed segmentation model (e.g. U-Net) for dynamic architecture adapting. Extensive experiments on BraTS 2019 and 2020 show that our method achieves comparable or better performance than previous state-of-the-art methods with much less model complexity. Compared with previous methods Med-DANet and TransBTS with dynamic and static architecture respectively, our framework improves the model efficiency by up to nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019.",2023-10-28T09:57:28Z,2023-10-28T09:57:28Z,http://arxiv.org/abs/2310.18656v1,http://arxiv.org/pdf/2310.18656v1,"eess.IV, cs.CV"
Feature Aggregation in Joint Sound Classification and Localization   Neural Networks,"Brendan Healy, Patrick McNamee, Zahra Nili Ahmadabadi","This study addresses the application of deep learning techniques in joint sound signal classification and localization networks. Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture. Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance. This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals. To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks. Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation. To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation sub-architectures into a SSL control architecture: Path Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network (BiFPN), and SEN. These sub-architectures were evaluated using two metrics for signal classification and two metrics for direction-of-arrival regression. PANet and BiFPN are established aggregators in computer vision models, while the proposed SEN is a more compact aggregator. The results suggest that models incorporating feature aggregations outperformed the control model, the Sound Event Localization and Detection network (SELDnet), in both sound signal classification and localization. The feature aggregation techniques enhance the performance of sound detection neural networks, particularly in direction-of-arrival regression.",2023-10-29T16:37:14Z,2024-01-27T20:45:13Z,http://arxiv.org/abs/2310.19063v2,http://arxiv.org/pdf/2310.19063v2,"cs.SD, cs.LG, eess.AS"
Kronecker-Factored Approximate Curvature for Modern Neural Network   Architectures,"Runa Eschenhagen, Alexander Immer, Richard E. Turner, Frank Schneider, Philipp Hennig","The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures.",2023-11-01T16:37:00Z,2024-01-11T17:32:26Z,http://arxiv.org/abs/2311.00636v2,http://arxiv.org/pdf/2311.00636v2,"cs.LG, stat.ML"
FATE: Feature-Agnostic Transformer-based Encoder for learning   generalized embedding spaces in flow cytometry data,"Lisa Weijler, Florian Kowarsch, Michael Reiter, Pedro Hermosilla, Margarita Maurer-Granofszky, Michael Dworzak","While model architectures and training strategies have become more generic and flexible with respect to different data modalities over the past years, a persistent limitation lies in the assumption of fixed quantities and arrangements of input features. This limitation becomes particularly relevant in scenarios where the attributes captured during data acquisition vary across different samples. In this work, we aim at effectively leveraging data with varying features, without the need to constrain the input space to the intersection of potential feature sets or to expand it to their union. We propose a novel architecture that can directly process data without the necessity of aligned feature modalities by learning a general embedding space that captures the relationship between features across data samples with varying sets of features. This is achieved via a set-transformer architecture augmented by feature-encoder layers, thereby enabling the learning of a shared latent feature space from data originating from heterogeneous feature spaces. The advantages of the model are demonstrated for automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where the features measured during acquisition often vary between samples. Our proposed architecture's capacity to operate seamlessly across incongruent feature spaces is particularly relevant in this context, where data scarcity arises from the low prevalence of the disease. The code is available for research purposes at https://github.com/lisaweijler/FATE.",2023-11-06T18:06:38Z,2023-11-06T18:06:38Z,http://arxiv.org/abs/2311.03314v1,http://arxiv.org/pdf/2311.03314v1,"eess.IV, cs.CV"
"Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs","Hongwu Peng, Caiwen Ding, Tong Geng, Sutanay Choudhury, Kevin Barker, Ang Li","The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern their strengths and unique capabilities. By conducting a series of benchmark evaluations on common DNN operators and other AI/ML workloads, we aim to illuminate the advantages of data-flow architectures over conventional processor designs and offer insights into the performance trade-offs of each platform. The findings from our study will serve as a valuable reference for the design and performance expectations of research prototypes, thereby facilitating the development of next-generation hardware accelerators tailored for the ever-evolving landscape of AI/ML applications. Through this analysis, we aspire to contribute to the broader understanding of current accelerator technologies and to provide guidance for future innovations in the field.",2023-11-08T01:06:25Z,2024-03-19T06:31:52Z,http://arxiv.org/abs/2311.04417v3,http://arxiv.org/pdf/2311.04417v3,"cs.AR, cs.DC, cs.LG, cs.PF, C.4"
Augmented Lagrangian Methods as Layered Control Architectures,"Anusha Srikanthan, Vijay Kumar, Nikolai Matni","For optimal control problems that involve planning and following a trajectory, two degree of freedom (2DOF) controllers are a ubiquitously used control architecture that decomposes the problem into a trajectory generation layer and a feedback control layer. However, despite the broad use and practical success of this layered control architecture, it remains a design choice that must be imposed $a\ priori$ on the control policy. To address this gap, this paper seeks to initiate a principled study of the design of layered control architectures, with an initial focus on the 2DOF controller. We show that applying the Alternating Direction Method of Multipliers (ADMM) algorithm to solve a strategically rewritten optimal control problem results in solutions that are naturally layered, and composed of a trajectory generation layer and a feedback control layer. Furthermore, these layers are coupled via Lagrange multipliers that ensure dynamic feasibility of the planned trajectory. We instantiate this framework in the context of deterministic and stochastic linear optimal control problems, and show how our approach automatically yields a feedforward/feedback-based control policy that exactly solves the original problem. We then show that the simplicity of the resulting controller structure suggests natural heuristic algorithms for approximately solving nonlinear optimal control problems. We empirically demonstrate improved performance of these layered nonlinear optimal controllers as compared to iLQR, and highlight their flexibility by incorporating both convex and nonconvex constraints.",2023-11-10T21:45:22Z,2023-11-10T21:45:22Z,http://arxiv.org/abs/2311.06404v1,http://arxiv.org/pdf/2311.06404v1,"math.OC, cs.SY, eess.SY"
Hybrid Precoding and Combining for mmWave Full-Duplex Joint Radar and   Communication Systems under Self-Interference,"Murat Bayraktar, Nuria González-Prelcic, Hao Chen","In the context of integrated sensing and communication (ISAC), a full-duplex (FD) transceiver can operate as a monostatic radar while maintaining communication capabilities. This paper investigates the design of precoders and combiners for a joint radar and communication (JRC) system at mmWave frequencies. The primary goal of the design is to guarantee certain performance in terms of some sensing and communication metrics while minimizing the self-interference (SI) caused by FD operation and taking into account the hardware limitations coming from a hybrid MIMO architecture. Specifically, we introduce a generalized eigenvalue-based precoder design that considers the downlink user rate, the radar gain, and the SI suppression. Since the hybrid analog/digital architecture degrades the SI mitigation capability of the precoder, we further enhance SI suppression with the analog combiner. Our numerical results demonstrate that the proposed architecture achieves the required radar gain and SI mitigation while incurring a small loss in downlink spectral efficiency. Additionally, the numerical experiments also show that the use of orthogonal frequency division multiplexing (OFDM) radar with the proposed beamforming architecture results in highly accurate range and velocity estimates for the detected targets.",2023-11-25T06:24:45Z,2024-04-01T20:06:40Z,http://arxiv.org/abs/2311.14942v2,http://arxiv.org/pdf/2311.14942v2,"eess.SP, cs.IT, math.IT"
Efficient and Scalable Architecture for Multiple-chip Implementation of   Simulated Bifurcation Machines,"Tomoya Kashimata, Masaya Yamasaki, Ryo Hidaka, Kosuke Tatsumura","Ising machines are specialized computers for finding the lowest energy states of Ising spin models, onto which many practical combinatorial optimization problems can be mapped. Simulated bifurcation (SB) is a quantum-inspired parallelizable algorithm for Ising problems that enables scalable multi-chip implementations of Ising machines. However, the computational performance of a previously proposed multi-chip architecture tends to saturate as the number of chips increases for a given problem size because both computation and communication are exclusive in the time domain. In this paper, we propose a streaming architecture for multi-chip implementations of SB-based Ising machines with full spin-to-spin connectivity. The data flow in in-chip computation is harmonized with the data flow in inter-chip communication, enabling the computation and communication to overlap and the communication time to be hidden. Systematic experiments demonstrate linear strong scaling of performance up to the vicinity of the ideal communication limit determined only by the latency of chip-to-chip communication. Our eight-FPGA (field-programmable gate array) cluster can compute a 32,768-spin problem with a high pipeline efficiency of 97.9%. The performance of a 79-FPGA cluster for a 100,000-spin problem, projected using a theoretical performance model validated on smaller experimental clusters, is comparable to that of a state-of-the-art 100,000-spin optical Ising machine.",2023-11-29T05:49:00Z,2023-11-29T05:49:00Z,http://arxiv.org/abs/2311.17370v1,http://arxiv.org/pdf/2311.17370v1,"cs.ET, cs.AR, cs.DC, cs.PF, 68M20, C.3; C.5.4"
Learning Arbitrary Complex Matrices by Interlacing Amplitude and Phase   Masks with Fixed Unitary Operations,"Matthew Markowitz, Kevin Zelaya, Mohammad-Ali Miri","Programmable photonic integrated circuits represent an emerging technology that amalgamates photonics and electronics, paving the way for light-based information processing at high speeds and low power consumption. Considering their wide range of applications as one of the most fundamental mathematical operations there has been a particular interest in programmable photonic circuits that perform matrix-vector multiplication. In this regard, there has been great interest in developing novel circuit architectures for performing matrix operations that are compatible with the existing photonic integrated circuit technology which can thus be reliably implemented. Recently, it has been shown that discrete linear unitary operations can be parameterized through diagonal phase parameters interlaced with a fixed operator that enables efficient photonic realization of unitary operations by cascading phase shifter arrays interlaced with a multiport component. Here, we show that such a decomposition is only a special case of a much broader class of factorizations that allow for parametrizing arbitrary complex matrices in terms of diagonal matrices alternating with a fixed unitary matrix. Thus, we introduce a novel architecture for physically implementing discrete linear operations. The proposed architecture is built on representing an $N \times N$ matrix operator in terms of $N+1$ amplitude-and-phase modulation layers interlaced with a fixed unitary layer that could be implemented via a coupled waveguide array. The proposed architecture enables the development of novel families of programmable photonic circuits for on-chip analog information processing.",2023-12-09T19:27:57Z,2023-12-09T19:27:57Z,http://arxiv.org/abs/2312.05648v1,http://arxiv.org/pdf/2312.05648v1,"physics.optics, quant-ph"
Stateful Conformer with Cache-based Inference for Streaming Automatic   Speech Recognition,"Vahid Noroozi, Somshubra Majumdar, Ankur Kumar, Jagadeesh Balam, Boris Ginsburg","In this paper, we propose an efficient and accurate streaming speech recognition model based on the FastConformer architecture. We adapted the FastConformer architecture for streaming applications through: (1) constraining both the look-ahead and past contexts in the encoder, and (2) introducing an activation caching mechanism to enable the non-autoregressive encoder to operate autoregressively during inference. The proposed model is thoughtfully designed in a way to eliminate the accuracy disparity between the train and inference time which is common for many streaming models. Furthermore, our proposed encoder works with various decoder configurations including Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders. Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a shared encoder with both a CTC and RNNT decoder to boost the accuracy and save computation. We evaluate the proposed model on LibriSpeech dataset and a multi-domain large scale dataset and demonstrate that it can achieve better accuracy with lower latency and inference time compared to a conventional buffered streaming model baseline. We also showed that training a model with multiple latencies can achieve better accuracy than single latency models while it enables us to support multiple latencies with a single model. Our experiments also showed the hybrid architecture would not only speedup the convergence of the CTC decoder but also improves the accuracy of streaming models compared to single decoder models.",2023-12-27T21:04:26Z,2024-05-02T21:38:10Z,http://arxiv.org/abs/2312.17279v3,http://arxiv.org/pdf/2312.17279v3,"cs.CL, eess.AS"
Interactive Multi-Objective Evolutionary Optimization of Software   Architectures,"Aurora Ramírez, José Raúl Romero, Sebastián Ventura","While working on a software specification, designers usually need to evaluate different architectural alternatives to be sure that quality criteria are met. Even when these quality aspects could be expressed in terms of multiple software metrics, other qualitative factors cannot be numerically measured, but they are extracted from the engineer's know-how and prior experiences. In fact, detecting not only strong but also weak points in the different solutions seems to fit better with the way humans make their decisions. Putting the human in the loop brings new challenges to the search-based software engineering field, especially for those human-centered activities within the early analysis phase. This paper explores how the interactive evolutionary computation can serve as a basis for integrating the human's judgment into the search process. An interactive approach is proposed to discover software architectures, in which both quantitative and qualitative criteria are applied to guide a multi-objective evolutionary algorithm. The obtained feedback is incorporated into the fitness function using architectural preferences allowing the algorithm to discern between promising and poor solutions. Experimentation with real users has revealed that the proposed interaction mechanism can effectively guide the search towards those regions of the search space that are of real interest to the expert.",2024-01-08T19:15:40Z,2024-01-08T19:15:40Z,http://arxiv.org/abs/2401.04192v1,http://arxiv.org/pdf/2401.04192v1,"cs.SE, cs.AI, cs.NE, 68, D.2.11; F.2.2"
Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering   Tasks,"Zihao Wang, Zhe Wu","Computational efficiency and robustness are essential in process modeling, optimization, and control for real-world engineering applications. While neural network-based approaches have gained significant attention in recent years, conventional neural networks often fail to address these two critical aspects simultaneously or even independently. Inspired by natural physical systems and established literature, input convex architectures are known to enhance computational efficiency in optimization tasks, whereas Lipschitz-constrained architectures improve robustness. However, combining these properties within a single model requires careful review, as inappropriate methods for enforcing one property can undermine the other. To overcome this, we introduce a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks (ICLRNNs). This architecture seamlessly integrates the benefits of convexity and Lipschitz continuity, enabling fast and robust neural network-based modeling and optimization. The ICLRNN outperforms existing recurrent units in both computational efficiency and robustness. Additionally, it has been successfully applied to practical engineering scenarios, such as modeling and control of chemical process and the modeling and real-world solar irradiance prediction for solar PV system planning at LHT Holdings in Singapore. Source code is available at https://github.com/killingbear999/ICLRNN.",2024-01-15T06:26:53Z,2025-01-24T15:48:22Z,http://arxiv.org/abs/2401.07494v5,http://arxiv.org/pdf/2401.07494v5,"cs.LG, cs.CE, cs.SY, eess.SY"
"Centralized active reconfigurable intelligent surface: Architecture,   path loss analysis and experimental verification","Changhao Liu, Fan Yang, Shenheng Xu, Yezhen Li, Maokun Li","Reconfigurable intelligent surfaces (RISs) are promising candidate for the 6G communication. Recently, active RIS has been proposed to compensate the multiplicative fading effect inherent in passive RISs. However, conventional distributed active RISs, with at least one amplifier per element, are costly, complex, and power-intensive. To address these challenges, this paper proposes a novel architecture of active RIS: the centralized active RIS (CA-RIS), which amplifies the energy using a centralized amplifying reflector to reduce the number of amplifiers. Under this architecture, only as low as one amplifier is needed for power amplification of the entire array, which can eliminate the mutual-coupling effect among amplifiers, and significantly reduce the cost, noise level, and power consumption. We evaluate the performance of CA-RIS, specifically its path loss, and compare it with conventional passive RISs, revealing a moderate amplification gain. Furthermore, the proposed CA-RIS and the path loss model are experimentally verified, achieving a 9.6 dB net gain over passive RIS at 4 GHz. The CA-RIS offers a substantial simplification of active RIS architecture while preserving performance, striking an optimal balance between system complexity and the performance, which is competitive in various scenarios.",2024-01-17T19:09:11Z,2024-01-19T02:19:13Z,http://arxiv.org/abs/2401.09552v2,http://arxiv.org/pdf/2401.09552v2,"physics.app-ph, eess.SP"
Wideband Beamforming for RIS Assisted Near-Field Communications,"Ji Wang, Jian Xiao, Yixuan Zou, Wenwu Xie, Yuanwei Liu","A near-field wideband beamforming scheme is investigated for reconfigurable intelligent surface (RIS) assisted multiple-input multiple-output (MIMO) systems, in which a deep learning-based end-to-end (E2E) optimization framework is proposed to maximize the system spectral efficiency. To deal with the near-field double beam split effect, the base station is equipped with frequency-dependent hybrid precoding architecture by introducing sub-connected true time delay (TTD) units, while two specific RIS architectures, namely true time delay-based RIS (TTD-RIS) and virtual subarray-based RIS (SA-RIS), are exploited to realize the frequency-dependent passive beamforming at the RIS. Furthermore, the efficient E2E beamforming models without explicit channel state information are proposed, which jointly exploits the uplink channel training module and the downlink wideband beamforming module. In the proposed network architecture of the E2E models, the classical communication signal processing methods, i.e., polarized filtering and sparsity transform, are leveraged to develop a signal-guided beamforming network. Numerical results show that the proposed E2E models have superior beamforming performance and robustness to conventional beamforming benchmarks. Furthermore, the tradeoff between the beamforming gain and the hardware complexity is investigated for different frequency-dependent RIS architectures, in which the TTD-RIS can achieve better spectral efficiency than the SA-RIS while requiring additional energy consumption and hardware cost.",2024-01-20T06:32:20Z,2025-01-07T10:50:25Z,http://arxiv.org/abs/2401.11141v4,http://arxiv.org/pdf/2401.11141v4,"cs.IT, eess.SP, math.IT"
Towards a Theory of Control Architecture: A quantitative framework for   layered multi-rate control,"Nikolai Matni, Aaron D. Ames, John C. Doyle","This paper focuses on the need for a rigorous theory of layered control architectures (LCAs) for complex engineered and natural systems, such as power systems, communication networks, autonomous robotics, bacteria, and human sensorimotor control. All deliver extraordinary capabilities, but they lack a coherent theory of analysis and design, partly due to the diverse domains across which LCAs can be found. In contrast, there is a core universal set of control concepts and theory that applies very broadly and accommodates necessary domain-specific specializations. However, control methods are typically used only to design algorithms in components within a larger system designed by others, typically with minimal or no theory. This points towards a need for natural but large extensions of robust performance from control to the full decision and control stack. It is encouraging that the successes of extant architectures from bacteria to the Internet are due to strikingly universal mechanisms and design patterns. This is largely due to convergent evolution by natural selection and not intelligent design, particularly when compared with the sophisticated design of components. Our aim here is to describe the universals of architecture and sketch tentative paths towards a useful design theory.",2024-01-26T20:06:45Z,2024-01-26T20:06:45Z,http://arxiv.org/abs/2401.15185v1,http://arxiv.org/pdf/2401.15185v1,"math.OC, cs.RO, cs.SY, eess.SY"
Digital requirements engineering with an INCOSE-derived SysML meta-model,"James S. Wheaton, Daniel R. Herber","Traditional requirements engineering tools do not readily access the SysML-defined system architecture model, often resulting in ad-hoc duplication of model elements that lacks the connectivity and expressive detail possible in a SysML-defined model. Without that model connectivity, requirement quality can suffer due to imprecision and inconsistent terminology, frustrating communication during system development. Further integration of requirements engineering activities with MBSE contributes to the Authoritative Source of Truth while facilitating deep access to system architecture model elements for V&V activities. The Model-Based Structured Requirement SysML Profile was extended to comply with the INCOSE Guide to Writing Requirements updated in 2023 while conforming to the ISO/IEC/IEEE 29148 standard requirement statement templates. Rules, Characteristics, and Attributes were defined in SysML according to the Guide to facilitate requirements definition and requirements V&V. The resulting SysML Profile was applied in two system architecture models at NASA Jet Propulsion Laboratory, allowing us to explore its applicability and value in real-world project environments. Initial results indicate that INCOSE-derived Model-Based Structured Requirements may rapidly improve requirement expression quality while complementing the NASA Systems Engineering Handbook checklist and guidance, but typical requirement management activities still have challenges related to automation and support with the system architecture modeling software.",2024-01-29T17:35:35Z,2024-02-21T03:48:45Z,http://arxiv.org/abs/2401.16330v2,http://arxiv.org/pdf/2401.16330v2,"eess.SY, cs.SY"
PirateNets: Physics-informed Deep Learning with Residual Adaptive   Networks,"Sifan Wang, Bowen Li, Yuhan Chen, Paris Perdikaris","While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture. We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks. All code and data accompanying this manuscript will be made publicly available at \url{https://github.com/PredictiveIntelligenceLab/jaxpi}.",2024-02-01T04:17:56Z,2024-02-11T22:53:59Z,http://arxiv.org/abs/2402.00326v3,http://arxiv.org/pdf/2402.00326v3,"cs.LG, cs.NA, math.NA"
Demystifying Datapath Accelerator Enhanced Off-path SmartNIC,"Xuzheng Chen, Jie Zhang, Ting Fu, Yifan Shen, Shu Ma, Kun Qian, Lingjun Zhu, Chao Shi, Yin Zhang, Ming Liu, Zeke Wang","Network speeds grow quickly in the modern cloud, so SmartNICs are introduced to offload network processing tasks, even application logic. However, typical multicore SmartNICs such as BlueFiled-2 are only capable of processing control-plane tasks with their embedded processors that have limited memory bandwidth and computing power. On the other hand, cloud applications evolve rapidly, such that a limited number of fixed hardware engines in a SmartNIC cannot satisfy the requirements of cloud applications. Therefore, SmartNIC programmers call for a programmable datapath accelerator (DPA) to process network traffic at line rate. However, no existing work has unveiled the performance characteristics of the existing DPA. To this end, we present the first architectural characterization of the latest DPA-enhanced BlueFiled-3 (BF3) SmartNIC. Our evaluation results indicate that BF3's DPA is significantly wimpier than the off-path Arm processor and the host CPU. However, we still identify that DPA has three unique architectural characteristics that unleash the performance potential of DPA. Specifically, we demonstrate how to take advantage of DPA's three architectural characteristics regarding computing, networking, and memory subsystems. Then we propose three important guidelines for programmers to fully unleash the potential of DPA. To demonstrate the effectiveness of our approach, we conduct detailed case studies regarding each guideline. Our case study on key-value aggregation achieves up to 4.3$\times$ higher throughput by using our guidelines to optimize memory combinations.",2024-02-05T14:25:08Z,2024-09-09T07:30:20Z,http://arxiv.org/abs/2402.03041v4,http://arxiv.org/pdf/2402.03041v4,"cs.NI, 68M10, C.2.1"
AFPR-CIM: An Analog-Domain Floating-Point RRAM-based Compute-In-Memory   Architecture with Dynamic Range Adaptive FP-ADC,"Haobo Liu, Zhengyang Qian, Wei Wu, Hongwei Ren, Zhiwei Liu, Leibin Ni","Power consumption has become the major concern in neural network accelerators for edge devices. The novel non-volatile-memory (NVM) based computing-in-memory (CIM) architecture has shown great potential for better energy efficiency. However, most of the recent NVM-CIM solutions mainly focus on fixed-point calculation and are not applicable to floating-point (FP) processing. In this paper, we propose an analog-domain floating-point CIM architecture (AFPR-CIM) based on resistive random-access memory (RRAM). A novel adaptive dynamic-range FP-ADC is designed to convert the analog computation results into FP codes. Output current with high dynamic range is converted to a normalized voltage range for readout, to prevent precision loss at low power consumption. Moreover, a novel FP-DAC is also implemented which reconstructs FP digital codes into analog values to perform analog computation. The proposed AFPR-CIM architecture enables neural network acceleration with FP8 (E2M5) activation for better accuracy and energy efficiency. Evaluation results show that AFPR-CIM can achieve 19.89 TFLOPS/W energy efficiency and 1474.56 GOPS throughput. Compared to traditional FP8 accelerator, digital FP-CIM, and analog INT8-CIM, this work achieves 4.135x, 5.376x, and 2.841x energy efficiency enhancement, respectively.",2024-02-21T13:27:14Z,2024-02-21T13:27:14Z,http://arxiv.org/abs/2402.13798v1,http://arxiv.org/pdf/2402.13798v1,"eess.SY, cs.SY"
Savvy: Trustworthy Autonomous Vehicles Architecture,"Ali Shoker, Rehana Yasmin, Paulo Esteves-Verissimo","The increasing interest in Autonomous Vehicles (AV) is notable due to business, safety, and performance reasons. While there is salient success in recent AV architectures, hinging on the advancements in AI models, there is a growing number of fatal incidents that impedes full AVs from going mainstream. This calls for the need to revisit the fundamentals of building safety-critical AV architectures. However, this direction should not deter leveraging the power of AI. To this end, we propose Savvy, a new trustworthy intelligent AV architecture that achieves the best of both worlds. Savvy makes a clear separation between the control plane and the data plane to guarantee the safety-first principles. The former assume control to ensure safety using design-time defined rules, while launching the latter for optimizing decisions as much as possible within safety time-bounds. This is achieved through guided Time-aware predictive quality degradation (TPQD): using dynamic ML models that can be tuned to provide either richer or faster outputs based on the available safety time bounds. For instance, Savvy allows to safely identify an elephant as an obstacle (a mere object) the earliest possible, rather than optimally recognizing it as an elephant when it is too late. This position paper presents the Savvy's motivations and concept, whereas empirical evaluation is a work in progress.",2024-02-08T07:24:45Z,2024-02-08T07:24:45Z,http://arxiv.org/abs/2402.14580v1,http://arxiv.org/pdf/2402.14580v1,"cs.AI, cs.SY, eess.SY"
A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless   Baseband Processing,"Limin Jiang, Yi Shi, Haiqin Hu, Qingyu Deng, Siyi Xu, Yintao Liu, Feng Yuan, Si Wang, Yihao Shen, Fangfang Ye, Shan Cao, Zhiyuan Jiang","Wireless baseband processing (WBP) is a key element of wireless communications, with a series of signal processing modules to improve data throughput and counter channel fading. Conventional hardware solutions, such as digital signal processors (DSPs) and more recently, graphic processing units (GPUs), provide various degrees of parallelism, yet they both fail to take into account the cyclical and consecutive character of WBP. Furthermore, the large amount of data in WBPs cannot be processed quickly in symmetric multiprocessors (SMPs) due to the unpredictability of memory latency. To address this issue, we propose a hierarchical dataflow-driven architecture to accelerate WBP. A pack-and-ship approach is presented under a non-uniform memory access (NUMA) architecture to allow the subordinate tiles to operate in a bundled access and execute manner. We also propose a multi-level dataflow model and the related scheduling scheme to manage and allocate the heterogeneous hardware resources. Experiment results demonstrate that our prototype achieves $2\times$ and $2.3\times$ speedup in terms of normalized throughput and single-tile clock cycles compared with GPU and DSP counterparts in several critical WBP benchmarks. Additionally, a link-level throughput of $288$ Mbps can be achieved with a $45$-core configuration.",2024-02-28T06:01:44Z,2024-02-28T06:01:44Z,http://arxiv.org/abs/2402.18070v1,http://arxiv.org/pdf/2402.18070v1,"cs.AR, eess.SP"
Multi-objective Differentiable Neural Architecture Search,"Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter","Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationally expensive search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences to trade-off performance and hardware metrics, yielding representative and diverse architectures across multiple devices in just a single search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments involving up to 19 hardware devices and 3 different objectives demonstrate the effectiveness and scalability of our method. Finally, we show that, without any additional costs, our method outperforms existing MOO NAS methods across a broad range of qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k, an encoder-decoder transformer space for machine translation and a decoder-only space for language modelling.",2024-02-28T10:09:04Z,2025-02-04T21:42:13Z,http://arxiv.org/abs/2402.18213v3,http://arxiv.org/pdf/2402.18213v3,"cs.LG, cs.CV, stat.ML"
Probing the Information Encoded in Neural-based Acoustic Models of   Automatic Speech Recognition Systems,"Quentin Raymondaud, Mickael Rouvier, Richard Dufour","Deep learning architectures have made significant progress in terms of performance in many research areas. The automatic speech recognition (ASR) field has thus benefited from these scientific and technological advances, particularly for acoustic modeling, now integrating deep neural network architectures. However, these performance gains have translated into increased complexity regarding the information learned and conveyed through these black-box architectures. Following many researches in neural networks interpretability, we propose in this article a protocol that aims to determine which and where information is located in an ASR acoustic model (AM). To do so, we propose to evaluate AM performance on a determined set of tasks using intermediate representations (here, at different layer levels). Regarding the performance variation and targeted tasks, we can emit hypothesis about which information is enhanced or perturbed at different architecture steps. Experiments are performed on both speaker verification, acoustic environment classification, gender classification, tempo-distortion detection systems and speech sentiment/emotion identification. Analysis showed that neural-based AMs hold heterogeneous information that seems surprisingly uncorrelated with phoneme recognition, such as emotion, sentiment or speaker identity. The low-level hidden layers globally appears useful for the structuring of information while the upper ones would tend to delete useless information for phoneme recognition.",2024-02-29T18:43:53Z,2024-02-29T18:43:53Z,http://arxiv.org/abs/2402.19443v1,http://arxiv.org/pdf/2402.19443v1,"cs.SD, cs.AI, eess.AS"
Solving Inverse Problems with Model Mismatch using Untrained Neural   Networks within Model-based Architectures,"Peimeng Guan, Naveed Iqbal, Mark A. Davenport, Mudassir Masood","Model-based deep learning methods such as loop unrolling (LU) and deep equilibrium model}(DEQ) extensions offer outstanding performance in solving inverse problems (IP). These methods unroll the optimization iterations into a sequence of neural networks that in effect learn a regularization function from data. While these architectures are currently state-of-the-art in numerous applications, their success heavily relies on the accuracy of the forward model. This assumption can be limiting in many physical applications due to model simplifications or uncertainties in the apparatus. To address forward model mismatch, we introduce an untrained forward model residual block within the model-based architecture to match the data consistency in the measurement domain for each instance. We propose two variants in well-known model-based architectures (LU and DEQ) and prove convergence under mild conditions. Our approach offers a unified solution that is less parameter-sensitive, requires no additional data, and enables simultaneous fitting of the forward model and reconstruction in a single pass, benefiting both linear and nonlinear inverse problems. The experiments show significant quality improvement in removing artifacts and preserving details across three distinct applications, encompassing both linear and nonlinear inverse problems. Moreover, we highlight reconstruction effectiveness in intermediate steps and showcase robustness to random initialization of the residual block and a higher number of iterations during evaluation. Code is available at \texttt{https://github.com/InvProbs/A-adaptive-model-based-methods}.",2024-03-07T19:02:13Z,2024-06-10T11:43:17Z,http://arxiv.org/abs/2403.04847v2,http://arxiv.org/pdf/2403.04847v2,"cs.LG, eess.SP"
The Goldilocks Principle of Learning Unitaries by Interlacing Fixed   Operators with Programmable Phase Shifters on a Photonic Chip,"Kevin Zelaya, Matthew Markowitz, Mohammad-Ali Miri","Programmable photonic integrated circuits represent an emerging technology that amalgamates photonics and electronics, paving the way for light-based information processing at high speeds and low power consumption. Programmable photonics provides a flexible platform that can be reconfigured to perform multiple tasks, thereby holding great promise for revolutionizing future optical networks and quantum computing systems. Over the past decade, there has been constant progress in developing several different architectures for realizing programmable photonic circuits that allow for realizing arbitrary discrete unitary operations with light. Here, we systematically investigate a general family of photonic circuits for realizing arbitrary unitaries based on a simple architecture that interlaces a fixed intervening layer with programmable phase shifter layers. We introduce a criterion for the intervening operator that guarantees the universality of this architecture for representing arbitrary $N \times N$ unitary operators with $N+1$ phase layers. We explore this criterion for different photonic components, including photonic waveguide lattices and meshes of directional couplers, which allows the identification of several families of photonic components that can serve as the intervening layers in the interlacing architecture. Our findings pave the way for efficiently designing and realizing novel families of programmable photonic integrated circuits for multipurpose analog information processing.",2024-03-15T16:59:03Z,2024-03-15T16:59:03Z,http://arxiv.org/abs/2403.10469v1,http://arxiv.org/pdf/2403.10469v1,"physics.optics, cs.ET"
Self-supervised Learning for Acoustic Few-Shot Classification,"Jingyong Liang, Bernd Meyer, Issac Ning Lee, Thanh-Toan Do","Labelled data are limited and self-supervised learning is one of the most important approaches for reducing labelling requirements. While it has been extensively explored in the image domain, it has so far not received the same amount of attention in the acoustic domain. Yet, reducing labelling is a key requirement for many acoustic applications. Specifically in bioacoustic, there are rarely sufficient labels for fully supervised learning available. This has led to the widespread use of acoustic recognisers that have been pre-trained on unrelated data for bioacoustic tasks. We posit that training on the actual task data and combining self-supervised pre-training with few-shot classification is a superior approach that has the ability to deliver high accuracy even when only a few labels are available. To this end, we introduce and evaluate a new architecture that combines CNN-based preprocessing with feature extraction based on state space models (SSMs). This combination is motivated by the fact that CNN-based networks alone struggle to capture temporal information effectively, which is crucial for classifying acoustic signals. SSMs, specifically S4 and Mamba, on the other hand, have been shown to have an excellent ability to capture long-range dependencies in sequence data. We pre-train this architecture using contrastive learning on the actual task data and subsequent fine-tuning with an extremely small amount of labelled data. We evaluate the performance of this proposed architecture for ($n$-shot, $n$-class) classification on standard benchmarks as well as real-world data. Our evaluation shows that it outperforms state-of-the-art architectures on the few-shot classification problem.",2024-09-15T07:45:11Z,2024-09-15T07:45:11Z,http://arxiv.org/abs/2409.09647v1,http://arxiv.org/pdf/2409.09647v1,"cs.SD, cs.AI, eess.AS"
User-Centric Cell-Free Massive MIMO With RIS-Integrated Antenna Arrays,"Özlem Tuğfe Demir, Emil Björnson","Cell-free massive MIMO (multiple-input multiple-output) is a promising network architecture for beyond 5G systems, which can particularly offer more uniform data rates across the coverage area. Recent works have shown how reconfigurable intelligent surfaces (RISs) can be used as relays in cell-free massive MIMO networks to improve data rates further. In this paper, we analyze an alternative architecture where an RIS is integrated into the antenna array at each access point and acts as an intelligent transmitting surface to expand the aperture area. This approach alleviates the multiplicative fading effect that normally makes RIS-aided systems inefficient and offers a cost-effective alternative to building large antenna arrays. We use a small number of antennas and a larger number of controllable RIS elements to match the performance of an antenna array whose size matches that of the RIS. In this paper, we explore this innovative transceiver architecture in the uplink of a cell-free massive MIMO system for the first time, demonstrating its potential benefits through analytic and numerical contributions. The simulation results validate the effectiveness of our proposed phase-shift configuration and highlight scenarios where the proposed architecture significantly enhances data rates.",2024-09-24T05:41:25Z,2024-09-24T05:41:25Z,http://arxiv.org/abs/2409.15765v1,http://arxiv.org/pdf/2409.15765v1,"eess.SP, cs.IT, math.IT"
"Blocks Architecture (BloArk): Efficient, Cost-Effective, and Incremental   Dataset Architecture for Wikipedia Revision History","Lingxi Li, Zonghai Yao, Sunjae Kwon, Hong Yu","Wikipedia (Wiki) is one of the most widely used and publicly available resources for natural language processing (NLP) applications. Wikipedia Revision History (WikiRevHist) shows the order in which edits were made to any Wiki page since its first modification. While the most up-to-date Wiki has been widely used as a training source, WikiRevHist can also be valuable resources for NLP applications. However, there are insufficient tools available to process WikiRevHist without having substantial computing resources, making additional customization, and spending extra time adapting others' works. Therefore, we report Blocks Architecture (BloArk), an efficiency-focused data processing architecture that reduces running time, computing resource requirements, and repeated works in processing WikiRevHist dataset. BloArk consists of three parts in its infrastructure: blocks, segments, and warehouses. On top of that, we build the core data processing pipeline: builder and modifier. The BloArk builder transforms the original WikiRevHist dataset from XML syntax into JSON Lines (JSONL) format for improving the concurrent and storage efficiency. The BloArk modifier takes previously-built warehouses to operate incremental modifications for improving the utilization of existing databases and reducing the cost of reusing others' works. In the end, BloArk can scale up easily in both processing Wikipedia Revision History and incrementally modifying existing dataset for downstream NLP use cases. The source code, documentations, and example usages are publicly available online and open-sourced under GPL-2.0 license.",2024-10-06T08:58:14Z,2024-10-06T08:58:14Z,http://arxiv.org/abs/2410.04410v1,http://arxiv.org/pdf/2410.04410v1,"cs.CL, I.7; I.2.7; E.1"
STA-Unet: Rethink the semantic redundant for Medical Imaging   Segmentation,"Vamsi Krishna Vasa, Wenhui Zhu, Xiwen Chen, Peijie Qiu, Xuanzhao Dong, Yalin Wang","In recent years, significant progress has been made in the medical image analysis domain using convolutional neural networks (CNNs). In particular, deep neural networks based on a U-shaped architecture (UNet) with skip connections have been adopted for several medical imaging tasks, including organ segmentation. Despite their great success, CNNs are not good at learning global or semantic features. Especially ones that require human-like reasoning to understand the context. Many UNet architectures attempted to adjust with the introduction of Transformer-based self-attention mechanisms, and notable gains in performance have been noted. However, the transformers are inherently flawed with redundancy to learn at shallow layers, which often leads to an increase in the computation of attention from the nearby pixels offering limited information. The recently introduced Super Token Attention (STA) mechanism adapts the concept of superpixels from pixel space to token space, using super tokens as compact visual representations. This approach tackles the redundancy by learning efficient global representations in vision transformers, especially for the shallow layers. In this work, we introduce the STA module in the UNet architecture (STA-UNet), to limit redundancy without losing rich information. Experimental results on four publicly available datasets demonstrate the superiority of STA-UNet over existing state-of-the-art architectures in terms of Dice score and IOU for organ segmentation tasks. The code is available at \url{https://github.com/Retinal-Research/STA-UNet}.",2024-10-13T07:19:46Z,2024-10-13T07:19:46Z,http://arxiv.org/abs/2410.11578v1,http://arxiv.org/pdf/2410.11578v1,"eess.IV, cs.AI, cs.CV"
MLLA-UNet: Mamba-like Linear Attention in an Efficient U-Shape Model for   Medical Image Segmentation,"Yufeng Jiang, Zongxi Li, Xiangyan Chen, Haoran Xie, Jing Cai","Recent advancements in medical imaging have resulted in more complex and diverse images, with challenges such as high anatomical variability, blurred tissue boundaries, low organ contrast, and noise. Traditional segmentation methods struggle to address these challenges, making deep learning approaches, particularly U-shaped architectures, increasingly prominent. However, the quadratic complexity of standard self-attention makes Transformers computationally prohibitive for high-resolution images. To address these challenges, we propose MLLA-UNet (Mamba-Like Linear Attention UNet), a novel architecture that achieves linear computational complexity while maintaining high segmentation accuracy through its innovative combination of linear attention and Mamba-inspired adaptive mechanisms, complemented by an efficient symmetric sampling structure for enhanced feature processing. Our architecture effectively preserves essential spatial features while capturing long-range dependencies at reduced computational complexity. Additionally, we introduce a novel sampling strategy for multi-scale feature fusion. Experiments demonstrate that MLLA-UNet achieves state-of-the-art performance on six challenging datasets with 24 different segmentation tasks, including but not limited to FLARE22, AMOS CT, and ACDC, with an average DSC of 88.32%. These results underscore the superiority of MLLA-UNet over existing methods. Our contributions include the novel 2D segmentation architecture and its empirical validation. The code is available via https://github.com/csyfjiang/MLLA-UNet.",2024-10-31T08:54:23Z,2024-10-31T08:54:23Z,http://arxiv.org/abs/2410.23738v1,http://arxiv.org/pdf/2410.23738v1,"eess.IV, cs.CV"
"Bio-xLSTM: Generative modeling, representation and in-context learning   of biological and chemical sequences","Niklas Schmidinger, Lisa Schneckenreiter, Philipp Seidl, Johannes Schimunek, Pieter-Jan Hoedt, Johannes Brandstetter, Andreas Mayr, Sohvi Luukkonen, Sepp Hochreiter, Günter Klambauer","Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their quadratic runtime dependency on the sequence length complicates their use for long genomic sequences and in-context learning on proteins and chemical sequences. Recently, the recurrent xLSTM architecture has been shown to perform favorably compared to Transformers and modern state-space model (SSM) architectures in the natural language domain. Similar to SSMs, xLSTMs have a linear runtime dependency on the sequence length and allow for constant-memory decoding at inference time, which makes them prime candidates for modeling long-range dependencies in biological and chemical sequences. In this work, we tailor xLSTM towards these domains and propose a suite of architectural variants called Bio-xLSTM. Extensive experiments in three large domains, genomics, proteins, and chemistry, were performed to assess xLSTM's ability to model biological and chemical sequences. The results show that models based on Bio-xLSTM a) can serve as proficient generative models for DNA, protein, and chemical sequences, b) learn rich representations for those modalities, and c) can perform in-context learning for proteins and small molecules.",2024-11-06T18:36:48Z,2024-11-06T18:36:48Z,http://arxiv.org/abs/2411.04165v1,http://arxiv.org/pdf/2411.04165v1,"q-bio.BM, cs.AI, cs.LG"
Architectural Exploration of Application-Specific Resonant SRAM   Compute-in-Memory (rCiM),"Dhandeep Challagundla, Ignatius Bezzam, Riadul Islam","While general-purpose computing follows Von Neumann's architecture, the data movement between memory and processor elements dictates the processor's performance. The evolving compute-in-memory (CiM) paradigm tackles this issue by facilitating simultaneous processing and storage within static random-access memory (SRAM) elements. Numerous design decisions taken at different levels of hierarchy affect the figure of merits (FoMs) of SRAM, such as power, performance, area, and yield. The absence of a rapid assessment mechanism for the impact of changes at different hierarchy levels on global FoMs poses a challenge to accurately evaluating innovative SRAM designs. This paper presents an automation tool designed to optimize the energy and latency of SRAM designs incorporating diverse implementation strategies for executing logic operations within the SRAM. The tool structure allows easy comparison across different array topologies and various design strategies to result in energy-efficient implementations. Our study involves a comprehensive comparison of over 6900+ distinct design implementation strategies for EPFL combinational benchmark circuits on the energy-recycling resonant compute-in-memory (rCiM) architecture designed using TSMC 28 nm technology. When provided with a combinational circuit, the tool aims to generate an energy-efficient implementation strategy tailored to the specified input memory and latency constraints. The tool reduces 80.9% of energy consumption on average across all benchmarks while using the six-topology implementation compared to baseline implementation of single-macro topology by considering the parallel processing capability of rCiM cache size ranging from 4KB to 192KB.",2024-11-14T16:01:05Z,2024-11-14T16:01:05Z,http://arxiv.org/abs/2411.09546v1,http://arxiv.org/pdf/2411.09546v1,"cs.AR, cs.CY, cs.ET, cs.SY, eess.SY"
SimPhony: A Device-Circuit-Architecture Cross-Layer Modeling and   Simulation Framework for Heterogeneous Electronic-Photonic AI System,"Ziang Yin, Meng Zhang, Amir Begovic, Rena Huang, Jeff Zhang, Jiaqi Gu","Electronic-photonic integrated circuits (EPICs) offer transformative potential for next-generation high-performance AI but require interdisciplinary advances across devices, circuits, architecture, and design automation. The complexity of hybrid systems makes it challenging even for domain experts to understand distinct behaviors and interactions across design stack. The lack of a flexible, accurate, fast, and easy-to-use EPIC AI system simulation framework significantly limits the exploration of hardware innovations and system evaluations on common benchmarks. To address this gap, we propose SimPhony, a cross-layer modeling and simulation framework for heterogeneous electronic-photonic AI systems. SimPhony offers a platform that enables (1) generic, extensible hardware topology representation that supports heterogeneous multi-core architectures with diverse photonic tensor core designs; (2) optics-specific dataflow modeling with unique multi-dimensional parallelism and reuse beyond spatial/temporal dimensions; (3) data-aware energy modeling with realistic device responses, layout-aware area estimation, link budget analysis, and bandwidth-adaptive memory modeling; and (4) seamless integration with model training framework for hardware/software co-simulation. By providing a unified, versatile, and high-fidelity simulation platform, SimPhony enables researchers to innovate and evaluate EPIC AI hardware across multiple domains, facilitating the next leap in emerging AI hardware. We open-source our codes at https://github.com/ScopeX-ASU/SimPhony",2024-11-20T21:21:54Z,2024-11-20T21:21:54Z,http://arxiv.org/abs/2411.13715v1,http://arxiv.org/pdf/2411.13715v1,"physics.optics, cs.AI, cs.AR, cs.ET, cs.LG"
Conditional t-independent spectral gap for random quantum circuits and   implications for t-design depths,"James Allen, Daniel Belkin, Bryan K. Clark","A fundamental question is understanding the rate at which random quantum circuits converge to the Haar measure. One quantity which is important in establishing this rate is the spectral gap of a random quantum ensemble. In this work we establish a new bound on the spectral gap of the t-th moment of a one-dimensional brickwork architecture on N qudits. This bound is independent of both t and N, provided t does not exceed the qudit dimension q. We also show that the bound is nearly optimal. The improved spectral gaps gives large improvements to the constant factors in known results on the approximate t-design depths of the 1D brickwork, of generic circuit architectures, and of specially-constructed architectures which scramble in depth O(log N). We moreover show that the spectral gap gives the dominant epsilon-dependence of the t-design depth at small epsilon. Our spectral gap bound is obtained by bounding the N-site 1D brickwork architecture by the spectra of 3-site operators. We then exploit a block-triangular hierarchy and a global symmetry in these operators in order to efficiently bound them. The technical methods used are a qualitatively different approach for bounding spectral gaps and and have little in common with previous techniques.",2024-11-20T22:46:10Z,2025-02-03T20:46:07Z,http://arxiv.org/abs/2411.13739v2,http://arxiv.org/pdf/2411.13739v2,"quant-ph, physics.comp-ph"
Loss Terms and Operator Forms of Koopman Autoencoders,"Dustin Enyeart, Guang Lin","Koopman autoencoders are a prevalent architecture in operator learning. But, the loss functions and the form of the operator vary significantly in the literature. This paper presents a fair and systemic study of these options. Furthermore, it introduces novel loss terms.",2024-12-05T19:48:13Z,2024-12-05T19:48:13Z,http://arxiv.org/abs/2412.04578v1,http://arxiv.org/pdf/2412.04578v1,"cs.LG, physics.comp-ph"
CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate   Segmentation of COVID-19 Lung Infections from CT Images,"Yijie Dang, Weijun Ma, Xiaohu Luo","Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: https://github.com/AmanoTooko-jie/CAD-Unet.",2024-12-09T09:08:31Z,2024-12-09T09:08:31Z,http://arxiv.org/abs/2412.06314v1,http://arxiv.org/pdf/2412.06314v1,"eess.IV, cs.AI, cs.CV"
UNet--: Memory-Efficient and Feature-Enhanced Network Architecture based   on U-Net with Reduced Skip-Connections,"Lingxiao Yin, Wei Tao, Dongyue Zhao, Tadayuki Ito, Kinya Osa, Masami Kato, Tse-Wei Chen","U-Net models with encoder, decoder, and skip-connections components have demonstrated effectiveness in a variety of vision tasks. The skip-connections transmit fine-grained information from the encoder to the decoder. It is necessary to maintain the feature maps used by the skip-connections in memory before the decoding stage. Therefore, they are not friendly to devices with limited resource. In this paper, we propose a universal method and architecture to reduce the memory consumption and meanwhile generate enhanced feature maps to improve network performance. To this end, we design a simple but effective Multi-Scale Information Aggregation Module (MSIAM) in the encoder and an Information Enhancement Module (IEM) in the decoder. The MSIAM aggregates multi-scale feature maps into single-scale with less memory. After that, the aggregated feature maps can be expanded and enhanced to multi-scale feature maps by the IEM. By applying the proposed method on NAFNet, a SOTA model in the field of image restoration, we design a memory-efficient and feature-enhanced network architecture, UNet--. The memory demand by the skip-connections in the UNet-- is reduced by 93.3%, while the performance is improved compared to NAFNet. Furthermore, we show that our proposed method can be generalized to multiple visual tasks, with consistent improvements in both memory consumption and network accuracy compared to the existing efficient architectures.",2024-12-24T08:38:34Z,2024-12-24T08:38:34Z,http://arxiv.org/abs/2412.18276v1,http://arxiv.org/pdf/2412.18276v1,"cs.CV, eess.IV"
Secure IAM on AWS with Multi-Account Strategy,Sungchan Yi,"Many recent IT companies use cloud services for deploying their products, mainly because of their convenience. As such, cloud assets have become a new attack surface, and the concept of cloud security has emerged. However, cloud security is not emphasized enough compared to on-premise security, resulting in many insecure cloud architectures. In particular, small organizations often don't have enough human resources to design a secure architecture, leaving them vulnerable to cloud security breaches.   We suggest the multi-account strategy for securing the cloud architecture. This strategy cost-effectively improves security by separating assets and reducing management overheads on the cloud infrastructure. When implemented, it automatically provides access restriction within the boundary of an account and eliminates redundancies in policy management. Since access control is a critical objective for constructing secure architectures, this practical method successfully enhances security even in small companies.   In this paper, we analyze the benefits of multi-accounts compared to single accounts and explain how to deploy multiple accounts effortlessly using the services provided by AWS. Then, we present possible design choices for multi-account structures with a concrete example. Finally, we illustrate two techniques for operational excellence on multi-account structures. We take an incremental approach to secure policy management with the principle of least privilege and introduce methods for auditing multiple accounts.",2025-01-04T05:42:27Z,2025-01-04T05:42:27Z,http://arxiv.org/abs/2501.02203v1,http://arxiv.org/pdf/2501.02203v1,"cs.CR, D.4.6"
Towards Federated Multi-Armed Bandit Learning for Content Dissemination   using Swarm of UAVs,"Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas","This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.",2025-01-15T20:55:13Z,2025-01-15T20:55:13Z,http://arxiv.org/abs/2501.09146v1,http://arxiv.org/pdf/2501.09146v1,"cs.LG, cs.NI, I.2.11"
Data mining the functional architecture of the brain's circuitry,Adam S. Charles,"The brain is a highly complex organ consisting of a myriad of subsystems that flexibly interact and adapt over time and context to enable perception, cognition, and behavior. Understanding the multi-scale nature of the brain, i.e., how circuit- and moleclular-level interactions build up the fundamental components of brain function, holds incredible potential for developing interventions for neurodegenerative and psychiatric diseases, as well as open new understanding into our very nature. Historically technological limitations have forced systems neuroscience to be local in anatomy (localized, small neural populations in single brain areas), in behavior (studying single tasks), in time (focusing on specific stages of learning or development), and in modality (focusing on imaging single biological quantities). New developments in neural recording technology and behavioral monitoring now provide the data needed to break free of local neuroscience to global neuroscience: i.e., understanding how the brain's many subsystem interact, adapt, and change across the multitude of behaviors animals and humans must perform to thrive. Specifically, while we have much knowledge of the anatomical architecture of the brain (i.e., the hardware), we finally are approaching the data needed to find the functional architecture and discover the fundamental properties of the software that runs on the hardware. We must take this opportunity to bridge between the vast amounts of data to discover this functional architecture which will face numerous challenges from low-level data alignment up to high level questions of interpretable mathematical models of behavior that can synthesize the myriad of datasets together.",2025-01-16T17:37:09Z,2025-01-16T17:37:09Z,http://arxiv.org/abs/2501.09684v1,http://arxiv.org/pdf/2501.09684v1,"q-bio.NC, stat.AP"
Test-time regression: a unifying framework for designing sequence models   with associative memory,"Ke Alexander Wang, Jiaxin Shi, Emily B. Fox","Sequences provide a remarkably general way to represent and process information. This powerful abstraction has placed sequence modeling at the center of modern deep learning applications, inspiring numerous architectures from transformers to recurrent networks. While this fragmented development has yielded powerful models, it has left us without a unified framework to understand their fundamental similarities and explain their effectiveness. We present a unifying framework motivated by an empirical observation: effective sequence models must be able to perform associative recall. Our key insight is that memorizing input tokens through an associative memory is equivalent to performing regression at test-time. This regression-memory correspondence provides a framework for deriving sequence models that can perform associative recall, offering a systematic lens to understand seemingly ad-hoc architectural choices. We show numerous recent architectures -- including linear attention models, their gated variants, state-space models, online learners, and softmax attention -- emerge naturally as specific approaches to test-time regression. Each architecture corresponds to three design choices: the relative importance of each association, the regressor function class, and the optimization algorithm. This connection leads to new understanding: we provide theoretical justification for QKNorm in softmax attention, and we motivate higher-order generalizations of softmax attention. Beyond unification, our work unlocks decades of rich statistical tools that can guide future development of more powerful yet principled sequence models.",2025-01-21T18:32:31Z,2025-01-21T18:32:31Z,http://arxiv.org/abs/2501.12352v1,http://arxiv.org/pdf/2501.12352v1,"cs.LG, cs.AI, cs.NE, stat.ML"
UAV-Assisted MEC Architecture for Collaborative Task Offloading in Urban   IoT Environment,"Subhrajit Barick, Chetna Singhal","Mobile edge computing (MEC) is a promising technology to meet the increasing demands and computing limitations of complex Internet of Things (IoT) devices. However, implementing MEC in urban environments can be challenging due to factors like high device density, complex infrastructure, and limited network coverage. Network congestion and connectivity issues can adversely affect user satisfaction. Hence, in this article, we use unmanned aerial vehicle (UAV)-assisted collaborative MEC architecture to facilitate task offloading of IoT devices in urban environments. We utilize the combined capabilities of UAVs and ground edge servers (ESs) to maximize user satisfaction and thereby also maximize the service provider's (SP) profit. We design IoT task-offloading as joint IoT-UAV-ES association and UAV-network topology optimization problem. Due to NP-hard nature, we break the problem into two subproblems: offload strategy optimization and UAV topology optimization. We develop a Three-sided Matching with Size and Cyclic preference (TMSC) based task offloading algorithm to find stable association between IoTs, UAVs, and ESs to achieve system objective. We also propose a K-means based iterative algorithm to decide the minimum number of UAVs and their positions to provide offloading services to maximum IoTs in the system. Finally, we demonstrate the efficacy of the proposed task offloading scheme over benchmark schemes through simulation-based evaluation. The proposed scheme outperforms by 19%, 12%, and 25% on average in terms of percentage of served IoTs, average user satisfaction, and SP profit, respectively, with 25% lesser UAVs, making it an effective solution to support IoT task requirements in urban environments using UAV-assisted MEC architecture.",2025-01-25T10:14:03Z,2025-01-31T18:58:18Z,http://arxiv.org/abs/2501.15164v2,http://arxiv.org/pdf/2501.15164v2,"cs.NI, cs.SY, eess.SP, eess.SY"
System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems,"Jasper H. Bussemaker, Paul Saves, Nathalie Bartoli, Thierry Lefebvre, Rémi Lafage","Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt.",2025-02-02T16:19:20Z,2025-02-02T16:19:20Z,http://arxiv.org/abs/2502.00838v1,http://arxiv.org/pdf/2502.00838v1,"math.OC, cs.DM, stat.AP"
Generative Autoregressive Transformers for Model-Agnostic Federated MRI   Reconstruction,"Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, Tolga Çukur","Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL builds a global model by aggregating locally trained model weights, inherently constraining all sites to a homogeneous model architecture. This rigid homogeneity requirement forces sites to forgo architectures tailored to their compute infrastructure and application-specific demands. Consequently, existing FL methods for MRI reconstruction fail to support model-heterogeneous settings, where individual sites are allowed to use distinct architectures. To overcome this fundamental limitation, here we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that captures the distribution of multi-site MR images. For enhanced fidelity, we propose a novel site-prompted GAT prior that controllably synthesizes MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its site-specific reconstruction model -- using its preferred architecture -- on a hybrid dataset comprising the local MRI dataset and GAT-generated synthetic MRI datasets for other sites. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT supports flexible collaborations while enjoying superior within-site and across-site reconstruction performance compared to state-of-the-art FL baselines.",2025-02-06T21:45:16Z,2025-02-06T21:45:16Z,http://arxiv.org/abs/2502.04521v1,http://arxiv.org/pdf/2502.04521v1,"eess.IV, cs.CV"
Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach   Integrating Convolutional and Attention Mechanisms with Frequency Domain   Features,"Kafi Anan, Anindya Bhattacharjee, Ashir Intesher, Kaidul Islam, Abrar Assaeem Fuad, Utsab Saha, Hafiz Imtiaz","Effective deepfake detection tools are becoming increasingly essential over the last few years due to the growing usage of deepfakes in unethical practices. There exists a diverse range of deepfake generation techniques, which makes it challenging to develop an accurate universal detection mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a diverse dataset of deepfake images, which are generated from multiple deepfake image generators, for training machine learning model(s) to emphasize the generalization of deepfake detection. To this end, we proposed an ensemble-based approach that employs three different neural network architectures: a ResNet-34-based architecture, a data-efficient image transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both local and global features of deepfakes. We visualize the specific regions that these models focus for classification using Grad-CAM, and empirically demonstrate the effectiveness of these models in grouping real and fake images into cohesive clusters using t-SNE plots. Individually, the ResNet-34 architecture has achieved 88.9% accuracy, whereas the Xception network and the DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With these networks, our weighted ensemble model achieves an excellent accuracy of 93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the confusion matrix and an Area Under the ROC curve of 97.44% further confirm the stability of our proposed method.",2025-02-15T06:02:11Z,2025-02-15T06:02:11Z,http://arxiv.org/abs/2502.10682v1,http://arxiv.org/pdf/2502.10682v1,"cs.CV, cs.LG, eess.IV"
Hybrid language processing in the Spoken Language Translator,"Manny Rayner, David Carter","The paper presents an overview of the Spoken Language Translator (SLT) system's hybrid language-processing architecture, focussing on the way in which rule-based and statistical methods are combined to achieve robust and efficient performance within a linguistically motivated framework. In general, we argue that rules are desirable in order to encode domain-independent linguistic constraints and achieve high-quality grammatical output, while corpus-derived statistics are needed if systems are to be efficient and robust; further, that hybrid architectures are superior from the point of view of portability to architectures which only make use of one type of information. We address the topics of ``multi-engine'' strategies for robust translation; robust bottom-up parsing using pruning and grammar specialization; rational development of linguistic rule-sets using balanced domain corpora; and efficient supervised training by interactive disambiguation. All work described is fully implemented in the current version of the SLT-2 system.",1997-01-02T17:25:54Z,1997-01-02T17:25:54Z,http://arxiv.org/abs/cmp-lg/9701002v1,http://arxiv.org/pdf/cmp-lg/9701002v1,"cmp-lg, cs.CL"
The Computing Research Repository: Promoting the Rapid Dissemination and   Archiving of Computer Science Research,"Joseph Y. Halpern, Carl Lagoze","We describe the Computing Research Repository (CoRR), a new electronic archive for rapid dissemination and archiving of computer science research results. CoRR was initiated in September 1998 through the cooperation of ACM, LANL (Los Alamos National Laboratory) e-Print archive, and NCSTRL (Networked Computer Science Technical Research Library. Through its implementation of the Dienst protocol, CoRR combines the open and extensible architecture of NCSTRL with the reliable access and well-established management practices of the LANL XXX e-Print repository. This architecture will allow integration with other e-Print archives and provides a foundation for a future broad-based scholarly digital library. We describe the decisions that were made in creating CoRR, the architecture of the CoRR/NCSTRL interoperation, and issues that have arisen during the operation of CoRR.",1998-12-22T11:49:29Z,1998-12-22T11:49:29Z,http://arxiv.org/abs/cs/9812020v1,http://arxiv.org/pdf/cs/9812020v1,"cs.DL, H.3.7"
DRAFT : Task System and Item Architecture (TSIA),Burkhard D. Burow,"During its execution, a task is independent of all other tasks. For an application which executes in terms of tasks, the application definition can be free of the details of the execution. Many projects have demonstrated that a task system (TS) can provide such an application with a parallel, distributed, heterogeneous, adaptive, dynamic, real-time, interactive, reliable, secure or other execution. A task consists of items and thus the application is defined in terms of items. An item architecture (IA) can support arrays, routines and other structures of items, thus allowing for a structured application definition. Taking properties from many projects, the support can extend through to currying, application defined types, conditional items, streams and other definition elements. A task system and item architecture (TSIA) thus promises unprecedented levels of support for application execution and definition.",1999-05-05T01:43:13Z,1999-05-05T01:43:13Z,http://arxiv.org/abs/cs/9905002v1,http://arxiv.org/pdf/cs/9905002v1,"cs.PL, cs.DC, cs.OS, A.1;D.1.1;D.1.3;D.1.4;D.2.3;D.2.11;D.3.2;D.3.3;D.3.4;D.4.1;D.4.5;
  D.4.7;E.1;F.1.2;F.3.3"
Integrating E-Commerce and Data Mining: Architecture and Challenges,"Suhail Ansari, Ron Kohavi, Llew Mason, Zijian Zheng","We show that the e-commerce domain can provide all the right ingredients for successful data mining and claim that it is a killer domain for data mining. We describe an integrated architecture, based on our expe-rience at Blue Martini Software, for supporting this integration. The architecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80% of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the web server) in order to support logging of data and metadata that is essential to the discovery process. We describe the data transformation bridges required from the transaction processing systems and customer event streams (e.g., clickstreams) to the data warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We con-clude with a set of challenges.",2000-07-14T00:33:12Z,2000-07-14T00:33:12Z,http://arxiv.org/abs/cs/0007026v1,http://arxiv.org/pdf/cs/0007026v1,"cs.LG, cs.AI, cs.CV, cs.DB, I.2.6;H.2.8"
System Support for Bandwidth Management and Content Adaptation in   Internet Applications,"David G. Andersen, Deepak Bansal, Dorothy Curtis, Srinivasan Seshan, Hari Balakrishnan","This paper describes the implementation and evaluation of an operating system module, the Congestion Manager (CM), which provides integrated network flow management and exports a convenient programming interface that allows applications to be notified of, and adapt to, changing network conditions. We describe the API by which applications interface with the CM, and the architectural considerations that factored into the design. To evaluate the architecture and API, we describe our implementations of TCP; a streaming layered audio/video application; and an interactive audio application using the CM, and show that they achieve adaptive behavior without incurring much end-system overhead. All flows including TCP benefit from the sharing of congestion information, and applications are able to incorporate new functionality such as congestion control and adaptive behavior.",2001-04-07T17:17:14Z,2001-04-07T17:17:14Z,http://arxiv.org/abs/cs/0104012v1,http://arxiv.org/pdf/cs/0104012v1,"cs.NI, cs.OS, D.4.4"
Architectural Framework for Large-Scale Multicast in Mobile Ad Hoc   Networks,Ahmed Helmy,"Emerging ad hoc networks are infrastructure-less networks consisting of wireless devices with various power constraints, capabilities and mobility characteristics. An essential capability in future ad hoc networks is the ability to provide scalable multicast services. This paper presents a novel adaptive architecture to support multicast services in large-scale wide-area ad hoc networks. Existing works on multicast in ad hoc networks address only small size networks. Our main design goals are scalability, robustness and efficiency. We propose a self-configuring hierarchy extending zone-based routing with the notion of contacts based on the small world graphs phenomenon and new metrics of stability and mobility. We introduce a new geographic-based multicast address allocation scheme coupled with adaptive anycast based on group popularity. Our scheme is the first of its kind and promises efficient and robust operation in the common case. Also, based on the new concept of rendezvous regions, we provide a bootstrap mechanism for the multicast service; a challenge generally ignored in previous work.",2001-09-05T04:31:14Z,2001-09-05T04:31:14Z,http://arxiv.org/abs/cs/0109005v1,http://arxiv.org/pdf/cs/0109005v1,"cs.NI, C.2; C.2.1"
Web Services for the Virtual Observatory,"Alexander S. Szalay, Tamas Budavari, Tanu Malika, Jim Gray, Ani Thakara","Web Services form a new, emerging paradigm to handle distributed access to resources over the Internet. There are platform independent standards (SOAP, WSDL), which make the developers? task considerably easier. This article discusses how web services could be used in the context of the Virtual Observatory. We envisage a multi-layer architecture, with interoperating services. A well-designed lower layer consisting of simple, standard services implemented by most data providers will go a long way towards establishing a modular architecture. More complex applications can be built upon this core layer. We present two prototype applications, the SdssCutout and the SkyQuery as examples of this layered architecture.",2002-08-07T22:58:37Z,2002-08-07T22:58:37Z,http://arxiv.org/abs/cs/0208014v1,http://arxiv.org/pdf/cs/0208014v1,"cs.DC, cs.DL, C.23.1;C.2.4;D.2.12;D.4.7;H.2;H.3;H.4;J.2;J.3"
Controlled hierarchical filtering: Model of neocortical sensory   processing,Andras Lorincz,"A model of sensory information processing is presented. The model assumes that learning of internal (hidden) generative models, which can predict the future and evaluate the precision of that prediction, is of central importance for information extraction. Furthermore, the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop. This generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop. Implicit memory phenomena; priming and prototype learning are emerging features of the model. Mathematical theorems ensure stability and attractive learning properties of the architecture. Connections to reinforcement learning are also established: both the control network, and the network with a hidden model converge to (near) optimal policy under suitable conditions. Falsifying predictions, including the role of the feedback connections between neocortical areas are made.",2003-08-16T07:31:57Z,2003-08-16T07:31:57Z,http://arxiv.org/abs/cs/0308025v1,http://arxiv.org/pdf/cs/0308025v1,"cs.NE, cs.AI, cs.LG, q-bio.NC, C.1.3; F.1.1.; I.2.0; I.2.6; I.2.10; I.4.3.; I.4.10; I.5.1"
Intelligent Systems: Architectures and Perspectives,Ajith Abraham,"The integration of different learning and adaptation techniques to overcome individual limitations and to achieve synergetic effects through the hybridization or fusion of these techniques has, in recent years, contributed to a large number of new intelligent system designs. Computational intelligence is an innovative framework for constructing intelligent hybrid architectures involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning (PR) and derivative free optimization techniques such as Evolutionary Computation (EC). Most of these hybridization approaches, however, follow an ad hoc design methodology, justified by success in certain application domains. Due to the lack of a common framework it often remains difficult to compare the various hybrid systems conceptually and to evaluate their performance comparatively. This chapter introduces the different generic architectures for integrating intelligent systems. The designing aspects and perspectives of different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC systems are presented. Some conclusions are also provided towards the end.",2004-05-04T23:48:39Z,2004-05-04T23:48:39Z,http://arxiv.org/abs/cs/0405009v1,http://arxiv.org/pdf/cs/0405009v1,"cs.AI, I.2.0"
Providing Service Guarantees in High-Speed Switching Systems with   Feedback Output Queuing,"Victor Firoiu, Xiaohui Zhang, Emre Gunduzhan, Nicolas Christin","We consider the problem of providing service guarantees in a high-speed packet switch. As basic requirements, the switch should be scalable to high speeds per port, a large number of ports and a large number of traffic flows with independent guarantees. Existing scalable solutions are based on Virtual Output Queuing, which is computationally complex when required to provide service guarantees for a large number of flows.   We present a novel architecture for packet switching that provides support for such service guarantees. A cost-effective fabric with small external speedup is combined with a feedback mechanism that enables the fabric to be virtually lossless, thus avoiding packet drops indiscriminate of flows. Through analysis and simulation, we show that this architecture provides accurate support for service guarantees, has low computational complexity and is scalable to very high port speeds.",2004-06-12T03:27:27Z,2004-06-12T03:27:27Z,http://arxiv.org/abs/cs/0406019v1,http://arxiv.org/pdf/cs/0406019v1,"cs.NI, C.2.1; C.2.6"
Programmable Ethernet Switches and Their Applications,"Srikant Sharma, Tzi-cker Chiueh","Modern Ethernet switches support many advanced features beyond route learning and packet forwarding such as VLAN tagging, IGMP snooping, rate limiting, and status monitoring, which can be controlled through a programmatic interface. Traditionally, these features are mostly used to statically configure a network. This paper proposes to apply them as dynamic control mechanisms to maximize physical network link resources, to minimize failure recovery time, to enforce QoS requirements, and to support link-layer multicast without broadcasting. With these advanced programmable control mechanisms, standard Ethernet switches can be used as effective building blocks for metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and computation cluster interconnects. We demonstrate the usefulness of this new level of control over Ethernet switches with a MEN architecture that features multi-fold throughput gains and sub-second failure recovery time.",2004-11-08T20:06:09Z,2004-11-08T20:06:09Z,http://arxiv.org/abs/cs/0411019v1,http://arxiv.org/pdf/cs/0411019v1,"cs.NI, cs.AR, cs.PF, C.2.5"
Using Wireless Sensor Networks to Narrow the Gap between Low-Level   Information and Context-Awareness,"Ioan Raicu, Owen Richter, Loren Schwiebert, Sherali Zeadally","Wireless sensor networks are finally becoming a reality. In this paper, we present a scalable architecture for using wireless sensor networks in combination with wireless Ethernet networks to provide a complete end-to-end solution to narrow the gap between the low-level information and context awareness. We developed and implemented a complete proximity detector in order to give a wearable computer, such as a PDA, location context. Since location is only one element of contextawareness, we pursued utilizing photo sensors and temperature sensors in learning as much as possible about the environment. We used the TinyOS RF Motes as our test bed WSN (Wireless Sensor Network), 802.11 compatible hardware as our wireless Ethernet network, and conventional PCs and wired 802.3 networks to build the upper levels of the architecture.",2004-11-12T22:52:33Z,2004-11-12T22:52:33Z,http://arxiv.org/abs/cs/0411039v1,http://arxiv.org/pdf/cs/0411039v1,"cs.NI, C.2.1"
"K-core decomposition of Internet graphs: hierarchies, self-similarity   and measurement biases","José Ignacio Alvarez-Hamelin, Luca Dall'Asta, Alain Barrat, Alessandro Vespignani",We consider the $k$-core decomposition of network models and Internet graphs at the autonomous system (AS) level. The $k$-core analysis allows to characterize networks beyond the degree distribution and uncover structural properties and hierarchies due to the specific architecture of the system. We compare the $k$-core structure obtained for AS graphs with those of several network models and discuss the differences and similarities with the real Internet architecture. The presence of biases and the incompleteness of the real maps are discussed and their effect on the $k$-core analysis is assessed with numerical experiments simulating biased exploration on a wide range of network models. We find that the $k$-core analysis provides an interesting characterization of the fluctuations and incompleteness of maps as well as information helping to discriminate the original underlying structure.,2005-11-02T08:06:51Z,2008-04-16T06:10:41Z,http://arxiv.org/abs/cs/0511007v4,http://arxiv.org/pdf/cs/0511007v4,"cs.NI, cond-mat.stat-mech"
An Architecture for the Aggregation and Analysis of Scholarly Usage Data,"Johan Bollen, Herbert Van de Sompel","Although recording of usage data is common in scholarly information services, its exploitation for the creation of value-added services remains limited due to concerns regarding, among others, user privacy, data validity, and the lack of accepted standards for the representation, sharing and aggregation of usage data. This paper presents a technical, standards-based architecture for sharing usage information, which we have designed and implemented. In this architecture, OpenURL-compliant linking servers aggregate usage information of a specific user community as it navigates the distributed information environment that it has access to. This usage information is made OAI-PMH harvestable so that usage information exposed by many linking servers can be aggregated to facilitate the creation of value-added services with a reach beyond that of a single community or a single information service. This paper also discusses issues that were encountered when implementing the proposed approach, and it presents preliminary results obtained from analyzing a usage data set containing about 3,500,000 requests aggregated by a federation of linking servers at the California State University system over a 20 month period.",2006-05-24T18:06:49Z,2006-05-24T18:06:49Z,http://arxiv.org/abs/cs/0605113v1,http://arxiv.org/pdf/cs/0605113v1,"cs.DL, H.3.7"
Application of a design space exploration tool to enhance interleaver   generation,"Cyrille Chavet, Philippe Coussy, Pascal Urard, Eric Martin","This paper presents a methodology to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall performance of the system is significantly affected by communication architectures, as a consequence the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). Design space exploration is then performed through associated tools, to synthesize a STAR component under time-to-market constraints. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.",2007-06-20T15:19:01Z,2007-06-20T15:19:01Z,http://arxiv.org/abs/0706.3009v1,http://arxiv.org/pdf/0706.3009v1,"cs.AR, cs.IT, math.IT"
Detailed Network Measurements Using Sparse Graph Counters: The Theory,"Yi Lu, Andrea Montanari, Balaji Prabhakar","Measuring network flow sizes is important for tasks like accounting/billing, network forensics and security. Per-flow accounting is considered hard because it requires that many counters be updated at a very high speed; however, the large fast memories needed for storing the counters are prohibitively expensive. Therefore, current approaches aim to obtain approximate flow counts; that is, to detect large elephant flows and then measure their sizes.   Recently the authors and their collaborators have developed [1] a novel method for per-flow traffic measurement that is fast, highly memory efficient and accurate. At the core of this method is a novel counter architecture called ""counter braids.'' In this paper, we analyze the performance of the counter braid architecture under a Maximum Likelihood (ML) flow size estimation algorithm and show that it is optimal; that is, the number of bits needed to store the size of a flow matches the entropy lower bound. While the ML algorithm is optimal, it is too complex to implement. In [1] we have developed an easy-to-implement and efficient message passing algorithm for estimating flow sizes.",2007-10-02T22:03:28Z,2007-10-07T05:42:39Z,http://arxiv.org/abs/0710.0658v2,http://arxiv.org/pdf/0710.0658v2,"cs.NI, cs.IT, math.IT"
"NCore: Architecture and Implementation of a Flexible, Collaborative   Digital Library","Dean B. Krafft, Aaron Birkland, Ellen J. Cramer","NCore is an open source architecture and software platform for creating flexible, collaborative digital libraries. NCore was developed by the National Science Digital Library (NSDL) project, and it serves as the central technical infrastructure for NSDL. NCore consists of a central Fedora-based digital repository, a specific data model, an API, and a set of backend services and frontend tools that create a new model for collaborative, contributory digital libraries. This paper describes NCore, presents and analyzes its architecture, tools and services; and reports on the experience of NSDL in building and operating a major digital library on it over the past year and the experience of the Digital Library for Earth Systems Education in porting their existing digital library and tools to the NCore platform.",2008-03-10T21:06:31Z,2008-03-10T21:06:31Z,http://arxiv.org/abs/0803.1500v1,http://arxiv.org/pdf/0803.1500v1,"cs.DL, H.3.7"
Emergence of Spontaneous Order Through Neighborhood Formation in   Peer-to-Peer Recommender Systems,"Ernesto Diaz-Aviles, Lars Schmidt-Thieme, Cai-Nicolas Ziegler","The advent of the Semantic Web necessitates paradigm shifts away from centralized client/server architectures towards decentralization and peer-to-peer computation, making the existence of central authorities superfluous and even impossible. At the same time, recommender systems are gaining considerable impact in e-commerce, providing people with recommendations that are personalized and tailored to their very needs. These recommender systems have traditionally been deployed with stark centralized scenarios in mind, operating in closed communities detached from their host network's outer perimeter. We aim at marrying these two worlds, i.e., decentralized peer-to-peer computing and recommender systems, in one agent-based framework. Our architecture features an epidemic-style protocol maintaining neighborhoods of like-minded peers in a robust, selforganizing fashion. In order to demonstrate our architecture's ability to retain scalability, robustness and to allow for convergence towards high-quality recommendations, we conduct offline experiments on top of the popular MovieLens dataset.",2008-12-23T23:26:27Z,2008-12-23T23:26:27Z,http://arxiv.org/abs/0812.4460v1,http://arxiv.org/pdf/0812.4460v1,"cs.AI, cs.IR, cs.MA, C.2.4; H.3.3"
An artificial spiking synapse made of molecules and nanoparticles,"F. Alibart, S. Pleutin, D. Guerin, C. Gamrat, D. Vuillaume","Molecule-based devices are envisioned to complement silicon devices by providing new functions or already existing functions at a simpler process level and at a lower cost by virtue of their self-organization capabilities, moreover, they are not bound to von Neuman architecture and this may open the way to other architectural paradigms. Here we demonstrate a device made of conjugated molecules and metal nanoparticles (NPs) which behaves as a spiking synapse suitable for integration in neural network architectures. We demonstrate that this device exhibits the main behavior of a biological synapse. These results open the way to rate coding utilization of the NOMFET in perceptron and Hopfield networks. We can also envision the NOMFET as a building block of neuroelectronics for interfacing neurons or neuronal logic devices made from patterned neuronal cultures with solid-state devices and circuits.",2009-04-06T09:56:08Z,2009-04-06T09:56:08Z,http://arxiv.org/abs/0904.0884v1,http://arxiv.org/pdf/0904.0884v1,"cond-mat.mtrl-sci, cond-mat.dis-nn, q-bio.NC"
A Parallelism-Based Approach to Network Anonymization,Igor Margasinski,"Considering topologies of anonymous networks we used to organizing anonymous communication into hard to trace paths hiding its origin or destination. In anonymity the company is crucial, however the serial transportation imposes a costly tradeoff between a level of privacy and a speed of communication.   This paper introduces a framework of a novel architecture for anonymous networks that hides initiators of communications by parallelization of anonymous links. The new approach, which is based on the grounds of the anonymous P2P network called P2Priv, does not require content forwarding via a chain of proxy nodes to assure high degree of anonymity. Contrary to P2Priv, the new architecture can be suited to anonymization of various network communications, including anonymous access to distributed as well as client-server services. In particular, it can be considered as an anonymization platform for these network applications where both privacy and low delays are required.",2009-06-21T20:55:13Z,2009-06-21T20:55:13Z,http://arxiv.org/abs/0906.3895v1,http://arxiv.org/pdf/0906.3895v1,"cs.CR, cs.NI, C.2.0; C.2.1; C.2.3; C.2.4"
Green Cellular - Optimizing the Cellular Network for Minimal Emission   from Mobile Stations,"Doron Ezri, Shimi Shilo","Wireless systems, which include cellular phones, have become an essential part of the modern life. However the mounting evidence that cellular radiation might adversely affect the health of its users, leads to a growing concern among authorities and the general public. Radiating antennas in the proximity of the user, such as antennas of mobile phones are of special interest for this matter. In this paper we suggest a new architecture for wireless networks, aiming at minimal emission from mobile stations, without any additional radiation sources. The new architecture, dubbed Green Cellular, abandons the classical transceiver base station design and suggests the augmentation of transceiver base stations with receive only devices. These devices, dubbed Green Antennas, are not aiming at coverage extension but rather at minimizing the emission from mobile stations. We discuss the implications of the Green Cellular architecture on 3G and 4G cellular technologies. We conclude by showing that employing the Green Cellular approach may lead to a significant decrease in the emission from mobile stations, especially in indoor scenarios. This is achieved without exposing the user to any additional radiation source.",2009-06-29T15:03:35Z,2009-06-29T15:03:35Z,http://arxiv.org/abs/0906.5289v1,http://arxiv.org/pdf/0906.5289v1,"cs.IT, math.IT"
Robust Failure Detection Architecture for Large Scale Distributed   Systems,"Ciprian Mihai Dobre, Florin Pop, Alexandru Costan, Mugurel Ionut Andreica, Valentin Cristea","Failure detection is a fundamental building block for ensuring fault tolerance in large scale distributed systems. There are lots of approaches and implementations in failure detectors. Providing flexible failure detection in off-the-shelf distributed systems is difficult. In this paper we present an innovative solution to this problem. Our approach is based on adaptive, decentralized failure detectors, capable of working asynchronous and independent on the application flow. The proposed solution considers an architecture for the failure detectors, based on clustering, the use of a gossip-based algorithm for detection at local level and the use of a hierarchical structure among clusters of detectors along which traffic is channeled. The solution can scale to a large number of nodes, considers the QoS requirements of both applications and resources, and includes fault tolerance and system orchestration mechanisms, added in order to asses the reliability and availability of distributed systems.",2009-10-05T10:05:19Z,2009-10-05T10:05:19Z,http://arxiv.org/abs/0910.0708v1,http://arxiv.org/pdf/0910.0708v1,"cs.DC, cs.NI, C.2.4; C.4; D.4.5"
Critical Analysis of Middleware Architectures for Large Scale   Distributed Systems,"Florin Pop, Ciprian Mihai Dobre, Alexandru Costan, Mugurel Ionut Andreica, Eliana-Dina Tirsa, Corina Stratan, Valentin Cristea","Distributed computing is increasingly being viewed as the next phase of Large Scale Distributed Systems (LSDSs). However, the vision of large scale resource sharing is not yet a reality in many areas - Grid computing is an evolving area of computing, where standards and technology are still being developed to enable this new paradigm. Hence, in this paper we analyze the current development of middleware tools for LSDS, from multiple perspectives: architecture, applications and market research. For each perspective we are interested in relevant technologies used in undergoing projects, existing products or services and useful design issues. In the end, based on this approach, we draw some conclusions regarding the future research directions in this area.",2009-10-15T17:14:55Z,2009-10-15T17:14:55Z,http://arxiv.org/abs/0910.2942v1,http://arxiv.org/pdf/0910.2942v1,"cs.DC, cs.NI, C.2.4; D.4"
Using Premia and Nsp for Constructing a Risk Management Benchmark for   Testing Parallel Architecture,"Jean-Philippe Chancelier, Jérôme Lelong, Bernard Lapeyre","Financial institutions have massive computations to carry out overnight which are very demanding in terms of the consumed CPU. The challenge is to price many different products on a cluster-like architecture. We have used the Premia software to valuate the financial derivatives. In this work, we explain how Premia can be embedded into Nsp, a scientific software like Matlab, to provide a powerful tool to valuate a whole portfolio. Finally, we have integrated an MPI toolbox into Nsp to enable to use Premia to solve a bunch of pricing problems on a cluster. This unified framework can then be used to test different parallel architectures.",2010-01-19T07:54:16Z,2012-05-21T19:13:53Z,http://arxiv.org/abs/1001.3213v2,http://arxiv.org/pdf/1001.3213v2,"cs.CE, cs.DC, cs.MS, cs.NA, q-fin.CP, q-fin.PR"
Computing Networks: A General Framework to Contrast Neural and Swarm   Cognitions,Carlos Gershenson,"This paper presents the Computing Networks (CNs) framework. CNs are used to generalize neural and swarm architectures. Artificial neural networks, ant colony optimization, particle swarm optimization, and realistic biological models are used as examples of instantiations of CNs. The description of these architectures as CNs allows their comparison. Their differences and similarities allow the identification of properties that enable neural and swarm architectures to perform complex computations and exhibit complex cognitive abilities. In this context, the most relevant characteristics of CNs are the existence multiple dynamical and functional scales. The relationship between multiple dynamical and functional scales with adaptation, cognition (of brains and swarms) and computation is discussed.",2010-01-28T18:55:53Z,2010-07-26T21:21:02Z,http://arxiv.org/abs/1001.5244v3,http://arxiv.org/pdf/1001.5244v3,"cs.NE, cs.AI, nlin.AO, F.1.1; I.2.0; H.1.1"
A Novel VLSI Architecture of Fixed-complexity Sphere Decoder,"Bin Wu, Guido Masera","Fixed-complexity Sphere Decoder (FSD) is a recently proposed technique for Multiple-Input Multiple-Output (MIMO) detection. It has several outstanding features such as constant throughput and large potential parallelism, which makes it suitable for efficient VLSI implementation. However, to our best knowledge, no VLSI implementation of FSD has been reported in the literature, although some FPGA prototypes of FSD with pipeline architecture have been developed. These solutions achieve very high throughput but at very high cost of hardware resources, making them impractical in real applications. In this paper, we present a novel four-nodes-per-cycle parallel architecture of FSD, with a breadth-first processing that allows for short critical path. The implementation achieves a throughput of 213.3 Mbps at 400 MHz clock frequency, at a cost of 0.18 mm2 Silicon area on 0.13{\mu}m CMOS technology. The proposed solution is much more economical compared with the existing FPGA implementations, and very suitable for practicl applications because of its balanced performance and hardware-complexity; moreover it has the flexibility to be expanded into an eight-nodes-per-cycle version in order to double the throughput.",2010-06-21T10:55:56Z,2010-06-21T10:55:56Z,http://arxiv.org/abs/1006.4030v1,http://arxiv.org/pdf/1006.4030v1,"cs.IT, math.IT"
Analysing Astronomy Algorithms for GPUs and Beyond,"Benjamin R. Barsdell, David G. Barnes, Christopher J. Fluke","Astronomy depends on ever increasing computing power. Processor clock-rates have plateaued, and increased performance is now appearing in the form of additional processor cores on a single chip. This poses significant challenges to the astronomy software community. Graphics Processing Units (GPUs), now capable of general-purpose computation, exemplify both the difficult learning-curve and the significant speedups exhibited by massively-parallel hardware architectures. We present a generalised approach to tackling this paradigm shift, based on the analysis of algorithms. We describe a small collection of foundation algorithms relevant to astronomy and explain how they may be used to ease the transition to massively-parallel computing architectures. We demonstrate the effectiveness of our approach by applying it to four well-known astronomy problems: Hogbom CLEAN, inverse ray-shooting for gravitational lensing, pulsar dedispersion and volume rendering. Algorithms with well-defined memory access patterns and high arithmetic intensity stand to receive the greatest performance boost from massively-parallel architectures, while those that involve a significant amount of decision-making may struggle to take advantage of the available processing power.",2010-07-09T20:00:21Z,2010-07-09T20:00:21Z,http://arxiv.org/abs/1007.1660v1,http://arxiv.org/pdf/1007.1660v1,"astro-ph.IM, cs.DS"
Immune System Inspired Strategies for Distributed Systems,"Soumya Banerjee, Melanie Moses","Many components of the IS are constructed as modular units which do not need to communicate with each other such that the number of components increases but the size remains constant. However, a sub-modular IS architecture in which lymph node number and size both increase sublinearly with body size is shown to efficiently balance the requirements of communication and migration, consistent with experimental data. We hypothesize that the IS architecture optimizes the tradeoff between local search for pathogens and global response using antibodies. Similar to natural immune systems, physical space and resource are also important constraints on Artificial Immune Systems (AIS), especially distributed systems applications used to connect low-powered sensors using short-range wireless communication. AIS problems like distributed robot control will also require a sub-modular architecture to efficiently balance the tradeoff between local search for a solution and global response or proliferation of the solution between different components.",2010-08-17T01:06:44Z,2010-08-17T01:06:44Z,http://arxiv.org/abs/1008.2799v1,http://arxiv.org/pdf/1008.2799v1,"cs.DC, math.OC, q-bio.CB"
Preliminary Functional-Structural Modeling on Poplar (Salicaceae),"Dongxiang Liu, Meng Zhen Kang, Véronique Letort, Meijun Xing, Yang Gang, Xinyuan Huang, Weiqun Cao","Poplar is one of the best fast-growing trees in the world, widely used for windbreak and wood product. Although architecture of poplar has direct impact on its applications, it has not been descried in previous poplar models, probably because of the difficulties raised by measurement, data processing and parameterization. In this paper, the functional-structural model GreenLab is calibrated by using poplar data of 3, 4, 5, 6 years old. The data was acquired by simplifying measurement. The architecture was also simplified by classifying the branches into several types (physiological age) using clustering analysis, which decrease the number of parameters. By multi-fitting the sampled data of each tree, the model parameters were identified and the plant architectures at different tree ages were simulated.",2010-12-15T10:57:44Z,2010-12-15T10:57:44Z,http://arxiv.org/abs/1012.3274v1,http://arxiv.org/pdf/1012.3274v1,"math.DS, q-bio.TO"
An Agent-based Architecture for a Knowledge-work Support System,Arijit Laha,"Enhancement of technology-based system support for knowledge workers is an issue of great importance. The ""Knowledge work Support System (KwSS)"" framework analyzes this issue from a holistic perspective. KwSS proposes a set of design principles for building a comprehensive IT-based support system, which enhances the capability of a human agent for performing a set of complex and interrelated knowledge-works relevant to one or more target task-types within a domain of professional activities. In this paper, we propose a high-level, software-agent based architecture for realizing a KwSS system that incorporates these design principles. Here we focus on developing a number of crucial enabling components of the architecture, including (1) an Activity Theory-based novel modeling technique for knowledgeintensive activities; (2) a graph theoretic formalism for representing these models in a knowledge base in conjunction with relevant entity taxonomies/ontologies; and (3) an algorithm for reasoning, using the knowledge base, about various aspects of possible supports for activities at performance-time.",2011-04-08T03:52:04Z,2011-04-08T03:52:04Z,http://arxiv.org/abs/1104.1477v1,http://arxiv.org/pdf/1104.1477v1,"cs.HC, cs.AI, cs.MA, H.4; I.2.4"
Space and Time as a Primary Classification Criterion for Information   Retrieval in Distributed Social Networking,"Georg Groh, Florian Straub, Andreas Donaubauer, Benjamin Koster","We discuss in a compact way how the implicit relations between spatiotemporal relatedness of information items, spatiotemporal relatedness of users, social relatedness of users and semantic relatedness of information items may be exploited for an information retrieval architecture that operates along the lines of human ways of searching. The decentralized and agent oriented architecture mirrors emerging trends such as upcoming mobile and decentralized social networking as a new paradigm in social computing and is targetted to satisfy broader and more subtly interlinked information demands beyond immediate information needs which can be readily satisfied with current IR services. We briefly discuss why using spatio-temporal references as primary information criterion implicitly conserves other relations and is thus suitable for such an architecture. We finally shortly point to results from a large evaluation study using Wikipedia articles.",2011-04-12T12:54:25Z,2011-04-12T12:54:25Z,http://arxiv.org/abs/1104.2196v1,http://arxiv.org/pdf/1104.2196v1,"cs.IR, cs.SI, physics.soc-ph"
The Value of Feedback in Decentralized Detection,Wee Peng Tay,"We consider the decentralized binary hypothesis testing problem in networks with feedback, where some or all of the sensors have access to compressed summaries of other sensors' observations. We study certain two-message feedback architectures, in which every sensor sends two messages to a fusion center, with the second message based on full or partial knowledge of the first messages of the other sensors. We also study one-message feedback architectures, in which each sensor sends one message to a fusion center, with a group of sensors having full or partial knowledge of the messages from the sensors not in that group. Under either a Neyman-Pearson or a Bayesian formulation, we show that the asymptotically optimal (in the limit of a large number of sensors) detection performance (as quantified by error exponents) does not benefit from the feedback messages, if the fusion center remembers all sensor messages. However, feedback can improve the Bayesian detection performance in the one-message feedback architecture if the fusion center has limited memory; for that case, we determine the corresponding optimal error exponents.",2011-08-31T03:33:10Z,2011-08-31T03:33:10Z,http://arxiv.org/abs/1108.6121v1,http://arxiv.org/pdf/1108.6121v1,"cs.IT, math.IT, stat.AP"
Incorporating Agile with MDA Case Study: Online Polling System,"Pritha Guha, Kinjal Shah, Shiv Shankar Prasad Shukla, Shweta Singh","Nowadays agile software development is used in greater extend but for small organizations only, whereas MDA is suitable for large organizations but yet not standardized. In this paper the pros and cons of Model Driven Architecture (MDA) and Extreme programming have been discussed. As both of them have some limitations and cannot be used in both large scale and small scale organizations a new architecture has been proposed. In this model it is tried to opt the advantages and important values to overcome the limitations of both the software development procedures. In support to the proposed architecture the implementation of it on Online Polling System has been discussed and all the phases of software development have been explained.",2011-10-31T18:10:57Z,2011-10-31T18:10:57Z,http://arxiv.org/abs/1110.6879v1,http://arxiv.org/pdf/1110.6879v1,"cs.SE, 68"
Sum-Product Networks: A New Deep Architecture,"Hoifung Poon, Pedro Domingos","The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.",2012-02-14T16:41:17Z,2012-02-14T16:41:17Z,http://arxiv.org/abs/1202.3732v1,http://arxiv.org/pdf/1202.3732v1,"cs.LG, cs.AI, stat.ML"
A model driven method for promoting reuse in SOA-solutions by managing   variability,"Boutaina Chakir, Mounia Fredj, Mahmoud Nassar","Service Oriented Architecture (SOA) is an architectural paradigm that describes how organizations, people and systems provide and use services to achieve their goals and enhance productivity. Moreover, with the evolution of SOA, the focus in software development has shifted from applications to reusable services. However, the reuse in SOA is more seen as composition of fine-grained services rather than reuse of services implementation to build new services with additional functionalities. This can have some performance repercussions. Hence, in this paper, we propose a model driven method for managing Web service's variability based on MDA (Model Driven Architecture) as a way to promote reuse. In fact, through MDA, the method enables the automation of Web service's realization regardless of the supported platforms. Moreover, we present a WSDL extension meta-model called VarWSDL which enhances Web services by variability notions.",2012-07-11T18:43:10Z,2012-07-11T18:43:10Z,http://arxiv.org/abs/1207.2742v1,http://arxiv.org/pdf/1207.2742v1,"cs.SE, D.2.1; D.2.2; D.2.9; D.2.10; D.2.13; D.3.2; D.3.3"
Ethernet Packet Processor for SoC Application,"Raja Jitendra Nayaka, R. C. Biradar","As the demand for Internet expands significantly in numbers of users, servers, IP addresses, switches and routers, the IP based network architecture must evolve and change. The design of domain specific processors that require high performance, low power and high degree of programmability is the bottleneck in many processor based applications. This paper describes the design of ethernet packet processor for system-on-chip (SoC) which performs all core packet processing functions, including segmentation and reassembly, packetization classification, route and queue management which will speedup switching/routing performance. Our design has been configured for use with multiple projects ttargeted to a commercial configurable logic device the system is designed to support 10/100/1000 links with a speed advantage. VHDL has been used to implement and simulated the required functions in FPGA.",2012-07-21T13:47:55Z,2012-07-21T13:47:55Z,http://arxiv.org/abs/1207.5138v1,http://arxiv.org/pdf/1207.5138v1,"cs.AR, cs.NI, Airccse Conferences"
A 2D Nearest-Neighbor Quantum Architecture for Factoring in   Polylogarithmic Depth,"Paul Pham, Krysta M. Svore","We contribute a 2D nearest-neighbor quantum architecture for Shor's algorithm to factor an $n$-bit number in $O(\log^2(n))$ depth. Our implementation uses parallel phase estimation, constant-depth fanout and teleportation, and constant-depth carry-save modular addition. We derive upper bounds on the circuit resources of our architecture under a new 2D nearest-neighbor model which allows a classical controller and parallel, communicating modules. We also contribute a novel constant-depth circuit for unbounded quantum unfanout in our new model. Finally, we provide a comparison to all previous nearest-neighbor factoring implementations. Our circuit results in an exponential improvement in nearest-neighbor circuit depth at the cost of a polynomial increase in circuit size and width.",2012-07-27T21:12:11Z,2013-04-22T05:26:32Z,http://arxiv.org/abs/1207.6655v2,http://arxiv.org/pdf/1207.6655v2,"quant-ph, cs.DS, cs.ET"
Composite Design Pattern for Feature Oriented Service Injection and   Composition of Web Services for Distributed Computing Systems with Service   Oriented Architecture,"Vishnuvardhan Mannava, T. Ramesh","With the advent of newly introduced programming models like Feature-Oriented Programming (FOP), we feel that it will be more flexible to include the new service invocation function into the service providing server as a Feature Module for the self-adaptive distributed systems. A composite design patterns shows a synergy that makes the composition more than just the sum of its parts which leads to ready-made software architectures. In this paper we describe the amalgamation of Visitor and Case-Based Reasoning Design Patterns to the development of the Service Invocation and Web Services Composition through SOA with the help of JWS technologies and FOP. As far as we know, there are no studies on composition of design patterns for self adaptive distributed computing domain. We have provided with the sample code developed for the application and simple UML class diagram is used to describe the architecture.",2012-08-19T13:57:10Z,2012-08-19T13:57:10Z,http://arxiv.org/abs/1208.3836v1,http://arxiv.org/pdf/1208.3836v1,"cs.SE, cs.DC, cs.PL, D.2.11; D.2.10; D.3.3"
PIRATTE: Proxy-based Immediate Revocation of ATTribute-based Encryption,"Sonia Jahid, Nikita Borisov","Access control to data in traditional enterprises is typically enforced through reference monitors. However, as more and more enterprise data is outsourced, trusting third party storage servers is getting challenging. As a result, cryptography, specifically Attribute-based encryption (ABE) is getting popular for its expressiveness. The challenge of ABE is revocation.   To address this challenge, we propose PIRATTE, an architecture that supports fine-grained access control policies and dynamic group membership. PIRATTE is built using attribute-based encryption; a key and novel feature of our architecture, however, is that it is possible to remove access from a user without issuing new keys to other users or re-encrypting existing ciphertexts. We achieve this by introducing a proxy that participates in the decryption process and enforces revocation constraints. The proxy is minimally trusted and cannot decrypt ciphertexts or provide access to previously revoked users. We describe the PIRATTE construction and provide a security analysis along with performance evaluation.We also describe an architecture for online social network that can use PIRATTE, and prototype application of PIRATTE on Facebook.",2012-08-23T23:42:22Z,2012-08-23T23:42:22Z,http://arxiv.org/abs/1208.4877v1,http://arxiv.org/pdf/1208.4877v1,"cs.CR, cs.SI, E.3; K.6.5"
Diametrical Mesh Of Tree (D2D-MoT) Architecture: A Novel Routing   Solution For NoC,"Prasun Ghosal, Sankar Karmakar","Network-on-chip (NoC) is a new aspect for designing of future System-On-Chips (SoC) where a vast number of IP cores are connected through interconnection network. The communication between the nodes occurred by routing packets rather than wires. It supports high degree of scalability, reusability and parallelism in communication. In this paper, we present a Mesh routing architecture, which is called Diametrical 2D Mesh of Tree, based on Mesh-of-Tree (MoT) routing and Diametrical 2D Mesh. It has the advantage of having small diameter as well as large bisection width and small node degree clubbed with being the fastest network in terms of speed. The routing algorithm ensures that the packets will always reach from source to sink through shortest path and is deadlock free.",2012-12-12T16:37:23Z,2012-12-12T16:37:23Z,http://arxiv.org/abs/1212.2874v1,http://arxiv.org/pdf/1212.2874v1,"cs.ET, cs.AR, 68-06, C.1.2; C.1.3; C.2.2"
Concurrent object-oriented development with behavioral design patterns,"Benjamin Morandi, Scott West, Sebastian Nanz, Hassan Gomaa","The development of concurrent applications is challenging because of the complexity of concurrent designs and the hazards of concurrent programming. Architectural modeling using the Unified Modeling Language (UML) can support the development process, but the problem of mapping the model to a concurrent implementation remains. This paper addresses this problem by defining a scheme to map concurrent UML designs to a concurrent object-oriented program. Using the COMET method for the architectural design of concurrent object-oriented systems, each component and connector is annotated with a stereotype indicating its behavioral design pattern. For each of these patterns, a reference implementation is provided using SCOOP, a concurrent object-oriented programming model. We evaluate this development process using a case study of an ATM system, obtaining a fully functional implementation based on the systematic mapping of the individual patterns. Given the strong execution guarantees of the SCOOP model, which is free of data races by construction, this development method eliminates a source of intricate concurrent programming errors.",2012-12-21T15:33:16Z,2012-12-21T15:33:16Z,http://arxiv.org/abs/1212.5491v1,http://arxiv.org/pdf/1212.5491v1,"cs.SE, D.2.11"
Coupled Neural Associative Memories,"Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi","We propose a novel architecture to design a neural associative memory that is capable of learning a large number of patterns and recalling them later in presence of noise. It is based on dividing the neurons into local clusters and parallel plains, very similar to the architecture of the visual cortex of macaque brain. The common features of our proposed architecture with those of spatially-coupled codes enable us to show that the performance of such networks in eliminating noise is drastically better than the previous approaches while maintaining the ability of learning an exponentially large number of patterns. Previous work either failed in providing good performance during the recall phase or in offering large pattern retrieval (storage) capacities. We also present computational experiments that lend additional support to the theoretical analysis.",2013-01-08T14:55:45Z,2013-08-23T14:26:16Z,http://arxiv.org/abs/1301.1555v5,http://arxiv.org/pdf/1301.1555v5,"cs.NE, cs.IT, cs.LG, math.IT"
A New Constructive Method to Optimize Neural Network Architecture and   Generalization,"Hou Muzhou, Moon Ho Lee","In this paper, after analyzing the reasons of poor generalization and overfitting in neural networks, we consider some noise data as a singular value of a continuous function - jump discontinuity point. The continuous part can be approximated with the simplest neural networks, which have good generalization performance and optimal network architecture, by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training, BP algorithm, ELM algorithm, various constructive algorithm, RBF approximation and SVM. At the same time, we will construct RBF neural networks to fit the singular value with every error in, and we prove that a function with jumping discontinuity points can be approximated by the simplest neural networks with a decay RBF neural networks in by each error, and a function with jumping discontinuity point can be constructively approximated by a decay RBF neural networks in by each error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance, reduce the overfitting phenomenon by avoid fitting the noisy data.",2013-02-02T00:43:42Z,2013-02-02T00:43:42Z,http://arxiv.org/abs/1302.0324v1,http://arxiv.org/pdf/1302.0324v1,"cs.NE, 41A99, 65D15"
An Algorithm for Training Polynomial Networks,"Roi Livni, Shai Shalev-Shwartz, Ohad Shamir","We consider deep neural networks, in which the output of each node is a quadratic function of its inputs. Similar to other deep architectures, these networks can compactly represent any function on a finite training set. The main goal of this paper is the derivation of an efficient layer-by-layer algorithm for training such networks, which we denote as the \emph{Basis Learner}. The algorithm is a universal learner in the sense that the training error is guaranteed to decrease at every iteration, and can eventually reach zero under mild conditions. We present practical implementations of this algorithm, as well as preliminary experimental results. We also compare our deep architecture to other shallow architectures for learning polynomials, in particular kernel learning.",2013-04-26T00:35:37Z,2014-02-20T13:14:59Z,http://arxiv.org/abs/1304.7045v2,http://arxiv.org/pdf/1304.7045v2,"cs.LG, cs.AI, stat.ML"
Secrets from the GPU,"Jean-Marie Chauvet, Eric Mahé","Acceleration of cryptographic applications on massively parallel computing platforms, such as Graphics Processing Units (GPUs), becomes a real challenge as their decreasing cost and mass production makes practical implementations attractive. We propose a layered trusted architecture integrating random bits generation and parallelized RSA cryptographic computations on such platforms. The GPU-resident, three-tier, MR architecture consists of a RBG, using the GPU as a deep entropy pool; a bignum modular arithmetic library using the Residue Number System; and GPU APIs for RSA key generation, encryption and decryption. Evaluation results of an experimental OpenCL implementation show a 32-40 GB/s throughput of random integers, and encryptions with up to 16,128-bit long exponents on a commercial mid-range GPUs. This suggests an ubiquitous solution for autonomous trusted architectures combining low cost and high throughput.",2013-05-16T07:48:27Z,2013-05-16T07:48:27Z,http://arxiv.org/abs/1305.3699v1,http://arxiv.org/pdf/1305.3699v1,"cs.CR, E.3"
Adaptive Controller Placement for Wireless Sensor-Actuator Networks with   Erasure Channels,"Daniel E. Quevedo, Karl H. Johansson, Anders Ahlen, Isabel Jurado","Wireless sensor-actuator networks offer flexibility for control design. One novel element which may arise in networks with multiple nodes is that the role of some nodes does not need to be fixed. In particular, there is no need to pre-allocate which nodes assume controller functions and which ones merely relay data. We present a flexible architecture for networked control using multiple nodes connected in series over analog erasure channels without acknowledgments. The control architecture proposed adapts to changes in network conditions, by allowing the role played by individual nodes to depend upon transmission outcomes. We adopt stochastic models for transmission outcomes and characterize the distribution of controller location and the covariance of system states. Simulation results illustrate that the proposed architecture has the potential to give better performance than limiting control calculations to be carried out at a fixed node.",2013-08-08T03:38:38Z,2013-08-08T03:38:38Z,http://arxiv.org/abs/1308.1744v1,http://arxiv.org/pdf/1308.1744v1,"cs.SY, math.OC, 93C41"
Remote Control of Mobile Devices in Android Platform,"Angel Gonzalez Villan, Josep Jorba","Remote control systems are a very useful element to control and monitor devices quickly and easily. This paper proposes a new architecture for remote control of Android mobile devices, analyzing the different alternatives and seeking the optimal solution in each case. Although the area of remote control, in case of mobile devices, is little explored, it may provide important advantages for testing software and hardware developments in several real devices. It can also allow an efficient management of various devices of different types for performing different tasks, related for example to security or forensic tasks.   The main idea behind the proposed architecture was the design of a system to use it as a platform which provides the services needed to perform remote control of mobile devices. As a result of this research, a proof of concept was implemented. An Android application running a group of server programs on the device, connected to the network or USB interface, depending on availability. This servers can be controlled through a small client written in Java and runnable both on desktop and web systems.",2013-10-22T09:22:04Z,2013-10-22T09:22:04Z,http://arxiv.org/abs/1310.5850v1,http://arxiv.org/pdf/1310.5850v1,"cs.HC, cs.NI, I.3.2; K.6.5; C.2.1; C.1.4; C.2.4"
Many-core applications to online track reconstruction in HEP experiments,"S. Amerio, D. Bastieri, M. Corvo, A. Gianelle, W. Ketchum, T. Liu, A. Lonardo, D. Lucchesi, S. Poprocki, R. Rivera, L. Tosoratto, P. Vicini, P. Wittich","Interest in parallel architectures applied to real time selections is growing in High Energy Physics (HEP) experiments. In this paper we describe performance measurements of Graphic Processing Units (GPUs) and Intel Many Integrated Core architecture (MIC) when applied to a typical HEP online task: the selection of events based on the trajectories of charged particles. We use as benchmark a scaled-up version of the algorithm used at CDF experiment at Tevatron for online track reconstruction - the SVT algorithm - as a realistic test-case for low-latency trigger systems using new computing architectures for LHC experiment. We examine the complexity/performance trade-off in porting existing serial algorithms to many-core devices. Measurements of both data processing and data transfer latency are shown, considering different I/O strategies to/from the parallel devices.",2013-11-02T14:37:09Z,2013-11-11T20:58:15Z,http://arxiv.org/abs/1311.0380v2,http://arxiv.org/pdf/1311.0380v2,"physics.ins-det, cs.DC"
Long Short-Term Memory Based Recurrent Neural Network Architectures for   Large Vocabulary Speech Recognition,"Haşim Sak, Andrew Senior, Françoise Beaufays","Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.",2014-02-05T19:01:51Z,2014-02-05T19:01:51Z,http://arxiv.org/abs/1402.1128v1,http://arxiv.org/pdf/1402.1128v1,"cs.NE, cs.CL, cs.LG, stat.ML"
Avoiding pathologies in very deep networks,"David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani","Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.",2014-02-24T14:27:40Z,2016-07-08T22:59:45Z,http://arxiv.org/abs/1402.5836v3,http://arxiv.org/pdf/1402.5836v3,"stat.ML, cs.LG"
Active Switching: Packet Steering Flow Annotations,"Saul St. John, Aditya Akella","Our previous experience building systems for middlebox chain composition and scaling in software-defined networks has revealed that existing mechanisms of flow annotation commonly do not survive middlebox-traversals, or suffer from extreme identifier domain limitations resulting in excessive flow table size. In this paper, we analyze the structural artifacts resulting in these challenges, and offer a framework for describing the behavior of middleboxes based on actions taken on traversing packets. We then present a novel mechanism for flow annotation that features an identifier domain significantly larger than existing techniques, that is transparent to hosts traversed, and that conserves flow-table resources by requiring only a small number of match rules and actions in most switches. We evaluate said technique, showing that it requires less per-switch state than conventional techniques. We then describe extensions allowing implementation of this architecture within a broader class of systems. Finally, we close with architectural suggestions for enabling straightforward integration of middleboxes within software-defined networks.",2014-03-27T16:23:41Z,2014-03-27T16:23:41Z,http://arxiv.org/abs/1403.7115v1,http://arxiv.org/pdf/1403.7115v1,"cs.NI, 68M10, C.2.1"
An Efficient Solution for Model Checking Abstract State Machine Using   Bogor,Saeed Doostali,"Nowadays, publish subscribe and event based architecture are frequently used for developing loosely coupled distributed systems. Hence, it is desirable to find a proper solution to specify different systems through these architectures. Abstract state machine (ASM) is a useful means to visually and formally model publish subscribe and event based architectures. However, modeling per se is not enough since the designers want to be able to verify the designed models. As the model checking is a proper approach to verify software and hardware systems. In this paper, we present an approach to verify ASM models specified in terms of AsmetaL language using Bogor. In our approach, the AsmetaL specification is automatically encoded to BIR, the input language of the Bogor.",2014-04-07T06:47:45Z,2014-04-07T06:47:45Z,http://arxiv.org/abs/1404.2155v1,http://arxiv.org/pdf/1404.2155v1,"cs.SE, F.4.3; D.2.4; D.3.1; F.1.1"
Eight-fold signal amplification of a superconducting nanowire   single-photon detector using a multiple-avalanche architecture,"Qingyuan Zhao, Adam McCaughan, Andrew Dane, Faraz Najafi, Francesco Bellei, Domenico De Fazio, Kristen Sunter, Yachin Ivry, Karl K. Berggren","Superconducting nanowire avalanche single-photon detectors (SNAPs) with n parallel nanowires are advantageous over single-nanowire detectors because their output signal amplitude scales linearly with n. However, the SNAP architecture has not been viably demonstrated for n > 4. To increase n for larger signal amplification, we designed a multi-stage, successive-avalanche architecture which used nanowires, connected via choke inductors in a binary-tree layout. We demonstrated an avalanche detector with n = 8 parallel nanowires and achieved eight-fold signal amplification, with a timing jitter of 54 ps.",2014-08-05T21:46:48Z,2014-08-05T21:46:48Z,http://arxiv.org/abs/1408.1124v1,http://arxiv.org/pdf/1408.1124v1,"physics.ins-det, physics.optics, quant-ph"
"The Influence of Architectural Styles on Security, Using the Example of   a Certification Authority",Michael Tänzer,"Often, security is considered in an advanced stage of the implementation of a system, rather than integrating it into the system design. This leads to less secure systems, as the security mechanisms are only applied as an afterthought and therefore do not integrate well with the rest of the design. Also, several statistics about discovered vulnerabilities in existing systems suggest, that most of the vulnerabilities of a system are not caused by errors in the cryptographic primitives, but in other parts of the implementation. So integrating security concerns early in the design process seems a promising approach for increasing the security of the resulting system.   This work evaluates how the choice of the architectural style affects the security of the resulting system. The evaluation is done on the example of an existing certification authority (CA). The requirements for the system are gathered and multiple designs according to different architectural styles are drafted and evaluated using a risk evaluation method. Then the evaluated designs are compared to find out whether there are significant differences.",2014-08-12T16:03:09Z,2014-08-12T16:03:09Z,http://arxiv.org/abs/1408.2758v1,http://arxiv.org/pdf/1408.2758v1,"cs.CR, cs.SE, D.2.11"
The RD53 Collaboration's SystemVerilog-UVM Simulation Framework and its   General Applicability to Design of Advanced Pixel Readout Chips,"S. Marconi, E. Conti, P. Placidi, J. Christiansen, T. Hemperek","The foreseen Phase 2 pixel upgrades at the LHC have very challenging requirements for the design of hybrid pixel readout chips. A versatile pixel simulation platform is as an essential development tool for the design, verification and optimization of both the system architecture and the pixel chip building blocks (Intellectual Properties, IPs). This work is focused on the implemented simulation and verification environment named VEPIX53, built using the SystemVerilog language and the Universal Verification Methodology (UVM) class library in the framework of the RD53 Collaboration. The environment supports pixel chips at different levels of description: its reusable components feature the generation of different classes of parameterized input hits to the pixel matrix, monitoring of pixel chip inputs and outputs, conformity checks between predicted and actual outputs and collection of statistics on system performance. The environment has been tested performing a study of shared architectures of the trigger latency buffering section of pixel chips. A fully shared architecture and a distributed one have been described at behavioral level and simulated; the resulting memory occupancy statistics and hit loss rates have subsequently been compared.",2014-08-14T09:43:44Z,2014-08-14T09:43:44Z,http://arxiv.org/abs/1408.3232v1,http://arxiv.org/pdf/1408.3232v1,"physics.ins-det, hep-ex"
HISQ inverter on Intel Xeon Phi and NVIDIA GPUs,"O. Kaczmarek, C. Schmidt, P. Steinbrecher, Swagato Mukherjee, M. Wagner","The runtime of a Lattice QCD simulation is dominated by a small kernel, which calculates the product of a vector by a sparse matrix known as the ""Dslash"" operator. Therefore, this kernel is frequently optimized for various HPC architectures. In this contribution we compare the performance of the Intel Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate gradient solver. By exposing more parallelism to the accelerator through inverting multiple vectors at the same time we obtain a performance 250 GFlop/s on both architectures. This more than doubles the performance of the inversions. We give a short overview of both architectures, discuss some details of the implementation and the effort required to obtain the achieved performance.",2014-09-04T18:13:30Z,2014-09-04T18:13:30Z,http://arxiv.org/abs/1409.1510v1,http://arxiv.org/pdf/1409.1510v1,"cs.DC, hep-lat"
Plug-and-play fault diagnosis and control-reconfiguration for a class of   nonlinear large-scale constrained systems,"Stefano Riverso, Francesca Boem, Giancarlo Ferrari-Trecate, Thomas Parisini","This paper deals with a novel Plug-and-Play (PnP) architecture for the control and monitoring of Large-Scale Systems (LSSs). The proposed approach integrates a distributed Model Predictive Control (MPC) strategy with a distributed Fault Detection (FD) architecture and methodology in a PnP framework. The basic concept is to use the FD scheme as an autonomous decision support system: once a fault is detected, the faulty subsystem can be unplugged to avoid the propagation of the fault in the interconnected LSS. Analogously, once the issue has been solved, the disconnected subsystem can be re-plugged-in. PnP design of local controllers and detectors allow these operations to be performed safely, i.e. without spoiling stability and constraint satisfaction for the whole LSS. The PnP distributed MPC is derived for a class of nonlinear LSS and an integrated PnP distributed FD architecture is proposed. Simulation results show the effectiveness and the potential of the general methodology.",2014-09-18T08:26:54Z,2014-09-18T08:26:54Z,http://arxiv.org/abs/1409.5224v1,http://arxiv.org/pdf/1409.5224v1,"cs.SY, math.OC"
Perspective: network-guided pattern formation of neural dynamics,"Marc-Thorsten Huett, Marcus Kaiser, Claus C. Hilgetag","The understanding of neural activity patterns is fundamentally linked to an understanding of how the brain's network architecture shapes dynamical processes. Established approaches rely mostly on deviations of a given network from certain classes of random graphs. Hypotheses about the supposed role of prominent topological features (for instance, the roles of modularity, network motifs, or hierarchical network organization) are derived from these deviations. An alternative strategy could be to study deviations of network architectures from regular graphs (rings, lattices) and consider the implications of such deviations for self-organized dynamic patterns on the network. Following this strategy, we draw on the theory of spatiotemporal pattern formation and propose a novel perspective for analyzing dynamics on networks, by evaluating how the self-organized dynamics are confined by network architecture to a small set of permissible collective states. In particular, we discuss the role of prominent topological features of brain connectivity, such as hubs, modules and hierarchy, in shaping activity patterns. We illustrate the notion of network-guided pattern formation with numerical simulations and outline how it can facilitate the understanding of neural dynamics.",2014-09-18T12:17:16Z,2014-09-18T12:17:16Z,http://arxiv.org/abs/1409.5280v1,http://arxiv.org/pdf/1409.5280v1,"q-bio.NC, nlin.AO, physics.soc-ph"
Entanglement Distribution in Optical Networks,"Alex Ciurana, Vicente Martin, Jesus Martinez-Mateo, Bernhard Schrenk, Momtchil Peev, Andreas Poppe","The ability to generate entangled photon-pairs over a broad wavelength range opens the door to the simultaneous distribution of entanglement to multiple users in a network by using centralized sources and flexible wavelength-division multiplexing schemes. Here we show the design of a metropolitan optical network consisting of tree-type access networks whereby entangled photon-pairs are distributed to any pair of users, independent of their location. The network is constructed employing commercial off-the-shelf components and uses the existing infrastructure, which allows for moderate deployment costs. We further develop a channel plan and a network-architecture design to provide a direct optical path between any pair of users, thus allowing classical and one-way quantum communication as well as entanglement distribution. This allows the simultaneous operation of multiple quantum information technologies. Finally, we present a more flexible backbone architecture that pushes away the load limitations of the original network design by extending its reach, number of users and capabilities.",2014-09-21T11:04:32Z,2014-09-21T11:04:32Z,http://arxiv.org/abs/1409.5965v1,http://arxiv.org/pdf/1409.5965v1,"quant-ph, cs.NI, physics.optics"
Scalable Quantum Computing Architecture with Mixed Species Ion Chains,"John Wright, Carolyn Auchter, Chen-Kuan Chou, Richard D. Graham, Thomas W. Noel, Tomasz Sakrejda, Zichao Zhou, Boris B. Blinov","We report on progress towards implementing mixed ion species quantum information processing for a scalable ion trap architecture. Mixed species chains may help solve several problems with scaling ion trap quantum computation to large numbers of qubits. Initial temperature measurements of linear Coulomb crystals containing barium and ytterbium ions indicate that the mass difference does not significantly impede cooling at low ion numbers. Average motional occupation numbers are estimated to be $\bar{n} \approx 130$ quanta per mode for chains with small numbers of ions, which is within a factor of three of the Doppler limit for barium ions in our trap. We also discuss generation of ion-photon entanglement with barium ions with a fidelity of $F \ge 0.84$, which is an initial step towards remote ion-ion coupling in a more scalable quantum information architecture. Further, we are working to implement these techniques in surface traps in order to exercise greater control over ion chain ordering and positioning.",2014-09-30T20:18:25Z,2014-09-30T20:18:25Z,http://arxiv.org/abs/1410.0037v1,http://arxiv.org/pdf/1410.0037v1,"quant-ph, physics.atom-ph"
On Metric Sorting for Successive Cancellation List Decoding of Polar   Codes,"Alexios Balatsoukas-Stimming, Mani Bastani Parizi, Andreas Burg","We focus on the metric sorter unit of successive cancellation list decoders for polar codes, which lies on the critical path in all current hardware implementations of the decoder. We review existing metric sorter architectures and we propose two new architectures that exploit the structure of the path metrics in a log-likelihood ratio based formulation of successive cancellation list decoding. Our synthesis results show that, for the list size of $L=32$, our first proposed sorter is $14\%$ faster and $45\%$ smaller than existing sorters, while for smaller list sizes, our second sorter has a higher delay in return for up to $36\%$ reduction in the area.",2014-10-16T15:07:54Z,2015-01-26T21:05:54Z,http://arxiv.org/abs/1410.4460v2,http://arxiv.org/pdf/1410.4460v2,"cs.AR, cs.IT, math.IT"
Conjugate gradient solvers on Intel Xeon Phi and NVIDIA GPUs,"O. Kaczmarek, C. Schmidt, P. Steinbrecher, M. Wagner","Lattice Quantum Chromodynamics simulations typically spend most of the runtime in inversions of the Fermion Matrix. This part is therefore frequently optimized for various HPC architectures. Here we compare the performance of the Intel Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate gradient solver. By exposing more parallelism to the accelerator through inverting multiple vectors at the same time, we obtain a performance greater than 300 GFlop/s on both architectures. This more than doubles the performance of the inversions. We also give a short overview of the Knights Corner architecture, discuss some details of the implementation and the effort required to obtain the achieved performance.",2014-11-17T11:27:55Z,2014-11-17T11:27:55Z,http://arxiv.org/abs/1411.4439v1,http://arxiv.org/pdf/1411.4439v1,"physics.comp-ph, cs.MS, hep-lat"
Design of a Unified Transport Triggered Processor for LDPC/Turbo Decoder,"Shahriar Shahabuddin, Janne Janhunen, Muhammet Fatih Bayramoglu, Markku Juntti, Amanullah Ghazi, Olli Silven","This paper summarizes the design of a programmable processor with transport triggered architecture (TTA) for decoding LDPC and turbo codes. The processor architecture is designed in such a manner that it can be programmed for LDPC or turbo decoding for the purpose of internetworking and roaming between different networks. The standard trellis based maximum a posteriori (MAP) algorithm is used for turbo decoding. Unlike most other implementations, a supercode based sum-product algorithm is used for the check node message computation for LDPC decoding. This approach ensures the highest hardware utilization of the processor architecture for the two different algorithms. Up to our knowledge, this is the first attempt to design a TTA processor for the LDPC decoder. The processor is programmed with a high level language to meet the time-to-market requirement. The optimization techniques and the usage of the function units for both algorithms are explained in detail. The processor achieves 22.64 Mbps throughput for turbo decoding with a single iteration and 10.12 Mbps throughput for LDPC decoding with five iterations for a clock frequency of 200 MHz.",2015-01-31T06:36:34Z,2015-01-31T06:36:34Z,http://arxiv.org/abs/1502.00076v1,http://arxiv.org/pdf/1502.00076v1,"cs.IT, math.IT"
Self-Organization in Disaster Resilient Heterogeneous Small Cell   Networks,"Haijun Zhang, Chunxiao Jiang, Rose Qingyang Hu, Yi Qian","Heterogeneous small cell networks with overlay femtocells and macrocell is a promising solution for future heterogeneous wireless cellular communications. However, great resilience is needed in heterogeneous small cells in case of accidents, attacks and natural disasters. In this article, we first describe the network architecture of disaster resilient heterogeneous small cell networks (DRHSCNs), where several self-organization inspired approaches are applied. Based on the proposed resilient heterogeneous small cell network architecture, self-configuring (power, physical cell ID and neighbor cell list self-configuration) and self-optimizing (coverage and capacity optimization and mobility robustness optimization) techniques are investigated in the DRHSCN. Simulation results show that self-configuration and self-optimization can effectively improve the performance of the deployment and operation of the small cell networks in disaster scenarios.",2015-05-13T01:31:26Z,2015-05-13T01:31:26Z,http://arxiv.org/abs/1505.03209v1,http://arxiv.org/pdf/1505.03209v1,"cs.NI, cs.IT, math.IT"
Image aesthetic evaluation using paralleled deep convolution neural   network,"Guo Lihua, Li Fudi","Image aesthetic evaluation has attracted much attention in recent years. Image aesthetic evaluation methods heavily depend on the effective aesthetic feature. Traditional meth-ods always extract hand-crafted features. However, these hand-crafted features are always designed to adapt particu-lar datasets, and extraction of them needs special design. Rather than extracting hand-crafted features, an automati-cally learn of aesthetic features based on deep convolutional neural network (DCNN) is first adopt in this paper. As we all know, when the training dataset is given, the DCNN architecture with high complexity may meet the over-fitting problem. On the other side, the DCNN architecture with low complexity would not efficiently extract effective features. For these reasons, we further propose a paralleled convolutional neural network (PDCNN) with multi-level structures to automatically adapt to the training dataset. Experimental results show that our proposed PDCNN architecture achieves better performance than other traditional methods.",2015-05-20T02:03:23Z,2015-05-20T02:03:23Z,http://arxiv.org/abs/1505.05225v1,http://arxiv.org/pdf/1505.05225v1,"cs.CV, cs.MM, 68U10, I.4.7"
Instant Learning: Parallel Deep Neural Networks and Convolutional   Bootstrapping,Andrew J. R. Simpson,"Although deep neural networks (DNN) are able to scale with direct advances in computational power (e.g., memory and processing speed), they are not well suited to exploit the recent trends for parallel architectures. In particular, gradient descent is a sequential process and the resulting serial dependencies mean that DNN training cannot be parallelized effectively. Here, we show that a DNN may be replicated over a massive parallel architecture and used to provide a cumulative sampling of local solution space which results in rapid and robust learning. We introduce a complimentary convolutional bootstrapping approach that enhances performance of the parallel architecture further. Our parallelized convolutional bootstrapping DNN out-performs an identical fully-trained traditional DNN after only a single iteration of training.",2015-05-22T07:24:14Z,2016-05-21T09:33:33Z,http://arxiv.org/abs/1505.05972v2,http://arxiv.org/pdf/1505.05972v2,"cs.LG, 68Txx"
"Emergence, self-organization and network efficiency in gigantic   termite-nest-networks build using simple rules","Diego Griffon, Carmen Andara, Klaus Jaffe","Termites, like many social insects, build nests of complex architecture. These constructions have been proposed to optimize different structural features. Here we describe the nest network of the termite Nasutitermes ephratae, which is among the largest nest-network reported for termites and show that it optimizes diverse parameters defining the network architecture. The network structure avoids multiple crossing of galleries and minimizes the overlap of foraging territories. Thus, these termites are able to minimize the number of galleries they build, while maximizing the foraging area available at the nest mounds. We present a simple computer algorithm that reproduces the basics characteristics of this termite nest network, showing that simple rules can produce complex architectural designs efficiently.",2015-06-04T07:27:03Z,2015-12-16T13:17:06Z,http://arxiv.org/abs/1506.01487v2,http://arxiv.org/pdf/1506.01487v2,"nlin.AO, nlin.PS, q-bio.PE"
Learning Contextualized Semantics from Co-occurring Terms via a Siamese   Architecture,"Ubai Sandouk, Ke Chen","One of the biggest challenges in Multimedia information retrieval and understanding is to bridge the semantic gap by properly modeling concept semantics in context. The presence of out of vocabulary (OOV) concepts exacerbates this difficulty. To address the semantic gap issues, we formulate a problem on learning contextualized semantics from descriptive terms and propose a novel Siamese architecture to model the contextualized semantics from descriptive terms. By means of pattern aggregation and probabilistic topic models, our Siamese architecture captures contextualized semantics from the co-occurring descriptive terms via unsupervised learning, which leads to a concept embedding space of the terms in context. Furthermore, the co-occurring OOV concepts can be easily represented in the learnt concept embedding space. The main properties of the concept embedding space are demonstrated via visualization. Using various settings in semantic priming, we have carried out a thorough evaluation by comparing our approach to a number of state-of-the-art methods on six annotation corpora in different domains, i.e., MagTag5K, CAL500 and Million Song Dataset in the music domain as well as Corel5K, LabelMe and SUNDatabase in the image domain. Experimental results on semantic priming suggest that our approach outperforms those state-of-the-art methods considerably in various aspects.",2015-06-17T23:03:43Z,2015-06-17T23:03:43Z,http://arxiv.org/abs/1506.05514v1,http://arxiv.org/pdf/1506.05514v1,"cs.IR, cs.CL, cs.LG, I.2.6"
Inherent Diversity in Replicated Architectures,"Peter Okech, Nicholas Mc Guire, William Okelo-Odongo","In this paper, we report our ongoing investigations of the inherent non-determinism in contemporary execution environments that can potentially lead to divergence in state of a multi-channel hardware/software system. Our approach involved setting up of experiments to study execution path variability of a simple program by tracing its execution at the kernel level. In the first of the two experiments, we analyzed the execution path by repeated execution of the program. In the second, we executed in parallel two instances of the same program, each pinned to a separate processor core. Our results show that for a program executing in a contemporary hardware/software platform , there is sufcient path non-determinism in kernel space that can potentially lead to diversity in replicated architectures. We believe the execution non-determinism can impact the activation of residual systematic faults in software. If this is true, then the inherent diversity can be used together with architectural means to protect safety related systems against residual systematic faults in the operating systems.",2015-10-07T14:57:45Z,2015-10-07T14:57:45Z,http://arxiv.org/abs/1510.02086v1,http://arxiv.org/pdf/1510.02086v1,"cs.DC, cs.IT, math.IT"
Radio Resource Allocation in LTE-Advanced Cellular Networks with M2M   Communications,"Kan Zheng, Fanglong Hu, Wei Xiang, Mischa Dohler, Wenbo Wang","Machine-to-machine (M2M) communications are expected to provide ubiquitous connectivity between machines without the need of human intervention. To support such a large number of autonomous devices, the M2M system architecture needs to be extremely power and spectrally efficient. This article thus briefly reviews the features of M2M services in the third generation (3G) long-term evolution and its advancement (LTE-Advanced) networks. Architectural enhancements are then presented for supporting M2M services in LTE-Advanced cellular networks. To increase spectral efficiency, the same spectrum is expected to be utilized for human-to-human (H2H) communications as well as M2M communications. We therefore present various radio resource allocation schemes and quantify their utility in LTE-Advanced cellular networks. System-level simulation results are provided to validate the performance effectiveness of M2M communications in LTE-Advanced cellular networks.",2015-10-22T10:38:53Z,2015-10-22T10:38:53Z,http://arxiv.org/abs/1510.06572v1,http://arxiv.org/pdf/1510.06572v1,"cs.IT, cs.NI, math.IT"
Energy Efficient Resource Allocation for Control Data Separation   Architecture based H-CRAN with Heterogeneous Fronthaul,"Qiang Liu, Gang Wu, Yingchu Guo, Yusong Zhang","Control data separation architecture (CDSA) is a more efficient architecture to overcome the overhead issue than the conventional cellular networks, especially for the huge bursty traffic like Internet of Things, and over-the-top (OTT) content service. In this paper, we study the optimization issue of network energy efficiency of the CDSA-based heterogeneous cloud radio access networks (H-CRAN) networks, which has heterogeneous fronthaul between control base station (CBS) and data base stations (DBSs). We first present a modified power consumption model for the CDSA-based H-CRAN, and then formulate the optimization problem with constraint of overall capacity of wireless fronthaul. We work out the resource assignment and power allocation by the convex relaxation approach Using fractional programming method and Lagrangian dual decomposition method, we derive the close-form optimal solution and verify it by comprehensive system-level simulation. The simulation results show that our proposed algorithm has 8% EE gain compared to the static algorithm, and the CDSA-based H-CRAN networks can achieve up to 16% EE gain compared to the conventional network even under strict fronthaul capacity limit.",2015-11-06T01:59:25Z,2015-11-23T12:18:20Z,http://arxiv.org/abs/1511.01969v2,http://arxiv.org/pdf/1511.01969v2,"cs.IT, math.IT"
Deep Kernel Learning,"Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing","We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$ for $n$ training points, and predictions cost $O(1)$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.",2015-11-06T20:38:08Z,2015-11-06T20:38:08Z,http://arxiv.org/abs/1511.02222v1,http://arxiv.org/pdf/1511.02222v1,"cs.LG, cs.AI, stat.ME, stat.ML"
Waveform Optimization for SWIPT with Nonlinear Energy Harvester Modeling,Bruno Clerckx,"Simultaneous Wireless Information and Power Transfer (SWIPT) has attracted significant attention in the communication community. The problem of waveform design for SWIPT has however never been addressed so far. In this paper, a novel SWIPT transceiver architecture is introduced relying on the superposition of multisine and OFDM waveforms at the transmitter and a power-splitter receiver equipped with an energy harvester and an information decoder capable of cancelling the multisine waveforms. The SWIPT multisine/OFDM waveforms are optimized so as to maximize the rate-energy region of the whole system. They are adaptive to the channel state information and result from a posynomial maximization problem that originates from the non-linearity of the energy harvester. Numerical results illustrate the performance of the derived waveforms and SWIPT architecture.",2016-02-02T19:54:33Z,2016-02-02T19:54:33Z,http://arxiv.org/abs/1602.01061v1,http://arxiv.org/pdf/1602.01061v1,"cs.IT, cs.NI, math.IT"
Convolutional Tables Ensemble: classification in microseconds,"Aharon Bar-Hillel, Eyal Krupka, Noam Bloom","We study classifiers operating under severe classification time constraints, corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble (CTE), an inherently fast architecture for object category recognition. The architecture is based on convolutionally-applied sparse feature extraction, using trees or ferns, and a linear voting layer. Several structure and optimization variants are considered, including novel decision functions, tree learning algorithm, and distillation from CNN to CTE architecture. Accuracy improvements of 24-45% over related art of similar speed are demonstrated on standard object recognition benchmarks. Using Pareto speed-accuracy curves, we show that CTE can provide better accuracy than Convolutional Neural Networks (CNN) for a certain range of classification time constraints, or alternatively provide similar error rates with 5-200X speedup.",2016-02-14T19:21:17Z,2016-02-14T19:21:17Z,http://arxiv.org/abs/1602.04489v1,http://arxiv.org/pdf/1602.04489v1,"cs.CV, cs.LG, 68T45"
Hardware Architecture of Complex K-best MIMO Decoder,"Mehnaz Rahman, Gwan S. Choi","This paper presents a hardware architecture of complex K-best Multiple Input Multiple Output (MIMO) decoder reducing the complexity of Maximum Likelihood (ML) detector. We develop a novel low-power VLSI design of complex K-best decoder for 8x8 MIMO and 64 QAM modulation scheme. Use of Schnorr-Euchner (SE) enumeration and a new parameter, Rlimit in the design reduce the complexity of calculating K-best nodes to a certain level with increased performance. The total word length of only 16 bits has been adopted for the hardware design limiting the bit error rate (BER) degradation to 0.3 dB with list size, K and Rlimit equal to 4. The proposed VLSI architecture is modeled in Verilog HDL using Xilinx and synthesized using Synopsys Design Vision in 45 nm CMOS technology. According to the synthesize result, it achieves 1090.8 Mbps throughput with power consumption of 782 mW and latency of 0.044 us. The maximum frequency the design proposed is 181.8 MHz.",2016-02-19T00:29:39Z,2016-02-19T00:29:39Z,http://arxiv.org/abs/1602.06009v1,http://arxiv.org/pdf/1602.06009v1,"cs.IT, math.IT"
Deep Spiking Networks,"Peter O'Connor, Max Welling","We introduce an algorithm to do backpropagation on a spiking network. Our network is ""spiking"" in the sense that our neurons accumulate their activation into a potential over time, and only send out a signal (a ""spike"") when this potential crosses a threshold and the neuron is reset. Neurons only update their states when receiving signals from other neurons. Total computation of the network thus scales with the number of spikes caused by an input rather than network size. We show that the spiking Multi-Layer Perceptron behaves identically, during both prediction and training, to a conventional deep network of rectified-linear units, in the limiting case where we run the spiking network for a long time. We apply this architecture to a conventional classification problem (MNIST) and achieve performance very close to that of a conventional Multi-Layer Perceptron with the same architecture. Our network is a natural architecture for learning based on streaming event-based data, and is a stepping stone towards using spiking neural networks to learn efficiently on streaming data.",2016-02-26T13:54:47Z,2016-11-07T12:38:17Z,http://arxiv.org/abs/1602.08323v2,http://arxiv.org/pdf/1602.08323v2,"cs.NE, 68T01, F.1.1"
A portable platform for accelerated PIC codes and its application to   GPUs using OpenACC,"F. Hariri, T. M. Tran, A. Jocksch, E. Lanti, J. Progsch, P. Messmer, S. Brunner, G. Gheller, L. Villard","We present a portable platform, called PIC_ENGINE, for accelerating Particle-In-Cell (PIC) codes on heterogeneous many-core architectures such as Graphic Processing Units (GPUs). The aim of this development is efficient simulations on future exascale systems by allowing different parallelization strategies depending on the application problem and the specific architecture. To this end, this platform contains the basic steps of the PIC algorithm and has been designed as a test bed for different algorithmic options and data structures. Among the architectures that this engine can explore, particular attention is given here to systems equipped with GPUs. The study demonstrates that our portable PIC implementation based on the OpenACC programming model can achieve performance closely matching theoretical predictions. Using the Cray XC30 system, Piz Daint, at the Swiss National Supercomputing Centre (CSCS), we show that PIC_ENGINE running on an NVIDIA Kepler K20X GPU can outperform the one on an Intel Sandybridge 8-core CPU by a factor of 3.4.",2016-03-09T13:55:56Z,2016-03-09T13:55:56Z,http://arxiv.org/abs/1603.02886v1,http://arxiv.org/pdf/1603.02886v1,"physics.comp-ph, cs.DC, physics.plasm-ph"
A practical multi-party computation algorithm for a secure distributed   online voting system,Juanjo Bermúdez,"We present an online voting architecture based on partitioning the election in small clusters of voters and using a new Multi-party Computation algorithm for obtaining voting results from the clusters. This new algorithm has some practical advantages over other previously known algorithms and isn't bound to any specific cryptographic concept; so it can be adapted to future cryptographic exigencies. Compared with other online voting technologies, we see that this new architecture is less vulnerable to hacker attacks and attacks from dishonest authorities, given that no sensitive information is stored in any public server and there is no need for any trustee to safeguard the legality of the election process. Even in case of an attack succeeding, the risks associated with the overall election are far lower than with any other voting system. This architecture can also be combined with any other voting system, inheriting advantages from both systems.",2016-03-14T12:10:47Z,2016-03-14T12:10:47Z,http://arxiv.org/abs/1603.04228v1,http://arxiv.org/pdf/1603.04228v1,"cs.CR, cs.DC, 68P99, E.m"
"Recent Advances in Cloud Radio Access Networks: System Architectures,   Key Techniques, and Open Issues","Mugen Peng, Yaohua Sun, Xuelong Li, Zhendong Mao, Chonggang Wang","As a promising paradigm to reduce both capital and operating expenditures, the cloud radio access network (C-RAN) has been shown to provide high spectral efficiency and energy efficiency. Motivated by its significant theoretical performance gains and potential advantages, C-RANs have been advocated by both the industry and research community. This paper comprehensively surveys the recent advances of C-RANs, including system architectures, key techniques, and open issues. The system architectures with different functional splits and the corresponding characteristics are comprehensively summarized and discussed. The state-of-the-art key techniques in C-RANs are classified as: the fronthaul compression, large-scale collaborative processing, and channel estimation in the physical layer; and the radio resource allocation and optimization in the upper layer. Additionally, given the extensiveness of the research area, open issues and challenges are presented to spur future investigations, in which the involvement of edge cache, big data mining, social-aware device-to-device, cognitive radio, software defined network, and physical layer security for C-RANs are discussed, and the progress of testbed development and trial test are introduced as well.",2016-04-03T07:39:01Z,2016-04-03T07:39:01Z,http://arxiv.org/abs/1604.00607v1,http://arxiv.org/pdf/1604.00607v1,"cs.IT, math.IT"
Deep Learning Convolutional Networks for Multiphoton Microscopy   Vasculature Segmentation,"Petteri Teikari, Marc Santos, Charissa Poon, Kullervo Hynynen","Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures.",2016-06-08T02:57:00Z,2016-06-08T02:57:00Z,http://arxiv.org/abs/1606.02382v1,http://arxiv.org/pdf/1606.02382v1,"cs.CV, cs.AI, I.2.6; I.5.1; I.5.4; I.4.6"
Conditional Generation and Snapshot Learning in Neural Dialogue Systems,"Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, Steve Young","Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.",2016-06-10T14:56:19Z,2016-06-10T14:56:19Z,http://arxiv.org/abs/1606.03352v1,http://arxiv.org/pdf/1606.03352v1,"cs.CL, cs.NE, stat.ML"
SCOR: Software-defined Constrained Optimal Routing Platform for SDN,"Siamak Layeghy, Farzaneh Pakzad, Marius Portmann","A Software-defined Constrained Optimal Routing (SCOR) platform is introduced as a Northbound interface in SDN architecture. It is based on constraint programming techniques and is implemented in MiniZinc modelling language. Using constraint programming techniques in this Northbound interface has created an efficient tool for implementing complex Quality of Service routing applications in a few lines of code. The code includes only the problem statement and the solution is found by a general solver program. A routing framework is introduced based on SDN's architecture model which uses SCOR as its Northbound interface and an upper layer of applications implemented in SCOR. Performance of a few implemented routing applications are evaluated in different network topologies, network sizes and various number of concurrent flows.",2016-07-12T07:07:52Z,2016-07-12T07:07:52Z,http://arxiv.org/abs/1607.03243v1,http://arxiv.org/pdf/1607.03243v1,"cs.NI, 68M10, 90B18, 90B20, 90B22, 90C09, 90C27, 90C29, 90C35, C.2.2; D.3.2; D.3.3; F.2.2; F.4.1; G.2.2"
"Enhanced VIP Algorithms for Forwarding, Caching, and Congestion Control   in Named Data Networks","Ying Cui, Fan Lai, Edmund Yeh, Ran Liu","Emerging Information-Centric Networking (ICN) architectures seek to optimally utilize both bandwidth and storage for efficient content distribution over the network. The Virtual Interest Packet (VIP) framework has been proposed to enable joint design of forwarding, caching, and congestion control strategies within the Named Data Networking (NDN) architecture. While the existing VIP algorithms exhibit good performance, they are primarily focused on maximizing network throughput and utility, and do not explicitly consider user delay. In this paper, we develop a new class of enhanced algorithms for joint dynamic forwarding, caching and congestion control within the VIP framework. These enhanced VIP algorithms adaptively stabilize the network and maximize network utility, while improving the delay performance by intelligently making use of VIP information beyond one hop. Generalizing Lyapunov drift techniques, we prove the throughput optimality and characterize the utility-delay tradeoff of the enhanced VIP algorithms. Numerical experiments demonstrate the superior performance of the resulting enhanced algorithms for handling Interest Packets and Data Packets within the actual plane, in terms of low network delay and high network utility.",2016-07-12T09:02:17Z,2016-07-12T09:02:17Z,http://arxiv.org/abs/1607.03270v1,http://arxiv.org/pdf/1607.03270v1,"cs.IT, cs.NI, math.IT"
Optimizing Recurrent Neural Networks Architectures under Time   Constraints,"Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang","Recurrent neural network (RNN)'s architecture is a key factor influencing its performance. We propose algorithms to optimize hidden sizes under running time constraint. We convert the discrete optimization into a subset selection problem. By novel transformations, the objective function becomes submodular and constraint becomes supermodular. A greedy algorithm with bounds is suggested to solve the transformed problem. And we show how transformations influence the bounds. To speed up optimization, surrogate functions are proposed which balance exploration and exploitation. Experiments show that our algorithms can find more accurate models or faster models than manually tuned state-of-the-art and random search. We also compare popular RNN architectures using our algorithms.",2016-08-29T02:14:48Z,2018-02-21T03:45:44Z,http://arxiv.org/abs/1608.07892v3,http://arxiv.org/pdf/1608.07892v3,"stat.ML, cs.LG"
A linear-time benchmarking tool for generalized surface codes,"Nicolas Delfosse, Pavithran Iyer, David Poulin","Quantum information processors need to be protected against errors and faults. One of the most widely considered fault-tolerant architecture is based on surface codes. While the general principles of these codes are well understood and basic code properties such as minimum distance and rate are easy to characterize, a code's average performance depends on the detailed geometric layout of the qubits. To date, optimizing a surface code architecture and comparing different geometric layouts relies on costly numerical simulations. Here, we propose a benchmarking algorithm for simulating the performance of surface codes, and generalizations thereof, that runs in linear time. We implemented this algorithm in a software that generates performance reports and allows to quickly compare different architectures.",2016-11-14T06:01:56Z,2016-11-14T06:01:56Z,http://arxiv.org/abs/1611.04256v1,http://arxiv.org/pdf/1611.04256v1,"quant-ph, cs.IT, math.IT"
Parallelizing Word2Vec in Multi-Core and Many-Core Architectures,"Shihao Ji, Nadathur Satish, Sheng Li, Pradeep Dubey","Word2vec is a widely used algorithm for extracting low-dimensional vector representations of words. State-of-the-art algorithms including those by Mikolov et al. have been parallelized for multi-core CPU architectures, but are based on vector-vector operations with ""Hogwild"" updates that are memory-bandwidth intensive and do not efficiently use computational resources. In this paper, we propose ""HogBatch"" by improving reuse of various data structures in the algorithm through the use of minibatching and negative sample sharing, hence allowing us to express the problem using matrix multiply operations. We also explore different techniques to distribute word2vec computation across nodes in a compute cluster, and demonstrate good strong scalability up to 32 nodes. The new algorithm is particularly suitable for modern multi-core/many-core architectures, especially Intel's latest Knights Landing processors, and allows us to scale up the computation near linearly across cores and nodes, and process hundreds of millions of words per second, which is the fastest word2vec implementation to the best of our knowledge.",2016-11-18T17:47:44Z,2016-12-23T19:20:25Z,http://arxiv.org/abs/1611.06172v2,http://arxiv.org/pdf/1611.06172v2,"cs.DC, stat.ML"
"Learning Radio Resource Management in 5G Networks: Framework,   Opportunities and Challenges","Francesco Davide Calabrese, Li Wang, Euhanna Ghadimi, Gunnar Peters, Pablo Soldati","In the fifth generation (5G) of mobile broadband systems, Radio Resources Management (RRM) will reach unprecedented levels of complexity. To cope with the ever more sophisticated RRM functionalities and with the growing variety of scenarios, while carrying out the prompt decisions required in 5G, this manuscript presents a lean 5G RRM architecture that capitalizes on recent advances in the field of machine learning in combination with the large amount of data readily available in the network from measurements and system observations. The architecture relies on a single general-purpose learning framework conceived for RRM directly using the data gathered in the network. The complexity of RRM is shifted to the design of the framework, whilst the RRM algorithms derived from this framework are executed in a computationally efficient distributed manner at the radio access nodes. The potential of this approach is verified in a pair of pertinent scenarios and future directions on applications of machine learning to RRM are discussed.",2016-11-30T16:18:32Z,2018-05-20T20:53:04Z,http://arxiv.org/abs/1611.10253v3,http://arxiv.org/pdf/1611.10253v3,"cs.NI, cs.IT, math.IT, math.OC"
FaaS: Federation-as-a-Service,"Francesco Paolo Schiavo, Vladimiro Sassone, Luca Nicoletti, Andrea Margheri","This document is the main high-level architecture specification of the SUNFISH cloud federation solution. Its main objective is to introduce the concept of Federation-as-a-Service (FaaS) and the SUNFISH platform. FaaS is the new and innovative cloud federation service proposed by the SUNFISH project. The document defines the functionalities of FaaS, its governance and precise objectives. With respect to these objectives, the document proposes the high-level architecture of the SUNFISH platform: the software architecture that permits realising a FaaS federation. More specifically, the document describes all the components forming the platform, the offered functionalities and their high-level interactions underlying the main FaaS functionalities. The document concludes by outlining the main implementation strategies towards the actual implementation of the proposed cloud federation solution.",2016-12-12T21:31:55Z,2016-12-12T21:31:55Z,http://arxiv.org/abs/1612.03937v1,http://arxiv.org/pdf/1612.03937v1,"cs.DC, D.2.1; D.2.11"
Performance Optimisation of Smoothed Particle Hydrodynamics Algorithms   for Multi/Many-Core Architectures,"Fabio Baruffa, Luigi Iapichino, Nicolay J. Hammer, Vasileios Karakasis","We describe a strategy for code modernisation of Gadget, a widely used community code for computational astrophysics. The focus of this work is on node-level performance optimisation, targeting current multi/many-core IntelR architectures. We identify and isolate a sample code kernel, which is representative of a typical Smoothed Particle Hydrodynamics (SPH) algorithm. The code modifications include threading parallelism optimisation, change of the data layout into Structure of Arrays (SoA), auto-vectorisation and algorithmic improvements in the particle sorting. We obtain shorter execution time and improved threading scalability both on Intel XeonR ($2.6 \times$ on Ivy Bridge) and Xeon PhiTM ($13.7 \times$ on Knights Corner) systems. First few tests of the optimised code result in $19.1 \times$ faster execution on second generation Xeon Phi (Knights Landing), thus demonstrating the portability of the devised optimisation solutions to upcoming architectures.",2016-12-19T09:45:25Z,2017-05-10T14:34:36Z,http://arxiv.org/abs/1612.06090v2,http://arxiv.org/pdf/1612.06090v2,"cs.DC, astro-ph.IM, physics.comp-ph"
Highway and Residual Networks learn Unrolled Iterative Estimation,"Klaus Greff, Rupesh K. Srivastava, Jürgen Schmidhuber","The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.   In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation -- a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of Highway and Residual networks. Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.",2016-12-22T19:57:35Z,2017-03-14T21:27:03Z,http://arxiv.org/abs/1612.07771v3,http://arxiv.org/pdf/1612.07771v3,"cs.NE, cs.AI, cs.LG, I.2.6; I.5.1"
HADAS Green Assistant: designing energy-efficient applications,"Nadia Gamez, Monica Pinto, Lidia Fuentes","The number of works addressing the role of energy efficiency in the software development has been increasing recently. But, designers and programmers still complain about the lack of tools that help them to make energy-efficiency decisions. Some works show that energy-aware design decisions tend to have a larger impact in the power consumed by applications, than code optimizations. In this paper we present the HADAS green assistant, which helps developers to identify the energy-consuming concerns of their applications (i.e., points in the application that consume more energy, like storing or transferring data), and also to model, analyse and reason about different architectural solutions for each of these concerns. This tool models the variability of more or less green architectural practices and the dependencies between different energy-consuming concerns using variabilty models. Finally, this tool will automatically generate the architectural configuration derived from the selections made by the developer from an energy consumption point of view.",2016-12-23T21:02:46Z,2016-12-23T21:02:46Z,http://arxiv.org/abs/1612.08095v1,http://arxiv.org/pdf/1612.08095v1,"cs.SE, D.2.11"
Reducing Competitive Cache Misses in Modern Processor Architectures,"Milcho Prisagjanec, Pece Mitrevski","The increasing number of threads inside the cores of a multicore processor, and competitive access to the shared cache memory, become the main reasons for an increased number of competitive cache misses and performance decline. Inevitably, the development of modern processor architectures leads to an increased number of cache misses. In this paper, we make an attempt to implement a technique for decreasing the number of competitive cache misses in the first level of cache memory. This technique enables competitive access to the entire cache memory when there is a hit - but, if there are cache misses, memory data (by using replacement techniques) is put in a virtual part given to threads, so that competitive cache misses are avoided. By using a simulator tool, the results show a decrease in the number of cache misses and performance increase for up to 15%. The conclusion that comes out of this research is that cache misses are a real challenge for future processor designers, in order to hide memory latency.",2017-01-06T13:32:36Z,2017-01-06T13:32:36Z,http://arxiv.org/abs/1701.01630v1,http://arxiv.org/pdf/1701.01630v1,"cs.AR, B.3.2; B.3.3"
Robust Distributed Control of DC Microgrids with Time-Varying Power   Sharing,"Mayank Baranwal, Alireza Askarian, Srinivasa M. Salapaka","This paper addresses the problem of output voltage regulation for multiple DC/DC converters connected to a microgrid, and prescribes a scheme for sharing power among different sources. This architecture is structured in such a way that it admits quantifiable analysis of the closed-loop performance of the network of converters; the analysis simplifies to studying closed-loop performance of an equivalent {\em single-converter} system. The proposed architecture allows for the proportion in which the sources provide power to vary with time; thus overcoming limitations of our previous designs. Additionally, the proposed control framework is suitable to both centralized and decentralized implementations, i.e., the same control architecture can be employed for voltage regulation irrespective of the availability of common load-current (or power) measurement, without the need to modify controller parameters. The performance becomes quantifiably better with better communication of the demanded load to all the controllers at all the converters (in the centralized case); however guarantees viability when such communication is absent. Case studies comprising of battery, PV and generic sources are presented and demonstrate the enhanced performance of prescribed optimal controllers for voltage regulation and power sharing.",2017-01-11T17:17:48Z,2017-01-11T17:17:48Z,http://arxiv.org/abs/1701.03065v1,http://arxiv.org/pdf/1701.03065v1,"math.OC, cs.SY"
Convolutional Neural Networks for Page Segmentation of Historical   Document Images,"Kai Chen, Mathias Seuret","This paper presents a Convolutional Neural Network (CNN) based page segmentation method for handwritten historical document images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.",2017-04-05T15:12:25Z,2017-04-07T10:16:49Z,http://arxiv.org/abs/1704.01474v2,http://arxiv.org/pdf/1704.01474v2,"cs.CV, cs.LG, stat.ML"
Limits of End-to-End Learning,Tobias Glasmachers,"End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning system is specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all ""peripheral"" modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex.   In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.",2017-04-26T19:12:37Z,2017-04-26T19:12:37Z,http://arxiv.org/abs/1704.08305v1,http://arxiv.org/pdf/1704.08305v1,"cs.LG, stat.ML"
Frequency-domain Compressive Channel Estimation for Frequency-Selective   Hybrid mmWave MIMO Systems,"Javier Rodríguez-Fernández, Nuria González-Prelcic, Kiran Venugopal, Robert W. Heath Jr","Channel estimation is useful in millimeter wave (mmWave) MIMO communication systems. Channel state information allows optimized designs of precoders and combiners under different metrics such as mutual information or signal-to-interference-noise (SINR) ratio. At mmWave, MIMO precoders and combiners are usually hybrid, since this architecture provides a means to trade-off power consumption and achievable rate. Channel estimation is challenging when using these architectures, however, since there is no direct access to the outputs of the different antenna elements in the array. The MIMO channel can only be observed through the analog combining network, which acts as a compression stage of the received signal. Most of prior work on channel estimation for hybrid architectures assumes a frequency-flat mmWave channel model. In this paper, we consider a frequency-selective mmWave channel and propose compressed-sensing-based strategies to estimate the channel in the frequency domain. We evaluate different algorithms and compute their complexity to expose trade-offs in complexity-overhead-performance as compared to those of previous approaches.",2017-04-27T13:51:19Z,2017-04-27T13:51:19Z,http://arxiv.org/abs/1704.08572v1,http://arxiv.org/pdf/1704.08572v1,"cs.IT, math.IT"
Joint Design of Multi-Tap Analog Cancellation and Digital Beamforming   for Reduced Complexity Full Duplex MIMO Systems,"George C. Alexandropoulos, Melissa Duarte","Incorporating full duplex operation in Multiple Input Multiple Output (MIMO) systems provides the potential of boosting throughput performance. However, the hardware complexity of the analog self-interference canceller scales with the number of transmit and receive antennas, thus exploiting the benefits of analog cancellation becomes impractical for full duplex MIMO transceivers. In this paper, we present a novel architecture for the analog canceller comprising of reduced number of taps (tap refers to a line of fixed delay and variable phase shifter and attenuator) and simple multiplexers for efficient signal routing among the transmit and receive radio frequency chains. In contrast to the available analog cancellation architectures, the values for each tap and the configuration of the multiplexers are jointly designed with the digital beamforming filters according to certain performance objectives. Focusing on a narrowband flat fading channel model as an example, we present a general optimization framework for the joint design of analog cancellation and digital beamforming. We also detail a particular optimization objective together with its derived solution for the latter architectural components. Representative computer simulation results demonstrate the superiority of the proposed low complexity full duplex MIMO system over lately available ones.",2017-05-15T10:26:43Z,2017-05-15T10:26:43Z,http://arxiv.org/abs/1705.05148v1,http://arxiv.org/pdf/1705.05148v1,"cs.IT, math.IT"
Instruction Set Architectures for Quantum Processing Units,"Keith A. Britt, Travis S. Humble","Progress in quantum computing hardware raises questions about how these devices can be controlled, programmed, and integrated with existing computational workflows. We briefly describe several prominent quantum computational models, their associated quantum processing units (QPUs), and the adoption of these devices as accelerators within high-performance computing systems. Emphasizing the interface to the QPU, we analyze instruction set architectures based on reduced and complex instruction sets, i.e., RISC and CISC architectures. We clarify the role of conventional constraints on memory addressing and instruction widths within the quantum computing context. Finally, we examine existing quantum computing platforms, including the D-Wave 2000Q and IBM Quantum Experience, within the context of future ISA development and HPC needs.",2017-07-19T17:12:48Z,2017-07-19T17:12:48Z,http://arxiv.org/abs/1707.06202v1,http://arxiv.org/pdf/1707.06202v1,"cs.ET, quant-ph"
Principles for optimal cooperativity in allosteric materials,"Le Yan, Riccardo Ravasio, Carolina Brito, Matthieu Wyart","Allosteric proteins transmit a mechanical signal induced by binding a ligand. However, understanding the nature of the information transmitted and the architectures optimizing such transmission remains a challenge. Here we show using an {\it in-silico} evolution scheme and theoretical arguments that architectures optimized to be cooperative, which propagate efficiently energy, {qualitatively} differ from previously investigated materials optimized to propagate strain. Although we observe a large diversity of functioning cooperative architectures (including shear, hinge and twist designs), they all obey the same principle {of displaying a {\it mechanism}, i.e. an extended {soft} mode}. We show that its optimal frequency decreases with the spatial extension $L$ of the system as $L^{-d/2}$, where $d$ is the spatial dimension. For these optimal designs, cooperativity decays logarithmically with $L$ for $d=2$ and does not decay for $d=3$. Overall our approach leads to a natural explanation for several observations in allosteric proteins, and { indicates an experimental path to test if allosteric proteins lie close to optimality}.",2017-08-05T21:02:42Z,2018-06-04T13:27:08Z,http://arxiv.org/abs/1708.01820v3,http://arxiv.org/pdf/1708.01820v3,"physics.bio-ph, cond-mat.mtrl-sci, cond-mat.soft"
Multibiometric Secure System Based on Deep Learning,"Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi","In this paper, we propose a secure multibiometric system that uses deep neural networks and error-correction coding. We present a feature-level fusion framework to generate a secure multibiometric template from each user's multiple biometrics. Two fusion architectures, fully connected architecture and bilinear architecture, are implemented to develop a robust multibiometric shared representation. The shared representation is used to generate a cancelable biometric template that involves the selection of a different set of reliable and discriminative features for each user. This cancelable template is a binary vector and is passed through an appropriate error-correcting decoder to find a closest codeword and this codeword is hashed to generate the final secure template. The efficacy of the proposed approach is shown using a multimodal database where we achieve state-of-the-art matching performance, along with cancelability and security.",2017-08-07T21:35:26Z,2017-08-07T21:35:26Z,http://arxiv.org/abs/1708.02314v1,http://arxiv.org/pdf/1708.02314v1,"cs.AI, cs.CV, cs.IT, math.IT"
Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,Stanislav Fort,"We propose a novel architecture for $k$-shot classification on the Omniglot dataset. Building on prototypical networks, we extend their architecture to what we call Gaussian prototypical networks. Prototypical networks learn a map between images and embedding vectors, and use their clustering for classification. In our model, a part of the encoder output is interpreted as a confidence region estimate about the embedding point, and expressed as a Gaussian covariance matrix. Our network then constructs a direction and class dependent distance metric on the embedding space, using uncertainties of individual data points as weights. We show that Gaussian prototypical networks are a preferred architecture over vanilla prototypical networks with an equivalent number of parameters. We report state-of-the-art performance in 1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot 5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset. We explore artificially down-sampling a fraction of images in the training set, which improves our performance even further. We therefore hypothesize that Gaussian prototypical networks might perform better in less homogeneous, noisier datasets, which are commonplace in real world applications.",2017-08-09T06:53:31Z,2017-08-09T06:53:31Z,http://arxiv.org/abs/1708.02735v1,http://arxiv.org/pdf/1708.02735v1,"cs.LG, cs.CV, cs.NE, stat.ML"
A hybrid architecture for astronomical computing,"Changhua Li, Chenzhou Cui, Boliang He, Dongwei Fan, Linying Mi, Shanshan Li, Sisi Yang, Yunfei Xu, Jun Han, Junyi Chen, Hailong Zhang, Ce Yu, Jian Xiao, Chuanjun Wang, Zihuang Cao, Yufeng Fan, Liang Liu, Xiao Chen, Wenming Song, Kangyu Du","With many large science equipment constructing and putting into use, astronomy has stepped into the big data era. The new method and infrastructure of big data processing has become a new requirement of many astronomers. Cloud computing, Map/Reduce, Hadoop, Spark, etc. many new technology has sprung up in recent years. Comparing to the high performance computing(HPC), Data is the center of these new technology. So, a new computing architecture infrastructure is necessary, which can be shared by both HPC and big data processing. Based on Astronomy Cloud project of Chinese Virtual Observatory (China-VO), we have made much efforts to optimize the designation of the hybrid computing platform. which include the hardware architecture, cluster management, Job and Resource scheduling.",2018-01-19T03:01:49Z,2018-01-19T03:01:49Z,http://arxiv.org/abs/1801.07548v1,http://arxiv.org/pdf/1801.07548v1,"cs.DC, astro-ph.IM"
Text-Independent Speaker Verification Using Long Short-Term Memory   Networks,"Aryan Mobiny, Mohammad Najarian","In this paper, an architecture based on Long Short-Term Memory Networks has been proposed for the text-independent scenario which is aimed to capture the temporal speaker-related information by operating over traditional speech features. For speaker verification, at first, a background model must be created for speaker representation. Then, in enrollment stage, the speaker models will be created based on the enrollment utterances. For this work, the model will be trained in an end-to-end fashion to combine the first two stages. The main goal of end-to-end training is the model being optimized to be consistent with the speaker verification protocol. The end- to-end training jointly learns the background and speaker models by creating the representation space. The LSTM architecture is trained to create a discrimination space for validating the match and non-match pairs for speaker verification. The proposed architecture demonstrate its superiority in the text-independent compared to other traditional methods.",2018-05-02T02:30:20Z,2018-09-07T14:41:25Z,http://arxiv.org/abs/1805.00604v3,http://arxiv.org/pdf/1805.00604v3,"eess.AS, cs.CL, cs.SD"
A Multi-component CNN-RNN Approach for Dimensional Emotion Recognition   in-the-wild,"Dimitrios Kollias, Stefanos Zafeiriou","This paper presents our approach to the One-Minute Gradual-Emotion Recognition (OMG-Emotion) Challenge, focusing on dimensional emotion recognition through visual analysis of the provided emotion videos. The approach is based on a Convolutional and Recurrent (CNN-RNN) deep neural architecture we have developed for the relevant large AffWild Emotion Database. We extended and adapted this architecture, by letting a combination of multiple features generated in the CNN component be explored by RNN subnets. Our target has been to obtain best performance on the OMG-Emotion visual validation data set, while learning the respective visual training data set. Extended experimentation has led to best architectures for the estimation of the values of the valence and arousal emotion dimensions over these data sets.",2018-05-03T17:54:44Z,2019-12-13T23:32:41Z,http://arxiv.org/abs/1805.01452v5,http://arxiv.org/pdf/1805.01452v5,"cs.CV, cs.AI, cs.HC, eess.IV, stat.ML"
Automatic Classification of Object Code Using Machine Learning,John Clemens,"Recent research has repeatedly shown that machine learning techniques can be applied to either whole files or file fragments to classify them for analysis. We build upon these techniques to show that for samples of un-labeled compiled computer object code, one can apply the same type of analysis to classify important aspects of the code, such as its target architecture and endianess. We show that using simple byte-value histograms we retain enough information about the opcodes within a sample to classify the target architecture with high accuracy, and then discuss heuristic-based features that exploit information within the operands to determine endianess. We introduce a dataset with over 16000 code samples from 20 architectures and experimentally show that by using our features, classifiers can achieve very high accuracy with relatively small sample sizes.",2018-05-06T03:49:48Z,2018-05-06T03:49:48Z,http://arxiv.org/abs/1805.02146v1,http://arxiv.org/pdf/1805.02146v1,"stat.ML, cs.CR, cs.LG"
Highly Integrated Organic-Inorganic Hybrid Architectures by Non-Covalent   Exfoliation of Graphite and Assembly with Zinc Oxide Nanoparticles,"Mario Marcia, Chau Vinh, Christian Dolle, Gonzalo Abellán, Jörg Schönamsgruber, Torsten Schunk, Benjamin Butz, Erdmann Spiecker, Frank Hauke, Andreas Hirsch","Herein, we report an easy, straight forward, and versatile approach to build 0D/2D hybrid nanoparticle/graphene architectures by means of non-covalent chemistry and a modified Layer-by-Layer assembly. Three water soluble perylene diimides were employed to efficiently exfoliate pristine graphite into positively charged few- and multilayer graphene flakes. Further combination of these cationic building blocks with anionic zinc oxide nanoparticles led to the formation of tailor-made hybrid films via electrostatic and van der Waals interactions. These supramolecular hybrid nano-structures were thoroughly characterized by UV/Vis and Raman spectroscopy, AFM as well as electron microscopy, showing outstanding long-range homogeneity and high integrity in the centimetre-scale, uniform nanometric thickness between 60-100 nm and a close contact between the different building blocks. Due to their straightforward assembly. These architectures can be considered as promising candidates for numerous advanced applications especially in the field of energy storage and conversion.",2018-05-04T10:03:02Z,2018-05-04T10:03:02Z,http://arxiv.org/abs/1805.09129v1,http://arxiv.org/pdf/1805.09129v1,"physics.chem-ph, cond-mat.mtrl-sci"
TAPAS: Train-less Accuracy Predictor for Architecture Search,"R. Istrate, F. Scheidegger, G. Mariani, D. Nikolopoulos, C. Bekas, A. C. I. Malossi","In recent years an increasing number of researchers and practitioners have been suggesting algorithms for large-scale neural network architecture search: genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. None of them, however, demonstrated high-performance without training new experiments in the presence of unseen datasets. We propose a new deep neural network accuracy predictor, that estimates in fractions of a second classification performance for unseen input datasets, without training. In contrast to previously proposed approaches, our prediction is not only calibrated on the topological network information, but also on the characterization of the dataset-difficulty which allows us to re-tune the prediction without any training. Our predictor achieves a performance which exceeds 100 networks per second on a single GPU, thus creating the opportunity to perform large-scale architecture search within a few minutes. We present results of two searches performed in 400 seconds on a single GPU. Our best discovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for CIFAR-100, verified by training. These networks are performance competitive with other automatically discovered state-of-the-art networks however we only needed a small fraction of the time to solution and computational resources.",2018-06-01T09:17:15Z,2018-06-01T09:17:15Z,http://arxiv.org/abs/1806.00250v1,http://arxiv.org/pdf/1806.00250v1,"cs.LG, stat.ML"
Autoencoders Learn Generative Linear Models,"Thanh V. Nguyen, Raymond K. W. Wong, Chinmay Hegde","We provide a series of results for unsupervised learning with autoencoders. Specifically, we study shallow two-layer autoencoder architectures with shared weights. We focus on three generative models for data that are common in statistical machine learning: (i) the mixture-of-gaussians model, (ii) the sparse coding model, and (iii) the sparsity model with non-negative coefficients. For each of these models, we prove that under suitable choices of hyperparameters, architectures, and initialization, autoencoders learned by gradient descent can successfully recover the parameters of the corresponding model. To our knowledge, this is the first result that rigorously studies the dynamics of gradient descent for weight-sharing autoencoders. Our analysis can be viewed as theoretical evidence that shallow autoencoder modules indeed can be used as feature learning mechanisms for a variety of data models, and may shed insight on how to train larger stacked architectures with autoencoders as basic building blocks.",2018-06-02T02:24:16Z,2019-02-15T07:40:13Z,http://arxiv.org/abs/1806.00572v3,http://arxiv.org/pdf/1806.00572v3,"stat.ML, cs.LG"
Data-driven Analytics for Business Architectures: Proposed Use of Graph   Theory,"Lei Huang, Guangjie Ren, Shun Jiang, Raphael Arar, Eric Young Liu","Business Architecture (BA) plays a significant role in helping organizations understand enterprise structures and processes, and align them with strategic objectives. However, traditional BAs are represented in fixed structure with static model elements and fail to dynamically capture business insights based on internal and external data. To solve this problem, this paper introduces the graph theory into BAs with aim of building extensible data-driven analytics and automatically generating business insights. We use IBM's Component Business Model (CBM) as an example to illustrate various ways in which graph theory can be leveraged for data-driven analytics, including what and how business insights can be obtained. Future directions for applying graph theory to business architecture analytics are discussed.",2018-06-05T06:25:25Z,2018-06-05T06:25:25Z,http://arxiv.org/abs/1806.03168v1,http://arxiv.org/pdf/1806.03168v1,"cs.SE, cs.DB, cs.LG, stat.ML"
FATALIC: A novel CMOS front-end readout ASIC for the ATLAS Tile   Calorimeter,"S. Angelidakis, W. M. Barbe, R. Bonnefoy, H. Chanal, C. Fayard, R. Madar, S. Manen, M. -L. Mercier, E. Nibigira, D. Pallin, N. Pillet, L. Royer, A. Soulier, R. Vandaële, F. Vazeille","The present article introduces a novel ASIC architecture, designed in the context of the ATLAS Tile Calorimeter upgrade program for the High-Luminosity phase of the Large Hadron Collider at CERN. The architecture is based on radiation-tolerant 130 nm Complementary Metal-Oxide-Semiconductor technology, embedding both analog and digital processing of detector signals. A detailed description of the ASIC is given in terms of motivation, design characteristics and simulated and measured performance. Experimental studies, based on 24 prototype units under real particle beam conditions are also presented in order to demonstrate the potential of the architecture as a reliable front-end readout electronic solution.",2018-09-01T17:32:02Z,2018-12-11T00:57:50Z,http://arxiv.org/abs/1809.00225v4,http://arxiv.org/pdf/1809.00225v4,"physics.ins-det, hep-ex"
Non-Orthogonal Multiplexing of Ultra-Reliable and Broadband Services in   Fog-Radio Architectures,"Rahif Kassab, Osvaldo Simeone, Petar Popovski, Toufiqul Islam","The fifth generation (5G) of cellular systems is introducing Ultra-Reliable Low-Latency Communications (URLLC) services alongside more conventional enhanced Mobile BroadBand (eMBB) traffic. Furthermore, the 5G cellular architecture is evolving from a base station-centric deployment to a fog-like set-up that accommodates a flexible functional split between cloud and edge. In this paper, a novel solution is proposed that enables the non-orthogonal coexistence of URLLC and eMBB services by processing URLLC traffic at the Edge Nodes (ENs), while eMBB communications are handled centrally at a cloud processor as in a Cloud-Radio Access Network (C-RAN) system. This solution guarantees the low-latency requirements of the URLLC service by means of edge processing, e.g., for vehicle-to-cellular use cases, as well as the high spectral efficiency for eMBB traffic via centralized baseband processing. Both uplink and downlink are analyzed by accounting for the heterogeneous performance requirements of eMBB and URLLC traffic and by considering practical aspects such as fading, lack of channel state information for URLLC transmitters, rate adaptation for eMBB transmitters, finite fronthaul capacity, and different coexistence strategies, such as puncturing.",2018-09-05T09:24:49Z,2018-12-21T15:18:55Z,http://arxiv.org/abs/1809.01399v2,http://arxiv.org/pdf/1809.01399v2,"cs.NI, cs.IT, math.IT"
Three-Stage Speaker Verification Architecture in Emotional Talking   Environments,"Ismail Shahin, Ali Bou Nassif","Speaker verification performance in neutral talking environment is usually high, while it is sharply decreased in emotional talking environments. This performance degradation in emotional environments is due to the problem of mismatch between training in neutral environment while testing in emotional environments. In this work, a three-stage speaker verification architecture has been proposed to enhance speaker verification performance in emotional environments. This architecture is comprised of three cascaded stages: gender identification stage followed by an emotion identification stage followed by a speaker verification stage. The proposed framework has been evaluated on two distinct and independent emotional speech datasets: in-house dataset and Emotional Prosody Speech and Transcripts dataset. Our results show that speaker verification based on both gender information and emotion information is superior to each of speaker verification based on gender information only, emotion information only, and neither gender information nor emotion information. The attained average speaker verification performance based on the proposed framework is very alike to that attained in subjective assessment by human listeners.",2018-09-03T09:25:35Z,2018-09-03T09:25:35Z,http://arxiv.org/abs/1809.01721v1,http://arxiv.org/pdf/1809.01721v1,"cs.SD, cs.AI, eess.AS"
ProdSumNet: reducing model parameters in deep neural networks via   product-of-sums matrix decompositions,Chai Wah Wu,"We consider a general framework for reducing the number of trainable model parameters in deep learning networks by decomposing linear operators as a product of sums of simpler linear operators. Recently proposed deep learning architectures such as CNN, KFC, Dilated CNN, etc. are all subsumed in this framework and we illustrate other types of neural network architectures within this framework. We show that good accuracy on MNIST and Fashion MNIST can be obtained using a relatively small number of trainable parameters. In addition, since implementation of the convolutional layer is resource-heavy, we consider an approach in the transform domain that obviates the need for convolutional layers. One of the advantages of this general framework over prior approaches is that the number of trainable parameters is not fixed and can be varied arbitrarily. In particular, we illustrate the tradeoff of varying the number of trainable variables and the corresponding error rate. As an example, by using this decomposition on a reference CNN architecture for MNIST with over 3x10^6 trainable parameters, we are able to obtain an accuracy of 98.44% using only 3554 trainable parameters.",2018-09-06T20:50:40Z,2019-05-23T20:34:09Z,http://arxiv.org/abs/1809.02209v2,http://arxiv.org/pdf/1809.02209v2,"cs.LG, stat.ML"
On the Turing Completeness of Modern Neural Network Architectures,"Jorge Pérez, Javier Marinković, Pablo Barceló","Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.",2019-01-10T23:21:35Z,2019-01-10T23:21:35Z,http://arxiv.org/abs/1901.03429v1,http://arxiv.org/pdf/1901.03429v1,"cs.LG, cs.FL, stat.ML"
Robust Design of AC Computing-Enabled Receiver Architecture for SWIPT   Networks,"Ha-Vu Tran, Georges Kaddoum","Inspired by the direct use of alternating current (AC) for computation, we propose a novel integrated information and energy receiver architecture for simultaneous wireless information and power transfer (SWIPT) networks. In this context, the AC computing method, in which wirelessly harvested AC energy is directly used to supply the computing block of receivers, enhances not only computational ability but also energy efficiency over the conventional direct current (DC) one. Further, we aim to manage the trade-off between the information decoding (ID) and energy harvesting (EH) optimally while taking imperfect channel estimation into account. It results in a worst-case optimization problem of maximizing the data rate under the constraints of an EH requirement, the energy needed for supplying the AC computational logic, and a transmit power budget. Then, we propose a method to derive closed-form optimal solutions. The numerical results demonstrate that the proposed architecture with AC computing significantly improves the rate-energy region.",2019-01-17T20:29:11Z,2019-01-17T20:29:11Z,http://arxiv.org/abs/1901.05999v1,http://arxiv.org/pdf/1901.05999v1,"cs.IT, math.IT"
3D-Printed Surface Architecture Enhancing Superhydrophobicity and   Viscous Droplet Repellency,"Gustav Graeber, Oskar B. Martin Kieliger, Thomas M. Schutzius, Dimos Poulikakos","Macro-textured superhydrophobic surfaces can reduce droplet-substrate contact times of impacting water droplets, however, surface designs with similar performance for significantly more viscous liquids are missing, despite their importance in nature and technology such as for chemical shielding, food staining repellency, and supercooled (viscous) water droplet removal in anti-icing applications. Here, we introduce a deterministic, controllable and up-scalable method to fabricate superhydrophobic surfaces with a 3D-printed architecture, combining arrays of alternating surface protrusions and indentations. We show a more than threefold contact time reduction of impacting viscous droplets up to a fluid viscosity of 3.7mPa s, which equals 3.7 times the viscosity of water at room temperature, covering the viscosity of many chemicals and supercooled water. Based on the combined consideration of the fluid flow within and the simultaneous droplet dynamics above the texture, we recommend future pathways to rationally architecture such surfaces, all realizable with the methodology presented here.",2018-12-10T17:00:39Z,2018-12-10T17:00:39Z,http://arxiv.org/abs/1901.07323v1,http://arxiv.org/pdf/1901.07323v1,"cond-mat.soft, physics.flu-dyn"
Understanding Geometry of Encoder-Decoder CNNs,"Jong Chul Ye, Woon Kyoung Sung","Encoder-decoder networks using convolutional neural network (CNN) architecture have been extensively used in deep learning literatures thanks to its excellent performance for various inverse problems. However, it is still difficult to obtain coherent geometric view why such an architecture gives the desired performance. Inspired by recent theoretical understanding on generalizability, expressivity and optimization landscape of neural networks, as well as the theory of convolutional framelets, here we provide a unified theoretical framework that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis representation using combinatorial convolution frames, whose expressibility increases exponentially with the network depth. We also demonstrate the importance of skipped connection in terms of expressibility, and optimization landscape.",2019-01-22T23:37:43Z,2019-05-07T13:56:00Z,http://arxiv.org/abs/1901.07647v2,http://arxiv.org/pdf/1901.07647v2,"cs.LG, cs.AI, cs.CV, stat.ML"
Self-Attention Networks for Connectionist Temporal Classification in   Speech Recognition,"Julian Salazar, Katrin Kirchhoff, Zhiheng Huang","The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and downsampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.",2019-01-22T21:37:07Z,2019-02-19T10:12:52Z,http://arxiv.org/abs/1901.10055v2,http://arxiv.org/pdf/1901.10055v2,"eess.AS, cs.CL, cs.LG, cs.SD"
Partially Exchangeable Networks and Architectures for Learning Summary   Statistics in Approximate Bayesian Computation,"Samuel Wiqvist, Pierre-Alexandre Mattei, Umberto Picchini, Jes Frellsen","We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation. The DeepSets architecture is a special case of PEN and we can therefore also target fully exchangeable data. We employ PENs to learn summary statistics in approximate Bayesian computation (ABC). When comparing PENs to previous deep learning methods for learning summary statistics, our results are highly competitive, both considering time series and static models. Indeed, PENs provide more reliable posterior samples even when using less training data.",2019-01-29T11:31:31Z,2019-05-17T14:19:59Z,http://arxiv.org/abs/1901.10230v2,http://arxiv.org/pdf/1901.10230v2,"stat.ML, cs.LG, stat.CO"
Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph   Neural Networks in Molecular Graph Analysis,"Katsuhiko Ishiguro, Shin-ichi Maeda, Masanori Koyama","Graph Neural Network (GNN) is a popular architecture for the analysis of chemical molecules, and it has numerous applications in material and medicinal science. Current lines of GNNs developed for molecular analysis, however, do not fit well on the training set, and their performance does not scale well with the complexity of the network. In this paper, we propose an auxiliary module to be attached to a GNN that can boost the representation power of the model without hindering with the original GNN architecture. Our auxiliary module can be attached to a wide variety of GNNs, including those that are used commonly in biochemical applications. With our auxiliary architecture, the performances of many GNNs used in practice improve more consistently, achieving the state-of-the-art performance on popular molecular graph datasets.",2019-02-04T03:23:40Z,2019-05-24T08:00:31Z,http://arxiv.org/abs/1902.01020v4,http://arxiv.org/pdf/1902.01020v4,"cs.LG, stat.ML"
Human Computer Interaction Design for Mobile Devices Based on a Smart   Healthcare Architecture,"Pu Liu, Sidney Fels, Nicholas West, Matthias Görges","Smart and IoT-enabled mobile devices have the potential to enhance healthcare services for both patients and healthcare providers. Human computer interaction design is key to realizing a useful and usable connection between the users and these smart healthcare technologies. Appropriate design of such devices enhances the usability, improves effective operation in an integrated healthcare system, and facilitates the collaboration and information sharing between patients, healthcare providers, and institutions. In this paper, the concept of smart healthcare is introduced, including its four-layer information architecture of sensing, communication, data integration, and application. Human Computer Interaction design principles for smart healthcare mobile devices are outlined, based on user-centered design. These include: ensuring safety, providing error-resistant displays and alarms, supporting the unique relationship between patients and healthcare providers, distinguishing end-user groups, accommodating legacy devices, guaranteeing low latency, allowing for personalization, and ensuring patient privacy. Results are synthesized in design suggestions ranging from personas, scenarios, workflow, and information architecture, to prototyping, testing and iterative development. Finally, future developments in smart healthcare and Human Computer Interaction design for mobile health devices are outlined.",2019-02-10T05:49:53Z,2019-02-10T05:49:53Z,http://arxiv.org/abs/1902.03541v1,http://arxiv.org/pdf/1902.03541v1,"cs.HC, cs.CY, H.5.2; H.1.2"
An Optimized Recurrent Unit for Ultra-Low-Power Keyword Spotting,"Justice Amoh, Kofi Odame","There is growing interest in being able to run neural networks on sensors, wearables and internet-of-things (IoT) devices. However, the computational demands of neural networks make them difficult to deploy on resource-constrained edge devices.   To meet this need, our work introduces a new recurrent unit architecture that is specifically adapted for on-device low power acoustic event detection (AED). The proposed architecture is based on the gated recurrent unit (`GRU') but features optimizations that make it implementable on ultra-low power micro-controllers such as the Arm Cortex M0+.   Our new architecture, the Embedded Gated Recurrent Unit (eGRU) is demonstrated to be highly efficient and suitable for short-duration AED and keyword spotting tasks. A single eGRU cell is 60x faster and 10x smaller than a GRU cell. Despite its optimizations, eGRU compares well with GRU across tasks of varying complexities.   The practicality of eGRU is investigated in a wearable acoustic event detection application. An eGRU model is implemented and tested on the Arm Cortex M0-based Atmel ATSAMD21E18 processor. The Arm M0+ implementation of the eGRU model compares favorably with a full precision GRU that is running on a workstation. The embedded eGRU model achieves a classification accuracy 95.3%, which is only 2% less than the full precision GRU.",2019-02-13T17:41:15Z,2019-02-13T17:41:15Z,http://arxiv.org/abs/1902.05026v1,http://arxiv.org/pdf/1902.05026v1,"cs.LG, stat.ML"
Improving Dense Crowd Counting Convolutional Neural Networks using   Inverse k-Nearest Neighbor Maps and Multiscale Upsampling,"Greg Olmschenk, Hao Tang, Zhigang Zhu","Gatherings of thousands to millions of people frequently occur for an enormous variety of events, and automated counting of these high-density crowds is useful for safety, management, and measuring significance of an event. In this work, we show that the regularly accepted labeling scheme of crowd density maps for training deep neural networks is less effective than our alternative inverse k-nearest neighbor (i$k$NN) maps, even when used directly in existing state-of-the-art network structures. We also provide a new network architecture MUD-i$k$NN, which uses multi-scale upsampling via transposed convolutions to take full advantage of the provided i$k$NN labeling. This upsampling combined with the i$k$NN maps further improves crowd counting accuracy. Our new network architecture performs favorably in comparison with the state-of-the-art. However, our labeling and upsampling techniques are generally applicable to existing crowd counting architectures.",2019-01-31T22:05:47Z,2019-03-29T20:59:03Z,http://arxiv.org/abs/1902.05379v3,http://arxiv.org/pdf/1902.05379v3,"cs.CV, cs.LG, stat.ML"
Neural-encoding Human Experts' Domain Knowledge to Warm Start   Reinforcement Learning,"Andrew Silva, Matthew Gombolay","Deep reinforcement learning has been successful in a variety of tasks, such as game playing and robotic manipulation. However, attempting to learn \textit{tabula rasa} disregards the logical structure of many domains as well as the wealth of readily available knowledge from domain experts that could help ""warm start"" the learning process. We present a novel reinforcement learning technique that allows for intelligent initialization of a neural network weights and architecture. Our approach permits the encoding domain knowledge directly into a neural decision tree, and improves upon that knowledge with policy gradient updates. We empirically validate our approach on two OpenAI Gym tasks and two modified StarCraft 2 tasks, showing that our novel architecture outperforms multilayer-perceptron and recurrent architectures. Our knowledge-based framework finds superior policies compared to imitation learning-based and prior knowledge-based approaches. Importantly, we demonstrate that our approach can be used by untrained humans to initially provide >80% increase in expected reward relative to baselines prior to training (p < 0.001), which results in a >60% increase in expected reward after policy optimization (p = 0.011).",2019-02-15T23:28:59Z,2020-09-23T22:17:29Z,http://arxiv.org/abs/1902.06007v4,http://arxiv.org/pdf/1902.06007v4,"cs.LG, cs.AI, stat.ML"
Towards Non-saturating Recurrent Units for Modelling Long-term   Dependencies,"Sarath Chandar, Chinnadhurai Sankar, Eugene Vorontsov, Samira Ebrahimi Kahou, Yoshua Bengio","Modelling long-term dependencies is a challenge for recurrent neural networks. This is primarily due to the fact that gradients vanish during training, as the sequence length increases. Gradients can be attenuated by transition operators and are attenuated or dropped by activation functions. Canonical architectures like LSTM alleviate this issue by skipping information through a memory mechanism. We propose a new recurrent architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but forgoes both saturating activation functions and saturating gates, in order to further alleviate vanishing gradients. In a series of synthetic and real world tasks, we demonstrate that the proposed model is the only model that performs among the top 2 models across all tasks with and without long-term dependencies, when compared against a range of other architectures.",2019-01-22T15:24:27Z,2019-01-22T15:24:27Z,http://arxiv.org/abs/1902.06704v1,http://arxiv.org/pdf/1902.06704v1,"cs.NE, cs.LG, stat.ML"
Learning Implicitly Recurrent CNNs Through Parameter Sharing,"Pedro Savarese, Michael Maire","We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy.   Our simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the design aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.   Our hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias. Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.",2019-02-26T02:02:09Z,2019-03-13T19:36:06Z,http://arxiv.org/abs/1902.09701v2,http://arxiv.org/pdf/1902.09701v2,"cs.LG, stat.ML"
A Deep DUAL-PATH Network for Improved Mammogram Image Processing,"Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies, Dave Laurenson","We present, for the first time, a novel deep neural network architecture called \dcn with a dual-path connection between the input image and output class label for mammogram image processing. This architecture is built upon U-Net, which non-linearly maps the input data into a deep latent space. One path of the \dcnn, the locality preserving learner, is devoted to hierarchically extracting and exploiting intrinsic features of the input, while the other path, called the conditional graph learner, focuses on modeling the input-mask correlations. The learned mask is further used to improve classification results, and the two learning paths complement each other. By integrating the two learners our new architecture provides a simple but effective way to jointly learn the segmentation and predict the class label. Benefiting from the powerful expressive capacity of deep neural networks a more discriminative representation can be learned, in which both the semantics and structure are well preserved. Experimental results show that \dcn achieves the best mammography segmentation and classification simultaneously, outperforming recent state-of-the-art models.",2019-03-01T11:51:47Z,2019-03-01T11:51:47Z,http://arxiv.org/abs/1903.00001v1,http://arxiv.org/pdf/1903.00001v1,"cs.CV, cs.LG, stat.ML"
Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space,"Zhou Fan, Rui Su, Weinan Zhang, Yong Yu","In this paper we propose a hybrid architecture of actor-critic algorithms for reinforcement learning in parameterized action space, which consists of multiple parallel sub-actor networks to decompose the structured action space into simpler action spaces along with a critic network to guide the training of all sub-actor networks. While this paper is mainly focused on parameterized action space, the proposed architecture, which we call hybrid actor-critic, can be extended for more general action spaces which has a hierarchical structure. We present an instance of the hybrid actor-critic architecture based on proximal policy optimization (PPO), which we refer to as hybrid proximal policy optimization (H-PPO). Our experiments test H-PPO on a collection of tasks with parameterized action space, where H-PPO demonstrates superior performance over previous methods of parameterized action reinforcement learning.",2019-03-04T16:33:15Z,2019-05-30T13:02:58Z,http://arxiv.org/abs/1903.01344v3,http://arxiv.org/pdf/1903.01344v3,"cs.LG, cs.AI, stat.ML"
A Novel Monocular Disparity Estimation Network with Domain   Transformation and Ambiguity Learning,"Juan Luis Gonzalez Bello, Munchurl Kim","Convolutional neural networks (CNN) have shown state-of-the-art results for low-level computer vision problems such as stereo and monocular disparity estimations, but still, have much room to further improve their performance in terms of accuracy, numbers of parameters, etc. Recent works have uncovered the advantages of using an unsupervised scheme to train CNN's to estimate monocular disparity, where only the relatively-easy-to-obtain stereo images are needed for training. We propose a novel encoder-decoder architecture that outperforms previous unsupervised monocular depth estimation networks by (i) taking into account ambiguities, (ii) efficient fusion between encoder and decoder features with rectangular convolutions and (iii) domain transformations between encoder and decoder. Our architecture outperforms the Monodepth baseline in all metrics, even with a considerable reduction of parameters. Furthermore, our architecture is capable of estimating a full disparity map in a single forward pass, whereas the baseline needs two passes. We perform extensive experiments to verify the effectiveness of our method on the KITTI dataset.",2019-03-20T14:20:35Z,2019-03-20T14:20:35Z,http://arxiv.org/abs/1903.08514v1,http://arxiv.org/pdf/1903.08514v1,"eess.IV, cs.LG"
Bandwidth Extension on Raw Audio via Generative Adversarial Networks,"Sung Kim, Visvesh Sathe","Neural network-based methods have recently demonstrated state-of-the-art results on image synthesis and super-resolution tasks, in particular by using variants of generative adversarial networks (GANs) with supervised feature losses. Nevertheless, previous feature loss formulations rely on the availability of large auxiliary classifier networks, and labeled datasets that enable such classifiers to be trained. Furthermore, there has been comparatively little work to explore the applicability of GAN-based methods to domains other than images and video. In this work we explore a GAN-based method for audio processing, and develop a convolutional neural network architecture to perform audio super-resolution. In addition to several new architectural building blocks for audio processing, a key component of our approach is the use of an autoencoder-based loss that enables training in the GAN framework, with feature losses derived from unlabeled data. We explore the impact of our architectural choices, and demonstrate significant improvements over previous works in terms of both objective and perceptual quality.",2019-03-21T14:32:02Z,2019-03-21T14:32:02Z,http://arxiv.org/abs/1903.09027v1,http://arxiv.org/pdf/1903.09027v1,"cs.SD, eess.AS"
Scheduling Algorithms for 5G Networks with Mid-haul Capacity Constraints,"Abhishek Sinha, Matthew Andrews, Prasanth Ananth","We consider a virtualized RAN architecture for 5G networks where the Remote Units are connected to a central unit via a mid-haul. To support high data rates, the midhaul is realized with a Passive Optical Network (PON). In this architecture, the data are stored at the central unit until the scheduler decides to transmit it through the mid-haul to an appropriate remote unit, and then over the air at the same slot. We study an optimal scheduling problem that arises in this context. This problem has two key features. First, multiple cells must be scheduled simultaneously for efficient operation. Second, the interplay between the time-varying wireless interface rates and the fixed capacity PON needs to be handled efficiently. In this paper, we take a comprehensive look at this resource allocation problem by formulating it as a utility-maximization problem. Using combinatorial techniques, we derive useful structural properties of the optimal allocation and utilize these results to design polynomial-time approximation algorithms and a pseudopolynomial-time optimal algorithm. Finally, we numerically compare the performance of the proposed algorithms to heuristics which are natural generalizations of the ubiquitous Proportional Fair algorithm.",2019-03-27T07:19:59Z,2019-03-27T07:19:59Z,http://arxiv.org/abs/1903.11270v1,http://arxiv.org/pdf/1903.11270v1,"cs.NI, cs.DM, math.OC"
Content Adaptive Optimization for Neural Image Compression,"Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah, Christopher Schroers","The field of neural image compression has witnessed exciting progress as recently proposed architectures already surpass the established transform coding based approaches. While, so far, research has mainly focused on architecture and model improvements, in this work we explore content adaptive optimization. To this end, we introduce an iterative procedure which adapts the latent representation to the specific content we wish to compress while keeping the parameters of the network and the predictive model fixed. Our experiments show that this allows for an overall increase in rate-distortion performance, independently of the specific architecture used. Furthermore, we also evaluate this strategy in the context of adapting a pretrained network to other content that is different in visual appearance or resolution. Here, our experiments show that our adaptation strategy can largely close the gap as compared to models specifically trained for the given content while having the benefit that no additional data in the form of model parameter updates has to be transmitted.",2019-06-04T06:39:50Z,2019-06-05T07:21:47Z,http://arxiv.org/abs/1906.01223v2,http://arxiv.org/pdf/1906.01223v2,"cs.CV, eess.IV"
Architecture Selection via the Trade-off Between Accuracy and Robustness,"Zhun Deng, Cynthia Dwork, Jialiang Wang, Yao Zhao","We provide a general framework for characterizing the trade-off between accuracy and robustness in supervised learning. We propose a method and define quantities to characterize the trade-off between accuracy and robustness for a given architecture, and provide theoretical insight into the trade-off. Specifically we introduce a simple trade-off curve, define and study an influence function that captures the sensitivity, under adversarial attack, of the optima of a given loss function. We further show how adversarial training regularizes the parameters in an over-parameterized linear model, recovering the LASSO and ridge regression as special cases, which also allows us to theoretically analyze the behavior of the trade-off curve. In experiments, we demonstrate the corresponding trade-off curves of neural networks and how they vary with respect to factors such as number of layers, neurons, and across different network structures. Such information provides a useful guideline to architecture selection.",2019-06-04T11:36:16Z,2019-06-04T11:36:16Z,http://arxiv.org/abs/1906.01354v1,http://arxiv.org/pdf/1906.01354v1,"cs.LG, stat.ML"
Neuromorphic Architecture Optimization for Task-Specific Dynamic   Learning,"Sandeep Madireddy, Angel Yanguas-Gil, Prasanna Balaprakash","The ability to learn and adapt in real time is a central feature of biological systems. Neuromorphic architectures demonstrating such versatility can greatly enhance our ability to efficiently process information at the edge. A key challenge, however, is to understand which learning rules are best suited for specific tasks and how the relevant hyperparameters can be fine-tuned. In this work, we introduce a conceptual framework in which the learning process is integrated into the network itself. This allows us to cast meta-learning as a mathematical optimization problem. We employ DeepHyper, a scalable, asynchronous model-based search, to simultaneously optimize the choice of meta-learning rules and their hyperparameters. We demonstrate our approach with two different datasets, MNIST and FashionMNIST, using a network architecture inspired by the learning center of the insect brain. Our results show that optimal learning rules can be dataset-dependent even within similar tasks. This dependency demonstrates the importance of introducing versatility and flexibility in the learning algorithms. It also illuminates experimental findings in insect neuroscience that have shown a heterogeneity of learning rules within the insect mushroom body.",2019-06-04T18:20:23Z,2019-06-04T18:20:23Z,http://arxiv.org/abs/1906.01668v1,http://arxiv.org/pdf/1906.01668v1,"cs.LG, cs.NE, stat.ML"
AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,"Wei Wen, Feng Yan, Yiran Chen, Hai Li","Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN. Our code is available at https://github.com/wenwei202/autogrow.",2019-06-07T05:54:41Z,2020-06-15T18:09:02Z,http://arxiv.org/abs/1906.02909v5,http://arxiv.org/pdf/1906.02909v5,"cs.LG, cs.CV, cs.NE, stat.ML, I.2.6"
Strategies to architect AI Safety: Defense to guard AI from Adversaries,"Rajagopal. A, Nirmala. V","The impact of designing for security of AI is critical for humanity in the AI era. With humans increasingly becoming dependent upon AI, there is a need for neural networks that work reliably, inspite of Adversarial attacks. The vision for Safe and secure AI for popular use is achievable. To achieve safety of AI, this paper explores strategies and a novel deep learning architecture. To guard AI from adversaries, paper explores combination of 3 strategies:   1. Introduce randomness at inference time to hide the representation learning from adversaries.   2. Detect presence of adversaries by analyzing the sequence of inferences.   3. Exploit visual similarity.   To realize these strategies, this paper designs a novel architecture, Dynamic Neural Defense, DND. This defense has 3 deep learning architectural features:   1. By hiding the way a neural network learns from exploratory attacks using a random computation graph, DND evades attack.   2. By analyzing input sequence to cloud AI inference engine with LSTM, DND detects attack sequence.   3. By inferring with visual similar inputs generated by VAE, any AI defended by DND approach does not succumb to hackers.   Thus, a roadmap to develop reliable, safe and secure AI is presented.",2019-06-08T14:34:47Z,2019-06-08T14:34:47Z,http://arxiv.org/abs/1906.03466v1,http://arxiv.org/pdf/1906.03466v1,"cs.AI, cs.CR, cs.CV, I.2.0"
Convolutional Bipartite Attractor Networks,"Michael Iuzzolino, Yoram Singer, Michael C. Mozer","In human perception and cognition, a fundamental operation that brains perform is interpretation: constructing coherent neural states from noisy, incomplete, and intrinsically ambiguous evidence. The problem of interpretation is well matched to an early and often overlooked architecture, the attractor network---a recurrent neural net that performs constraint satisfaction, imputation of missing features, and clean up of noisy data via energy minimization dynamics. We revisit attractor nets in light of modern deep learning methods and propose a convolutional bipartite architecture with a novel training loss, activation function, and connectivity constraints. We tackle larger problems than have been previously explored with attractor nets and demonstrate their potential for image completion and super-resolution. We argue that this architecture is better motivated than ever-deeper feedforward models and is a viable alternative to more costly sampling-based generative methods on a range of supervised and unsupervised tasks.",2019-06-08T19:13:26Z,2019-09-26T23:39:37Z,http://arxiv.org/abs/1906.03504v3,http://arxiv.org/pdf/1906.03504v3,"cs.LG, cs.NE, stat.ML"
Neural Network Models for Stock Selection Based on Fundamental Analysis,"Yuxuan Huang, Luiz Fernando Capretz, Danny Ho","Application of neural network architectures for financial prediction has been actively studied in recent years. This paper presents a comparative study that investigates and compares feed-forward neural network (FNN) and adaptive neural fuzzy inference system (ANFIS) on stock prediction using fundamental financial ratios. The study is designed to evaluate the performance of each architecture based on the relative return of the selected portfolios with respect to the benchmark stock index. The results show that both architectures possess the ability to separate winners and losers from a sample universe of stocks, and the selected portfolios outperform the benchmark. Our study argues that FNN shows superior performance over ANFIS.",2019-06-12T18:57:50Z,2019-06-12T18:57:50Z,http://arxiv.org/abs/1906.05327v1,http://arxiv.org/pdf/1906.05327v1,"q-fin.ST, cs.LG"
PredNet and Predictive Coding: A Critical Review,"Roshan Rane, Edit Szügyi, Vageesh Saxena, André Ofner, Sebastian Stober","PredNet, a deep predictive coding network developed by Lotter et al., combines a biologically inspired architecture based on the propagation of prediction error with self-supervised representation learning in video. While the architecture has drawn a lot of attention and various extensions of the model exist, there is a lack of a critical analysis. We fill in the gap by evaluating PredNet both as an implementation of the predictive coding theory and as a self-supervised video prediction model using a challenging video action classification dataset. We design an extended model to test if conditioning future frame predictions on the action class of the video improves the model performance. We show that PredNet does not yet completely follow the principles of predictive coding. The proposed top-down conditioning leads to a performance gain on synthetic data, but does not scale up to the more complex real-world action classification dataset. Our analysis is aimed at guiding future research on similar architectures based on the predictive coding theory.",2019-06-14T21:58:00Z,2020-05-18T09:52:22Z,http://arxiv.org/abs/1906.11902v3,http://arxiv.org/pdf/1906.11902v3,"cs.CV, cs.LG, eess.IV, stat.ML"
Convolution Based Spectral Partitioning Architecture for Hyperspectral   Image Classification,"Ringo S. W. Chu, Ho-Cheung Ng, Xiwei Wang, Wayne Luk","Hyperspectral images (HSIs) can distinguish materials with high number of spectral bands, which is widely adopted in remote sensing applications and benefits in high accuracy land cover classifications. However, HSIs processing are tangled with the problem of high dimensionality and limited amount of labelled data. To address these challenges, this paper proposes a deep learning architecture using three dimensional convolutional neural networks with spectral partitioning to perform effective feature extraction. We conduct experiments using Indian Pines and Salinas scenes acquired by NASA Airborne Visible/Infra-Red Imaging Spectrometer. In comparison to prior results, our architecture shows competitive performance for classification results over current methods.",2019-06-27T22:06:15Z,2019-06-27T22:06:15Z,http://arxiv.org/abs/1906.11981v1,http://arxiv.org/pdf/1906.11981v1,"cs.CV, cs.LG, eess.IV"
Adversarial Pixel-Level Generation of Semantic Images,"Emanuele Ghelfi, Paolo Galeone, Michele De Simoni, Federico Di Mattia","Generative Adversarial Networks (GANs) have obtained extraordinary success in the generation of realistic images, a domain where a lower pixel-level accuracy is acceptable. We study the problem, not yet tackled in the literature, of generating semantic images starting from a prior distribution. Intuitively this problem can be approached using standard methods and architectures. However, a better-suited approach is needed to avoid generating blurry, hallucinated and thus unusable images since tasks like semantic segmentation require pixel-level exactness. In this work, we present a novel architecture for learning to generate pixel-level accurate semantic images, namely Semantic Generative Adversarial Networks (SemGANs). The experimental evaluation shows that our architecture outperforms standard ones from both a quantitative and a qualitative point of view in many semantic image generation tasks.",2019-06-27T14:25:11Z,2019-06-27T14:25:11Z,http://arxiv.org/abs/1906.12195v1,http://arxiv.org/pdf/1906.12195v1,"cs.CV, cs.LG, eess.IV, stat.ML"
Semi Supervised Phrase Localization in a Bidirectional Caption-Image   Retrieval Framework,"Deepan Das, Noor Mohammed Ghouse, Shashank Verma, Yin Li","We introduce a novel deep neural network architecture that links visual regions to corresponding textual segments including phrases and words. To accomplish this task, our architecture makes use of the rich semantic information available in a joint embedding space of multi-modal data. From this joint embedding space, we extract the associative localization maps that develop naturally, without explicitly providing supervision during training for the localization task. The joint space is learned using a bidirectional ranking objective that is optimized using a $N$-Pair loss formulation. This training mechanism demonstrates the idea that localization information is learned inherently while optimizing a Bidirectional Retrieval objective. The model's retrieval and localization performance is evaluated on MSCOCO and Flickr30K Entities datasets. This architecture outperforms the state of the art results in the semi-supervised phrase localization setting.",2019-08-08T07:01:26Z,2019-08-08T07:01:26Z,http://arxiv.org/abs/1908.02950v1,http://arxiv.org/pdf/1908.02950v1,"cs.CV, eess.IV"
Unsupervised Neural Quantization for Compressed-Domain Similarity Search,"Stanislav Morozov, Artem Babenko","We tackle the problem of unsupervised visual descriptors compression, which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines, the existing state-of-the-art compression methods employ shallow architectures, and we aim to close this gap by our paper. In more detail, we introduce a DNN architecture for the unsupervised compressed-domain retrieval, based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin.",2019-08-11T10:46:16Z,2019-08-11T10:46:16Z,http://arxiv.org/abs/1908.03883v1,http://arxiv.org/pdf/1908.03883v1,"cs.LG, cs.CV, stat.ML"
Reinforcement Learning based Interconnection Routing for Adaptive   Traffic Optimization,"Sheng-Chun Kao, Chao-Han Huck Yang, Pin-Yu Chen, Xiaoli Ma, Tushar Krishna","Applying Machine Learning (ML) techniques to design and optimize computer architectures is a promising research direction. Optimizing the runtime performance of a Network-on-Chip (NoC) necessitates a continuous learning framework. In this work, we demonstrate the promise of applying reinforcement learning (RL) to optimize NoC runtime performance. We present three RL-based methods for learning optimal routing algorithms. The experimental results show the algorithms can successfully learn a near-optimal solution across different environment states. Reproducible Code: github.com/huckiyang/interconnect-routing-gym",2019-08-13T04:35:40Z,2019-08-13T04:35:40Z,http://arxiv.org/abs/1908.04484v1,http://arxiv.org/pdf/1908.04484v1,"cs.NI, cs.AI, cs.AR, cs.LG, cs.SY, eess.SY"
Experimental Analysis of Harvested Energy and Throughput Trade-off in a   Realistic SWIPT System,"Junghoon Kim, Bruno Clerckx, Paul D. Mitcheson","We build a realistic Simultaneous Wireless Information and Power Transfer (SWIPT) prototype and experimentally analyse the harvested energy and throughput trade-off. Both time-switching and power splitting receiver architectures are implemented, and the performance comparison is carried out. Systematic SWIPT transmission signal design methods are also considered and implemented on the prototype. The harvested energy-throughput (E-T) performance with different transmission signal designs, modulation schemes, and receiver architectures are evaluated and compared. The combination of the power splitting receiver architecture and the superposition transmission signal design technique shows significant expansion of the E-T region. The experimental results fully validate the observations predicted from the theoretical signal designs and confirm the benefits of systematic signal designs on the system performance. The observations give important insights on how to design a practical SWIPT system.",2019-08-22T09:20:58Z,2019-08-22T09:20:58Z,http://arxiv.org/abs/1908.08272v1,http://arxiv.org/pdf/1908.08272v1,"cs.IT, math.IT"
Assessing Knee OA Severity with CNN attention-based end-to-end   architectures,"Marc Górriz, Joseph Antony, Kevin McGuinness, Xavier Giró-i-Nieto, Noel E. O'Connor","This work proposes a novel end-to-end convolutional neural network (CNN) architecture to automatically quantify the severity of knee osteoarthritis (OA) using X-Ray images, which incorporates trainable attention modules acting as unsupervised fine-grained detectors of the region of interest (ROI). The proposed attention modules can be applied at different levels and scales across any CNN pipeline helping the network to learn relevant attention patterns over the most informative parts of the image at different resolutions. We test the proposed attention mechanism on existing state-of-the-art CNN architectures as our base models, achieving promising results on the benchmark knee OA datasets from the osteoarthritis initiative (OAI) and multicenter osteoarthritis study (MOST). All code from our experiments will be publicly available on the github repository: https://github.com/marc-gorriz/KneeOA-CNNAttention",2019-08-23T14:59:52Z,2019-08-23T14:59:52Z,http://arxiv.org/abs/1908.08856v1,http://arxiv.org/pdf/1908.08856v1,"eess.IV, cs.CV, cs.LG"
End-to-End Conditional GAN-based Architectures for Image Colourisation,"Marc Górriz, Marta Mrak, Alan F. Smeaton, Noel E. O'Connor","In this work recent advances in conditional adversarial networks are investigated to develop an end-to-end architecture based on Convolutional Neural Networks (CNNs) to directly map realistic colours to an input greyscale image. Observing that existing colourisation methods sometimes exhibit a lack of colourfulness, this paper proposes a method to improve colourisation results. In particular, the method uses Generative Adversarial Neural Networks (GANs) and focuses on improvement of training stability to enable better generalisation in large multi-class image datasets. Additionally, the integration of instance and batch normalisation layers in both generator and discriminator is introduced to the popular U-Net architecture, boosting the network capabilities to generalise the style changes of the content. The method has been tested using the ILSVRC 2012 dataset, achieving improved automatic colourisation results compared to other methods based on GANs.",2019-08-26T18:29:22Z,2019-09-05T12:05:53Z,http://arxiv.org/abs/1908.09873v2,http://arxiv.org/pdf/1908.09873v2,"eess.IV, cs.CV, cs.LG"
Fashion Image Retrieval with Capsule Networks,"Furkan Kınlı, Barış Özcan, Furkan Kıraç","In this study, we investigate in-shop clothing retrieval performance of densely-connected Capsule Networks with dynamic routing. To achieve this, we propose Triplet-based design of Capsule Network architecture with two different feature extraction methods. In our design, Stacked-convolutional (SC) and Residual-connected (RC) blocks are used to form the input of capsule layers. Experimental results show that both of our designs outperform all variants of the baseline study, namely FashionNet, without relying on the landmark information. Moreover, when compared to the SOTA architectures on clothing retrieval, our proposed Triplet Capsule Networks achieve comparable recall rates only with half of parameters used in the SOTA architectures.",2019-08-26T22:33:14Z,2019-08-26T22:33:14Z,http://arxiv.org/abs/1908.09943v1,http://arxiv.org/pdf/1908.09943v1,"cs.CV, cs.LG, stat.ML"
Global Planar Convolutions for improved context aggregation in Brain   Tumor Segmentation,"Santi Puch, Irina Sánchez, Aura Hernández, Gemma Piella, Vesna Prchkovska","In this work, we introduce the Global Planar Convolution module as a building-block for fully-convolutional networks that aggregates global information and, therefore, enhances the context perception capabilities of segmentation networks in the context of brain tumor segmentation. We implement two baseline architectures (3D UNet and a residual version of 3D UNet, ResUNet) and present a novel architecture based on these two architectures, ContextNet, that includes the proposed Global Planar Convolution module. We show that the addition of such module eliminates the need of building networks with several representation levels, which tend to be over-parametrized and to showcase slow rates of convergence. Furthermore, we provide a visual demonstration of the behavior of GPC modules via visualization of intermediate representations. We finally participate in the 2018 edition of the BraTS challenge with our best performing models, that are based on ContextNet, and report the evaluation scores on the validation and the test sets of the challenge.",2019-08-27T15:38:50Z,2019-08-27T15:38:50Z,http://arxiv.org/abs/1908.10281v1,http://arxiv.org/pdf/1908.10281v1,"eess.IV, cs.CV, stat.ML"
Multiresolution Transformer Networks: Recurrence is Not Essential for   Modeling Hierarchical Structure,"Vikas K. Garg, Inderjit S. Dhillon, Hsiang-Fu Yu","The architecture of Transformer is based entirely on self-attention, and has been shown to outperform models that employ recurrence on sequence transduction tasks such as machine translation. The superior performance of Transformer has been attributed to propagating signals over shorter distances, between positions in the input and the output, compared to the recurrent architectures. We establish connections between the dynamics in Transformer and recurrent networks to argue that several factors including gradient flow along an ensemble of multiple weakly dependent paths play a paramount role in the success of Transformer. We then leverage the dynamics to introduce {\em Multiresolution Transformer Networks} as the first architecture that exploits hierarchical structure in data via self-attention. Our models significantly outperform state-of-the-art recurrent and hierarchical recurrent models on two real-world datasets for query suggestion, namely, \aol and \amazon. In particular, on AOL data, our model registers at least 20\% improvement on each precision score, and over 25\% improvement on the BLEU score with respect to the best performing recurrent model. We thus provide strong evidence that recurrence is not essential for modeling hierarchical structure.",2019-08-27T18:51:50Z,2019-08-27T18:51:50Z,http://arxiv.org/abs/1908.10408v1,http://arxiv.org/pdf/1908.10408v1,"cs.LG, cs.IR, stat.ML"
Automated Architecture Design for Deep Neural Networks,Steven Abreu,"Machine learning has made tremendous progress in recent years and received large amounts of public attention. Though we are still far from designing a full artificially intelligent agent, machine learning has brought us many applications in which computers solve human learning tasks remarkably well. Much of this progress comes from a recent trend within machine learning, called deep learning. Deep learning models are responsible for many state-of-the-art applications of machine learning. Despite their success, deep learning models are hard to train, very difficult to understand, and often times so complex that training is only possible on very large GPU clusters. Lots of work has been done on enabling neural networks to learn efficiently. However, the design and architecture of such neural networks is often done manually through trial and error and expert knowledge. This thesis inspects different approaches, existing and novel, to automate the design of deep feedforward neural networks in an attempt to create less complex models with good performance that take away the burden of deciding on an architecture and make it more efficient to design and train such deep networks.",2019-08-22T00:57:45Z,2019-08-22T00:57:45Z,http://arxiv.org/abs/1908.10714v1,http://arxiv.org/pdf/1908.10714v1,"cs.LG, cs.NE, stat.ML"
Multi-Agent Coordination of Thermostatically Controlled Loads by Smart   Power Sockets for Electric Demand Side Management,"Mauro Franceschelli, Alessandro Pilloni, Andrea Gasparri","This paper presents a multi-agent control architecture and an online optimization method based on dynamic average consensus to coordinate the power consumption of a large population of Thermostatically Controlled Loads (TCLs). Our objective is to penalize peaks of power demand, smooth the load profile and enable Demand Side Management (DSM). The proposed architecture and methods exploit only local measurements of power consumption via Smart Power Sockets (SPSs) with no access to their internal temperature. No centralized aggregator of information is exploited and agents preserve their privacy by cooperating anonymously only through consensus-based distributed estimation, robust to node/link failure. The interactions among devices are designed to occur through an unstructured peer-to-peer (P2P) network over the internet. The architecture includes novel methods for parameter identification, state estimation and mixed logical modelling of TCLs and SPSs. It is designed from a multi-agent and plug-and-play perspective in which existing household appliances can interact with each other in an urban environment. Finally, a novel low cost testbed is proposed along with numerical tests and an experimental validation.",2019-08-29T16:01:08Z,2019-08-29T16:01:08Z,http://arxiv.org/abs/1908.11318v1,http://arxiv.org/pdf/1908.11318v1,"eess.SY, cs.SY"
Neural Architecture Search for Joint Optimization of Predictive Power   and Biological Knowledge,"Zijun Zhang, Linqi Zhou, Liangke Gou, Ying Nian Wu","We report a neural architecture search framework, BioNAS, that is tailored for biomedical researchers to easily build, evaluate, and uncover novel knowledge from interpretable deep learning models. The introduction of knowledge dissimilarity functions in BioNAS enables the joint optimization of predictive power and biological knowledge through searching architectures in a model space. By optimizing the consistency with existing knowledge, we demonstrate that BioNAS optimal models reveal novel knowledge in both simulated data and in real data of functional genomics. BioNAS provides a useful tool for domain experts to inject their prior belief into automated machine learning and therefore making deep learning easily accessible to practitioners. BioNAS is available at https://github.com/zj-zhang/BioNAS-pub.",2019-09-01T07:00:21Z,2019-09-01T07:00:21Z,http://arxiv.org/abs/1909.00337v1,http://arxiv.org/pdf/1909.00337v1,"stat.ML, cs.LG, q-bio.GN"
Poly-GAN: Multi-Conditioned GAN for Fashion Synthesis,"Nilesh Pandey, Andreas Savakis","We present Poly-GAN, a novel conditional GAN architecture that is motivated by Fashion Synthesis, an application where garments are automatically placed on images of human models at an arbitrary pose. Poly-GAN allows conditioning on multiple inputs and is suitable for many tasks, including image alignment, image stitching, and inpainting. Existing methods have a similar pipeline where three different networks are used to first align garments with the human pose, then perform stitching of the aligned garment and finally refine the results. Poly-GAN is the first instance where a common architecture is used to perform all three tasks. Our novel architecture enforces the conditions at all layers of the encoder and utilizes skip connections from the coarse layers of the encoder to the respective layers of the decoder. Poly-GAN is able to perform a spatial transformation of the garment based on the RGB skeleton of the model at an arbitrary pose. Additionally, Poly-GAN can perform image stitching, regardless of the garment orientation, and inpainting on the garment mask when it contains irregular holes. Our system achieves state-of-the-art quantitative results on Structural Similarity Index metric and Inception Score metric using the DeepFashion dataset.",2019-09-05T00:29:39Z,2019-09-05T00:29:39Z,http://arxiv.org/abs/1909.02165v1,http://arxiv.org/pdf/1909.02165v1,"cs.CV, cs.GR, eess.IV"
Analysis of UAV Communications in Cell-Free Massive MIMO systems,"Carmen D'Andrea, Adrian Garcia-Rodriguez, Giovanni Geraci, Lorenzo Galati Giordano, Stefano Buzzi","We study support for unmanned aerial vehicle (UAV) communications through a cell-free massive MIMO architecture, wherein a large number of access points (APs) is deployed in place of large co-located massive MIMO arrays. We consider also a variation of the pure cell-free architecture by applying a user-centric association approach, where each user is served only from a subset of APs in the network. Under the general assumption that the propagation channel between the mobile stations, either UAVs or ground users (GUEs), and the APs follows a Ricean distribution, we derive closed-form spectral efficiency lower bounds for uplink and downlink with linear minimum mean square error channel estimation. We consider several power allocation and user scheduling strategies for such a system, and, among these, also minimum-rate maximizing power allocation strategies to improve the system fairness. Our numerical results reveal that cell-free massive MIMO architecture and its low-complexity user-centric alternative may provide better performance than a traditional multi-cell massive MIMO network deployment.",2019-09-05T15:37:00Z,2020-01-10T14:58:30Z,http://arxiv.org/abs/1909.02485v2,http://arxiv.org/pdf/1909.02485v2,"cs.IT, eess.SP, math.IT"
TMA: Tera-MACs/W Neural Hardware Inference Accelerator with a   Multiplier-less Massive Parallel Processor,"Hyunbin Park, Dohyun Kim, Shiho Kim","Computationally intensive Inference tasks of Deep neural networks have enforced revolution of new accelerator architecture to reduce power consumption as well as latency. The key figure of merit in hardware inference accelerators is the number of multiply-and-accumulation operations per watt (MACs/W), where, the state-of-the-arts MACs/W remains several hundreds Giga-MACs/W. We propose a Tera-MACS/W neural hardware inference Accelerator (TMA) with 8-bit activations and scalable integer weights less than 1-byte. The architectures main feature is configurable neural processing element for matrix-vector operations. The proposed neural processing element has Multiplier-less Massive Parallel Processor to work without any multiplications, which makes it attractive for energy efficient high-performance neural network applications. We benchmark our systems latency, power, and performance using Alexnet trained on ImageNet. Finally, we compared our accelerators throughput and power consumption to the prior works. The proposed accelerator outperforms the state of the art in terms of energy and area achieving 2.3 TMACS/W@1.0 V, 65 nm CMOS technology.",2019-09-08T14:18:13Z,2019-09-08T14:18:13Z,http://arxiv.org/abs/1909.04551v1,http://arxiv.org/pdf/1909.04551v1,"cs.DC, cs.AR, eess.SP"
Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data,"Sergei Popov, Stanislav Morozov, Artem Babenko","Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",2019-09-13T16:11:28Z,2019-09-19T13:30:23Z,http://arxiv.org/abs/1909.06312v2,http://arxiv.org/pdf/1909.06312v2,"cs.LG, stat.ML"
"Comparison of UNet, ENet, and BoxENet for Segmentation of Mast Cells in   Scans of Histological Slices","Alexander Karimov, Artem Razumov, Ruslana Manbatchurina, Ksenia Simonova, Irina Donets, Anastasia Vlasova, Yulia Khramtsova, Konstantin Ushenin","Deep neural networks show high accuracy in theproblem of semantic and instance segmentation of biomedicaldata. However, this approach is computationally expensive. Thecomputational cost may be reduced with network simplificationafter training or choosing the proper architecture, which providessegmentation with less accuracy but does it much faster. In thepresent study, we analyzed the accuracy and performance ofUNet and ENet architectures for the problem of semantic imagesegmentation. In addition, we investigated the ENet architecture by replacing of some convolution layers with box-convolutionlayers. The analysis performed on the original dataset consisted of histology slices with mast cells. These cells provide a region forsegmentation with different types of borders, which vary fromclearly visible to ragged. ENet was less accurate than UNet byonly about 1-2%, but ENet performance was 8-15 times faster than UNet one.",2019-09-15T17:26:56Z,2019-11-22T16:14:21Z,http://arxiv.org/abs/1909.06840v3,http://arxiv.org/pdf/1909.06840v3,"eess.IV, cs.CV, cs.LG"
Interpolation-Prediction Networks for Irregularly Sampled Time Series,"Satya Narayan Shukla, Benjamin M. Marlin","In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.",2019-09-13T21:18:06Z,2019-09-13T21:18:06Z,http://arxiv.org/abs/1909.07782v1,http://arxiv.org/pdf/1909.07782v1,"cs.LG, stat.ML"
Learning Sparse Mixture of Experts for Visual Question Answering,"Vardaan Pahuja, Jie Fu, Christopher J. Pal","There has been a rapid progress in the task of Visual Question Answering with improved model architectures. Unfortunately, these models are usually computationally intensive due to their sheer size which poses a serious challenge for deployment. We aim to tackle this issue for the specific task of Visual Question Answering (VQA). A Convolutional Neural Network (CNN) is an integral part of the visual processing pipeline of a VQA model (assuming the CNN is trained along with entire VQA model). In this project, we propose an efficient and modular neural architecture for the VQA task with focus on the CNN module. Our experiments demonstrate that a sparsely activated CNN based VQA model achieves comparable performance to a standard CNN based VQA model architecture.",2019-09-19T18:55:54Z,2019-09-19T18:55:54Z,http://arxiv.org/abs/1909.09192v1,http://arxiv.org/pdf/1909.09192v1,"cs.LG, cs.CL, cs.CV, stat.ML"
"6G Wireless Communication Systems: Applications, Requirements,   Technologies, Challenges, and Research Directions","Mostafa Zaman Chowdhury, Md. Shahjalal, Shakil Ahmed, Yeong Min Jang","Fifth-generation (5G) communication, which has many more features than fourth-generation communication, will be officially launched very soon. A new paradigm of wireless communication, the sixth-generation (6G) system, with the full support of artificial intelligence is expected to be deployed between 2027 and 2030. In beyond 5G, there are some fundamental issues, which need to be addressed are higher system capacity, higher data rate, lower latency, and improved quality of service (QoS) compared to 5G system. This paper presents the vision of future 6G wireless communication and its network architecture. We discuss the emerging technologies such as artificial intelligence, terahertz communications, optical wireless technology, free space optic network, blockchain, three-dimensional networking, quantum communications, unmanned aerial vehicle, cell-free communications, integration of wireless information and energy transfer, integration of sensing and communication, integration of access-backhaul networks, dynamic network slicing, holographic beamforming, and big data analytics that can assist the 6G architecture development in guaranteeing the QoS. We present the expected applications with the requirements and the possible technologies for 6G communication. We also outline the possible challenges and research directions to reach this goal.",2019-09-25T07:27:18Z,2019-09-25T07:27:18Z,http://arxiv.org/abs/1909.11315v1,http://arxiv.org/pdf/1909.11315v1,"cs.NI, eess.SP"
A Hierarchical Approach for Visual Storytelling Using Image Description,"Md Sultan Al Nahian, Tasmia Tasrin, Sagar Gandhi, Ryan Gaines, Brent Harrison","One of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate human-like stories. In this paper, we propose a hierarchical deep learning architecture based on encoder-decoder networks to address this problem. To better help our network maintain this context while also generating long and diverse sentences, we incorporate natural language image descriptions along with the images themselves to generate each story sentence. We evaluate our system on the Visual Storytelling (VIST) dataset and show that our method outperforms state-of-the-art techniques on a suite of different automatic evaluation metrics. The empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling.",2019-09-26T21:25:41Z,2019-09-26T21:25:41Z,http://arxiv.org/abs/1909.12401v1,http://arxiv.org/pdf/1909.12401v1,"cs.CV, cs.CL, cs.LG, stat.ML"
Implicit Discriminator in Variational Autoencoder,"Prateek Munjal, Akanksha Paul, Narayanan C. Krishnan","Recently generative models have focused on combining the advantages of variational autoencoders (VAE) and generative adversarial networks (GAN) for good reconstruction and generative abilities. In this work we introduce a novel hybrid architecture, Implicit Discriminator in Variational Autoencoder (IDVAE), that combines a VAE and a GAN, which does not need an explicit discriminator network. The fundamental premise of the IDVAE architecture is that the encoder of a VAE and the discriminator of a GAN utilize common features and therefore can be trained as a shared network, while the decoder of the VAE and the generator of the GAN can be combined to learn a single network. This results in a simple two-tier architecture that has the properties of both a VAE and a GAN. The qualitative and quantitative experiments on real-world benchmark datasets demonstrates that IDVAE perform better than the state of the art hybrid approaches. We experimentally validate that IDVAE can be easily extended to work in a conditional setting and demonstrate its performance on complex datasets.",2019-09-28T10:12:28Z,2019-09-28T10:12:28Z,http://arxiv.org/abs/1909.13062v1,http://arxiv.org/pdf/1909.13062v1,"cs.LG, cs.CV, stat.ML"
Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning   in Autonomous Driving,"Maria Huegle, Gabriel Kalweit, Moritz Werling, Joschka Boedecker","The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the benefits of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or traffic signs. At the same time, the architecture has to be able to cover interactions between traffic participants in order to find the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-of-the-art methods in evaluations with the publicly available traffic simulator SUMO.",2019-09-30T10:59:11Z,2019-09-30T10:59:11Z,http://arxiv.org/abs/1909.13582v1,http://arxiv.org/pdf/1909.13582v1,"cs.LG, cs.AI, cs.RO, stat.ML"
Communication Complexity of the Fast Multipole Method and its Algebraic   Variants,"Rio Yokota, George Turkiyyah, David Keyes","A combination of hierarchical tree-like data structures and data access patterns from fast multipole methods and hierarchical low-rank approximation of linear operators from H-matrix methods appears to form an algorithmic path forward for efficient implementation of many linear algebraic operations of scientific computing at the exascale. The combination provides asymptotically optimal computational and communication complexity and applicability to large classes of operators that commonly arise in scientific computing applications. A convergence of the mathematical theories of the fast multipole and H-matrix methods has been underway for over a decade. We recap this mathematical unification and describe implementation aspects of a hybrid of these two compelling hierarchical algorithms on hierarchical distributed-shared memory architectures, which are likely to be the first to reach the exascale. We present a new communication complexity estimate for fast multipole methods on such architectures. We also show how the data structures and access patterns of H-matrices for low-rank operators map onto those of fast multipole, leading to an algebraically generalized form of fast multipole that compromises none of its architecturally ideal properties.",2014-06-08T11:48:44Z,2014-06-08T11:48:44Z,http://arxiv.org/abs/1406.1974v1,http://arxiv.org/pdf/1406.1974v1,"cs.DC, cs.NA, 70F10, D.1.2; D.1.3; G.1.0; G.1.2"
The Mean Interference-to-Signal Ratio and its Key Role in Cellular and   Amorphous Networks,Martin Haenggi,"We introduce a simple yet powerful and versatile analytical framework to approximate the SIR distribution in the downlink of cellular systems. It is based on the mean interference-to-signal ratio and yields the horizontal gap (SIR gain) between the SIR distribution in question and a reference SIR distribution. As applications, we determine the SIR gain for base station silencing, cooperation, and lattice deployment over a baseline architecture that is based on a Poisson deployment of base stations and strongest-base station association. The applications demonstrate that the proposed approach unifies several recent results and provides a convenient framework for the analysis and comparison of future network architectures and transmission schemes, including amorphous networks where a user is served by multiple base stations and, consequently, (hard) cell association becomes obsolete.",2014-06-11T06:55:45Z,2014-06-11T06:55:45Z,http://arxiv.org/abs/1406.2794v1,http://arxiv.org/pdf/1406.2794v1,"cs.IT, cs.NI, math.IT"
A Cascade Neural Network Architecture investigating Surface Plasmon   Polaritons propagation for thin metals in OpenMP,"Francesco Bonanno, Giacomo Capizzi, Grazia Lo Sciuto, Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana","Surface plasmon polaritons (SPPs) confined along metal-dielectric interface have attracted a relevant interest in the area of ultracompact photonic circuits, photovoltaic devices and other applications due to their strong field confinement and enhancement. This paper investigates a novel cascade neural network (NN) architecture to find the dependance of metal thickness on the SPP propagation. Additionally, a novel training procedure for the proposed cascade NN has been developed using an OpenMP-based framework, thus greatly reducing training time. The performed experiments confirm the effectiveness of the proposed NN architecture for the problem at hand.",2014-06-12T08:40:04Z,2014-06-12T08:40:04Z,http://arxiv.org/abs/1406.3149v1,http://arxiv.org/pdf/1406.3149v1,"cs.NE, cond-mat.mes-hall, cond-mat.mtrl-sci, cs.DC, cs.LG, 68T05, I.2.6; I.5.1; I.2.11"
Selective Match-Line Energizer Content Addressable Memory(SMLE -CAM),"Mohammed Zackriya. V, Harish M Kittur","A Content Addressable Memory (CAM) is a memory primarily designed for high speed search operation. Parallel search scheme forms the basis of CAM, thus power reduction is the challenge associated with a large amount of parallel active circuits. We are presenting a novel algorithm and architecture described as Selective Match-Line Energizer Content Addressable Memory (SMLE-CAM) which energizes only those MLs (Match-Line) whose first three bits are conditionally matched with corresponding first three search bit using special architecture which comprises of novel XNOR-CAM cell and novel XOR-CAM cell. The rest of the CAM chain is followed by NOR-CAM cell. The 256 X 144 bit SMLE-CAM is implemented in TSMC 90 nm technology and its robustness across PVT variation is verified. The post-layout simulation result shows, it has energy metric of 0.115 fJ/bit/search with search time 361.6 ps, the best reported so far. The maximum operating frequency is 1GHz.",2014-06-30T10:46:50Z,2014-06-30T10:46:50Z,http://arxiv.org/abs/1406.7662v1,http://arxiv.org/pdf/1406.7662v1,"cs.AR, B.7.1"
Full-Duplex Transceiver for Future Cellular Network: A Smart Antenna   Approach,"Chandan Pradhan, Garimella Rama Murthy","In this paper, we propose a transceiver architecture for full-duplex (FD) eNodeB (eNB) and FD user equipment (UE) transceiver. For FD communication,.i.e., simultaneous in-band uplink and downlink operation, same subcarriers can be allocated to UE in both uplink and downlink. Hence, contrary to traditional LTE, we propose using single-carrier frequency division multiple accesses (SC-FDMA) for downlink along with the conventional method of using it for uplink. The use of multiple antennas at eNB and singular value decomposition (SVD) in the downlink allows multiple users (MU) to operate on the same set of ubcarriers. In the uplink, successive interference cancellation with optimal ordering (SSIC-OO) algorithm is used to decouple signals of UEs operating in the same set of subcarriers. A smart antenna approach is adopted which prevents interference, in downlink of a UE, from uplink signals of other UEs sharing same subcarriers. The approach includes using multiple antennas at UEs to form directed beams towards eNode and nulls towards other UEs. The proposed architecture results in significant improvement of the overall spectrum efficiency per cell of the cellular network.",2015-09-10T03:50:46Z,2015-09-10T03:50:46Z,http://arxiv.org/abs/1509.03000v1,http://arxiv.org/pdf/1509.03000v1,"cs.NI, cs.IT, math.IT"
LIRA: A Location Independent Routing Layer based on Source-Provided   Ephemeral Names,"Ioannis Psaras, Konstantinos V. Katsaros, Lorenzo Saino, George Pavlou","We identify the obstacles hindering the deployment of Information Centric Networking (ICN) and the shift from the current IP architecture. In particular, we argue that scalability of name resolution and the lack of control of content access from content providers are two important barriers that keep ICN away from deployment. We design solutions to incentivise ICN deployment and present a new network architecture that incorporates an extra layer in the protocol stack (the Location Independent Routing Layer, LIRA) to integrate location-independent content delivery. According to our design, content names need not (and should not) be permanent, but rather should be ephemeral. Resolution of non-permanent names requires the involvement of content providers, enabling desirable features such as request logging and cache purging, while avoiding the need for the deployment of a new name resolution infrastructure. Our results show that with half of the network's nodes operating under the LIRA framework, we can get the full gain of the ICN mode of operation.",2015-09-18T11:12:58Z,2015-09-18T11:12:58Z,http://arxiv.org/abs/1509.05589v1,http://arxiv.org/pdf/1509.05589v1,"cs.NI, C.2.1"
Cell Grid Architecture for Maritime Route Prediction on AIS Data Streams,"Ciprian Amariei, Paul Diac, Emanuel Onica, Valentin Roşca","The 2018 Grand Challenge targets the problem of accurate predictions on data streams produced by automatic identification system (AIS) equipment, describing naval traffic. This paper reports the technical details of a custom solution, which exposes multiple tuning parameters, making its configurability one of the main strengths. Our solution employs a cell grid architecture essentially based on a sequence of hash tables, specifically built for the targeted use case. This makes it particularly effective in prediction on AIS data, obtaining a high accuracy and scalable performance results. Moreover, the architecture proposed accommodates also an optionally semi-supervised learning process besides the basic supervised mode.",2018-09-28T21:42:17Z,2018-09-28T21:42:17Z,http://arxiv.org/abs/1810.00090v1,http://arxiv.org/pdf/1810.00090v1,"cs.AI, cs.LG, stat.ML"
Optimization of Circuits for IBM's five-qubit Quantum Computers,"Gerhard W. Dueck, Anirban Pathak, Md Mazder Rahman, Abhishek Shukla, Anindita Banerjee","IBM has made several quantum computers available to researchers around the world via cloud services. Two architectures with five qubits, one with 16, and one with 20 qubits are available to run experiments. The IBM architectures implement gates from the Clifford+T gate library. However, each architecture only implements a subset of the possible CNOT gates. In this paper, we show how Clifford+T circuits can efficiently be mapped into the two IBM quantum computers with 5 qubits. We further present an algorithm and a set of circuit identities that may be used to optimize the Clifford+T circuits in terms of gate count and number of levels. It is further shown that the optimized circuits can considerably reduce the gate count and number of levels and thus produce results with better fidelity.",2018-09-29T01:20:53Z,2018-09-29T01:20:53Z,http://arxiv.org/abs/1810.00129v1,http://arxiv.org/pdf/1810.00129v1,"cs.ET, quant-ph"
On Self Modulation for Generative Adversarial Networks,"Ting Chen, Mario Lucic, Neil Houlsby, Sylvain Gelly","Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of $5\%-35\%$ in FID. Furthermore, all else being equal, adding this modification to the generator leads to improved performance in $124/144$ ($86\%$) of the studied settings. Self-modulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN.",2018-10-02T16:50:28Z,2019-05-02T07:20:50Z,http://arxiv.org/abs/1810.01365v2,http://arxiv.org/pdf/1810.01365v2,"cs.LG, cs.CV, stat.ML"
A Closer Look at Structured Pruning for Neural Network Compression,"Elliot J. Crowley, Jack Turner, Amos Storkey, Michael O'Boyle","Structured pruning is a popular method for compressing a neural network: given a large trained network, one alternates between removing channel connections and fine-tuning; reducing the overall width of the network. However, the efficacy of structured pruning has largely evaded scrutiny. In this paper, we examine ResNets and DenseNets obtained through structured pruning-and-tuning and make two interesting observations: (i) reduced networks---smaller versions of the original network trained from scratch---consistently outperform pruned networks; (ii) if one takes the architecture of a pruned network and then trains it from scratch it is significantly more competitive. Furthermore, these architectures are easy to approximate: we can prune once and obtain a family of new, scalable network architectures that can simply be trained from scratch. Finally, we compare the inference speed of reduced and pruned networks on hardware, and show that reduced networks are significantly faster. Code is available at https://github.com/BayesWatch/pytorch-prunes.",2018-10-10T16:30:02Z,2019-06-07T14:23:14Z,http://arxiv.org/abs/1810.04622v3,http://arxiv.org/pdf/1810.04622v3,"stat.ML, cs.CV, cs.LG"
On the applicability of distributed ledger architectures to peer-to-peer   energy trading framework,"Van Hoa Nguyen, Yvon Besanger, Quoc Tuan Tran, Minh Tri Le","As more and more distributed renewable energy resources are integrated to the grid, the traditional consumers have become the prosumers who can sell back their surplus energy to the others who are in energy shortage. This peer-to-peer (P2P) energy transaction framework benefits the end users, financially and in term of energy security; and the network operators, in term of flexibility in DRES management, peak load shifting and regulation of voltage/frequency. Environmentally, P2P energy transaction also helps to reduce carbon footprint, reduces DRES payback period and incentivizes the installation of DRES. The current centralized market model is no longer suitable and it is therefore necessary to develop an adapted decentralized architecture for the advanced P2P energy transaction framework intra/inter-microgrid. In this paper, we discuss several distributed ledger approaches for such framework: Blockchain, Block Lattice and Directed Acyclic Graph (the Tangle). The technical advantages of these architectures as well as the persistent challenges are then considered.",2018-10-11T11:34:22Z,2018-10-11T11:34:22Z,http://arxiv.org/abs/1810.05541v1,http://arxiv.org/pdf/1810.05541v1,"cs.CY, eess.SP"
Exploring Adversarial Examples in Malware Detection,"Octavian Suciu, Scott E. Coull, Jeffrey Johns","The convolutional neural network (CNN) architecture is increasingly being applied to new domains, such as malware detection, where it is able to learn malicious behavior from raw bytes extracted from executables. These architectures reach impressive performance with no feature engineering effort involved, but their robustness against active attackers is yet to be understood. Such malware detectors could face a new attack vector in the form of adversarial interference with the classification model. Existing evasion attacks intended to cause misclassification on test-time instances, which have been extensively studied for image classifiers, are not applicable because of the input semantics that prevents arbitrary changes to the binaries. This paper explores the area of adversarial examples for malware detection. By training an existing model on a production-scale dataset, we show that some previous attacks are less effective than initially reported, while simultaneously highlighting architectural weaknesses that facilitate new attack strategies for malware classification. Finally, we explore how generalizable different attack strategies are, the trade-offs when aiming to increase their effectiveness, and the transferability of single-step attacks.",2018-10-18T21:26:27Z,2019-04-13T23:21:45Z,http://arxiv.org/abs/1810.08280v3,http://arxiv.org/pdf/1810.08280v3,"cs.LG, cs.CR, stat.ML"
Compositional Attention Networks for Interpretability in Natural   Language Question Answering,"Muru Selvakumar, Suriyadeepan Ramamoorthy, Vaidheeswaran Archana, Malaikannan Sankarasubbu","MAC Net is a compositional attention network designed for Visual Question Answering. We propose a modified MAC net architecture for Natural Language Question Answering. Question Answering typically requires Language Understanding and multi-step Reasoning. MAC net's unique architecture - the separation between memory and control, facilitates data-driven iterative reasoning. This makes it an ideal candidate for solving tasks that involve logical reasoning. Our experiments with 20 bAbI tasks demonstrate the value of MAC net as a data-efficient and interpretable architecture for Natural Language Question Answering. The transparent nature of MAC net provides a highly granular view of the reasoning steps taken by the network in answering a query.",2018-10-30T12:23:35Z,2018-10-30T12:23:35Z,http://arxiv.org/abs/1810.12698v1,http://arxiv.org/pdf/1810.12698v1,"cs.LG, cs.AI, cs.CL, stat.ML"
Cascade-Net: a New Deep Learning Architecture for OFDM Detection,"Qisheng Huang, Chunming Zhao, Ming Jiang, Xiaoming Li, Jing Liang","In this paper, we consider using deep neural network for OFDM symbol detection and demonstrate its performance advantages in combating large Doppler Shift. In particular, a new architecture named Cascade-Net is proposed for detection, where deep neural network is cascading with a zero-forcing preprocessor to prevent the network stucking in a saddle point or a local minimum point. In addition, we propose a sliding detection approach in order to detect OFDM symbols with large number of subcarriers. We evaluate this new architecture, as well as the sliding algorithm, using the Rayleigh channel with large Doppler spread, which could degrade detection performance in an OFDM system and is especially severe for high frequency band and mmWave communications. The numerical results of OFDM detection in SISO scenario show that cascade-net can achieve better performance than zero-forcing method while providing robustness against ill conditioned channels. We also show the better performance of the sliding cascade network (SCN) compared to sliding zero-forcing detector through numerical simulation.",2018-11-30T19:07:57Z,2018-11-30T19:07:57Z,http://arxiv.org/abs/1812.00023v1,http://arxiv.org/pdf/1812.00023v1,"eess.SP, cs.IT, math.IT"
Examining Deep Learning Architectures for Crime Classification and   Prediction,"Panagiotis Stalidis, Theodoros Semertzidis, Petros Daras","In this paper, a detailed study on crime classification and prediction using deep learning architectures is presented. We examine the effectiveness of deep learning algorithms on this domain and provide recommendations for designing and training deep learning systems for predicting crime areas, using open data from police reports. Having as training data time-series of crime types per location, a comparative study of 10 state-of-the-art methods against 3 different deep learning configurations is conducted. In our experiments with five publicly available datasets, we demonstrate that the deep learning-based methods consistently outperform the existing best-performing methods. Moreover, we evaluate the effectiveness of different parameters in the deep learning architectures and give insights for configuring them in order to achieve improved performance in crime classification and finally crime prediction.",2018-12-03T08:40:50Z,2018-12-03T08:40:50Z,http://arxiv.org/abs/1812.00602v1,http://arxiv.org/pdf/1812.00602v1,"cs.LG, cs.CY, stat.ML"
A Graph-CNN for 3D Point Cloud Classification,"Yingxue Zhang, Michael Rabbat","Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to handle data that is supported on a graph. Major challenges when working with data on graphs are that the support set (the vertices of the graph) do not typically have a natural ordering, and in general, the topology of the graph is not regular (i.e., vertices do not all have the same number of neighbors). Thus, Graph-CNNs have huge potential to deal with 3D point cloud data which has been obtained from sampling a manifold. In this paper, we develop a Graph-CNN for classifying 3D point cloud data, called PointGCN. The architecture combines localized graph convolutions with two types of graph downsampling operations (also known as pooling). By the effective exploration of the point cloud local structure using the Graph-CNN, the proposed architecture achieves competitive performance on the 3D object classification benchmark ModelNet, and our architecture is more stable than competing schemes.",2018-11-28T17:00:34Z,2018-11-28T17:00:34Z,http://arxiv.org/abs/1812.01711v1,http://arxiv.org/pdf/1812.01711v1,"cs.CV, cs.LG, stat.ML"
Hierarchical Bipartite Graph Convolution Networks,Marcel Nassar,"Recently, graph neural networks have been adopted in a wide variety of applications ranging from relational representations to modeling irregular data domains such as point clouds and social graphs. However, the space of graph neural network architectures remains highly fragmented impeding the development of optimized implementations similar to what is available for convolutional neural networks. In this work, we present BiGraphNet, a graph neural network architecture that generalizes many popular graph neural network models and enables new efficient operations similar to those supported by ConvNets. By explicitly separating the input and output nodes, BiGraphNet: (i) generalizes the graph convolution to support new efficient operations such as coarsened graph convolutions (similar to strided convolution in convnets), multiple input graphs convolution and graph expansions (unpooling) which can be used to implement various graph architectures such as graph autoencoders, and graph residual nets; and (ii) accelerates and scales the computations and memory requirements in hierarchical networks by performing computations only at specified output nodes.",2018-11-17T02:43:59Z,2018-12-13T02:05:11Z,http://arxiv.org/abs/1812.03813v2,http://arxiv.org/pdf/1812.03813v2,"cs.LG, cs.CV, stat.ML"
Conditional Graph Neural Processes: A Functional Autoencoder Approach,"Marcel Nassar, Xin Wang, Evren Tumer","We introduce a novel encoder-decoder architecture to embed functional processes into latent vector spaces. This embedding can then be decoded to sample the encoded functions over any arbitrary domain. This autoencoder generalizes the recently introduced Conditional Neural Process (CNP) model of random processes. Our architecture employs the latest advances in graph neural networks to process irregularly sampled functions. Thus, we refer to our model as Conditional Graph Neural Process (CGNP). Graph neural networks can effectively exploit `local' structures of the metric spaces over which the functions/processes are defined. The contributions of this paper are twofold: (i) a novel graph-based encoder-decoder architecture for functional and process embeddings, and (ii) a demonstration of the importance of using the structure of metric spaces for this type of representations.",2018-12-13T00:52:56Z,2018-12-13T00:52:56Z,http://arxiv.org/abs/1812.05212v1,http://arxiv.org/pdf/1812.05212v1,"cs.LG, cs.AI, stat.ML"
Autoencoder Based Architecture For Fast & Real Time Audio Style Transfer,"Dhruv Ramani, Samarjit Karmakar, Anirban Panda, Asad Ahmed, Pratham Tangri","Recently, there has been great interest in the field of audio style transfer, where a stylized audio is generated by imposing the style of a reference audio on the content of a target audio. We improve on the current approaches which use neural networks to extract the content and the style of the audio signal and propose a new autoencoder based architecture for the task. This network generates a stylized audio for a content audio in a single forward pass. The proposed network architecture proves to be advantageous over the quality of audio produced and the time taken to train the network. The network is experimented on speech signals to confirm the validity of our proposal.",2018-12-18T04:04:38Z,2018-12-26T15:30:16Z,http://arxiv.org/abs/1812.07159v2,http://arxiv.org/pdf/1812.07159v2,"cs.SD, cs.LG, eess.AS, stat.ML"
Analysing Mathematical Reasoning Abilities of Neural Models,"David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli","Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.",2019-04-02T17:26:41Z,2019-04-02T17:26:41Z,http://arxiv.org/abs/1904.01557v1,http://arxiv.org/pdf/1904.01557v1,"cs.LG, stat.ML"
Rep the Set: Neural Networks for Learning Set Representations,"Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, Michalis Vazirgiannis","In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts. Many conventional machine learning algorithms are unable to process this kind of representations, since sets may vary in cardinality and elements lack a meaningful ordering. In this paper, we present a new neural network architecture, called RepSet, that can handle examples that are represented as sets of vectors. The proposed model computes the correspondences between an input set and some hidden sets by solving a series of network flow problems. This representation is then fed to a standard neural network architecture to produce the output. The architecture allows end-to-end gradient-based learning. We demonstrate RepSet on classification tasks, including text categorization, and graph classification, and we show that the proposed neural network achieves performance better or comparable to state-of-the-art algorithms.",2019-04-03T12:25:54Z,2020-02-28T19:19:12Z,http://arxiv.org/abs/1904.01962v2,http://arxiv.org/pdf/1904.01962v2,"cs.LG, stat.ML"
Unsupervised Progressive Learning and the STAM Architecture,"James Smith, Cameron Taylor, Seth Baer, Constantine Dovrolis","We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, learning a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using clustering and classification tasks. While there are no existing learning scenarios that are directly comparable to UPL, we compare the STAM architecture with two recent continual learning models, Memory Aware Synapses (MAS) and Gradient Episodic Memories (GEM), after adapting them in the UPL setting.",2019-04-03T14:25:08Z,2021-05-13T17:55:25Z,http://arxiv.org/abs/1904.02021v6,http://arxiv.org/pdf/1904.02021v6,"cs.LG, q-bio.NC, stat.ML, I.2.6"
Teaching GANs to Sketch in Vector Format,"Varshaneya V, S Balasubramanian, Vineeth N Balasubramanian","Sketching is more fundamental to human cognition than speech. Deep Neural Networks (DNNs) have achieved the state-of-the-art in speech-related tasks but have not made significant development in generating stroke-based sketches a.k.a sketches in vector format. Though there are Variational Auto Encoders (VAEs) for generating sketches in vector format, there is no Generative Adversarial Network (GAN) architecture for the same. In this paper, we propose a standalone GAN architecture SkeGAN and a VAE-GAN architecture VASkeGAN, for sketch generation in vector format. SkeGAN is a stochastic policy in Reinforcement Learning (RL), capable of generating both multidimensional continuous and discrete outputs. VASkeGAN hybridizes a VAE and a GAN, in order to couple the efficient representation of data by VAE with the powerful generating capabilities of a GAN, to produce visually appealing sketches. We also propose a new metric called the Ske-score which quantifies the quality of vector sketches. We have validated that SkeGAN and VASkeGAN generate visually appealing sketches by using Human Turing Test and Ske-score.",2019-04-07T10:23:47Z,2019-04-07T10:23:47Z,http://arxiv.org/abs/1904.03620v1,http://arxiv.org/pdf/1904.03620v1,"cs.GR, cs.LG, stat.ML"
Time Domain Audio Visual Speech Separation,"Jian Wu, Yong Xu, Shi-Xiong Zhang, Lian-Wu Chen, Meng Yu, Lei Xie, Dong Yu","Audio-visual multi-modal modeling has been demonstrated to be effective in many speech related tasks, such as speech recognition and speech enhancement. This paper introduces a new time-domain audio-visual architecture for target speaker extraction from monaural mixtures. The architecture generalizes the previous TasNet (time-domain speech separation network) to enable multi-modal learning and at meanwhile it extends the classical audio-visual speech separation from frequency-domain to time-domain. The main components of proposed architecture include an audio encoder, a video encoder that extracts lip embedding from video streams, a multi-modal separation network and an audio decoder. Experiments on simulated mixtures based on recently released LRS2 dataset show that our method can bring 3dB+ and 4dB+ Si-SNR improvements on two- and three-speaker cases respectively, compared to audio-only TasNet and frequency-domain audio-visual networks",2019-04-07T22:30:23Z,2019-09-22T14:32:32Z,http://arxiv.org/abs/1904.03760v2,http://arxiv.org/pdf/1904.03760v2,"eess.AS, cs.SD"
"ASAP: Architecture Search, Anneal and Prune","Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, Lihi Zelnik-Manor","Automatic methods for Neural Architecture Search (NAS) have been shown to produce state-of-the-art network models. Yet, their main drawback is the computational complexity of the search process. As some primal methods optimized over a discrete search space, thousands of days of GPU were required for convergence. A recent approach is based on constructing a differentiable search space that enables gradient-based optimization, which reduces the search time to a few days. While successful, it still includes some noncontinuous steps, e.g., the pruning of many weak connections at once. In this paper, we propose a differentiable search space that allows the annealing of architecture weights, while gradually pruning inferior operations. In this way, the search converges to a single output network in a continuous manner. Experiments on several vision datasets demonstrate the effectiveness of our method with respect to the search cost and accuracy of the achieved model. Specifically, with $0.2$ GPU search days we achieve an error rate of $1.68\%$ on CIFAR-10.",2019-04-08T15:16:16Z,2019-10-10T08:59:52Z,http://arxiv.org/abs/1904.04123v2,http://arxiv.org/pdf/1904.04123v2,"stat.ML, cs.LG"
SWNet: Small-World Neural Networks and Rapid Convergence,"Mojan Javaheripi, Bita Darvish Rouhani, Farinaz Koushanfar","Training large and highly accurate deep learning (DL) models is computationally costly. This cost is in great part due to the excessive number of trained parameters, which are well-known to be redundant and compressible for the execution phase. This paper proposes a novel transformation which changes the topology of the DL architecture such that it reaches an optimal cross-layer connectivity. This transformation leverages our important observation that for a set level of accuracy, convergence is fastest when network topology reaches the boundary of a Small-World Network. Small-world graphs are known to possess a specific connectivity structure that enables enhanced signal propagation among nodes. Our small-world models, called SWNets, provide several intriguing benefits: they facilitate data (gradient) flow within the network, enable feature-map reuse by adding long-range connections and accommodate various network architectures/datasets. Compared to densely connected networks (e.g., DenseNets), SWNets require a substantially fewer number of training parameters while maintaining a similar level of classification accuracy. We evaluate our networks on various DL model architectures and image classification datasets, namely, CIFAR10, CIFAR100, and ILSVRC (ImageNet). Our experiments demonstrate an average of ~2.1x improvement in convergence speed to the desired accuracy",2019-04-09T18:41:26Z,2019-04-09T18:41:26Z,http://arxiv.org/abs/1904.04862v1,http://arxiv.org/pdf/1904.04862v1,"cs.LG, cs.AI, cs.CV, stat.ML"
DNN Architecture for High Performance Prediction on Natural Videos Loses   Submodule's Ability to Learn Discrete-World Dataset,"Lana Sinapayen, Atsushi Noda","Is cognition a collection of loosely connected functions tuned to different tasks, or can there be a general learning algorithm? If such an hypothetical general algorithm did exist, tuned to our world, could it adapt seamlessly to a world with different laws of nature? We consider the theory that predictive coding is such a general rule, and falsify it for one specific neural architecture known for high-performance predictions on natural videos and replication of human visual illusions: PredNet. Our results show that PredNet's high performance generalizes without retraining on a completely different natural video dataset. Yet PredNet cannot be trained to reach even mediocre accuracy on an artificial video dataset created with the rules of the Game of Life (GoL). We also find that a submodule of PredNet, a Convolutional Neural Network trained alone, reaches perfect accuracy on the GoL while being mediocre for natural videos, showing that PredNet's architecture itself is responsible for both the high performance on natural videos and the loss of performance on the GoL. Just as humans cannot predict the dynamics of the GoL, our results suggest that there might be a trade-off between high performance on sensory inputs with different sets of rules.",2019-04-16T20:35:09Z,2019-04-16T20:35:09Z,http://arxiv.org/abs/1904.07969v1,http://arxiv.org/pdf/1904.07969v1,"cs.CV, cs.LG, stat.ML"
Novel heterojunction bipolar transistor architectures for the practical   implementation of high-efficiency three-terminal solar cells,"Pablo G. Linares, Elisa Antolín, Antonio Martí","Practical device architectures are proposed here for the implementation of three-terminal heterojunction bipolar transistor solar cells (3T-HBTSCs). These photovoltaic devices, which have a potential efficiency similar to that of multijunction cells, exhibit reduced spectral sensitivity compared with monolithically and series-connected tandem solar cells. In addition, the simplified n-p-n (or p-n-p) structure does not require the use of tunnel junctions. In this framework, four architectures are proposed and discussed in this paper: 1) one in which the top cell is based on silicon and the bottom cell is based on a heterojunction between silicon and III-V nanomaterials; 2) one in which the top cell is made of amorphous silicon and the bottom cell is made of an amorphous silicon-silicon heterojunction; 3) one based on the use of III-V semiconductors aimed at space applications; and 4) one in which the top cell is based on a perovskite material and the bottom cell is made of a perovskite-silicon heterostructure.",2019-04-28T11:12:37Z,2019-04-28T11:12:37Z,http://arxiv.org/abs/1904.12305v1,http://arxiv.org/pdf/1904.12305v1,"physics.app-ph, cond-mat.mtrl-sci"
Neural Language Priors,"Joseph Enguehard, Dan Busbridge, Vitalii Zhelezniak, Nils Hammerla","The choice of sentence encoder architecture reflects assumptions about how a sentence's meaning is composed from its constituent words. We examine the contribution of these architectures by holding them randomly initialised and fixed, effectively treating them as as hand-crafted language priors, and evaluating the resulting sentence encoders on downstream language tasks. We find that even when encoders are presented with additional information that can be used to solve tasks, the corresponding priors do not leverage this information, except in an isolated case. We also find that apparently uninformative priors are just as good as seemingly informative priors on almost all tasks, indicating that learning is a necessary component to leverage information provided by architecture choice.",2019-10-04T16:44:33Z,2019-10-04T16:44:33Z,http://arxiv.org/abs/1910.03492v1,http://arxiv.org/pdf/1910.03492v1,"cs.CL, cs.LG, cs.NE, stat.ML"
A Software-Defined Opto-Acoustic Network Architecture for Internet of   Underwater Things,"Abdulkadir Celik, Nasir Saeed, Basem Shihada, Tareq Y. Al-Naffouri, Mohamed-Slim Alouini","In this paper, we envision a hybrid opto-acoustic network design for the internet of underwater things (IoUT). Software-defined underwater networking (SDUN) is presented as an enabler of hybridizing benefits of optic and acoustic systems and adapting IoUT nodes to the challenging and dynamically changing underwater environment. We explain inextricably interwoven relations among functionalities of different layers and analyze their impacts on key network attributes. Network function virtualization (NFV) concept is then introduced to realize application specific cross-layer protocol suites through an NFV management and orchestration system. We finally discuss how SDUN and NFV can slice available network resources as per the diverging service demands of different underwater applications. Such a revolutionary architectural paradigm shift is not only a cure for chronicle underwater networking problems but also a way of smoothly integrating IoUT and IoT ecosystems.",2019-09-30T12:40:14Z,2019-09-30T12:40:14Z,http://arxiv.org/abs/1910.05306v1,http://arxiv.org/pdf/1910.05306v1,"cs.NI, cs.SY, eess.SY"
Optic-Net: A Novel Convolutional Neural Network for Diagnosis of Retinal   Diseases from Optical Tomography Images,"Sharif Amit Kamran, Sourajit Saha, Ali Shihab Sabbir, Alireza Tavakkoli","Diagnosing different retinal diseases from Spectral Domain Optical Coherence Tomography (SD-OCT) images is a challenging task. Different automated approaches such as image processing, machine learning and deep learning algorithms have been used for early detection and diagnosis of retinal diseases. Unfortunately, these are prone to error and computational inefficiency, which requires further intervention from human experts. In this paper, we propose a novel convolution neural network architecture to successfully distinguish between different degeneration of retinal layers and their underlying causes. The proposed novel architecture outperforms other classification models while addressing the issue of gradient explosion. Our approach reaches near perfect accuracy of 99.8% and 100% for two separately available Retinal SD-OCT data-set respectively. Additionally, our architecture predicts retinal diseases in real time while outperforming human diagnosticians.",2019-10-13T03:02:35Z,2019-10-13T03:02:35Z,http://arxiv.org/abs/1910.05672v1,http://arxiv.org/pdf/1910.05672v1,"eess.IV, cs.CV"
Path homologies of deep feedforward networks,"Samir Chowdhury, Thomas Gebhart, Steve Huntsman, Matvey Yutin","We provide a characterization of two types of directed homology for fully-connected, feedforward neural network architectures. These exact characterizations of the directed homology structure of a neural network architecture are the first of their kind. We show that the directed flag homology of deep networks reduces to computing the simplicial homology of the underlying undirected graph, which is explicitly given by Euler characteristic computations. We also show that the path homology of these networks is non-trivial in higher dimensions and depends on the number and size of the layers within the network. These results provide a foundation for investigating homological differences between neural network architectures and their realized structure as implied by their parameters.",2019-10-16T21:14:55Z,2019-10-16T21:14:55Z,http://arxiv.org/abs/1910.07617v1,http://arxiv.org/pdf/1910.07617v1,"math.AT, cs.LG, stat.ML"
Universal programmable photonic architecture for quantum information   processing,"Ben Bartlett, Shanhui Fan","We present a photonic integrated circuit architecture for a quantum programmable gate array (QPGA) capable of preparing arbitrary quantum states and operators. The architecture consists of a lattice of phase-modulated Mach-Zehnder interferometers, which perform rotations on path-encoded photonic qubits, and embedded quantum emitters, which use a two-photon scattering process to implement a deterministic controlled-$\sigma_z$ operation between adjacent qubits. By appropriately setting phase shifts within the lattice, the device can be programmed to implement any quantum circuit without hardware modifications. We provide algorithms for exactly preparing arbitrary quantum states and operators on the device and we show that gradient-based optimization can train a simulated QPGA to automatically implement highly compact approximations to important quantum circuits with near-unity fidelity.",2019-10-22T17:57:15Z,2019-10-22T17:57:15Z,http://arxiv.org/abs/1910.10141v1,http://arxiv.org/pdf/1910.10141v1,"quant-ph, physics.optics"
Injecting Hierarchy with U-Net Transformers,"David Donahue, Vladislav Lialin, Anna Rumshisky","The Transformer architecture has become increasingly popular over the past two years, owing to its impressive performance on a number of natural language processing (NLP) tasks. However, all Transformer computations occur at the level of word representations and therefore, it may be argued that Transformer models do not explicitly attempt to learn hierarchical structure which is widely assumed to be integral to language. In the present work, we introduce hierarchical processing into the Transformer model, taking inspiration from the U-Net architecture, popular in computer vision for its hierarchical view of natural images. We empirically demonstrate that the proposed architecture outperforms both the vanilla Transformer and some strong baselines in the domain of chit-chat dialogue.",2019-10-16T15:48:46Z,2021-04-01T19:41:09Z,http://arxiv.org/abs/1910.10488v2,http://arxiv.org/pdf/1910.10488v2,"cs.LG, cs.CL, stat.ML"
Interrupted and cascaded permutation invariant training for speech   separation,"Gene-Ping Yang, Szu-Lin Wu, Yao-Wen Mao, Hung-yi Lee, Lin-shan Lee","Permutation Invariant Training (PIT) has long been a stepping stone method for training speech separation model in handling the label ambiguity problem. With PIT selecting the minimum cost label assignments dynamically, very few studies considered the separation problem to be optimizing both the model parameters and the label assignments, but focused on searching for good model architecture and parameters. In this paper, we investigate instead for a given model architecture the various flexible label assignment strategies for training the model, rather than directly using PIT. Surprisingly, we discover a significant performance boost compared to PIT is possible if the model is trained with fixed label assignments and a good set of labels is chosen. With fixed label training cascaded between two sections of PIT, we achieved the state-of-the-art performance on WSJ0-2mix without changing the model architecture at all.",2019-10-28T14:28:12Z,2019-10-28T14:28:12Z,http://arxiv.org/abs/1910.12706v1,http://arxiv.org/pdf/1910.12706v1,"cs.SD, cs.LG, eess.AS"
IMPACT: Importance Weighted Asynchronous Architectures with Clipped   Target Networks,"Michael Luo, Jiahao Yao, Richard Liaw, Eric Liang, Ion Stoica","The practical usage of reinforcement learning agents is often bottlenecked by the duration of training time. To accelerate training, practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate the training process. However, modern methods for scalable reinforcement learning (RL) often tradeoff between the throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, as one increases sample throughput (i.e. increasing parallelization in IMPALA), sample efficiency drops significantly. To address this, we propose a new distributed reinforcement learning algorithm, IMPACT. IMPACT extends IMPALA with three changes: a target network for stabilizing the surrogate objective, a circular buffer, and truncated importance sampling. In discrete action-space environments, we show that IMPACT attains higher reward and, simultaneously, achieves up to 30% decrease in training wall-time than that of IMPALA. For continuous control environments, IMPACT trains faster than existing scalable agents while preserving the sample efficiency of synchronous PPO.",2019-11-30T09:44:19Z,2020-01-23T07:30:51Z,http://arxiv.org/abs/1912.00167v3,http://arxiv.org/pdf/1912.00167v3,"cs.LG, stat.ML"
MetalGAN: Multi-Domain Label-Less Image Synthesis Using cGANs and   Meta-Learning,"Tomaso Fontanini, Eleonora Iotti, Luca Donati, Andrea Prati","Image synthesis is currently one of the most addressed image processing topic in computer vision and deep learning fields of study. Researchers have tackled this problem focusing their efforts on its several challenging problems, e.g. image quality and size, domain and pose changing, architecture of the networks, and so on. Above all, producing images belonging to different domains by using a single architecture is a very relevant goal for image generation. In fact, a single multi-domain network would allow greater flexibility and robustness in the image synthesis task than other approaches. This paper proposes a novel architecture and a training algorithm, which are able to produce multi-domain outputs using a single network. A small portion of a dataset is intentionally used, and there are no hard-coded labels (or classes). This is achieved by combining a conditional Generative Adversarial Network (cGAN) for image generation and a Meta-Learning algorithm for domain switch, and we called our approach MetalGAN. The approach has proved to be appropriate for solving the multi-domain problem and it is validated on facial attribute transfer, using CelebA dataset.",2019-12-05T10:47:08Z,2020-06-25T09:40:52Z,http://arxiv.org/abs/1912.02494v2,http://arxiv.org/pdf/1912.02494v2,"cs.LG, cs.CV, stat.ML"
Neural Networks with Cheap Differential Operators,"Ricky T. Q. Chen, David Duvenaud","Gradients of neural networks can be computed efficiently for any architecture, but some applications require differential operators with higher time complexity. We describe a family of restricted neural network architectures that allow efficient computation of a family of differential operators involving dimension-wise derivatives, used in cases such as computing the divergence. Our proposed architecture has a Jacobian matrix composed of diagonal and hollow (non-diagonal) components. We can then modify the backward computation graph to extract dimension-wise derivatives efficiently with automatic differentiation. We demonstrate these cheap differential operators for solving root-finding subproblems in implicit ODE solvers, exact density evaluation for continuous normalizing flows, and evaluating the Fokker--Planck equation for training stochastic differential equation models.",2019-12-08T00:08:50Z,2019-12-08T00:08:50Z,http://arxiv.org/abs/1912.03579v1,http://arxiv.org/pdf/1912.03579v1,"cs.LG, stat.ML"
Realizing the Frugal 5G Network,"Meghna Khaturia, Pranav Jha, Abhay Karandikar","In order to make effective use of the Internet, broadband connectivity is a pre-requisite. However, in the majority of rural areas in developing countries, high-speed connectivity is unavailable. The Frugal 5G network architecture presented in this paper aims at enabling broadband in rural areas by addressing the challenges associated with it. The work presented in this paper is a development over our previous work, in which we proposed abstract network architecture for Frugal 5G. In this paper, we provide an innovative solution to realize the Frugal 5G network. We identify the key system requirements and show that the proposed solution enables an uncomplicated and flexible realization of the Frugal 5G network. We are currently building a testbed to implement the proposed changes.",2019-12-09T11:10:47Z,2019-12-09T11:10:47Z,http://arxiv.org/abs/1912.03965v1,http://arxiv.org/pdf/1912.03965v1,"cs.NI, eess.SP"
Deep Connectomics Networks: Neural Network Architectures Inspired by   Neuronal Networks,"Nicholas Roberts, Dian Ang Yap, Vinay Uday Prabhu","The interplay between inter-neuronal network topology and cognition has been studied deeply by connectomics researchers and network scientists, which is crucial towards understanding the remarkable efficacy of biological neural networks. Curiously, the deep learning revolution that revived neural networks has not paid much attention to topological aspects. The architectures of deep neural networks (DNNs) do not resemble their biological counterparts in the topological sense. We bridge this gap by presenting initial results of Deep Connectomics Networks (DCNs) as DNNs with topologies inspired by real-world neuronal networks. We show high classification accuracy obtained by DCNs whose architecture was inspired by the biological neuronal networks of C. Elegans and the mouse visual cortex.",2019-12-19T01:59:20Z,2019-12-19T01:59:20Z,http://arxiv.org/abs/1912.08986v1,http://arxiv.org/pdf/1912.08986v1,"cs.LG, cs.NE, stat.ML"
TextNAS: A Neural Architecture Search Space tailored for Text   Representation,"Yujing Wang, Yaming Yang, Yiren Chen, Jing Bai, Ce Zhang, Guinan Su, Xiaoyu Kou, Yunhai Tong, Mao Yang, Lidong Zhou","Learning text representation is crucial for text classification and other language related tasks. There are a diverse set of text representation networks in the literature, and how to find the optimal one is a non-trivial problem. Recently, the emerging Neural Architecture Search (NAS) techniques have demonstrated good potential to solve the problem. Nevertheless, most of the existing works of NAS focus on the search algorithms and pay little attention to the search space. In this paper, we argue that the search space is also an important human prior to the success of NAS in different applications. Thus, we propose a novel search space tailored for text representation. Through automatic search, the discovered network architecture outperforms state-of-the-art models on various public datasets on text classification and natural language inference tasks. Furthermore, some of the design principles found in the automatic network agree well with human intuition.",2019-12-23T10:51:58Z,2019-12-23T10:51:58Z,http://arxiv.org/abs/1912.10729v1,http://arxiv.org/pdf/1912.10729v1,"cs.LG, cs.CL, cs.NE, stat.ML"
Random CapsNet Forest Model for Imbalanced Malware Type Classification   Task,"Aykut Çayır, Uğur Ünal, Hasan Dağ","Behavior of a malware varies with respect to malware types. Therefore,knowing type of a malware affects strategies of system protection softwares. Many malware type classification models empowered by machine and deep learning achieve superior accuracies to predict malware types.Machine learning based models need to do heavy feature engineering and feature engineering is dominantly effecting performance of models.On the other hand, deep learning based models require less feature engineering than machine learning based models. However, traditional deep learning architectures and components cause very complex and data sensitive models. Capsule network architecture minimizes this complexity and data sensitivity unlike classical convolutional neural network architectures. This paper proposes an ensemble capsule network model based on bootstrap aggregating technique. The proposed method are tested on two malware datasets, whose the-state-of-the-art results are well-known.",2019-12-20T06:40:40Z,2020-08-23T20:21:04Z,http://arxiv.org/abs/1912.10836v4,http://arxiv.org/pdf/1912.10836v4,"cs.CR, cs.CV, cs.LG, stat.ML"
Neural Architecture Search on Acoustic Scene Classification,"Jixiang Li, Chuming Liang, Bo Zhang, Zhao Wang, Fei Xiang, Xiangxiang Chu","Convolutional neural networks are widely adopted in Acoustic Scene Classification (ASC) tasks, but they generally carry a heavy computational burden. In this work, we propose a lightweight yet high-performing baseline network inspired by MobileNetV2, which replaces square convolutional kernels with unidirectional ones to extract features alternately in temporal and frequency dimensions. Furthermore, we explore a dynamic architecture space built on the basis of the proposed baseline with the recent Neural Architecture Search (NAS) paradigm, which first trains a supernet that incorporates all candidate networks and then applies a well-known evolutionary algorithm NSGA-II to discover more efficient networks with higher accuracy and lower computational cost. Experimental results demonstrate that our searched network is competent in ASC tasks, which achieves 90.3% F1-score on the DCASE2018 task 5 evaluation set, marking a new state-of-the-art performance while saving 25% of FLOPs compared to our baseline network.",2019-12-30T06:35:12Z,2020-08-05T04:58:06Z,http://arxiv.org/abs/1912.12825v2,http://arxiv.org/pdf/1912.12825v2,"cs.SD, cs.LG, eess.AS, stat.ML"
Self-supervised Neural Architecture Search,"Sapir Kaplan, Raja Giryes","Neural Architecture Search (NAS) has been used recently to achieve improved performance in various tasks and most prominently in image classification. Yet, current search strategies rely on large labeled datasets, which limit their usage in the case where only a smaller fraction of the data is annotated. Self-supervised learning has shown great promise in training neural networks using unlabeled data. In this work, we propose a self-supervised neural architecture search (SSNAS) that allows finding novel network models without the need for labeled data. We show that such a search leads to comparable results to supervised training with a ""fully labeled"" NAS and that it can improve the performance of self-supervised learning. Moreover, we demonstrate the advantage of the proposed approach when the number of labels in the search is relatively small.",2020-07-03T05:09:30Z,2020-07-03T05:09:30Z,http://arxiv.org/abs/2007.01500v1,http://arxiv.org/pdf/2007.01500v1,"cs.LG, cs.CV, stat.ML"
Parametric machines: a fresh approach to architecture search,"Pietro Vertechi, Mattia G. Bergomi","Using tools from topology and functional analysis, we provide a framework where artificial neural networks, and their architectures, can be formally described. We define the notion of machine in a general topological context and show how simple machines can be combined into more complex ones. We explore finite- and infinite-depth machines, which generalize neural networks and neural ordinary differential equations. Borrowing ideas from functional analysis and kernel methods, we build complete, normed, infinite-dimensional spaces of machines, and we discuss how to find optimal architectures and parameters -- within those spaces -- to solve a given computational problem. In our numerical experiments, these kernel-inspired networks can outperform classical neural networks when the training dataset is small.",2020-07-06T14:27:06Z,2022-11-29T13:03:04Z,http://arxiv.org/abs/2007.02777v3,http://arxiv.org/pdf/2007.02777v3,"cs.LG, stat.ML, 18A20, 47L05, I.2.6"
Meta-Learning Symmetries by Reparameterization,"Allan Zhou, Tom Knowles, Chelsea Finn","Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know the symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is an approach for learning equivariances from data, without needing to design custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably represent equivariance-inducing parameter sharing for any finite group of symmetry transformations. Our experiments suggest that it can automatically learn to encode equivariances to common transformations used in image processing tasks. We provide our experiment code at https://github.com/AllanYangZhou/metalearning-symmetries.",2020-07-06T17:59:54Z,2021-03-30T06:44:43Z,http://arxiv.org/abs/2007.02933v3,http://arxiv.org/pdf/2007.02933v3,"cs.LG, stat.ML"
Journey Towards Tiny Perceptual Super-Resolution,"Royson Lee, Łukasz Dudziak, Mohamed Abdelfattah, Stylianos I. Venieris, Hyeji Kim, Hongkai Wen, Nicholas D. Lane","Recent works in single-image perceptual super-resolution (SR) have demonstrated unprecedented performance in generating realistic textures by means of deep convolutional networks. However, these convolutional models are excessively large and expensive, hindering their effective deployment to end devices. In this work, we propose a neural architecture search (NAS) approach that integrates NAS and generative adversarial networks (GANs) with recent advances in perceptual SR and pushes the efficiency of small perceptual SR models to facilitate on-device execution. Specifically, we search over the architectures of both the generator and the discriminator sequentially, highlighting the unique challenges and key observations of searching for an SR-optimized discriminator and comparing them with existing discriminator architectures in the literature. Our tiny perceptual SR (TPSR) models outperform SRGAN and EnhanceNet on both full-reference perceptual metric (LPIPS) and distortion metric (PSNR) while being up to 26.4$\times$ more memory efficient and 33.6$\times$ more compute efficient respectively.",2020-07-08T18:24:40Z,2020-07-08T18:24:40Z,http://arxiv.org/abs/2007.04356v1,http://arxiv.org/pdf/2007.04356v1,"eess.IV, cs.CV"
Searching for Efficient Architecture for Instrument Segmentation in   Robotic Surgery,"Daniil Pakhomov, Nassir Navab","Segmentation of surgical instruments is an important problem in robot-assisted surgery: it is a crucial step towards full instrument pose estimation and is directly used for masking of augmented reality overlays during surgical procedures. Most applications rely on accurate real-time segmentation of high-resolution surgical images. While previous research focused primarily on methods that deliver high accuracy segmentation masks, majority of them can not be used for real-time applications due to their computational cost. In this work, we design a light-weight and highly-efficient deep residual architecture which is tuned to perform real-time inference of high-resolution images. To account for reduced accuracy of the discovered light-weight deep residual network and avoid adding any additional computational burden, we perform a differentiable search over dilation rates for residual units of our network. We test our discovered architecture on the EndoVis 2017 Robotic Instruments dataset and verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff with a speed of up to 125 FPS on high resolution images.",2020-07-08T21:38:29Z,2020-07-08T21:38:29Z,http://arxiv.org/abs/2007.04449v1,http://arxiv.org/pdf/2007.04449v1,"cs.CV, eess.IV"
AutoEmbedder: A semi-supervised DNN embedding system for clustering,"Abu Quwsar Ohi, M. F. Mridha, Farisa Benta Safir, Md. Abdul Hamid, Muhammad Mostafa Monowar","Clustering is widely used in unsupervised learning method that deals with unlabeled data. Deep clustering has become a popular study area that relates clustering with Deep Neural Network (DNN) architecture. Deep clustering method downsamples high dimensional data, which may also relate clustering loss. Deep clustering is also introduced in semi-supervised learning (SSL). Most SSL methods depend on pairwise constraint information, which is a matrix containing knowledge if data pairs can be in the same cluster or not. This paper introduces a novel embedding system named AutoEmbedder, that downsamples higher dimensional data to clusterable embedding points. To the best of our knowledge, this is the first research endeavor that relates to traditional classifier DNN architecture with a pairwise loss reduction technique. The training process is semi-supervised and uses Siamese network architecture to compute pairwise constraint loss in the feature learning phase. The AutoEmbedder outperforms most of the existing DNN based semi-supervised methods tested on famous datasets.",2020-07-11T19:00:45Z,2020-07-11T19:00:45Z,http://arxiv.org/abs/2007.05830v1,http://arxiv.org/pdf/2007.05830v1,"cs.LG, cs.CV, stat.ML"
MS-NAS: Multi-Scale Neural Architecture Search for Medical Image   Segmentation,"Xingang Yan, Weiwen Jiang, Yiyu Shi, Cheng Zhuo","The recent breakthroughs of Neural Architecture Search (NAS) have motivated various applications in medical image segmentation. However, most existing work either simply rely on hyper-parameter tuning or stick to a fixed network backbone, thereby limiting the underlying search space to identify more efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS) framework that is featured with multi-scale search space from network backbone to cell operation, and multi-scale fusion capability to fuse features with different sizes. To mitigate the computational overhead due to the larger search space, a partial channel connection scheme and a two-step decoding method are utilized to reduce computational overhead while maintaining optimization quality. Experimental results show that on various datasets for segmentation, MS-NAS outperforms the state-of-the-art methods and achieves 0.6-5.4% mIOU and 0.4-3.5% DSC improvements, while the computational resource consumption is reduced by 18.0-24.9%.",2020-07-13T02:02:00Z,2020-07-13T02:02:00Z,http://arxiv.org/abs/2007.06151v1,http://arxiv.org/pdf/2007.06151v1,"eess.IV, cs.CV"
Representation Transfer by Optimal Transport,"Xuhong Li, Yves Grandvalet, Rémi Flamary, Nicolas Courty, Dejing Dou","Learning generic representations with deep networks requires massive training samples and significant computer resources. To learn a new specific task, an important issue is to transfer the generic teacher's representation to a student network. In this paper, we propose to use a metric between representations that is based on a functional view of neurons. We use optimal transport to quantify the match between two representations, yielding a distance that embeds some invariances inherent to the representation of deep networks. This distance defines a regularizer promoting the similarity of the student's representation with that of the teacher. Our approach can be used in any learning context where representation transfer is applicable. We experiment here on two standard settings: inductive transfer learning, where the teacher's representation is transferred to a student network of same architecture for a new related task, and knowledge distillation, where the teacher's representation is transferred to a student of simpler architecture for the same task (model compression). Our approach also lends itself to solving new learning problems; we demonstrate this by showing how to directly transfer the teacher's representation to a simpler architecture student for a new related task.",2020-07-13T23:42:06Z,2021-02-26T06:34:09Z,http://arxiv.org/abs/2007.06737v2,http://arxiv.org/pdf/2007.06737v2,"cs.LG, stat.ML"
Area- Efficient VLSI Implementation of Serial-In Parallel-Out Multiplier   Using Polynomial Representation in Finite Field GF(2m),"Saeideh Nabipour, Gholamreza Zare Fatin, Javad Javidan","Finite field multiplier is mainly used in error-correcting codes and signal processing. Finite field multiplier is regarded as the bottleneck arithmetic unit for such applications and it is the most complicated operation over finite field GF(2m) which requires a huge amount of logic resources. In this paper, a new modified serial-in parallel-out multiplication algorithm with interleaved modular reduction is suggested. The proposed method offers efficient area architecture as compared to proposed algorithms in the literature. The reduced finite field multiplier complexity is achieved by means of utilizing logic NAND gate in a particular architecture. The efficiency of the proposed architecture is evaluated based on criteria such as time (latency, critical path) and space (gate-latch number) complexity. A detailed comparative analysis indicates that, the proposed finite field multiplier based on logic NAND gate outperforms previously known results",2020-07-16T12:20:30Z,2023-09-13T19:11:53Z,http://arxiv.org/abs/2007.08284v5,http://arxiv.org/pdf/2007.08284v5,"cs.IT, math.IT, 94D99, B.2"
HPIPE: Heterogeneous Layer-Pipelined and Sparse-Aware CNN Inference for   FPGAs,"Mathew Hall, Vaughn Betz","We present both a novel Convolutional Neural Network (CNN) accelerator architecture and a network compiler for FPGAs that outperforms all prior work. Instead of having generic processing elements that together process one layer at a time, our network compiler statically partitions available device resources and builds custom-tailored hardware for each layer of a CNN. By building hardware for each layer we can pack our controllers into fewer lookup tables and use dedicated routing. These efficiencies enable our accelerator to utilize 2x the DSPs and operate at more than 2x the frequency of prior work on sparse CNN acceleration on FPGAs. We evaluate the performance of our architecture on both sparse Resnet-50 and dense MobileNet Imagenet classifiers on a Stratix 10 2800 FPGA. We find that the sparse Resnet-50 model has throughput at a batch size of 1 of 4550 images/s, which is nearly 4x the throughput of NVIDIA's fastest machine learning targeted GPU, the V100, and outperforms all prior work on FPGAs.",2020-07-20T20:17:58Z,2020-07-20T20:17:58Z,http://arxiv.org/abs/2007.10451v1,http://arxiv.org/pdf/2007.10451v1,"cs.AR, B.5.1"
A Preliminary Exploration into an Alternative CellLineNet: An   Evolutionary Approach,"Akwarandu Ugo Nwachuku, Xavier Lewis-Palmer, Darlington Ahiale Akogo","Within this paper, the exploration of an evolutionary approach to an alternative CellLineNet: a convolutional neural network adept at the classification of epithelial breast cancer cell lines, is presented. This evolutionary algorithm introduces control variables that guide the search of architectures in the search space of inverted residual blocks, bottleneck blocks, residual blocks and a basic 2x2 convolutional block. The promise of EvoCELL is predicting what combination or arrangement of the feature extracting blocks that produce the best model architecture for a given task. Therein, the performance of how the fittest model evolved after each generation is shown. The final evolved model CellLineNet V2 classifies 5 types of epithelial breast cell lines consisting of two human cancer lines, 2 normal immortalized lines, and 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A and HC11). The Multiclass Cell Line Classification Convolutional Neural Network extends our earlier work on a Binary Breast Cancer Cell Line Classification model. This paper presents an on-going exploratory approach to neural network architecture design and is presented for further study.",2020-07-26T02:36:56Z,2020-07-26T02:36:56Z,http://arxiv.org/abs/2007.13044v1,http://arxiv.org/pdf/2007.13044v1,"cs.NE, cs.CV, eess.IV"
EagerNet: Early Predictions of Neural Networks for Computationally   Efficient Intrusion Detection,"Fares Meghdouri, Maximilian Bachl, Tanja Zseby","Fully Connected Neural Networks (FCNNs) have been the core of most state-of-the-art Machine Learning (ML) applications in recent years and also have been widely used for Intrusion Detection Systems (IDSs). Experimental results from the last years show that generally deeper neural networks with more layers perform better than shallow models. Nonetheless, with the growing number of layers, obtaining fast predictions with less resources has become a difficult task despite the use of special hardware such as GPUs. We propose a new architecture to detect network attacks with minimal resources. The architecture is able to deal with either binary or multiclass classification problems and trades prediction speed for the accuracy of the network. We evaluate our proposal with two different network intrusion detection datasets. Results suggest that it is possible to obtain comparable accuracies to simple FCNNs without evaluating all layers for the majority of samples, thus obtaining early predictions and saving energy and computational efforts.",2020-07-27T11:31:37Z,2020-10-15T16:58:16Z,http://arxiv.org/abs/2007.13444v2,http://arxiv.org/pdf/2007.13444v2,"cs.LG, cs.NI, stat.ML"
Parsimonious Inference,"Jed A. Duersch, Thomas A. Catanach","Bayesian inference provides a uniquely rigorous approach to obtain principled justification for uncertainty in predictions, yet it is difficult to articulate suitably general prior belief in the machine learning context, where computational architectures are pure abstractions subject to frequent modifications by practitioners attempting to improve results. Parsimonious inference is an information-theoretic formulation of inference over arbitrary architectures that formalizes Occam's Razor; we prefer simple and sufficient explanations. Our universal hyperprior assigns plausibility to prior descriptions, encoded as sequences of symbols, by expanding on the core relationships between program length, Kolmogorov complexity, and Solomonoff's algorithmic probability. We then cast learning as information minimization over our composite change in belief when an architecture is specified, training data are observed, and model parameters are inferred. By distinguishing model complexity from prediction information, our framework also quantifies the phenomenon of memorization.   Although our theory is general, it is most critical when datasets are limited, e.g. small or skewed. We develop novel algorithms for polynomial regression and random forests that are suitable for such data, as demonstrated by our experiments. Our approaches combine efficient encodings with prudent sampling strategies to construct predictive ensembles without cross-validation, thus addressing a fundamental challenge in how to efficiently obtain predictions from data.",2021-03-03T04:13:14Z,2021-03-03T04:13:14Z,http://arxiv.org/abs/2103.02165v1,http://arxiv.org/pdf/2103.02165v1,"stat.ML, cs.IT, cs.LG, math.IT"
Fused Deep Features Based Classification Framework for COVID-19   Classification with Optimized MLP,"Saban Ozturk, Enes Yigit, Umut Ozkaya","The new type of Coronavirus disease called COVID-19 continues to spread quite rapidly. Although it shows some specific symptoms, this disease, which can show different symptoms in almost every individual, has caused hundreds of thousands of patients to die. Although healthcare professionals work hard to prevent further loss of life, the rate of disease spread is very high. For this reason, the help of computer aided diagnosis (CAD) and artificial intelligence (AI) algorithms is vital. In this study, a method based on optimization of convolutional neural network (CNN) architecture, which is the most effective image analysis method of today, is proposed to fulfill the mentioned COVID-19 detection needs. First, COVID-19 images are trained using ResNet-50 and VGG-16 architectures. Then, features in the last layer of these two architectures are combined with feature fusion. These new image features matrices obtained with feature fusion are classified for COVID detection. A multi-layer perceptron (MLP) structure optimized by the whale optimization algorithm is used for the classification process. The obtained results show that the performance of the proposed framework is almost 4.5% higher than VGG-16 performance and almost 3.5% higher than ResNet-50 performance.",2021-03-15T14:30:12Z,2021-03-15T14:30:12Z,http://arxiv.org/abs/2103.09904v1,http://arxiv.org/pdf/2103.09904v1,"cs.LG, eess.IV, F.2.2"
OmniPose: A Multi-Scale Framework for Multi-Person Pose Estimation,"Bruno Artacho, Andreas Savakis","We propose OmniPose, a single-pass, end-to-end trainable framework, that achieves state-of-the-art results for multi-person pose estimation. Using a novel waterfall module, the OmniPose architecture leverages multi-scale feature representations that increase the effectiveness of backbone feature extractors, without the need for post-processing. OmniPose incorporates contextual information across scales and joint localization with Gaussian heatmap modulation at the multi-scale feature extractor to estimate human pose with state-of-the-art accuracy. The multi-scale representations, obtained by the improved waterfall module in OmniPose, leverage the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Our results on multiple datasets demonstrate that OmniPose, with an improved HRNet backbone and waterfall module, is a robust and efficient architecture for multi-person pose estimation that achieves state-of-the-art results.",2021-03-18T11:30:31Z,2021-03-18T11:30:31Z,http://arxiv.org/abs/2103.10180v1,http://arxiv.org/pdf/2103.10180v1,"cs.CV, cs.LG, eess.IV"
Quantum Machine Learning with HQC Architectures using non-Classically   Simulable Feature Maps,"Syed Farhan Ahmad, Raghav Rawat, Minal Moharir","Hybrid Quantum-Classical (HQC) Architectures are used in near-term NISQ Quantum Computers for solving Quantum Machine Learning problems. The quantum advantage comes into picture due to the exponential speedup offered over classical computing. One of the major challenges in implementing such algorithms is the choice of quantum embeddings and the use of a functionally correct quantum variational circuit. In this paper, we present an application of QSVM (Quantum Support Vector Machines) to predict if a person will require mental health treatment in the tech world in the future using the dataset from OSMI Mental Health Tech Surveys. We achieve this with non-classically simulable feature maps and prove that NISQ HQC Architectures for Quantum Machine Learning can be used alternatively to create good performance models in near-term real-world applications.",2021-03-21T12:28:46Z,2024-04-14T01:07:38Z,http://arxiv.org/abs/2103.11381v2,http://arxiv.org/pdf/2103.11381v2,"quant-ph, cs.LG"
Spatially Dependent U-Nets: Highly Accurate Architectures for Medical   Imaging Segmentation,"João B. S. Carvalho, João A. Santinha, Đorđe Miladinović, Joachim M. Buhmann","In clinical practice, regions of interest in medical imaging often need to be identified through a process of precise image segmentation. The quality of this image segmentation step critically affects the subsequent clinical assessment of the patient status. To enable high accuracy, automatic image segmentation, we introduce a novel deep neural network architecture that exploits the inherent spatial coherence of anatomical structures and is well equipped to capture long-range spatial dependencies in the segmented pixel/voxel space. In contrast to the state-of-the-art solutions based on convolutional layers, our approach leverages on recently introduced spatial dependency layers that have an unbounded receptive field and explicitly model the inductive bias of spatial coherence. Our method performs favourably to commonly used U-Net and U-Net++ architectures as demonstrated by improved Dice and Jaccardscore in three different medical segmentation tasks: nuclei segmentation in microscopy images, polyp segmentation in colonoscopy videos, and liver segmentation in abdominal CT scans.",2021-03-22T10:37:20Z,2021-03-22T10:37:20Z,http://arxiv.org/abs/2103.11713v1,http://arxiv.org/pdf/2103.11713v1,"eess.IV, cs.CV"
Structured Inverted-File k-Means Clustering for High-Dimensional Sparse   Data,"Kazuo Aoyama, Kazumi Saito","This paper presents an architecture-friendly k-means clustering algorithm called SIVF for a large-scale and high-dimensional sparse data set. Algorithm efficiency on time is often measured by the number of costly operations such as similarity calculations. In practice, however, it depends greatly on how the algorithm adapts to an architecture of the computer system which it is executed on. Our proposed SIVF employs invariant centroid-pair based filter (ICP) to decrease the number of similarity calculations between a data object and centroids of all the clusters. To maximize the ICP performance, SIVF exploits for a centroid set an inverted-file that is structured so as to reduce pipeline hazards. We demonstrate in our experiments on real large-scale document data sets that SIVF operates at higher speed and with lower memory consumption than existing algorithms. Our performance analysis reveals that SIVF achieves the higher speed by suppressing performance degradation factors of the number of cache misses and branch mispredictions rather than less similarity calculations.",2021-03-30T07:54:02Z,2021-03-30T07:54:02Z,http://arxiv.org/abs/2103.16141v1,http://arxiv.org/pdf/2103.16141v1,"stat.ML, cs.AR, cs.LG"
Parallel architectures for fuzzy triadic similarity learning,"Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui","In a context of document co-clustering, we define a new similarity measure which iteratively computes similarity while combining fuzzy sets in a three-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal with uncertainty offers by the fuzzy sets. Moreover, with the development of the Web and the high availability of storage spaces, more and more documents become accessible. Documents can be provided from multiple sites and make similarity computation an expensive processing. This problem motivated us to use parallel computing. In this paper, we introduce parallel architectures which are able to treat large and multi-source data sets by a sequential, a merging or a splitting-based process. Then, we proceed to a local and a central (or global) computing using the basic FT-Sim measure. The idea behind these architectures is to reduce both time and space complexities thanks to parallel computation.",2013-12-21T16:51:26Z,2013-12-21T16:51:26Z,http://arxiv.org/abs/1312.6273v1,http://arxiv.org/pdf/1312.6273v1,"cs.DC, cs.LG, stat.ML"
Large-scale Antenna Operation in Heterogeneous Cloud Radio Access   Networks: A Partial Centralization Approach,"Sangkyu Park, Chan-Byoung Chae, Saewoong Bahk","To satisfy the ever-increasing capacity demand and quality of service (QoS) requirements of users, 5G cellular systems will take the form of heterogeneous networks (HetNets) that consist of macro cells and small cells. To build and operate such systems, mobile operators have given significant attention to cloud radio access networks (C-RANs) due to their beneficial features of performance optimization and cost effectiveness. Along with the architectural enhancement of C-RAN, large-scale antennas (a.k.a. massive MIMO) at cell sites contribute greatly to increased network capacity either with higher spectral efficiency or through permitting many users at once. In this article, we discuss the challenging issues of C-RAN based HetNets (H-CRAN), especially with respect to large-scale antenna operation. We provide an overview of existing C-RAN architectures in terms of large-scale antenna operation and promote a partially centralized approach. This approach reduces, remarkably, fronthaul overheads in CRANs with large-scale antennas. We also provide some insights into its potential and applicability in the fronthaul bandwidthlimited H-CRAN with large-scale antennas.",2015-04-12T10:27:56Z,2015-04-17T18:07:40Z,http://arxiv.org/abs/1504.02954v2,http://arxiv.org/pdf/1504.02954v2,"cs.NI, cs.IT, math.IT"
Cooperative Interference Mitigation and Handover Management for   Heterogeneous Cloud Small Cell Networks,"Haijun Zhang, Chunxiao Jiang, Julian Cheng, Victor C. M. Leung","Heterogeneous small cell network has attracted much attention to satisfy users' explosive data traffic requirements. Heterogeneous cloud small cell network (HCSNet), which combines cloud computing and heterogeneous small cell network, will likely play an important role in 5G mobile communication networks. However, with massive deployment of small cells, co-channel interference and handover management are two important problems in HCSNet, especially for cell edge users. In this article, we examine the problems of cooperative interference mitigation and handover management in HCSNet. A network architecture is described to combine cloud radio access network with small cells. An effective coordinated multi-point (CoMP) clustering scheme using affinity propagation is adopted to mitigate cell edge users' interference. A low complexity handover management scheme is presented, and its signaling procedure is analyzed in HCSNet. Numerical results show that the proposed network architecture, CoMP clustering scheme and handover management scheme can significantly increase the capacity of HCSNet while maintaining users' quality of service.",2015-04-30T04:06:37Z,2015-05-01T01:25:04Z,http://arxiv.org/abs/1504.08076v2,http://arxiv.org/pdf/1504.08076v2,"cs.IT, cs.NI, math.IT"
Block Copolymer Derived Multifunctional Gyroidal Monoliths for 3-D   Electrical Energy Storage Applications,"Jörg G. Werner, Gabriel G. Rodríguez-Calero, Héctor D. Abruña, Ulrich Wiesner","Multifunctional three-dimensional (3-D) nano-architectures, integrating all device components within tens of nanometers, offer great promise for next generation electrical energy storage applications, but have remained challenging to achieve. The lack of appropriate synthesis methods, enabling precise 3-D spatial control at the nanoscale, remains a key issue holding back the development of such intricate architectures. Here we present an approach to such systems based on the bottom-up synthesis of penta-continuous nanohybrid monoliths with four functional components integrated in a triblock terpolymer derived core-shell double gyroid architecture. Two distinct 3 D interpenetrating networks serving as cathode and current collector are separated from a carbon anode matrix by continuous, ultrathin polymer electrolyte shells. All periodically ordered domains are less than 20 nm in their layer dimensions and integrated throughout the macroscopic monolith. Initial electrochemical measurements with the Li-ion/S system exhibit reversible battery-like charge-discharge characteristics with orders of magnitude decreases in footprint area over conventional flat thin layer designs.",2017-06-07T11:20:37Z,2017-06-07T11:20:37Z,http://arxiv.org/abs/1706.02134v1,http://arxiv.org/pdf/1706.02134v1,"physics.chem-ph, cond-mat.mtrl-sci"
An Online Learning Approach to Generative Adversarial Networks,"Paulina Grnarova, Kfir Y. Levy, Aurelien Lucchi, Thomas Hofmann, Andreas Krause","We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",2017-06-10T18:49:07Z,2017-06-10T18:49:07Z,http://arxiv.org/abs/1706.03269v1,http://arxiv.org/pdf/1706.03269v1,"cs.LG, stat.ML"
Sparse Neural Networks Topologies,"Alfred Bourely, John Patrick Boueri, Krzysztof Choromonski","We propose Sparse Neural Network architectures that are based on random or structured bipartite graph topologies. Sparse architectures provide compression of the models learned and speed-ups of computations, they can also surpass their unstructured or fully connected counterparts. As we show, even more compact topologies of the so-called SNN (Sparse Neural Network) can be achieved with the use of structured graphs of connections between consecutive layers of neurons. In this paper, we investigate how the accuracy and training speed of the models depend on the topology and sparsity of the neural network. Previous approaches using sparcity are all based on fully connected neural network models and create sparcity during training phase, instead we explicitly define a sparse architectures of connections before the training. Building compact neural network models is coherent with empirical observations showing that there is much redundancy in learned neural network models. We show experimentally that the accuracy of the models learned with neural networks depends on expander-like properties of the underlying topologies such as the spectral gap and algebraic connectivity rather than the density of the graphs of connections.",2017-06-18T16:30:25Z,2017-06-18T16:30:25Z,http://arxiv.org/abs/1706.05683v1,http://arxiv.org/pdf/1706.05683v1,"cs.LG, cs.NE, stat.ML"
Multi-tap Digital Canceller for Full-Duplex Applications,"Paul Ferrand, Melissa Duarte",We identify phase noise as a bottleneck for the performance of digital self-interference cancellers that utilize a single auxiliary receiver---single-tap digital cancellers---and operate in multipath propagation environments. Our analysis demonstrates that the degradation due to phase noise is caused by a mismatch between the analog delay of the auxiliary receiver and the different delays of the multipath components of the self-interference signal. We propose a novel multi-tap digital self-interference canceller architecture that is based on multiple auxiliary receivers and a customized Normalized-Least-Mean-Squared (NLMS) filtering for self-interference regeneration. Our simulation results demonstrate that our proposed architecture is more robust to phase noise impairments and can in some cases achieve 10~dB larger self-interference cancellation than the single-tap architecture.,2017-06-29T14:03:31Z,2017-06-29T14:03:31Z,http://arxiv.org/abs/1706.09764v1,http://arxiv.org/pdf/1706.09764v1,"cs.IT, math.IT"
Moonshine: Distilling with Cheap Convolutions,"Elliot J. Crowley, Gavin Gray, Amos Storkey","Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data.",2017-11-07T17:21:06Z,2019-01-17T12:26:19Z,http://arxiv.org/abs/1711.02613v4,http://arxiv.org/pdf/1711.02613v4,"stat.ML, cs.CV, cs.LG"
Geometry-constrained Degrees of Freedom Analysis for Imaging Systems:   Monostatic and Multistatic,"Babak Mamandipoor, Amin Arbabian, Upamanyu Madhow","In this paper, we develop a theoretical framework for analyzing the measurable information content of an unknown scene through an active electromagnetic imaging array. We consider monostatic and multistatic array architectures in a one-dimensional setting. Our main results include the following: (a) we introduce the space-bandwidth product (SBP), and show that, under the Born approximation, it provides an accurate prediction of the number of the degrees of freedom (DoF) as constrained by the geometry of the scene and the imaging system; (b) we show that both monostatic and multistatic architectures have the same number of DoF; (c) we show that prior DoF analysis based on the more restrictive Fresnel approximation are obtained by specializing our results; (d) we investigate matched-filter (back-propagation) and pseudoinverse image reconstruction schemes, and analyze the achievable resolution through these methods. Our analytical framework opens up new avenues to investigate image formation techniques that aim to reconstruct the reflectivity function of the scene by solving an inverse scattering problem, and provides insights on achievable resolution. For example, we show that matched-filter reconstruction leads to a significant resolution loss for multistatic architectures.",2017-11-09T20:26:59Z,2018-08-04T23:40:04Z,http://arxiv.org/abs/1711.03585v2,http://arxiv.org/pdf/1711.03585v2,"cs.IT, math.IT"
Attend and Diagnose: Clinical Time Series Analysis using Attention   Models,"Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias","With widespread adoption of electronic health records, there is an increased emphasis for predictive models that can effectively deal with clinical time-series data. Powered by Recurrent Neural Network (RNN) architectures with Long Short-Term Memory (LSTM) units, deep neural networks have achieved state-of-the-art results in several clinical prediction tasks. Despite the success of RNNs, its sequential nature prohibits parallelized computing, thus making it inefficient particularly when processing long sequences. Recently, architectures which are based solely on attention mechanisms have shown remarkable success in transduction tasks in NLP, while being computationally superior. In this paper, for the first time, we utilize attention models for clinical time-series modeling, thereby dispensing recurrence entirely. We develop the \textit{SAnD} (Simply Attend and Diagnose) architecture, which employs a masked, self-attention mechanism, and uses positional encoding and dense interpolation strategies for incorporating temporal order. Furthermore, we develop a multi-task variant of \textit{SAnD} to jointly infer models with multiple diagnosis tasks. Using the recent MIMIC-III benchmark datasets, we demonstrate that the proposed approach achieves state-of-the-art performance in all tasks, outperforming LSTM models and classical baselines with hand-engineered features.",2017-11-10T16:26:14Z,2017-11-19T21:19:12Z,http://arxiv.org/abs/1711.03905v2,http://arxiv.org/pdf/1711.03905v2,"stat.ML, cs.LG"
Spatial Channel Covariance Estimation for the Hybrid MIMO Architecture:   A Compressive Sensing Based Approach,"Sungwoo Park, Robert W. Heath Jr","Spatial channel covariance information can replace full knowledge of the entire channel matrix for designing analog precoders in hybrid multiple-input-multiple-output (MIMO) architecture. Spatial channel covariance estimation, however, is challenging for the hybrid MIMO architecture because the estimator operating at baseband can only obtain a lower dimensional pre-combined signal through fewer radio frequency (RF) chains than antennas. In this paper, we propose two approaches for covariance estimation based on compressive sensing techniques. One is to apply a time-varying sensing matrix, and the other is to exploit the prior knowledge that the covariance matrix is Hermitian. We present the rationale of the two ideas and validate the superiority of the proposed methods by theoretical analysis and numerical simulations. We conclude the paper by extending the proposed algorithms from narrowband massive MIMO systems with a single receive antenna to wideband systems with multiple receive antennas.",2017-11-11T23:22:20Z,2017-11-11T23:22:20Z,http://arxiv.org/abs/1711.04207v1,http://arxiv.org/pdf/1711.04207v1,"cs.IT, math.IT"
Simple And Efficient Architecture Search for Convolutional Neural   Networks,"Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter","Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6% in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5%.",2017-11-13T11:23:36Z,2017-11-13T11:23:36Z,http://arxiv.org/abs/1711.04528v1,http://arxiv.org/pdf/1711.04528v1,"stat.ML, cs.AI, cs.LG"
Higher aggregation of gNodeBs in Cloud-RAN architectures via parallel   computing,"Veronica Quintuna Rodriguez, Fabrice Guillemin","In this paper, we address the virtualization and the centralization of real-time network functions, notably in the framework of Cloud RAN (C-RAN). We thoroughly analyze the required fronthaul capacity for the deployment of the proposed C-RAN architecture. We are specifically interested in the performance of the software based channel coding function. We develop a dynamic multi-threading approach to achieve parallel computing on a multi-core platform. Measurements from an OAI-based testbed show important gains in terms of latency; this enables the increase of the distance between the radio elements and the virtualized RAN functions and thus a higher aggregation of gNodeBs in edge data centers, referred to as Central Offices (COs).",2019-04-11T13:08:01Z,2019-04-11T13:08:01Z,http://arxiv.org/abs/1905.01141v1,http://arxiv.org/pdf/1905.01141v1,"cs.NI, cs.PF, 68M10"
High Frequency Residual Learning for Multi-Scale Image Classification,"Bowen Cheng, Rong Xiao, Jianfeng Wang, Thomas Huang, Lei Zhang","We present a novel high frequency residual learning framework, which leads to a highly efficient multi-scale network (MSNet) architecture for mobile and embedded vision problems. The architecture utilizes two networks: a low resolution network to efficiently approximate low frequency components and a high resolution network to learn high frequency residuals by reusing the upsampled low resolution features. With a classifier calibration module, MSNet can dynamically allocate computation resources during inference to achieve a better speed and accuracy trade-off. We evaluate our methods on the challenging ImageNet-1k dataset and observe consistent improvements over different base networks. On ResNet-18 and MobileNet with alpha=1.0, MSNet gains 1.5% accuracy over both architectures without increasing computations. On the more efficient MobileNet with alpha=0.25, our method gains 3.8% accuracy with the same amount of computations.",2019-05-07T15:47:27Z,2019-05-07T15:47:27Z,http://arxiv.org/abs/1905.02649v1,http://arxiv.org/pdf/1905.02649v1,"cs.CV, cs.LG, stat.ML"
Convolutional Neural Networks Considering Local and Global features for   Image Enhancement,"Yuma Kinoshita, Hitoshi Kiya","In this paper, we propose a novel convolutional neural network (CNN) architecture considering both local and global features for image enhancement. Most conventional image enhancement methods, including Retinex-based methods, cannot restore lost pixel values caused by clipping and quantizing. CNN-based methods have recently been proposed to solve the problem, but they still have a limited performance due to network architectures not handling global features. To handle both local and global features, the proposed architecture consists of three networks: a local encoder, a global encoder, and a decoder. In addition, high dynamic range (HDR) images are used for generating training data for our networks. The use of HDR images makes it possible to train CNNs with better-quality images than images directly captured with cameras. Experimental results show that the proposed method can produce higher-quality images than conventional image enhancement methods including CNN-based methods, in terms of various objective quality metrics: TMQI, entropy, NIQE, and BRISQUE.",2019-05-07T08:20:30Z,2019-05-07T08:20:30Z,http://arxiv.org/abs/1905.02899v1,http://arxiv.org/pdf/1905.02899v1,"eess.IV, cs.CV, cs.MM"
Training CNNs with Selective Allocation of Channels,"Jongheon Jeong, Jinwoo Shin","Recent progress in deep convolutional neural networks (CNNs) have enabled a simple paradigm of architecture design: larger models typically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more important to design models that generalize well under certain resource constraints, e.g. the number of parameters. In this paper, we propose a simple way to improve the capacity of any CNN model having large-scale features, without adding more parameters. In particular, we modify a standard convolutional layer to have a new functionality of channel-selectivity, so that the layer is trained to select important channels to re-distribute their parameters. Our experimental results under various CNN architectures and datasets demonstrate that the proposed new convolutional layer allows new optima that generalize better via efficient resource utilization, compared to the baseline.",2019-05-11T12:00:55Z,2019-05-11T12:00:55Z,http://arxiv.org/abs/1905.04509v1,http://arxiv.org/pdf/1905.04509v1,"cs.LG, cs.CV, stat.ML"
Vector Field Neural Networks,"Daniel Vieira, Joao Paixao","This work begins by establishing a mathematical formalization between different geometrical interpretations of Neural Networks, providing a first contribution. From this starting point, a new interpretation is explored, using the idea of implicit vector fields moving data as particles in a flow. A new architecture, Vector Fields Neural Networks(VFNN), is proposed based on this interpretation, with the vector field becoming explicit. A specific implementation of the VFNN using Euler's method to solve ordinary differential equations (ODEs) and gaussian vector fields is tested. The first experiments present visual results remarking the important features of the new architecture and providing another contribution with the geometrically interpretable regularization of model parameters. Then, the new architecture is evaluated for different hyperparameters and inputs, with the objective of evaluating the influence on model performance, computational time, and complexity. The VFNN model is compared against the known basic models Naive Bayes, Feed Forward Neural Networks, and Support Vector Machines(SVM), showing comparable, or better, results for different datasets. Finally, the conclusion provides many new questions and ideas for improvement of the model that can be used to increase model performance.",2019-05-16T21:09:10Z,2019-05-16T21:09:10Z,http://arxiv.org/abs/1905.07033v1,http://arxiv.org/pdf/1905.07033v1,"cs.LG, stat.ML"
DeepSwarm: Optimising Convolutional Neural Networks using Swarm   Intelligence,"Edvinas Byla, Wei Pang","In this paper we propose DeepSwarm, a novel neural architecture search (NAS) method based on Swarm Intelligence principles. At its core DeepSwarm uses Ant Colony Optimization (ACO) to generate ant population which uses the pheromone information to collectively search for the best neural architecture. Furthermore, by using local and global pheromone update rules our method ensures the balance between exploitation and exploration. On top of this, to make our method more efficient we combine progressive neural architecture search with weight reusability. Furthermore, due to the nature of ACO our method can incorporate heuristic information which can further speed up the search process. After systematic and extensive evaluation, we discover that on three different datasets (MNIST, Fashion-MNIST, and CIFAR-10) when compared to existing systems our proposed method demonstrates competitive performance. Finally, we open source DeepSwarm as a NAS library and hope it can be used by more deep learning researchers and practitioners.",2019-05-17T16:13:38Z,2019-05-17T16:13:38Z,http://arxiv.org/abs/1905.07350v1,http://arxiv.org/pdf/1905.07350v1,"cs.LG, cs.NE, stat.ML, I.2.6"
A Neural Network Architecture for Learning Word-Referent Associations in   Multiple Contexts,"Hansenclever F. Bassani, Aluizio F. R. Araujo","This article proposes a biologically inspired neurocomputational architecture which learns associations between words and referents in different contexts, considering evidence collected from the literature of Psycholinguistics and Neurolinguistics. The multi-layered architecture takes as input raw images of objects (referents) and streams of word's phonemes (labels), builds an adequate representation, recognizes the current context, and associates label with referents incrementally, by employing a Self-Organizing Map which creates new association nodes (prototypes) as required, adjusts the existing prototypes to better represent the input stimuli and removes prototypes that become obsolete/unused. The model takes into account the current context to retrieve the correct meaning of words with multiple meanings. Simulations show that the model can reach up to 78% of word-referent association accuracy in ambiguous situations and approximates well the learning rates of humans as reported by three different authors in five Cross-Situational Word Learning experiments, also displaying similar learning patterns in the different learning conditions.",2019-05-20T19:05:12Z,2019-05-20T19:05:12Z,http://arxiv.org/abs/1905.08300v1,http://arxiv.org/pdf/1905.08300v1,"cs.LG, cs.CL, stat.ML"
TurboNet: A Model-driven DNN Decoder Based on Max-Log-MAP Algorithm for   Turbo Code,"Yunfeng He, Jing Zhang, Chao-Kai Wen, Shi Jin","This paper presents TurboNet, a novel model-driven deep learning (DL) architecture for turbo decoding that combines DL with the traditional max-log-maximum a posteriori (MAP) algorithm. To design TurboNet, we unfold the original iterative structure for turbo decoding and replace each iteration by a deep neural network (DNN) decoding unit. In particular, the DNN decoding unit is obtained by parameterizing the max-log-MAP algorithm rather than replace the whole decoder with a black box fully connected DNN architecture. With the proposed architecture, the parameters can be efficiently learned from training data, and thus TurboNet learns to appropriately use systematic and parity information to offer higher error correction capabilities and decrease computational complexity compared with existing methods. Furthermore, simulation results prove TurboNet's superiority in signal-to-noise ratio generalizations.",2019-05-25T02:46:19Z,2019-05-25T02:46:19Z,http://arxiv.org/abs/1905.10502v1,http://arxiv.org/pdf/1905.10502v1,"eess.SP, cs.IT, math.IT"
Graph Attention Auto-Encoders,"Amin Salehi, Hasan Davulcu","Auto-encoders have emerged as a successful framework for unsupervised learning. However, conventional auto-encoders are incapable of utilizing explicit relations in structured data. To take advantage of relations in graph-structured data, several graph auto-encoders have recently been proposed, but they neglect to reconstruct either the graph structure or node attributes. In this paper, we present the graph attention auto-encoder (GATE), a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is able to reconstruct graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. In the encoder, by considering node attributes as initial node representations, each layer generates new representations of nodes by attending over their neighbors' representations. In the decoder, we attempt to reverse the encoding process to reconstruct node attributes. Moreover, node representations are regularized to reconstruct the graph structure. Our proposed architecture does not need to know the graph structure upfront, and thus it can be applied to inductive learning. Our experiments demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even exceeding the performance of supervised learning baselines in most cases.",2019-05-26T02:47:58Z,2019-05-26T02:47:58Z,http://arxiv.org/abs/1905.10715v1,http://arxiv.org/pdf/1905.10715v1,"cs.LG, cs.SI, stat.ML"
Rapid flipping of parametric phase states,"Martin Frimmer, Toni L. Heugel, Žiga Nosan, Felix Tebbenjohanns, David Hälg, Abdulkadir Akin, Christian L. Degen, Lukas Novotny, R. Chitra, Oded Zilberberg, Alexander Eichler","Since the invention of the solid-state transistor, the overwhelming majority of computers followed the von Neumann architecture that strictly separates logic operations and memory. Today, there is a revived interest in alternative computation models accompanied by the necessity to develop corresponding hardware architectures. The Ising machine, for example, is a variant of the celebrated Hopfield network based on the Ising model. It can be realized with artifcial spins such as the `parametron' that arises in driven nonlinear resonators. The parametron encodes binary information in the phase state of its oscillation. It enables, in principle, logic operations without energy transfer and the corresponding speed limitations. In this work, we experimentally demonstrate flipping of parametron phase states on a timescale of an oscillation period, much faster than the ringdown time \tau that is often (erroneously) deemed a fundamental limit for resonator operations. Our work establishes a new paradigm for resonator-based logic architectures.",2019-05-28T06:34:14Z,2019-12-20T08:27:02Z,http://arxiv.org/abs/1905.11630v2,http://arxiv.org/pdf/1905.11630v2,"physics.app-ph, cond-mat.mes-hall"
Near-Term Quantum-Classical Associative Adversarial Networks,"Eric R. Anschuetz, Cristian Zanoci","We introduce a new hybrid quantum-classical adversarial machine learning architecture called a quantum-classical associative adversarial network (QAAN). This architecture consists of a classical generative adversarial network with a small auxiliary quantum Boltzmann machine that is simultaneously trained on an intermediate layer of the discriminator of the generative network. We numerically study the performance of QAANs compared to their classical counterparts on the MNIST and CIFAR-10 data sets, and show that QAANs attain a higher quality of learning when evaluated using the Inception score and the Fr\'{e}chet Inception distance. As the QAAN architecture only relies on sampling simple local observables of a small quantum Boltzmann machine, this model is particularly amenable for implementation on the current and next generations of quantum devices.",2019-05-30T17:48:51Z,2019-05-30T17:48:51Z,http://arxiv.org/abs/1905.13205v1,http://arxiv.org/pdf/1905.13205v1,"cs.LG, quant-ph"
Efficient Forward Architecture Search,"Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee, Eric Horvitz, Debadeepta Dey","We propose a neural architecture search (NAS) algorithm, Petridish, to iteratively add shortcut connections to existing network layers. The added shortcut connections effectively perform gradient boosting on the augmented layers. The proposed algorithm is motivated by the feature selection algorithm forward stage-wise linear regression, since we consider NAS as a generalization of feature selection for regression, where NAS selects shortcuts among layers instead of selecting features. In order to reduce the number of trials of possible connection combinations, we train jointly all possible connections at each stage of growth while leveraging feature selection techniques to choose a subset of them. We experimentally show this process to be an efficient forward architecture search algorithm that can find competitive models using few GPU days in both the search space of repeatable network modules (cell-search) and the space of general networks (macro-search). Petridish is particularly well-suited for warm-starting from existing models crucial for lifelong-learning scenarios.",2019-05-31T00:10:17Z,2019-05-31T00:10:17Z,http://arxiv.org/abs/1905.13360v1,http://arxiv.org/pdf/1905.13360v1,"cs.LG, stat.ML"
Switching Variational Auto-Encoders for Noise-Agnostic Audio-visual   Speech Enhancement,"Mostafa Sadeghi, Xavier Alameda-Pineda","Recently, audio-visual speech enhancement has been tackled in the unsupervised settings based on variational auto-encoders (VAEs), where during training only clean data is used to train a generative model for speech, which at test time is combined with a noise model, e.g. nonnegative matrix factorization (NMF), whose parameters are learned without supervision. Consequently, the proposed model is agnostic to the noise type. When visual data are clean, audio-visual VAE-based architectures usually outperform the audio-only counterpart. The opposite happens when the visual data are corrupted by clutter, e.g. the speaker not facing the camera. In this paper, we propose to find the optimal combination of these two architectures through time. More precisely, we introduce the use of a latent sequential variable with Markovian dependencies to switch between different VAE architectures through time in an unsupervised manner: leading to switching variational auto-encoder (SwVAE). We propose a variational factorization to approximate the computationally intractable posterior distribution. We also derive the corresponding variational expectation-maximization algorithm to estimate the parameters of the model and enhance the speech signal. Our experiments demonstrate the promising performance of SwVAE.",2021-02-08T11:45:02Z,2021-02-08T11:45:02Z,http://arxiv.org/abs/2102.04144v1,http://arxiv.org/pdf/2102.04144v1,"eess.AS, cs.CV, cs.SD"
Improved Brain Age Estimation with Slice-based Set Networks,"Umang Gupta, Pradeep K. Lam, Greg Ver Steeg, Paul M. Thompson","Deep Learning for neuroimaging data is a promising but challenging direction. The high dimensionality of 3D MRI scans makes this endeavor compute and data-intensive. Most conventional 3D neuroimaging methods use 3D-CNN-based architectures with a large number of parameters and require more time and data to train. Recently, 2D-slice-based models have received increasing attention as they have fewer parameters and may require fewer samples to achieve comparable performance. In this paper, we propose a new architecture for BrainAGE prediction. The proposed architecture works by encoding each 2D slice in an MRI with a deep 2D-CNN model. Next, it combines the information from these 2D-slice encodings using set networks or permutation invariant layers. Experiments on the BrainAGE prediction problem, using the UK Biobank dataset, showed that the model with the permutation invariant layers trains faster and provides better predictions compared to other state-of-the-art approaches.",2021-02-08T18:54:15Z,2021-02-09T16:15:45Z,http://arxiv.org/abs/2102.04438v2,http://arxiv.org/pdf/2102.04438v2,"eess.IV, cs.LG, q-bio.QM"
Modeling Extremes with d-max-decreasing Neural Networks,"Ali Hasan, Khalil Elkhalil, Yuting Ng, Joao M. Pereira, Sina Farsiu, Jose H. Blanchet, Vahid Tarokh","We propose a novel neural network architecture that enables non-parametric calibration and generation of multivariate extreme value distributions (MEVs). MEVs arise from Extreme Value Theory (EVT) as the necessary class of models when extrapolating a distributional fit over large spatial and temporal scales based on data observed in intermediate scales. In turn, EVT dictates that $d$-max-decreasing, a stronger form of convexity, is an essential shape constraint in the characterization of MEVs. As far as we know, our proposed architecture provides the first class of non-parametric estimators for MEVs that preserve these essential shape constraints. We show that our architecture approximates the dependence structure encoded by MEVs at parametric rate. Moreover, we present a new method for sampling high-dimensional MEVs using a generative model. We demonstrate our methodology on a wide range of experimental settings, ranging from environmental sciences to financial mathematics and verify that the structural properties of MEVs are retained compared to existing methods.",2021-02-17T22:02:47Z,2022-03-01T19:47:23Z,http://arxiv.org/abs/2102.09042v2,http://arxiv.org/pdf/2102.09042v2,"stat.ML, cs.LG, stat.CO"
HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search,"Niv Nayman, Yonathan Aflalo, Asaf Noy, Lihi Zelnik-Manor","Realistic use of neural networks often requires adhering to multiple constraints on latency, energy and memory among others. A popular approach to find fitting networks is through constrained Neural Architecture Search (NAS), however, previous methods enforce the constraint only softly. Therefore, the resulting networks do not exactly adhere to the resource constraint and their accuracy is harmed. In this work we resolve this by introducing Hard Constrained diffeRentiable NAS (HardCoRe-NAS), that is based on an accurate formulation of the expected resource requirement and a scalable search method that satisfies the hard constraint throughout the search. Our experiments show that HardCoRe-NAS generates state-of-the-art architectures, surpassing other NAS methods, while strictly satisfying the hard resource constraints without any tuning required.",2021-02-23T11:56:30Z,2021-02-23T11:56:30Z,http://arxiv.org/abs/2102.11646v1,http://arxiv.org/pdf/2102.11646v1,"cs.LG, cs.AI, cs.CV, math.OC, stat.ML, 68T09, 68T45, G.1.6; G.3; I.2.8; I.2.10; I.5.1"
Hardware architectures for Successive Cancellation Decoding of Polar   Codes,"Camille Leroux, Ido Tal, Alexander Vardy, Warren J. Gross","The recently-discovered polar codes are widely seen as a major breakthrough in coding theory. These codes achieve the capacity of many important channels under successive cancellation decoding. Motivated by the rapid progress in the theory of polar codes, we propose a family of architectures for efficient hardware implementation of successive cancellation decoders. We show that such decoders can be implemented with O(n) processing elements and O(n) memory elements, while providing constant throughput. We also propose a technique for overlapping the decoding of several consecutive codewords, thereby achieving a significant speed-up factor. We furthermore show that successive cancellation decoding can be implemented in the logarithmic domain, thereby eliminating the multiplication and division operations and greatly reducing the complexity of each processing element.",2010-11-12T14:38:52Z,2010-11-12T14:38:52Z,http://arxiv.org/abs/1011.2919v1,http://arxiv.org/pdf/1011.2919v1,"cs.AR, cs.IT, math.IT"
Mixed-ADC Massive MIMO Uplink in Frequency-Selective Channels,"Ning Liang, Wenyi Zhang","The aim of this paper is to investigate the recently developed mixed-ADC architecture for frequency-selective channels. Multi-carrier techniques such as orthogonal frequency division multiplexing (OFDM) are employed to handle inter-symbol interference (ISI). A frequency-domain equalizer is designed for mitigating the inter-carrier interference (ICI) introduced by the nonlinearity of one-bit quantization. For static single-input-multiple-output (SIMO) channels, a closed-form expression of the generalized mutual information (GMI) is derived, and based on which the linear frequency-domain equalizer is optimized. The analysis is then extended to ergodic time-varying SIMO channels with estimated channel state information (CSI), where numerically tight lower and upper bounds of the GMI are derived. The analytical framework is naturally applicable to the multi-user scenario, for both static and time-varying channels. Extensive numerical studies reveal that the mixed-ADC architecture with a small proportion of high-resolution ADCs does achieve a dominant portion of the achievable rate of ideal conventional architecture, and that it remarkably improves the performance as compared with one-bit massive MIMO.",2016-01-09T06:08:37Z,2016-09-26T07:31:30Z,http://arxiv.org/abs/1601.02082v3,http://arxiv.org/pdf/1601.02082v3,"cs.IT, math.IT"
Cellular Communications on License-Exempt Spectrum: A Tutorial,"Binyin Ren, Mao Wang, Jingjing Zhang, Wenjie Yang, Jun Zou, Min Hua","A traditional cellular system (e.g., LTE) operates only on the licensed spectrum. This tutorial explains the concept of cellular communications on both licensed and license-exempt spectrum under a unified architecture. The purpose to extend a cellular system into the bandwidth-rich license-exempt spectrum is to form a larger cellular network for all spectrum types. This would result in an ultimate mobile converged cellular network. This tutorial examines the benefits of this concept, the technical challenges, and provides a conceptual LTE-based design example that helps to show how a traditional cellular system like the LTE can adapt itself to a different spectrum type, conform to the regulatory requirements, and harmoniously co-exist with the incumbent systems such as Wi-Fi. In order to cope with the interference and regulation rules on license-exempt spectrum, a special medium access mechanism is introduced into the existing LTE transmission frame structure to exploit the full benefits of coordinated and managed cellular architecture.",2016-01-15T07:15:33Z,2016-01-15T07:15:33Z,http://arxiv.org/abs/1601.03829v1,http://arxiv.org/pdf/1601.03829v1,"cs.NI, cs.IT, math.IT"
Linear State Estimation via 5G C-RAN Cellular Networks using Gaussian   Belief Propagation,"Mirsad Cosovic, Dejan Vukobratovic, Vladimir Stankovic","Machine-type communications and large-scale information processing architectures are among key (r)evolutionary enhancements of emerging fifth-generation (5G) mobile cellular networks. Massive data acquisition and processing will make 5G network an ideal platform for large-scale system monitoring and control with applications in future smart transportation, connected industry, power grids, etc. In this work, we investigate a capability of such a 5G network architecture to provide the state estimate of an underlying linear system from the input obtained via large-scale deployment of measurement devices. Assuming that the measurements are communicated via densely deployed cloud radio access network (C-RAN), we formulate and solve the problem of estimating the system state from the set of signals collected at C-RAN base stations. Our solution, based on the Gaussian Belief-Propagation (GBP) framework, allows for large-scale and distributed deployment within the emerging 5G information processing architectures. The presented numerical study demonstrates the accuracy, convergence behavior and scalability of the proposed GBP-based solution to the large-scale state estimation problem.",2017-10-24T09:27:51Z,2018-02-02T09:45:16Z,http://arxiv.org/abs/1710.08671v2,http://arxiv.org/pdf/1710.08671v2,"cs.IT, math.IT"
Sample-level CNN Architectures for Music Auto-tagging Using Raw   Waveforms,"Taejun Kim, Jongpil Lee, Juhan Nam","Recent work has shown that the end-to-end approach using convolutional neural network (CNN) is effective in various types of machine learning tasks. For audio signals, the approach takes raw waveforms as input using an 1-D convolution layer. In this paper, we improve the 1-D CNN architecture for music auto-tagging by adopting building blocks from state-of-the-art image classification models, ResNets and SENets, and adding multi-level feature aggregation to it. We compare different combinations of the modules in building CNN architectures. The results show that they achieve significant improvements over previous state-of-the-art models on the MagnaTagATune dataset and comparable results on Million Song Dataset. Furthermore, we analyze and visualize our model to show how the 1-D CNN operates.",2017-10-28T11:55:50Z,2018-02-14T04:39:50Z,http://arxiv.org/abs/1710.10451v2,http://arxiv.org/pdf/1710.10451v2,"cs.SD, cs.LG, cs.MM, cs.NE, eess.AS"
Empirical analysis of non-linear activation functions for Deep Neural   Networks in classification tasks,Giovanni Alcantara,"We provide an overview of several non-linear activation functions in a neural network architecture that have proven successful in many machine learning applications. We conduct an empirical analysis on the effectiveness of using these function on the MNIST classification task, with the aim of clarifying which functions produce the best results overall. Based on this first set of results, we examine the effects of building deeper architectures with an increasing number of hidden layers. We also survey the impact of using, on the same task, different initialisation schemes for the weights of our neural network. Using these sets of experiments as a base, we conclude by providing a optimal neural network architecture that yields impressive results in accuracy on the MNIST classification task.",2017-10-30T23:45:57Z,2017-10-30T23:45:57Z,http://arxiv.org/abs/1710.11272v1,http://arxiv.org/pdf/1710.11272v1,"cs.LG, stat.ML"
milliProxy: a TCP Proxy Architecture for 5G mmWave Cellular Systems,"Michele Polese, Marco Mezzavilla, Menglei Zhang, Jing Zhu, Sundeep Rangan, Shivendra Panwar, Michele Zorzi","TCP is the most widely used transport protocol in the internet. However, it offers suboptimal performance when operating over high bandwidth mmWave links. The main issues introduced by communications at such high frequencies are (i) the sensitivity to blockage and (ii) the high bandwidth fluctuations due to Line of Sight (LOS) to Non Line of Sight (NLOS) transitions and vice versa. In particular, TCP has an abstract view of the end-to-end connection, which does not properly capture the dynamics of the wireless mmWave link. The consequence is a suboptimal utilization of the available resources. In this paper we propose a TCP proxy architecture that improves the performance of TCP flows without any modification at the remote sender side. The proxy is installed in the Radio Access Network, and exploits information available at the gNB in order to maximize throughput and minimize latency.",2017-12-07T16:37:11Z,2017-12-07T16:37:11Z,http://arxiv.org/abs/1712.02700v1,http://arxiv.org/pdf/1712.02700v1,"cs.NI, cs.IT, math.IT"
How to Start Training: The Effect of Initialization and Architecture,"Boris Hanin, David Rolnick","We identify and study two common failure modes for early training in deep ReLU nets. For each we give a rigorous proof of when it occurs and how to avoid it, for fully connected and residual architectures. The first failure mode, exploding/vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in and, for ResNets, by correctly weighting the residual modules. We prove that the second failure mode, exponentially large variance of activation length, never occurs in residual nets once the first failure mode is avoided. In contrast, for fully connected nets, we prove that this failure mode can happen and is avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training. In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained.",2018-03-05T15:17:50Z,2018-11-13T14:52:46Z,http://arxiv.org/abs/1803.01719v3,http://arxiv.org/pdf/1803.01719v3,"stat.ML, cs.LG"
Homomorphic Encryption for Speaker Recognition: Protection of Biometric   Templates and Vendor Model Parameters,"Andreas Nautsch, Sergey Isadskiy, Jascha Kolberg, Marta Gomez-Barrero, Christoph Busch","Data privacy is crucial when dealing with biometric data. Accounting for the latest European data privacy regulation and payment service directive, biometric template protection is essential for any commercial application. Ensuring unlinkability across biometric service operators, irreversibility of leaked encrypted templates, and renewability of e.g., voice models following the i-vector paradigm, biometric voice-based systems are prepared for the latest EU data privacy legislation. Employing Paillier cryptosystems, Euclidean and cosine comparators are known to ensure data privacy demands, without loss of discrimination nor calibration performance. Bridging gaps from template protection to speaker recognition, two architectures are proposed for the two-covariance comparator, serving as a generative model in this study. The first architecture preserves privacy of biometric data capture subjects. In the second architecture, model parameters of the comparator are encrypted as well, such that biometric service providers can supply the same comparison modules employing different key pairs to multiple biometric service operators. An experimental proof-of-concept and complexity analysis is carried out on the data from the 2013-2014 NIST i-vector machine learning challenge.",2018-03-09T15:25:32Z,2018-03-09T15:25:32Z,http://arxiv.org/abs/1803.03559v1,http://arxiv.org/pdf/1803.03559v1,"cs.CR, cs.SD, eess.AS"
Modular Linear Optical Circuits,"Paolo L. Mennea, William R. Clements, Devin H. Smith, James C. Gates, Benjamin J. Metcalf, Rex H. S. Bannerman, Roel Burgwal, Jelmer J. Renema, W. Steven Kolthammer, Ian A. Walmsley, Peter G. R. Smith","We propose and demonstrate a modular architecture for reconfigurable on-chip linear-optical circuits. Each module contains 10 independent phase-controlled Mach-Zehnder interferometers; several such modules can be connected to each other to build large reconfigurable interferometers. With this architecture, large interferometers are easier to build and characterize than with traditional, bespoke, monolithic designs. We demonstrate our approach by fabricating three modules in the form of UV-written silica-on-silicon chips. We characterize these chips, connect them to each other, and implement a wide range of linear optical transformations. We envisage that this architecture will enable many future experiments in quantum optics.",2018-03-12T21:18:14Z,2018-03-12T21:18:14Z,http://arxiv.org/abs/1803.04539v1,http://arxiv.org/pdf/1803.04539v1,"quant-ph, physics.optics"
A Distributed Architecture for Edge Service Orchestration with   Guarantees,"Gabriele Castellano, Flavio Esposito, Fulvio Risso","The Network Function Virtualization paradigm is attracting the interest of service providers, that may greatly benefit from its flexibility and scalability properties. However, the diversity of possible orchestrated services, rises the necessity of adopting specific orchestration strategies for each service request that are unknown a priori. This paper presents Senate, a distributed architecture that enables precise orchestration of heterogeneous services over a common edge infrastructure. To assign shared resources to service orchestrators, Senate uses the Distributed Orchestration Resource Assignment (DORA), an approximation algorithm that we designed to guarantee both a bound on convergence time and an optimal (1-1/e)-approximation with respect to the Pareto optimal resource assignment. We evaluate advantages of service orchestration with Senate and performance of DORA through a prototype implementation.",2018-03-14T20:21:29Z,2018-03-14T20:21:29Z,http://arxiv.org/abs/1803.05499v1,http://arxiv.org/pdf/1803.05499v1,"cs.NI, cs.DS, 68-06 (Primary) 68W15, 68W25 (Secondary), C.2.1; C.2.4; C.2.6"
Learning Long Term Dependencies via Fourier Recurrent Units,"Jiong Zhang, Yibo Lin, Zhao Song, Inderjit S. Dhillon","It is a known fact that training recurrent neural networks for tasks that have long term dependencies is challenging. One of the main reasons is the vanishing or exploding gradient problem, which prevents gradient information from propagating to early layers. In this paper we propose a simple recurrent architecture, the Fourier Recurrent Unit (FRU), that stabilizes the gradients that arise in its training while giving us stronger expressive power. Specifically, FRU summarizes the hidden states $h^{(t)}$ along the temporal dimension with Fourier basis functions. This allows gradients to easily reach any layer due to FRU's residual learning structure and the global support of trigonometric functions. We show that FRU has gradient lower and upper bounds independent of temporal dimension. We also show the strong expressivity of sparse Fourier basis, from which FRU obtains its strong expressive power. Our experimental study also demonstrates that with fewer parameters the proposed architecture outperforms other recurrent architectures on many tasks.",2018-03-17T23:06:31Z,2018-03-17T23:06:31Z,http://arxiv.org/abs/1803.06585v1,http://arxiv.org/pdf/1803.06585v1,"cs.LG, stat.ML"
Pattern Analysis with Layered Self-Organizing Maps,David Friedlander,"This paper defines a new learning architecture, Layered Self-Organizing Maps (LSOMs), that uses the SOM and supervised-SOM learning algorithms. The architecture is validated with the MNIST database of hand-written digit images. LSOMs are similar to convolutional neural nets (covnets) in the way they sample data, but different in the way they represent features and learn. LSOMs analyze (or generate) image patches with maps of exemplars determined by the SOM learning algorithm rather than feature maps from filter-banks learned via backprop.   LSOMs provide an alternative to features derived from covnets. Multi-layer LSOMs are trained bottom-up, without the use of backprop and therefore may be of interest as a model of the visual cortex. The results show organization at multiple levels. The algorithm appears to be resource efficient in learning, classifying and generating images. Although LSOMs can be used for classification, their validation accuracy for these exploratory runs was well below the state of the art. The goal of this article is to define the architecture and display the structures resulting from its application to the MNIST images.",2018-03-23T22:07:52Z,2018-03-28T17:07:18Z,http://arxiv.org/abs/1803.08996v2,http://arxiv.org/pdf/1803.08996v2,"cs.CV, cs.LG, stat.ML"
Multi-Modal Data Augmentation for End-to-End ASR,"Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, Shinji Watanabe","We present a new end-to-end architecture for automatic speech recognition (ASR) that can be trained using \emph{symbolic} input in addition to the traditional acoustic input. This architecture utilizes two separate encoders: one for acoustic input and another for symbolic input, both sharing the attention and decoder parameters. We call this architecture a multi-modal data augmentation network (MMDA), as it can support multi-modal (acoustic and symbolic) input and enables seamless mixing of large text datasets with significantly smaller transcribed speech corpora during training. We study different ways of transforming large text corpora into a symbolic form suitable for training our MMDA network. Our best MMDA setup obtains small improvements on character error rate (CER), and as much as 7-10\% relative word error rate (WER) improvement over a baseline both with and without an external language model.",2018-03-27T20:12:39Z,2018-06-18T05:53:10Z,http://arxiv.org/abs/1803.10299v3,http://arxiv.org/pdf/1803.10299v3,"cs.CL, cs.SD, eess.AS"
Foreground object segmentation in RGB-D data implemented on GPU,"Piotr Janus, Tomasz Kryjak, Marek Gorgon","This paper presents a GPU implementation of two foreground object segmentation algorithms: Gaussian Mixture Model (GMM) and Pixel Based Adaptive Segmenter (PBAS) modified for RGB-D data support. The simultaneous use of colour (RGB) and depth (D) data allows to improve segmentation accuracy, especially in case of colour camouflage, illumination changes and occurrence of shadows. Three GPUs were used to accelerate calculations: embedded NVIDIA Jetson TX2 (Maxwell architecture), mobile NVIDIA GeForce GTX 1050m (Pascal architecture) and efficient NVIDIA RTX 2070 (Turing architecture). Segmentation accuracy comparable to previously published works was obtained. Moreover, the use of a GPU platform allowed to get real-time image processing. In addition, the system has been adapted to work with two RGB-D sensors: RealSense D415 and D435 from Intel.",2020-02-01T17:53:39Z,2020-02-01T17:53:39Z,http://arxiv.org/abs/2002.00250v1,http://arxiv.org/pdf/2002.00250v1,"cs.CV, eess.IV"
PDE-NetGen 1.0: from symbolic PDE representations of physical processes   to trainable neural network representations,"Olivier Pannekoucke, Ronan Fablet","Bridging physics and deep learning is a topical challenge. While deep learning frameworks open avenues in physical science, the design of physically-consistent deep neural network architectures is an open issue. In the spirit of physics-informed NNs, PDE-NetGen package provides new means to automatically translate physical equations, given as PDEs, into neural network architectures. PDE-NetGen combines symbolic calculus and a neural network generator. The later exploits NN-based implementations of PDE solvers using Keras. With some knowledge of a problem, PDE-NetGen is a plug-and-play tool to generate physics-informed NN architectures. They provide computationally-efficient yet compact representations to address a variety of issues, including among others adjoint derivation, model calibration, forecasting, data assimilation as well as uncertainty quantification. As an illustration, the workflow is first presented for the 2D diffusion equation, then applied to the data-driven and physics-informed identification of uncertainty dynamics for the Burgers equation.",2020-02-03T22:11:13Z,2020-02-03T22:11:13Z,http://arxiv.org/abs/2002.01029v1,http://arxiv.org/pdf/2002.01029v1,"physics.comp-ph, cs.LG, stat.ML"
Classification of Upper Limb Movements \newline Using Convolutional   Neural Network \newline with 3D Inception Block,"D. -Y. Lee, J. -H. Jeong, K. -H. Shim, D. -J. Kim","A brain-machine interface (BMI) based on electroencephalography (EEG) can overcome the movement deficits for patients and real-world applications for healthy people. Ideally, the BMI system detects user movement intentions transforms them into a control signal for a robotic arm movement. In this study, we made progress toward user intention decoding and successfully classified six different reaching movements of the right arm in the movement execution (ME). Notably, we designed an experimental environment using robotic arm movement and proposed a convolutional neural network architecture (CNN) with inception block for robust classify executed movements of the same limb. As a result, we confirmed the classification accuracies of six different directions show 0.45 for the executed session. The results proved that the proposed architecture has approximately 6~13% performance increase compared to its conventional classification models. Hence, we demonstrate the 3D inception CNN architecture to contribute to the continuous decoding of ME.",2020-02-04T04:33:49Z,2020-02-04T04:33:49Z,http://arxiv.org/abs/2002.01121v1,http://arxiv.org/pdf/2002.01121v1,"cs.HC, eess.SP"
Learning Hyperspectral Feature Extraction and Classification with   ResNeXt Network,"Divinah Nyasaka, Jing Wang, Haron Tinega","The Hyperspectral image (HSI) classification is a standard remote sensing task, in which each image pixel is given a label indicating the physical land-cover on the earth's surface. The achievements of image semantic segmentation and deep learning approaches on ordinary images have accelerated the research on hyperspectral image classification. Moreover, the utilization of both the spectral and spatial cues in hyperspectral images has shown improved classification accuracy in hyperspectral image classification. The use of only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and spectral cues from Hyperspectral images results in an explosion of parameters hence high computational cost. We propose network architecture called the MixedSN that utilizes the 3D convolutions to modeling spectral-spatial information in the early layers of the architecture and the 2D convolutions at the top layers which majorly deal with semantic abstraction. We constrain our architecture to ResNeXt block because of their performance and simplicity. Our model drastically reduced the number of parameters and achieved comparable classification performance with state-of-the-art methods on Indian Pine (IP) scene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset, and Botswana (BW) dataset.",2020-02-07T01:54:15Z,2020-02-07T01:54:15Z,http://arxiv.org/abs/2002.02585v1,http://arxiv.org/pdf/2002.02585v1,"cs.CV, cs.LG, eess.IV"
Variational Depth Search in ResNets,"Javier Antorán, James Urquhart Allingham, José Miguel Hernández-Lobato","One-shot neural architecture search allows joint learning of weights and network architecture, reducing computational cost. We limit our search space to the depth of residual networks and formulate an analytically tractable variational objective that allows for obtaining an unbiased approximate posterior over depths in one-shot. We propose a heuristic to prune our networks based on this distribution. We compare our proposed method against manual search over network depths on the MNIST, Fashion-MNIST, SVHN datasets. We find that pruned networks do not incur a loss in predictive performance, obtaining accuracies competitive with unpruned networks. Marginalising over depth allows us to obtain better-calibrated test-time uncertainty estimates than regular networks, in a single forward pass.",2020-02-06T16:00:03Z,2020-04-01T17:59:13Z,http://arxiv.org/abs/2002.02797v4,http://arxiv.org/pdf/2002.02797v4,"stat.ML, cs.LG"
A Data Efficient End-To-End Spoken Language Understanding Architecture,"Marco Dinarelli, Nikita Kapoor, Bassam Jabaian, Laurent Besacier","End-to-end architectures have been recently proposed for spoken language understanding (SLU) and semantic parsing. Based on a large amount of data, those models learn jointly acoustic and linguistic-sequential features. Such architectures give very good results in the context of domain, intent and slot detection, their application in a more complex semantic chunking and tagging task is less easy. For that, in many cases, models are combined with an external language model to enhance their performance.   In this paper we introduce a data efficient system which is trained end-to-end, with no additional, pre-trained external module. One key feature of our approach is an incremental training procedure where acoustic, language and semantic models are trained sequentially one after the other. The proposed model has a reasonable size and achieves competitive results with respect to state-of-the-art while using a small training dataset. In particular, we reach 24.02% Concept Error Rate (CER) on MEDIA/test while training on MEDIA/train without any additional data.",2020-02-14T10:24:42Z,2020-02-14T10:24:42Z,http://arxiv.org/abs/2002.05955v1,http://arxiv.org/pdf/2002.05955v1,"cs.CL, cs.LG, cs.SD, eess.AS"
DSNAS: Direct Neural Architecture Search without Parameter Retraining,"Shoukang Hu, Sirui Xie, Hehui Zheng, Chunxiao Liu, Jianping Shi, Xunying Liu, Dahua Lin","If NAS methods are solutions, what is the problem? Most existing NAS methods require two-stage parameter optimization. However, performance of the same architecture in the two stages correlates poorly. In this work, we propose a new problem definition for NAS, task-specific end-to-end, based on this observation. We argue that given a computer vision task for which a NAS method is expected, this definition can reduce the vaguely-defined NAS evaluation to i) accuracy of this task and ii) the total computation consumed to finally obtain a model with satisfying accuracy. Seeing that most existing methods do not solve this problem directly, we propose DSNAS, an efficient differentiable NAS framework that simultaneously optimizes architecture and parameters with a low-biased Monte Carlo estimate. Child networks derived from DSNAS can be deployed directly without parameter retraining. Comparing with two-stage methods, DSNAS successfully discovers networks with comparable accuracy (74.4%) on ImageNet in 420 GPU hours, reducing the total time by more than 34%. Our implementation is available at https://github.com/SNAS-Series/SNAS-Series.",2020-02-21T04:41:47Z,2020-04-01T00:31:37Z,http://arxiv.org/abs/2002.09128v2,http://arxiv.org/pdf/2002.09128v2,"cs.LG, stat.ML"
Learning Beam Codebooks with Neural Networks: Towards Environment-Aware   mmWave MIMO,"Yu Zhang, Muhammad Alrabeiah, Ahmed Alkhateeb","Scaling the number of antennas up is a key characteristic of current and future wireless communication systems. The hardware cost and power consumption, however, motivate large-scale MIMO systems, especially at millimeter wave (mmWave) bands, to rely on analog-only or hybrid analog/digital transceiver architectures. With these architectures, mmWave base stations normally use pre-defined beamforming codebooks for both initial access and data transmissions. Current beam codebooks, however, generally adopt single-lobe narrow beams and scan the entire angular space. This leads to high beam training overhead and loss in the achievable beamforming gains. In this paper, we propose a new machine learning framework for learning beamforming codebooks in hardware-constrained large-scale MIMO systems. More specifically, we develop a neural network architecture that accounts for the hardware constraints and learns beam codebooks that adapt to the surrounding environment and the user locations. Simulation results highlight the capability of the proposed solution in learning multi-lobe beams and reducing the codebook size, which leads to noticeable gains compared to classical codebook design approaches.",2020-02-25T04:46:37Z,2020-02-25T04:46:37Z,http://arxiv.org/abs/2002.10663v1,http://arxiv.org/pdf/2002.10663v1,"cs.IT, eess.SP, math.IT"
Planning for Compilation of a Quantum Algorithm for Graph Coloring,"Minh Do, Zhihui Wang, Bryan O'Gorman, Davide Venturelli, Eleanor Rieffel, Jeremy Frank","The problem of compiling general quantum algorithms for implementation on near-term quantum processors has been introduced to the AI community. Previous work demonstrated that temporal planning is an attractive approach for part of this compilationtask, specifically, the routing of circuits that implement the Quantum Alternating Operator Ansatz (QAOA) applied to the MaxCut problem on a quantum processor architecture. In this paper, we extend the earlier work to route circuits that implement QAOA for Graph Coloring problems. QAOA for coloring requires execution of more, and more complex, operations on the chip, which makes routing a more challenging problem. We evaluate the approach on state-of-the-art hardware architectures from leading quantum computing companies. Additionally, we apply a planning approach to qubit initialization. Our empirical evaluation shows that temporal planning compares well to reasonable analytic upper bounds, and that solving qubit initialization with a classical planner generally helps temporal planners in finding shorter-makespan compilations for QAOA for Graph Coloring. These advances suggest that temporal planning can be an effective approach for more complex quantum computing algorithms and architectures.",2020-02-23T03:09:57Z,2020-02-23T03:09:57Z,http://arxiv.org/abs/2002.10917v1,http://arxiv.org/pdf/2002.10917v1,"quant-ph, cs.AI, cs.ET"
DNN-Chip Predictor: An Analytical Performance Predictor for DNN   Accelerators with Various Dataflows and Hardware Architectures,"Yang Zhao, Chaojian Li, Yue Wang, Pengfei Xu, Yongan Zhang, Yingyan Lin","The recent breakthroughs in deep neural networks (DNNs) have spurred a tremendously increased demand for DNN accelerators. However, designing DNN accelerators is non-trivial as it often takes months/years and requires cross-disciplinary knowledge. To enable fast and effective DNN accelerator development, we propose DNN-Chip Predictor, an analytical performance predictor which can accurately predict DNN accelerators' energy, throughput, and latency prior to their actual implementation. Our Predictor features two highlights: (1) its analytical performance formulation of DNN ASIC/FPGA accelerators facilitates fast design space exploration and optimization; and (2) it supports DNN accelerators with different algorithm-to-hardware mapping methods (i.e., dataflows) and hardware architectures. Experiment results based on 2 DNN models and 3 different ASIC/FPGA implementations show that our DNN-Chip Predictor's predicted performance differs from those of chip measurements of FPGA/ASIC implementation by no more than 17.66% when using different DNN models, hardware architectures, and dataflows. We will release code upon acceptance.",2020-02-26T02:59:18Z,2021-04-16T02:52:32Z,http://arxiv.org/abs/2002.11270v2,http://arxiv.org/pdf/2002.11270v2,"cs.LG, cs.DC, eess.SP"
A Systematic Survey of General Sparse Matrix-Matrix Multiplication,"Jianhua Gao, Weixing Ji, Fangli Chang, Shiyu Han, Bingxin Wei, Zeming Liu, Yizhuo Wang","General Sparse Matrix-Matrix Multiplication (SpGEMM) has attracted much attention from researchers in graph analyzing, scientific computing, and deep learning. Many optimization techniques have been developed for different applications and computing architectures over the past decades. The objective of this paper is to provide a structured and comprehensive overview of the researches on SpGEMM. Existing researches have been grouped into different categories based on target architectures and design choices. Covered topics include typical applications, compression formats, general formulations, key problems and techniques, architecture-oriented optimizations, and programming models. The rationales of different algorithms are analyzed and summarized. This survey sufficiently reveals the latest progress of SpGEMM research to 2021. Moreover, a thorough performance comparison of existing implementations is presented. Based on our findings, we highlight future research directions, which encourage better design and implementations in later studies.",2020-02-26T03:15:05Z,2023-07-11T06:12:14Z,http://arxiv.org/abs/2002.11273v3,http://arxiv.org/pdf/2002.11273v3,"cs.DC, 68-02, 68W10, 65F50, A.1; D.1.3; G.1.3"
A Survey on Coarse-Grained Reconfigurable Architectures from a   Performance Perspective,"Artur Podobas, Kentaro Sano, Satoshi Matsuoka","With the end of both Dennard's scaling and Moore's law, computer users and researchers are aggressively exploring alternative forms of computing in order to continue the performance scaling that we have come to enjoy. Among the more salient and practical of the post-Moore alternatives are reconfigurable systems, with Coarse-Grained Reconfigurable Architectures (CGRAs) seemingly capable of striking a balance between performance and programmability. In this paper, we survey the landscape of CGRAs. We summarize nearly three decades of literature on the subject, with a particular focus on the premise behind the different CGRAs and how they have evolved. Next, we compile metrics of available CGRAs and analyze their performance properties in order to understand and discover knowledge gaps and opportunities for future CGRA research specialized towards High-Performance Computing (HPC). We find that there are ample opportunities for future research on CGRAs, in particular with respect to size, functionality, support for parallel programming models, and to evaluate more complex applications.",2020-04-09T12:12:05Z,2020-09-15T15:25:04Z,http://arxiv.org/abs/2004.04509v2,http://arxiv.org/pdf/2004.04509v2,"cs.AR, A.1; B.0; C.1; C.3"
Hcore-Init: Neural Network Initialization based on Graph Degeneracy,"Stratis Limnios, George Dasoulas, Dimitrios M. Thilikos, Michalis Vazirgiannis","Neural networks are the pinnacle of Artificial Intelligence, as in recent years we witnessed many novel architectures, learning and optimization techniques for deep learning. Capitalizing on the fact that neural networks inherently constitute multipartite graphs among neuron layers, we aim to analyze directly their structure to extract meaningful information that can improve the learning process. To our knowledge graph mining techniques for enhancing learning in neural networks have not been thoroughly investigated. In this paper we propose an adapted version of the k-core structure for the complete weighted multipartite graph extracted from a deep learning architecture. As a multipartite graph is a combination of bipartite graphs, that are in turn the incidence graphs of hypergraphs, we design k-hypercore decomposition, the hypergraph analogue of k-core degeneracy. We applied k-hypercore to several neural network architectures, more specifically to convolutional neural networks and multilayer perceptrons for image recognition tasks after a very short pretraining. Then we used the information provided by the hypercore numbers of the neurons to re-initialize the weights of the neural network, thus biasing the gradient optimization scheme. Extensive experiments proved that k-hypercore outperforms the state-of-the-art initialization methods.",2020-04-16T12:57:14Z,2022-09-09T11:24:33Z,http://arxiv.org/abs/2004.07636v2,http://arxiv.org/pdf/2004.07636v2,"cs.LG, stat.ML"
Enriching the Transformer with Linguistic Factors for Low-Resource   Machine Translation,"Jordi Armengol-Estapé, Marta R. Costa-jussà, Carlos Escolano","Introducing factors, that is to say, word features such as linguistic information referring to the source tokens, is known to improve the results of neural machine translation systems in certain settings, typically in recurrent architectures. This study proposes enhancing the current state-of-the-art neural machine translation architecture, the Transformer, so that it allows to introduce external knowledge. In particular, our proposed modification, the Factored Transformer, uses linguistic factors that insert additional knowledge into the machine translation system. Apart from using different kinds of features, we study the effect of different architectural configurations. Specifically, we analyze the performance of combining words and features at the embedding level or at the encoder level, and we experiment with two different combination strategies. With the best-found configuration, we show improvements of 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task. Moreover, we experiment with the more challenging FLoRes English-to-Nepali benchmark, which includes both extremely low-resourced and very distant languages, and obtain an improvement of 1.2 BLEU.",2020-04-17T03:40:13Z,2020-12-24T09:06:18Z,http://arxiv.org/abs/2004.08053v2,http://arxiv.org/pdf/2004.08053v2,"cs.CL, I.2.7"
4D Deep Learning for Multiple Sclerosis Lesion Activity Segmentation,"Nils Gessert, Marcel Bengs, Julia Krüger, Roland Opfer, Ann-Christin Ostwaldt, Praveena Manogaran, Sven Schippling, Alexander Schlaefer","Multiple sclerosis lesion activity segmentation is the task of detecting new and enlarging lesions that appeared between a baseline and a follow-up brain MRI scan. While deep learning methods for single-scan lesion segmentation are common, deep learning approaches for lesion activity have only been proposed recently. Here, a two-path architecture processes two 3D MRI volumes from two time points. In this work, we investigate whether extending this problem to full 4D deep learning using a history of MRI volumes and thus an extended baseline can improve performance. For this purpose, we design a recurrent multi-encoder-decoder architecture for processing 4D data. We find that adding more temporal information is beneficial and our proposed architecture outperforms previous approaches with a lesion-wise true positive rate of 0.84 at a lesion-wise false positive rate of 0.19.",2020-04-20T11:41:01Z,2020-05-29T19:28:54Z,http://arxiv.org/abs/2004.09216v2,http://arxiv.org/pdf/2004.09216v2,"cs.CV, eess.IV, q-bio.QM"
Automatic Polyp Segmentation Using Convolutional Neural Networks,"Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J. Wesolowski, Kevin A. Schneider, Ralph Deters","Colorectal cancer is the third most common cancer-related death after lung cancer and breast cancer worldwide. The risk of developing colorectal cancer could be reduced by early diagnosis of polyps during a colonoscopy. Computer-aided diagnosis systems have the potential to be applied for polyp screening and reduce the number of missing polyps. In this paper, we compare the performance of different deep learning architectures as feature extractors, i.e. ResNet, DenseNet, InceptionV3, InceptionResNetV2 and SE-ResNeXt in the encoder part of a U-Net architecture. We validated the performance of presented ensemble models on the CVC-Clinic (GIANA 2018) dataset. The DenseNet169 feature extractor combined with U-Net architecture outperformed the other counterparts and achieved an accuracy of 99.15\%, Dice similarity coefficient of 90.87%, and Jaccard index of 83.82%.",2020-04-22T18:54:29Z,2020-04-22T18:54:29Z,http://arxiv.org/abs/2004.10792v1,http://arxiv.org/pdf/2004.10792v1,"eess.IV, cs.CV, cs.LG"
A two-dimensional architecture for fast large-scale trapped-ion quantum   computing,"Y. -K. Wu, L. -M. Duan","Building blocks of quantum computers have been demonstrated in small to intermediate-scale systems. As one of the leading platforms, the trapped ion system has attracted wide attention. A significant challenge in this system is to combine fast high-fidelity gates with scalability and convenience in ion trap fabrication. Here we propose an architecture for large-scale quantum computing with a two-dimensional array of atomic ions trapped at such large distance which is convenient for ion-trap fabrication but usually believed to be unsuitable for quantum computing as the conventional gates would be too slow. Using gate operations far outside of the Lamb-Dicke region, we show that fast and robust entangling gates can be realized in any large ion arrays. The gate operations are intrinsically parallel and robust to thermal noise, which, together with their high speed and scalability of the proposed architecture, makes this approach an attractive one for large-scale quantum computing.",2020-04-24T09:17:40Z,2020-04-24T09:17:40Z,http://arxiv.org/abs/2004.11608v1,http://arxiv.org/pdf/2004.11608v1,"quant-ph, physics.atom-ph"
EmbraceNet for Activity: A Deep Multimodal Fusion Architecture for   Activity Recognition,"Jun-Ho Choi, Jong-Seok Lee","Human activity recognition using multiple sensors is a challenging but promising task in recent decades. In this paper, we propose a deep multimodal fusion model for activity recognition based on the recently proposed feature fusion architecture named EmbraceNet. Our model processes each sensor data independently, combines the features with the EmbraceNet architecture, and post-processes the fused feature to predict the activity. In addition, we propose additional processes to boost the performance of our model. We submit the results obtained from our proposed model to the SHL recognition challenge with the team name ""Yonsei-MCML.""",2020-04-29T01:54:28Z,2020-04-29T01:54:28Z,http://arxiv.org/abs/2004.13918v1,http://arxiv.org/pdf/2004.13918v1,"eess.SP, cs.LG, stat.ML"
On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause   Detection and Identification,"Vidyasagar Sadhu, Saman Zonouz, Dario Pompili","With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and experiments on a real drone. Empirical results show that our solution is able to detect with over 90% accuracy and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 88% accuracy (experimental data)).",2020-04-03T22:46:34Z,2020-05-06T18:55:28Z,http://arxiv.org/abs/2005.00336v2,http://arxiv.org/pdf/2005.00336v2,"eess.SP, cs.CV, cs.LG, cs.RO"
Continuous sign language recognition from wearable IMUs using deep   capsule networks and game theory,"Karush Suri, Rinki Gupta","Sign Language is used by the deaf community all over world. The work presented here proposes a novel one-dimensional deep capsule network (CapsNet) architecture for continuous Indian Sign Language recognition by means of signals obtained from a custom designed wearable IMU system. The performance of the proposed CapsNet architecture is assessed by altering dynamic routing between capsule layers. The proposed CapsNet yields improved accuracy values of 94% for 3 routings and 92.50% for 5 routings in comparison with the convolutional neural network (CNN) that yields an accuracy of 87.99%. Improved learning of the proposed architecture is also validated by spatial activations depicting excited units at the predictive layer. Finally, a novel non-cooperative pick-and-predict competition is designed between CapsNet and CNN. Higher value of Nash equilibrium for CapsNet as compared to CNN indicates the suitability of the proposed approach.",2020-04-27T01:21:16Z,2020-04-27T01:21:16Z,http://arxiv.org/abs/2005.00409v1,http://arxiv.org/pdf/2005.00409v1,"eess.SP, cs.LG, stat.ML"
PON-based connectivity for fog computing,"Abdullah M. Alqahtani, Sanaa H. Mohamed, Taisir E. H. El-Gorashi, Jaafar M. H. Elmirghani","Fog computing plays a crucial role in satisfying the requirements of delay-sensitive applications such as connected vehicles, smart grids, and actuator networks by moving data processing close to end users. Passive optical networks (PONs) are widely used in access networks to reduce the power consumption while providing high bandwidth to end users under flexible designs. Typically, distributed fog computing units in access networks have limited processing and storage capacities that can be under or over utilized depending on instantaneous demands. To extend the available capacity in access network, this paper proposes a fog computing architecture based on SDN-enabled PONs to achieve full connectivity among distributed fog computing servers. The power consumption results show that this architecture can achieve up to about 80% power savings in comparison to legacy fog computing based on spine and leaf data centers with the same number of servers.",2020-05-02T14:20:24Z,2020-05-02T14:20:24Z,http://arxiv.org/abs/2005.00839v1,http://arxiv.org/pdf/2005.00839v1,"cs.NI, eess.SP"
Evaluation of Applications Latency in Server Centric Passive Optical   Network Based Data Centre Architectures,"Azza E. A. Eltraify, Mohamed O. I. Musa, Ahmed Al-Quzweeni, Jaafar M. H. Elmirghani","The number of applications running in the cloud has dramatically increased in the past decade as well as the number of users accessing them. Data centres resources, architectures and conditions define the performance of the applications running on them. One of the main measures of the network efficiency is the latency, which can have a huge effect on resources utilisation, power consumption and the overall performance. In this paper, the performance of a fog network is evaluated by measuring the latency while running a facial recognition software. The network consists of two processing cells, a core network and a PON cell. The results show how network latency is affected by running the facial recognition software in the end-to-end network setup introduced in this paper.",2020-05-05T18:59:16Z,2020-05-05T18:59:16Z,http://arxiv.org/abs/2005.02440v1,http://arxiv.org/pdf/2005.02440v1,"cs.NI, eess.SP"
AutoSpeech: Neural Architecture Search for Speaker Recognition,"Shaojin Ding, Tianlong Chen, Xinyu Gong, Weiwei Zha, Zhangyang Wang","Speaker recognition systems based on Convolutional Neural Networks (CNNs) are often built with off-the-shelf backbones such as VGG-Net or ResNet. However, these backbones were originally proposed for image classification, and therefore may not be naturally fit for speaker recognition. Due to the prohibitive complexity of manually exploring the design space, we propose the first neural architecture search approach approach for the speaker recognition tasks, named as AutoSpeech. Our algorithm first identifies the optimal operation combination in a neural cell and then derives a CNN model by stacking the neural cell for multiple times. The final speaker recognition model can be obtained by training the derived CNN model through the standard scheme. To evaluate the proposed approach, we conduct experiments on both speaker identification and speaker verification tasks using the VoxCeleb1 dataset. Results demonstrate that the derived CNN architectures from the proposed approach significantly outperform current speaker recognition systems based on VGG-M, ResNet-18, and ResNet-34 back-bones, while enjoying lower model complexity.",2020-05-07T02:53:47Z,2020-08-31T15:53:27Z,http://arxiv.org/abs/2005.03215v2,http://arxiv.org/pdf/2005.03215v2,"eess.AS, cs.LG"
Noisy Differentiable Architecture Search,"Xiangxiang Chu, Bo Zhang","Simplicity is the ultimate sophistication. Differentiable Architecture Search (DARTS) has now become one of the mainstream paradigms of neural architecture search. However, it largely suffers from the well-known performance collapse issue due to the aggregation of skip connections. It is thought to have overly benefited from the residual structure which accelerates the information flow. To weaken this impact, we propose to inject unbiased random noise to impede the flow. We name this novel approach NoisyDARTS. In effect, a network optimizer should perceive this difficulty at each training step and refrain from overshooting, especially on skip connections. In the long run, since we add no bias to the gradient in terms of expectation, it is still likely to converge to the right solution area. We also prove that the injected noise plays a role in smoothing the loss landscape, which makes the optimization easier. Our method features extreme simplicity and acts as a new strong baseline. We perform extensive experiments across various search spaces, datasets, and tasks, where we robustly achieve state-of-the-art results. Our code is available at https://github.com/xiaomi-automl/NoisyDARTS.",2020-05-07T15:53:52Z,2021-10-17T14:57:46Z,http://arxiv.org/abs/2005.03566v3,http://arxiv.org/pdf/2005.03566v3,"cs.LG, cs.CV, stat.ML"
Ring Reservoir Neural Networks for Graphs,"Claudio Gallicchio, Alessio Micheli","Machine Learning for graphs is nowadays a research topic of consolidated relevance. Common approaches in the field typically resort to complex deep neural network architectures and demanding training algorithms, highlighting the need for more efficient solutions. The class of Reservoir Computing (RC) models can play an important role in this context, enabling to develop fruitful graph embeddings through untrained recursive architectures. In this paper, we study progressive simplifications to the design strategy of RC neural networks for graphs. Our core proposal is based on shaping the organization of the hidden neurons to follow a ring topology. Experimental results on graph classification tasks indicate that ring-reservoirs architectures enable particularly effective network configurations, showing consistent advantages in terms of predictive performance.",2020-05-11T17:51:40Z,2020-05-11T17:51:40Z,http://arxiv.org/abs/2005.05294v1,http://arxiv.org/pdf/2005.05294v1,"cs.LG, cs.NE, stat.ML"
Fostering Event Compression using Gated Surprise,"Dania Humaidan, Sebastian Otte, Martin V. Butz","Our brain receives a dynamically changing stream of sensorimotor data. Yet, we perceive a rather organized world, which we segment into and perceive as events. Computational theories of cognitive science on event-predictive cognition suggest that our brain forms generative, event-predictive models by segmenting sensorimotor data into suitable chunks of contextual experiences. Here, we introduce a hierarchical, surprise-gated recurrent neural network architecture, which models this process and develops compact compressions of distinct event-like contexts. The architecture contains a contextual LSTM layer, which develops generative compressions of ongoing and subsequent contexts. These compressions are passed into a GRU-like layer, which uses surprise signals to update its recurrent latent state. The latent state is passed forward into another LSTM layer, which processes actual dynamic sensory flow in the light of the provided latent, contextual compression signals. Our model shows to develop distinct event compressions and achieves the best performance on multiple event processing tasks. The architecture may be very useful for the further development of resource-efficient learning, hierarchical model-based reinforcement learning, as well as the development of artificial event-predictive cognition and intelligence.",2020-05-12T11:57:46Z,2020-05-12T11:57:46Z,http://arxiv.org/abs/2005.05704v1,http://arxiv.org/pdf/2005.05704v1,"cs.LG, cs.NE, stat.ML"
Generalized Bayesian Posterior Expectation Distillation for Deep Neural   Networks,"Meet P. Vadera, Brian Jalaian, Benjamin M. Marlin","In this paper, we present a general framework for distilling expectations with respect to the Bayesian posterior distribution of a deep neural network classifier, extending prior work on the Bayesian Dark Knowledge framework. The proposed framework takes as input ""teacher"" and student model architectures and a general posterior expectation of interest. The distillation method performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples. We focus on the posterior predictive distribution and expected entropy as distillation targets. We investigate several aspects of this framework including the impact of uncertainty and the choice of student model architecture. We study methods for student model architecture search from a speed-storage-accuracy perspective and evaluate down-stream tasks leveraging entropy distillation including uncertainty ranking and out-of-distribution detection.",2020-05-16T21:40:47Z,2020-05-16T21:40:47Z,http://arxiv.org/abs/2005.08110v1,http://arxiv.org/pdf/2005.08110v1,"cs.LG, stat.ML"
How much complexity does an RNN architecture need to learn   syntax-sensitive dependencies?,"Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal","Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully.",2020-05-17T09:13:28Z,2020-05-25T10:18:27Z,http://arxiv.org/abs/2005.08199v2,http://arxiv.org/pdf/2005.08199v2,"cs.CL, q-bio.NC, I.2.6; I.2.7; J.5"
Free2Shard: Adaptive-adversary-resistant sharding via Dynamic Self   Allocation,"Ranvir Rana, Sreeram Kannan, David Tse, Pramod Viswanath","Propelled by the growth of large-scale blockchain deployments, much recent progress has been made in designing sharding protocols that achieve throughput scaling linearly in the number of nodes. However, existing protocols are not robust to an adversary adaptively corrupting a fixed fraction of nodes. In this paper, we propose Free2Shard -- a new architecture that achieves near-linear scaling while being secure against a fully adaptive adversary.   The focal point of this architecture is a dynamic self-allocation algorithm that lets users allocate themselves to shards in response to adversarial action, without requiring a central or cryptographic proof. This architecture has several attractive features unusual for sharding protocols, including: (a) the ability to handle the regime of large number of shards (relative to the number of nodes); (b) heterogeneous shard demands; (c) requiring only a small minority to follow the self-allocation; (d) asynchronous shard rotation; (e) operation in a purely identity-free proof-of-work setting. The key technical contribution is a deep mathematical connection to the classical work of Blackwell in dynamic game theory.",2020-05-19T17:31:39Z,2020-05-19T17:31:39Z,http://arxiv.org/abs/2005.09610v1,http://arxiv.org/pdf/2005.09610v1,"cs.CR, cs.DC, cs.GT, cs.IT, math.IT"
End-to-End Far-Field Speech Recognition with Unified Dereverberation and   Beamforming,"Wangyou Zhang, Aswin Shanmugam Subramanian, Xuankai Chang, Shinji Watanabe, Yanmin Qian","Despite successful applications of end-to-end approaches in multi-channel speech recognition, the performance still degrades severely when the speech is corrupted by reverberation. In this paper, we integrate the dereverberation module into the end-to-end multi-channel speech recognition system and explore two different frontend architectures. First, a multi-source mask-based weighted prediction error (WPE) module is incorporated in the frontend for dereverberation. Second, another novel frontend architecture is proposed, which extends the weighted power minimization distortionless response (WPD) convolutional beamformer to perform simultaneous separation and dereverberation. We derive a new formulation from the original WPD, which can handle multi-source input, and replace eigenvalue decomposition with the matrix inverse operation to make the back-propagation algorithm more stable. The above two architectures are optimized in a fully end-to-end manner, only using the speech recognition criterion. Experiments on both spatialized wsj1-2mix corpus and REVERB show that our proposed model outperformed the conventional methods in reverberant scenarios.",2020-05-21T06:29:49Z,2020-10-27T03:46:49Z,http://arxiv.org/abs/2005.10479v2,http://arxiv.org/pdf/2005.10479v2,"eess.AS, cs.SD"
Decentralized Deep Reinforcement Learning for a Distributed and Adaptive   Locomotion Controller of a Hexapod Robot,"Malte Schilling, Kai Konen, Frank W. Ohl, Timo Korthals","Locomotion is a prime example for adaptive behavior in animals and biological control principles have inspired control architectures for legged robots. While machine learning has been successfully applied to many tasks in recent years, Deep Reinforcement Learning approaches still appear to struggle when applied to real world robots in continuous control tasks and in particular do not appear as robust solutions that can handle uncertainties well. Therefore, there is a new interest in incorporating biological principles into such learning architectures. While inducing a hierarchical organization as found in motor control has shown already some success, we here propose a decentralized organization as found in insect motor control for coordination of different legs. A decentralized and distributed architecture is introduced on a simulated hexapod robot and the details of the controller are learned through Deep Reinforcement Learning. We first show that such a concurrent local structure is able to learn better walking behavior. Secondly, that the simpler organization is learned faster compared to holistic approaches.",2020-05-21T11:40:37Z,2020-05-21T11:40:37Z,http://arxiv.org/abs/2005.11164v1,http://arxiv.org/pdf/2005.11164v1,"cs.RO, cs.AI, stat.ML"
Tackling the Problem of Large Deformations in Deep Learning Based   Medical Image Registration Using Displacement Embeddings,"Lasse Hansen, Mattias P. Heinrich","Though, deep learning based medical image registration is currently starting to show promising advances, often, it still fells behind conventional frameworks in terms of registration accuracy. This is especially true for applications where large deformations exist, such as registration of interpatient abdominal MRI or inhale-to-exhale CT lung registration. Most current works use U-Net-like architectures to predict dense displacement fields from the input images in different supervised and unsupervised settings. We believe that the U-Net architecture itself to some level limits the ability to predict large deformations (even when using multilevel strategies) and therefore propose a novel approach, where the input images are mapped into a displacement space and final registrations are reconstructed from this embedding. Experiments on inhale-to-exhale CT lung registration demonstrate the ability of our architecture to predict large deformations in a single forward path through our network (leading to errors below 2 mm).",2020-05-27T13:06:24Z,2020-05-27T13:06:24Z,http://arxiv.org/abs/2005.13338v1,http://arxiv.org/pdf/2005.13338v1,"cs.CV, eess.IV"
An ENAS Based Approach for Constructing Deep Learning Models for Breast   Cancer Recognition from Ultrasound Images,"Mohammed Ahmed, Hongbo Du, Alaa AlZoubi","Deep Convolutional Neural Networks (CNN) provides an ""end-to-end"" solution for image pattern recognition with impressive performance in many areas of application including medical imaging. Most CNN models of high performance use hand-crafted network architectures that require expertise in CNNs to utilise their potentials. In this paper, we applied the Efficient Neural Architecture Search (ENAS) method to find optimal CNN architectures for classifying breast lesions from ultrasound (US) images. Our empirical study with a dataset of 524 US images shows that the optimal models generated by using ENAS achieve an average accuracy of 89.3%, surpassing other hand-crafted alternatives. Furthermore, the models are simpler in complexity and more efficient. Our study demonstrates that the ENAS approach to CNN model design is a promising direction for classifying ultrasound images of breast lesions.",2020-05-27T22:49:45Z,2020-05-27T22:49:45Z,http://arxiv.org/abs/2005.13695v1,http://arxiv.org/pdf/2005.13695v1,"eess.IV, cs.CV, cs.LG"
Algorithm for the replica redistribution in the implementation of   parallel annealing method on the hybrid supercomputer architecture,"Alexander Russkov, roman Chulkevich, Lev Shchur",The parallel annealing method is one of the promising approaches for large scale simulations as potentially scalable on any parallel architecture. We present an implementation of the algorithm on the hybrid program architecture combining CUDA and MPI. The problem is to keep all general-purpose graphics processing unit devices as busy as possible redistributing replicas and to do that efficiently. We provide details of the testing on Intel Skylake/Nvidia V100 based hardware running in parallel more than two million replicas of the Ising model sample. The results are quite optimistic because the acceleration grows toward the perfect line with the growing complexity of the simulated system.,2020-05-31T16:56:51Z,2020-05-31T16:56:51Z,http://arxiv.org/abs/2006.00561v1,http://arxiv.org/pdf/2006.00561v1,"physics.comp-ph, cond-mat.stat-mech"
AnalogNet: Convolutional Neural Network Inference on Analog Focal Plane   Sensor Processors,"Matthew Z. Wong, Benoit Guillard, Riku Murai, Sajad Saeedi, Paul H. J. Kelly","We present a high-speed, energy-efficient Convolutional Neural Network (CNN) architecture utilising the capabilities of a unique class of devices known as analog Focal Plane Sensor Processors (FPSP), in which the sensor and the processor are embedded together on the same silicon chip. Unlike traditional vision systems, where the sensor array sends collected data to a separate processor for processing, FPSPs allow data to be processed on the imaging device itself. This unique architecture enables ultra-fast image processing and high energy efficiency, at the expense of limited processing resources and approximate computations. In this work, we show how to convert standard CNNs to FPSP code, and demonstrate a method of training networks to increase their robustness to analog computation errors. Our proposed architecture, coined AnalogNet, reaches a testing accuracy of 96.9% on the MNIST handwritten digits recognition task, at a speed of 2260 FPS, for a cost of 0.7 mJ per frame.",2020-06-02T16:44:43Z,2020-06-21T17:19:36Z,http://arxiv.org/abs/2006.01765v2,http://arxiv.org/pdf/2006.01765v2,"eess.SP, cs.CV, cs.LG"
Learning to Rank Learning Curves,"Martin Wistuba, Tejaswini Pedapati","Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, we present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, we consider this task as a ranking and transfer learning problem. We qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other datasets, our model is able to effectively rank learning curves without having to observe many or very long learning curves. We further demonstrate that our method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments we analyze the quality of ranking, the influence of different model components as well as the predictive behavior of the model.",2020-06-05T10:49:52Z,2020-06-05T10:49:52Z,http://arxiv.org/abs/2006.03361v1,http://arxiv.org/pdf/2006.03361v1,"cs.LG, cs.CV, stat.ML"
Synchronization in Digital Twins for Industrial Control Systems,"Fatemeh Akbarian, Emma Fitzgerald, Maria Kihl","Digital twins, which are a new concept in industrial control systems (ICS), play a key role in realizing the vision of a smart factory, and they can have different effective use cases. With digital twins, we have virtual replicas of physical systems so that they precisely mirror the internal behavior of the physical systems. Hence, synchronization is necessary to keep the states of digital twins in sync with those of their physical counterparts. Otherwise, their behavior may be different from each other, and it can lead to wrong decisions about the system that can have catastrophic consequences. In this paper, we propose three different architectures for digital twins, and then by investigating their ability to follow the physical system's behavior, we will determine the best architecture, whose output has the lowest error compared with the physical system's output.",2020-06-05T13:44:50Z,2020-06-05T13:44:50Z,http://arxiv.org/abs/2006.03447v1,http://arxiv.org/pdf/2006.03447v1,"eess.SY, cs.NI, cs.SY"
Conditional Neural Architecture Search,"Sheng-Chun Kao, Arun Ramamurthy, Reed Williams, Tushar Krishna","Designing resource-efficient Deep Neural Networks (DNNs) is critical to deploy deep learning solutions over edge platforms due to diverse performance, power, and memory budgets. Unfortunately, it is often the case a well-trained ML model does not fit to the constraint of deploying edge platforms, causing a long iteration of model reduction and retraining process. Moreover, a ML model optimized for platform-A often may not be suitable when we deploy it on another platform-B, causing another iteration of model retraining. We propose a conditional neural architecture search method using GAN, which produces feasible ML models for different platforms. We present a new workflow to generate constraint-optimized DNN models. This is the first work of bringing in condition and adversarial technique into Neural Architecture Search domain. We verify the method with regression problems and classification on CIFAR-10. The proposed workflow can successfully generate resource-optimized MLP or CNN-based networks.",2020-06-06T20:39:33Z,2020-06-06T20:39:33Z,http://arxiv.org/abs/2006.03969v1,http://arxiv.org/pdf/2006.03969v1,"cs.LG, stat.ML"
VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net   architecture,"Da-Yi Wu, Yen-Hao Chen, Hung-Yi Lee","Voice conversion (VC) is a task that transforms the source speaker's timbre, accent, and tones in audio into another one's while preserving the linguistic content. It is still a challenging work, especially in a one-shot setting. Auto-encoder-based VC methods disentangle the speaker and the content in input speech without given the speaker's identity, so these methods can further generalize to unseen speakers. The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN). However, the imperfect disentanglement may harm the quality of output speech. In this work, to further improve audio quality, we use the U-Net architecture within an auto-encoder-based VC system. We find that to leverage the U-Net architecture, a strong information bottleneck is necessary. The VQ-based method, which quantizes the latent vectors, can serve the purpose. The objective and the subjective evaluations show that the proposed method performs well in both audio naturalness and speaker similarity.",2020-06-07T14:01:16Z,2020-06-07T14:01:16Z,http://arxiv.org/abs/2006.04154v1,http://arxiv.org/pdf/2006.04154v1,"eess.AS, cs.LG, cs.SD"
Serverless on FHIR: Deploying machine learning models for healthcare on   the cloud,"Bell Raj Eapen, Kamran Sartipi, Norm Archer","Machine Learning (ML) plays a vital role in implementing digital health. The advances in hardware and the democratization of software tools have revolutionized machine learning. However, the deployment of ML models -- the mathematical representation of the task to be performed -- for effective and efficient clinical decision support at the point of care is still a challenge. ML models undergo constant improvement of their accuracy and predictive power with a high turnover rate. Updating models consumed by downstream health information systems is essential for patient safety. We introduce a functional taxonomy and a four-tier architecture for cloud-based model deployment for digital health. The four tiers are containerized microservices for maintainability, serverless architecture for scalability, function as a service for portability and FHIR schema for discoverability. We call this architecture Serverless on FHIR and propose this as a standard to deploy digital health applications that can be consumed by downstream systems such as EMRs and visualization tools.",2020-06-08T16:57:30Z,2020-06-08T16:57:30Z,http://arxiv.org/abs/2006.04748v1,http://arxiv.org/pdf/2006.04748v1,"cs.CY, cs.LG, q-bio.QM"
Lorentz Group Equivariant Neural Network for Particle Physics,"Alexander Bogatskiy, Brandon Anderson, Jan T. Offermann, Marwah Roussi, David W. Miller, Risi Kondor","We present a neural network architecture that is fully equivariant with respect to transformations under the Lorentz group, a fundamental symmetry of space and time in physics. The architecture is based on the theory of the finite-dimensional representations of the Lorentz group and the equivariant nonlinearity involves the tensor product. For classification tasks in particle physics, we demonstrate that such an equivariant architecture leads to drastically simpler models that have relatively few learnable parameters and are much more physically interpretable than leading approaches that use CNNs and point cloud approaches. The competitive performance of the network is demonstrated on a public classification dataset [27] for tagging top quark decays given energy-momenta of jet constituents produced in proton-proton collisions.",2020-06-08T17:54:43Z,2020-06-08T17:54:43Z,http://arxiv.org/abs/2006.04780v1,http://arxiv.org/pdf/2006.04780v1,"hep-ph, cs.LG, hep-ex, physics.comp-ph, stat.ML"
The Curious Case of Convex Neural Networks,"Sarath Sivaprasad, Ankur Singh, Naresh Manwani, Vineet Gandhi","In this paper, we investigate a constrained formulation of neural networks where the output is a convex function of the input. We show that the convexity constraints can be enforced on both fully connected and convolutional layers, making them applicable to most architectures. The convexity constraints include restricting the weights (for all but the first layer) to be non-negative and using a non-decreasing convex activation function. Albeit simple, these constraints have profound implications on the generalization abilities of the network. We draw three valuable insights: (a) Input Output Convex Neural Networks (IOC-NNs) self regularize and reduce the problem of overfitting; (b) Although heavily constrained, they outperform the base multi layer perceptrons and achieve similar performance as compared to base convolutional architectures and (c) IOC-NNs show robustness to noise in train labels. We demonstrate the efficacy of the proposed idea using thorough experiments and ablation studies on standard image classification datasets with three different neural network architectures.",2020-06-09T08:16:38Z,2021-07-10T10:51:29Z,http://arxiv.org/abs/2006.05103v3,http://arxiv.org/pdf/2006.05103v3,"cs.LG, stat.ML"
Design Challenges of Neural Network Acceleration Using Stochastic   Computing,Alireza Khadem,"The enormous and ever-increasing complexity of state-of-the-art neural networks (NNs) has impeded the deployment of deep learning on resource-limited devices such as the Internet of Things (IoTs). Stochastic computing exploits the inherent amenability to approximation characteristic of NNs to reduce their energy and area footprint, two critical requirements of small embedded devices suitable for the IoTs. This report evaluates and compares two recently proposed stochastic-based NN designs, referred to as BISC (Binary Interfaced Stochastic Computing) by Sim and Lee, 2017, and ESL (Extended Stochastic Logic) by Canals et al., 2016. Using analysis and simulation, we compare three distinct implementations of these designs in terms of performance, power consumption, area, and accuracy. We also discuss the overall challenges faced in adopting stochastic computing for building NNs. We find that BISC outperforms the other architectures when executing the LeNet-5 NN model applied to the MNIST digit recognition dataset. Our analysis and simulation experiments indicate that this architecture is around 50X faster, occupies 5.7X and 2.9X less area, and consumes 7.8X and 1.8X less power than the two ESL architectures.",2020-06-08T16:06:56Z,2020-06-08T16:06:56Z,http://arxiv.org/abs/2006.05352v1,http://arxiv.org/pdf/2006.05352v1,"eess.SP, cs.LG"
Collegial Ensembles,"Etai Littwin, Ben Myara, Sima Sabah, Joshua Susskind, Shuangfei Zhai, Oren Golan","Modern neural network performance typically improves as model size increases. A recent line of research on the Neural Tangent Kernel (NTK) of over-parameterized networks indicates that the improvement with size increase is a product of a better conditioned loss landscape. In this work, we investigate a form of over-parameterization achieved through ensembling, where we define collegial ensembles (CE) as the aggregation of multiple independent models with identical architectures, trained as a single model. We show that the optimization dynamics of CE simplify dramatically when the number of models in the ensemble is large, resembling the dynamics of wide models, yet scale much more favorably. We use recent theoretical results on the finite width corrections of the NTK to perform efficient architecture search in a space of finite width CE that aims to either minimize capacity, or maximize trainability under a set of constraints. The resulting ensembles can be efficiently implemented in practical architectures using group convolutions and block diagonal layers. Finally, we show how our framework can be used to analytically derive optimal group convolution modules originally found using expensive grid searches, without having to train a single model.",2020-06-13T16:40:26Z,2020-06-17T15:33:22Z,http://arxiv.org/abs/2006.07678v2,http://arxiv.org/pdf/2006.07678v2,"cs.LG, stat.ML"
Bonsai-Net: One-Shot Neural Architecture Search via Differentiable   Pruners,"Rob Geada, Dennis Prangle, Andrew Stephen McGough","One-shot Neural Architecture Search (NAS) aims to minimize the computational expense of discovering state-of-the-art models. However, in the past year attention has been drawn to the comparable performance of naive random search across the same search spaces used by leading NAS algorithms. To address this, we explore the effects of drastically relaxing the NAS search space, and we present Bonsai-Net, an efficient one-shot NAS method to explore our relaxed search space. Bonsai-Net is built around a modified differential pruner and can consistently discover state-of-the-art architectures that are significantly better than random search with fewer parameters than other state-of-the-art methods. Additionally, Bonsai-Net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully-trained models from scratch.",2020-06-12T14:44:00Z,2021-06-04T15:40:29Z,http://arxiv.org/abs/2006.09264v3,http://arxiv.org/pdf/2006.09264v3,"cs.LG, stat.ML"
Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition,"Xinyuan Zhou, Emre Yılmaz, Yanhua Long, Yijie Li, Haizhou Li","Code-switching (CS) occurs when a speaker alternates words of two or more languages within a single sentence or across sentences. Automatic speech recognition (ASR) of CS speech has to deal with two or more languages at the same time. In this study, we propose a Transformer-based architecture with two symmetric language-specific encoders to capture the individual language attributes, that improve the acoustic representation of each language. These representations are combined using a language-specific multi-head attention mechanism in the decoder module. Each encoder and its corresponding attention module in the decoder are pre-trained using a large monolingual corpus aiming to alleviate the impact of limited CS training data. We call such a network a multi-encoder-decoder (MED) architecture. Experiments on the SEAME corpus show that the proposed MED architecture achieves 10.2% and 10.8% relative error rate reduction on the CS evaluation sets with Mandarin and English as the matrix language respectively.",2020-06-18T10:42:52Z,2020-06-18T10:42:52Z,http://arxiv.org/abs/2006.10414v1,http://arxiv.org/pdf/2006.10414v1,"eess.AS, cs.SD"
Systematic Attack Surface Reduction For Deployed Sentiment Analysis   Models,"Josh Kalin, David Noever, Gerry Dozier","This work proposes a structured approach to baselining a model, identifying attack vectors, and securing the machine learning models after deployment. This method for securing each model post deployment is called the BAD (Build, Attack, and Defend) Architecture. Two implementations of the BAD architecture are evaluated to quantify the adversarial life cycle for a black box Sentiment Analysis system. As a challenging diagnostic, the Jigsaw Toxic Bias dataset is selected as the baseline in our performance tool. Each implementation of the architecture will build a baseline performance report, attack a common weakness, and defend the incoming attack. As an important note: each attack surface demonstrated in this work is detectable and preventable. The goal is to demonstrate a viable methodology for securing a machine learning model in a production setting.",2020-06-19T13:41:38Z,2020-06-19T13:41:38Z,http://arxiv.org/abs/2006.11130v1,http://arxiv.org/pdf/2006.11130v1,"cs.CR, cs.LG, stat.ML"
A Hierarchical Architecture for the Coordination of an Ensemble of Steam   Generators,"Stefano Spinelli, Elia Longoni, Marcello Farina, Felix Petzke, Stefan Streif, Andrea Ballarino","This work presents a hierarchical architecture for the optimal management of an ensemble of steam generators, which needs to jointly sustain a common load. The coordination of independent subsystems is provided by a multi-layer control scheme. A high-level optimizer computes the optimal shares of production to be allocated to single generators. At medium level, a robust tube-based model predictive control (MPC) is proposed to track the time-varying demand of the ensemble using a centralized, but aggregated model, whose order does not scale with the number of subsystems. At low level, decentralized controllers are in place to stabilize the internal boiler pressure. The control architecture enables the dynamic modification of the ensemble configuration and plug and play operations. Simulation results are reported to demonstrate the potentialities of the proposed approach.",2020-06-22T16:30:21Z,2020-06-22T16:30:21Z,http://arxiv.org/abs/2006.12400v1,http://arxiv.org/pdf/2006.12400v1,"eess.SY, cs.SY"
Direct Feedback Alignment Scales to Modern Deep Learning Tasks and   Architectures,"Julien Launay, Iacopo Poli, François Boniface, Florent Krzakala","Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",2020-06-23T10:17:49Z,2020-12-11T14:31:35Z,http://arxiv.org/abs/2006.12878v2,http://arxiv.org/pdf/2006.12878v2,"stat.ML, cs.LG, cs.NE"
Image-to-image Mapping with Many Domains by Sparse Attribute Transfer,"Matthew Amodio, Rim Assouel, Victor Schmidt, Tristan Sylvain, Smita Krishnaswamy, Yoshua Bengio","Unsupervised image-to-image translation consists of learning a pair of mappings between two domains without known pairwise correspondences between points. The current convention is to approach this task with cycle-consistent GANs: using a discriminator to encourage the generator to change the image to match the target domain, while training the generator to be inverted with another mapping. While ending up with paired inverse functions may be a good end result, enforcing this restriction at all times during training can be a hindrance to effective modeling. We propose an alternate approach that directly restricts the generator to performing a simple sparse transformation in a latent layer, motivated by recent work from cognitive neuroscience suggesting an architectural prior on representations corresponding to consciousness. Our biologically motivated approach leads to representations more amenable to transformation by disentangling high-level abstract concepts in the latent space. We demonstrate that image-to-image domain translation with many different domains can be learned more effectively with our architecturally constrained, simple transformation than with previous unconstrained architectures that rely on a cycle-consistency loss.",2020-06-23T19:52:23Z,2020-06-23T19:52:23Z,http://arxiv.org/abs/2006.13291v1,http://arxiv.org/pdf/2006.13291v1,"cs.CV, cs.LG, eess.IV"
Implicit Convex Regularizers of CNN Architectures: Convex Optimization   of Two- and Three-Layer Networks in Polynomial Time,"Tolga Ergen, Mert Pilanci","We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an $\ell_2$ norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an $\ell_1$ regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.",2020-06-26T04:47:20Z,2021-03-18T15:30:26Z,http://arxiv.org/abs/2006.14798v3,http://arxiv.org/pdf/2006.14798v3,"cs.LG, cs.CC, stat.ML"
Traditional and accelerated gradient descent for neural architecture   search,"Nicolas Garcia Trillos, Felix Morales, Javier Morales","In this paper we introduce two algorithms for neural architecture search (NASGD and NASAGD) following the theoretical work by two of the authors [5] which used the geometric structure of optimal transport to introduce the conceptual basis for new notions of traditional and accelerated gradient descent algorithms for the optimization of a function on a semi-discrete space. Our algorithms, which use the network morphism framework introduced in [2] as a baseline, can analyze forty times as many architectures as the hill climbing methods [2, 14] while using the same computational resources and time and achieving comparable levels of accuracy. For example, using NASGD on CIFAR-10, our method designs and trains networks with an error rate of 4.06 in only 12 hours on a single GPU.",2020-06-26T21:28:35Z,2021-02-14T03:26:17Z,http://arxiv.org/abs/2006.15218v3,http://arxiv.org/pdf/2006.15218v3,"cs.LG, cs.NA, cs.NE, math.NA, stat.ML"
Chroma Intra Prediction with attention-based CNN architectures,"Marc Górriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta Mrak","Neural networks can be used in video coding to improve chroma intra-prediction. In particular, usage of fully-connected networks has enabled better cross-component prediction with respect to traditional linear models. Nonetheless, state-of-the-art architectures tend to disregard the location of individual reference samples in the prediction process. This paper proposes a new neural network architecture for cross-component intra-prediction. The network uses a novel attention module to model spatial relations between reference and predicted samples. The proposed approach is integrated into the Versatile Video Coding (VVC) prediction pipeline. Experimental results demonstrate compression gains over the latest VVC anchor compared with state-of-the-art chroma intra-prediction methods based on neural networks.",2020-06-27T12:11:17Z,2020-06-27T12:11:17Z,http://arxiv.org/abs/2006.15349v1,http://arxiv.org/pdf/2006.15349v1,"eess.IV, cs.CC, cs.CV, cs.LG, cs.MM"
GuavaNet: A deep neural network architecture for automatic sensory   evaluation to predict degree of acceptability for Guava by a consumer,Vipul Mehra,"This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables, Cheese and Fish based on Image Processing using Computer Vision and Deep Learning: A Review. It consists of a comprehensive review of image processing, computer vision and deep learning techniques applied to carry out analysis of fruits, vegetables, cheese and fish.This part also serves as a literature review for Part II.Part II: GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. This part introduces to an end-to-end deep neural network architecture that can predict the degree of acceptability by the consumer for a guava based on sensory evaluation.",2021-07-05T20:13:37Z,2021-07-05T20:13:37Z,http://arxiv.org/abs/2108.02563v1,http://arxiv.org/pdf/2108.02563v1,"cs.CV, cs.LG, eess.IV"
Biocompatible surface functionalization architecture for a diamond   quantum sensor,"Mouzhe Xie, Xiaofei Yu, Lila V. H. Rodgers, Daohong Xu, Ignacio Chi-Duran, Adrien Toros, Niels Quack, Nathalie P. de Leon, Peter C. Maurer","Quantum metrology enables some of the most precise measurements. In the life sciences, diamond-based quantum sensing has enabled a new class of biophysical sensors and diagnostic devices that are being investigated as a platform for cancer screening and ultra-sensitive immunoassays. However, a broader application in the life sciences based on nanoscale nuclear magnetic resonance spectroscopy has been hampered by the need to interface highly sensitive quantum bit (qubit) sensors with their biological targets. Here, we demonstrate a new approach that combines quantum engineering with single-molecule biophysics to immobilize individual proteins and DNA molecules on the surface of a bulk diamond crystal that hosts coherent nitrogen vacancy qubit sensors. Our thin (sub-5 nm) functionalization architecture provides precise control over protein adsorption density and results in near-surface qubit coherence approaching 100 {\mu}s. The developed architecture remains chemically stable under physiological conditions for over five days, making our technique compatible with most biophysical and biomedical applications.",2021-08-10T18:01:35Z,2021-08-10T18:01:35Z,http://arxiv.org/abs/2108.04843v1,http://arxiv.org/pdf/2108.04843v1,"quant-ph, physics.bio-ph"
Hardware-Aware Beamspace Precoding for All-Digital mmWave Massive   MU-MIMO,"Emre Gönültaş, Sueda Taner, Alexandra Gallyas-Sanhueza, Seyed Hadi Mirfarshbafan, Christoph Studer","Massive multi-user multiple-input multiple-output (MU-MIMO) wireless systems operating at millimeter-wave (mmWave) frequencies enable simultaneous wideband data transmission to a large number of users. In order to reduce the complexity of MU precoding in all-digital basestation architectures, we propose a two-stage precoding architecture that first performs precoding using a sparse matrix in the beamspace domain, followed by an inverse fast Fourier transform that converts the result to the antenna domain. The sparse precoding matrix requires a small number of multipliers and enables regular hardware architectures, which allows the design of hardware-efficient all-digital precoders. Simulation results demonstrate that our methods approach the error-rate of conventional Wiener filter precoding with more than 2x reduced complexity.",2021-08-13T13:25:50Z,2021-08-13T13:25:50Z,http://arxiv.org/abs/2108.06229v1,http://arxiv.org/pdf/2108.06229v1,"cs.IT, eess.SP, math.IT"
Can the Transformer Be Used as a Drop-in Replacement for RNNs in   Text-Generating GANs?,"Kevin Blin, Andrei Kucharavy","In this paper we address the problem of fine-tuned text generation with a limited computational budget. For that, we use a well-performing text generative adversarial network (GAN) architecture - Diversity-Promoting GAN (DPGAN), and attempted a drop-in replacement of the LSTM layer with a self-attention-based Transformer layer in order to leverage their efficiency. The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance, quality and diversity of generated text and stability. Computational experiments suggested that a transformer architecture is unable to drop-in replace the LSTM layer, under-performing during the pre-training phase and undergoing a complete mode collapse during the GAN tuning phase. Our results suggest that the transformer architecture need to be adapted before it can be used as a replacement for RNNs in text-generating GANs.",2021-08-26T14:15:36Z,2021-08-26T14:15:36Z,http://arxiv.org/abs/2108.12275v1,http://arxiv.org/pdf/2108.12275v1,"cs.LG, cs.CL, 68T50, 68T05, I.2.7"
Multi-input Architecture and Disentangled Representation Learning for   Multi-dimensional Modeling of Music Similarity,"Sebastian Ribecky, Jakob Abeßer, Hanna Lukashevich","In the context of music information retrieval, similarity-based approaches are useful for a variety of tasks that benefit from a query-by-example scenario. Music however, naturally decomposes into a set of semantically meaningful factors of variation. Current representation learning strategies pursue the disentanglement of such factors from deep representations, resulting in highly interpretable models. This allows the modeling of music similarity perception, which is highly subjective and multi-dimensional. While the focus of prior work is on metadata driven notions of similarity, we suggest to directly model the human notion of multi-dimensional music similarity. To achieve this, we propose a multi-input deep neural network architecture, which simultaneously processes mel-spectrogram, CENS-chromagram and tempogram in order to extract informative features for the different disentangled musical dimensions: genre, mood, instrument, era, tempo, and key. We evaluated the proposed music similarity approach using a triplet prediction task and found that the proposed multi-input architecture outperforms a state of the art method. Furthermore, we present a novel multi-dimensional analysis in order to evaluate the influence of each disentangled dimension on the perception of music similarity.",2021-11-02T16:23:46Z,2021-11-02T16:23:46Z,http://arxiv.org/abs/2111.01710v1,http://arxiv.org/pdf/2111.01710v1,"eess.AS, cs.SD"
Improving Pose Estimation through Contextual Activity Fusion,"David Poulton, Richard Klein","This research presents the idea of activity fusion into existing Pose Estimation architectures to enhance their predictive ability. This is motivated by the rise in higher level concepts found in modern machine learning architectures, and the belief that activity context is a useful piece of information for the problem of pose estimation. To analyse this concept we take an existing deep learning architecture and augment it with an additional 1x1 convolution to fuse activity information into the model. We perform evaluation and comparison on a common pose estimation dataset, and show a performance improvement over our baseline model, especially in uncommon poses and on typically difficult joints. Additionally, we perform an ablative analysis to indicate that the performance improvement does in fact draw from the activity information.",2021-11-03T19:58:13Z,2021-11-03T19:58:13Z,http://arxiv.org/abs/2111.02500v1,http://arxiv.org/pdf/2111.02500v1,"cs.CV, cs.LG, I.4.9"
Conformer-based Hybrid ASR System for Switchboard Dataset,"Mohammad Zeineldeen, Jingjing Xu, Christoph Lüscher, Wilfried Michel, Alexander Gerstenberger, Ralf Schlüter, Hermann Ney","The recently proposed conformer architecture has been successfully used for end-to-end automatic speech recognition (ASR) architectures achieving state-of-the-art performance on different datasets. To our best knowledge, the impact of using conformer acoustic model for hybrid ASR is not investigated. In this paper, we present and evaluate a competitive conformer-based hybrid model training recipe. We study different training aspects and methods to improve word-error-rate as well as to increase training speed. We apply time downsampling methods for efficient training and use transposed convolutions to upsample the output sequence again. We conduct experiments on Switchboard 300h dataset and our conformer-based hybrid model achieves competitive results compared to other architectures. It generalizes very well on Hub5'01 test set and outperforms the BLSTM-based hybrid model significantly.",2021-11-05T12:03:18Z,2022-02-19T21:53:03Z,http://arxiv.org/abs/2111.03442v2,http://arxiv.org/pdf/2111.03442v2,"cs.CL, eess.AS, stat.ML"
Gated Linear Model induced U-net for surrogate modeling and uncertainty   quantification,"Sai Krishna Mendu, Souvik Chakraborty","We propose a novel deep learning based surrogate model for solving high-dimensional uncertainty quantification and uncertainty propagation problems. The proposed deep learning architecture is developed by integrating the well-known U-net architecture with the Gaussian Gated Linear Network (GGLN) and referred to as the Gated Linear Network induced U-net or GLU-net. The proposed GLU-net treats the uncertainty propagation problem as an image to image regression and hence, is extremely data efficient. Additionally, it also provides estimates of the predictive uncertainty. The network architecture of GLU-net is less complex with 44\% fewer parameters than the contemporary works. We illustrate the performance of the proposed GLU-net in solving the Darcy flow problem under uncertainty under the sparse data scenario. We consider the stochastic input dimensionality to be up to 4225. Benchmark results are generated using the vanilla Monte Carlo simulation. We observe the proposed GLU-net to be accurate and extremely efficient even when no information about the structure of the inputs is provided to the network. Case studies are performed by varying the training sample size and stochastic input dimensionality to illustrate the robustness of the proposed approach.",2021-11-08T03:27:48Z,2021-11-08T03:27:48Z,http://arxiv.org/abs/2111.05123v1,http://arxiv.org/pdf/2111.05123v1,"stat.ML, cs.LG"
Preserving Dense Features for Ki67 Nuclei Detection,"Seyed Hossein Mirjahanmardi, Melanie Dawe, Anthony Fyles, Wei Shi, Fei-Fei Liu, Susan Done, April Khademi","Nuclei detection is a key task in Ki67 proliferation index estimation in breast cancer images. Deep learning algorithms have shown strong potential in nuclei detection tasks. However, they face challenges when applied to pathology images with dense medium and overlapping nuclei since fine details are often diluted or completely lost by early maxpooling layers. This paper introduces an optimized UV-Net architecture, specifically developed to recover nuclear details with high-resolution through feature preservation for Ki67 proliferation index computation. UV-Net achieves an average F1-score of 0.83 on held-out test patch data, while other architectures obtain 0.74-0.79. On tissue microarrays (unseen) test data obtained from multiple centers, UV-Net's accuracy exceeds other architectures by a wide margin, including 9-42\% on Ontario Veterinary College, 7-35\% on Protein Atlas and 0.3-3\% on University Health Network.",2021-11-10T01:50:23Z,2022-07-15T23:31:04Z,http://arxiv.org/abs/2111.05482v3,http://arxiv.org/pdf/2111.05482v3,"eess.IV, q-bio.QM"
Domain Generalization on Efficient Acoustic Scene Classification using   Residual Normalization,"Byeonggeun Kim, Seunghan Yang, Jangho Kim, Simyung Chang","It is a practical research topic how to deal with multi-device audio inputs by a single acoustic scene classification system with efficient design. In this work, we propose Residual Normalization, a novel feature normalization method that uses frequency-wise normalization % instance normalization with a shortcut path to discard unnecessary device-specific information without losing useful information for classification. Moreover, we introduce an efficient architecture, BC-ResNet-ASC, a modified version of the baseline architecture with a limited receptive field. BC-ResNet-ASC outperforms the baseline architecture even though it contains the small number of parameters. Through three model compression schemes: pruning, quantization, and knowledge distillation, we can reduce model complexity further while mitigating the performance degradation. The proposed system achieves an average test accuracy of 76.3% in TAU Urban Acoustic Scenes 2020 Mobile, development dataset with 315k parameters, and average test accuracy of 75.3% after compression to 61.0KB of non-zero parameters. The proposed method won the 1st place in DCASE 2021 challenge, TASK1A.",2021-11-12T01:57:36Z,2021-11-12T01:57:36Z,http://arxiv.org/abs/2111.06531v1,http://arxiv.org/pdf/2111.06531v1,"cs.SD, cs.LG, eess.AS"
Modeling Irregular Time Series with Continuous Recurrent Units,"Mona Schirmer, Mazin Eltayeb, Stefan Lessmann, Maja Rudolph","Recurrent neural networks (RNNs) are a popular choice for modeling sequential data. Modern RNN architectures assume constant time-intervals between observations. However, in many datasets (e.g. medical records) observation times are irregular and can carry important information. To address this challenge, we propose continuous recurrent units (CRUs) -- a neural architecture that can naturally handle irregular intervals between observations. The CRU assumes a hidden state, which evolves according to a linear stochastic differential equation and is integrated into an encoder-decoder framework. The recursive computations of the CRU can be derived using the continuous-discrete Kalman filter and are in closed form. The resulting recurrent architecture has temporal continuity between hidden states and a gating mechanism that can optimally integrate noisy observations. We derive an efficient parameterization scheme for the CRU that leads to a fast implementation f-CRU. We empirically study the CRU on a number of challenging datasets and find that it can interpolate irregular time series better than methods based on neural ordinary differential equations.",2021-11-22T16:49:15Z,2022-07-26T15:12:26Z,http://arxiv.org/abs/2111.11344v3,http://arxiv.org/pdf/2111.11344v3,"cs.LG, stat.ML"
Toward Next Generation Open Radio Access Network--What O-RAN Can and   Cannot Do!,"Aly S. Abdalla, Pratheek S. Upadhyaya, Vijay K. Shah, Vuk Marojevic","The open radio access network (O-RAN) describes an industry-driven open architecture and interfaces for building next generation RANs with artificial intelligence (AI) controllers. We circulated a survey among researchers, developers, and practitioners to gather their perspectives on O-RAN as a framework for 6G wireless research and development (R&D). The majority responded in favor of O-RAN and identified R&D of interest to them. Motivated by these responses, this paper identifies the limitations of the current O-RAN specifications and the technologies for overcoming them. We recognize end-to-end security, deterministic latency, physical layer real-time control, and testing of AI-based RAN control applications as the critical features to enable and discuss R&D opportunities for extending the architectural capabilities of O-RAN as a platform for 6G wireless.",2021-11-26T21:57:23Z,2022-03-26T01:32:47Z,http://arxiv.org/abs/2111.13754v2,http://arxiv.org/pdf/2111.13754v2,"cs.NI, cs.SY, eess.SY"
Architectural improvements and technological enhancements for the   APEnet+ interconnect system,"R. Ammendola, A. Biagioni, O. Frezza, A. Lonardo, F. Lo Cicero, M. Martinelli, P. S. Paolucci, E. Pastorelli, D. Rossetti, F. Simula, L. Tosoratto, P. Vicini","The APEnet+ board delivers a point-to-point, low-latency, 3D torus network interface card. In this paper we describe the latest generation of APEnet NIC, APEnet v5, integrated in a PCIe Gen3 board based on a state-of-the-art, 28 nm Altera Stratix V FPGA. The NIC features a network architecture designed following the Remote DMA paradigm and tailored to tightly bind the computing power of modern GPUs to the communication fabric. For the APEnet v5 board we show characterizing figures as achieved bandwidth and BER obtained by exploiting new high performance ALTERA transceivers and PCIe Gen3 compliancy.",2022-01-04T11:10:49Z,2022-01-04T11:10:49Z,http://arxiv.org/abs/2201.01088v1,http://arxiv.org/pdf/2201.01088v1,"physics.comp-ph, cs.AR"
Spectrum Sharing for Secrecy Performance Enhancement in D2D-enabled UAV   Networks,"B. Yang, T. Taleb, Z. Wu, L. Ma","With the assistance of device-to-device (D2D) communications, unmanned aerial vehicle (UAV) networks are anticipated to support widespread applications in the fifth generation (5G) and beyond wireless systems, by providing seamless coverage, flexible deployment, and high channel rate. However, the networks face significant security threats from malicious eavesdroppers due to the inherent broadcast and openness nature of wireless channels. To ensure secure communications of such networks, physical layer security is a promising technique, which utilizes the randomness and noise of wireless channels to enhance secrecy performance. This article investigates physical layer security performance via spectrum sharing in D2D-enabled UAV networks. We first present two typical network architectures where each UAV serves as either a flying base station or an aerial user equipment. Then, we propose a spectrum sharing strategy to fully exploit interference incurred by spectrum reuse for improving secrecy performance. We further conduct two case studies to evaluate the spectrum sharing strategy in these two typical network architectures, and also show secrecy performance gains compared to traditional spectrum sharing strategy. Finally, we discuss some future research directions in D2D-enabled UAV networks.",2022-01-05T07:23:54Z,2022-01-05T07:23:54Z,http://arxiv.org/abs/2201.03370v1,http://arxiv.org/pdf/2201.03370v1,"cs.NI, eess.SP"
A Novel Temporal Attentive-Pooling based Convolutional Recurrent   Architecture for Acoustic Signal Enhancement,"Tassadaq Hussain, Wei-Chien Wang, Mandar Gogate, Kia Dashtipour, Yu Tsao, Xugang Lu, Adeel Ahsan, Amir Hussain","In acoustic signal processing, the target signals usually carry semantic information, which is encoded in a hierarchal structure of short and long-term contexts. However, the background noise distorts these structures in a nonuniform way. The existing deep acoustic signal enhancement (ASE) architectures ignore this kind of local and global effect. To address this problem, we propose to integrate a novel temporal attentive-pooling (TAP) mechanism into a conventional convolutional recurrent neural network, termed as TAP-CRNN. The proposed approach considers both global and local attention for ASE tasks. Specifically, we first utilize a convolutional layer to extract local information of the acoustic signals and then a recurrent neural network (RNN) architecture is used to characterize temporal contextual information. Second, we exploit a novelattention mechanism to contextually process salient regions of the noisy signals. The proposed ASE system is evaluated using a benchmark infant cry dataset and compared with several well-known methods. It is shown that the TAPCRNN can more effectively reduce noise components from infant cry signals in unseen background noises at challenging signal-to-noise levels.",2022-01-24T19:13:44Z,2022-01-24T19:13:44Z,http://arxiv.org/abs/2201.09913v1,http://arxiv.org/pdf/2201.09913v1,"eess.AS, cs.SD"
Convolutional Xformers for Vision,"Pranav Jeevan, Amit sethi","Vision transformers (ViTs) have found only limited practical use in processing images, in spite of their state-of-the-art accuracy on certain benchmarks. The reason for their limited use include their need for larger training datasets and more computational resources compared to convolutional neural networks (CNNs), owing to the quadratic complexity of their self-attention mechanism. We propose a linear attention-convolution hybrid architecture -- Convolutional X-formers for Vision (CXV) -- to overcome these limitations. We replace the quadratic attention with linear attention mechanisms, such as Performer, Nystr\""omformer, and Linear Transformer, to reduce its GPU usage. Inductive prior for image data is provided by convolutional sub-layers, thereby eliminating the need for class token and positional embeddings used by the ViTs. We also propose a new training method where we use two different optimizers during different phases of training and show that it improves the top-1 image classification accuracy across different architectures. CXV outperforms other architectures, token mixers (e.g. ConvMixer, FNet and MLP Mixer), transformer models (e.g. ViT, CCT, CvT and hybrid Xformers), and ResNets for image classification in scenarios with limited data and GPU resources (cores, RAM, power).",2022-01-25T12:32:09Z,2022-01-25T12:32:09Z,http://arxiv.org/abs/2201.10271v1,http://arxiv.org/pdf/2201.10271v1,"cs.CV, cs.AI, cs.LG, I.4.0; I.4.1; I.4.7; I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2;
  I.5.4"
The BrainScaleS-2 accelerated neuromorphic system with hybrid plasticity,"Christian Pehle, Sebastian Billaudelle, Benjamin Cramer, Jakob Kaiser, Korbinian Schreiber, Yannik Stradmann, Johannes Weis, Aron Leibfried, Eric Müller, Johannes Schemmel","Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks - sometimes referred to as the third generation of neural networks - are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network.",2022-01-26T17:13:46Z,2022-02-03T16:18:25Z,http://arxiv.org/abs/2201.11063v2,http://arxiv.org/pdf/2201.11063v2,"cs.NE, cond-mat.dis-nn, q-bio.NC"
Hyperparameter Tuning for Deep Reinforcement Learning Applications,"Mariam Kiran, Melis Ozyildirim","Reinforcement learning (RL) applications, where an agent can simply learn optimal behaviors by interacting with the environment, are quickly gaining tremendous success in a wide variety of applications from controlling simple pendulums to complex data centers. However, setting the right hyperparameters can have a huge impact on the deployed solution performance and reliability in the inference models, produced via RL, used for decision-making. Hyperparameter search itself is a laborious process that requires many iterations and computationally expensive to find the best settings that produce the best neural network architectures. In comparison to other neural network architectures, deep RL has not witnessed much hyperparameter tuning, due to its algorithm complexity and simulation platforms needed. In this paper, we propose a distributed variable-length genetic algorithm framework to systematically tune hyperparameters for various RL applications, improving training time and robustness of the architecture, via evolution. We demonstrate the scalability of our approach on many RL problems (from simple gyms to complex applications) and compared with Bayesian approach. Our results show that with more generations, optimal solutions that require fewer training episodes and are computationally cheap while being more robust for deployment. Our results are imperative to advance deep reinforcement learning controllers for real-world problems.",2022-01-26T20:43:13Z,2022-01-26T20:43:13Z,http://arxiv.org/abs/2201.11182v1,http://arxiv.org/pdf/2201.11182v1,"cs.LG, cs.NE, I.2.6; I.2.11"
Autoencoding Hyperbolic Representation for Adversarial Generation,"Eric Qu, Dongmian Zou","With the recent advance of geometric deep learning, neural networks have been extensively used for data in non-Euclidean domains. In particular, hyperbolic neural networks have proved successful in processing hierarchical information of data. However, many hyperbolic neural networks are numerically unstable during training, which precludes using complex architectures. This crucial problem makes it difficult to build hyperbolic generative models for real and complex data. In this work, we propose a hyperbolic generative network in which we design novel architecture and layers to improve stability in training. Our proposed network contains three parts: first, a hyperbolic autoencoder (AE) that produces hyperbolic embedding for input data; second, a hyperbolic generative adversarial network (GAN) for generating the hyperbolic latent embedding of the AE from simple noise; third, a generator that inherits the decoder from the AE and the generator from the GAN. We call this network the hyperbolic AE-GAN, or HAEGAN for short. The architecture of HAEGAN fosters expressive representation in the hyperbolic space, and the specific design of layers ensures numerical stability. Experiments show that HAEGAN is able to generate complex data with state-of-the-art structure-related performance.",2022-01-30T14:14:15Z,2023-01-23T15:04:57Z,http://arxiv.org/abs/2201.12825v3,http://arxiv.org/pdf/2201.12825v3,"cs.LG, q-bio.BM, stat.ML"
Maze Learning using a Hyperdimensional Predictive Processing Cognitive   Architecture,"Alexander Ororbia, M. Alex Kelly","We present the COGnitive Neural GENerative system (CogNGen), a cognitive architecture that combines two neurobiologically-plausible, computational models: predictive processing and hyperdimensional/vector-symbolic models. We draw inspiration from architectures such as ACT-R and Spaun/Nengo. CogNGen is in broad agreement with these, providing a level of detail between ACT-R's high-level symbolic description of human cognition and Spaun's low-level neurobiological description, furthermore creating the groundwork for designing agents that learn continually from diverse tasks and model human performance at larger scales than what is possible with current systems. We test CogNGen on four maze-learning tasks, including those that test memory and planning, and find that CogNGen matches performance of deep reinforcement learning models and exceeds on a task designed to test memory.",2022-03-31T04:44:28Z,2022-08-09T01:12:23Z,http://arxiv.org/abs/2204.00619v2,http://arxiv.org/pdf/2204.00619v2,"cs.AI, cs.LG, cs.NE, q-bio.NC"
RL4ReAl: Reinforcement Learning for Register Allocation,"S. VenkataKeerthy, Siddharth Jain, Anilava Kundu, Rohit Aggarwal, Albert Cohen, Ramakrishna Upadrasta","We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.",2022-04-05T06:30:03Z,2023-02-06T05:22:34Z,http://arxiv.org/abs/2204.02013v3,http://arxiv.org/pdf/2204.02013v3,"cs.LG, cs.AR, cs.PL, D.2; I.2.5"
Scandium Nitride as a Gateway III-Nitride Semiconductor for   Optoelectronic Artificial Synaptic Devices,"Dheemahi Rao, Bivas Saha","Traditional computation based on von Neumann architecture is limited by the time and energy consumption due to data transfer between the storage and the processing units. The von Neumann architecture is also inefficient in solving unstructured, probabilistic, and real-time problems. To address these challenges, a new brain-inspired neuromorphic computational architecture is required. Due to absence of resistance-capacitance (RC) delay, high bandwidth and low power consumption, optoelectronic artificial synaptic devices are highly attractive. Yet stable, scalable, and complementary-metal-oxide-semiconductor (CMOS)-compatible synapses have not been demonstrated. In this work, persistence in the photoconductivity of undoped and magnesium-doped scandium nitride (ScN) is equated to the inhibitory and excitatory synaptic plasticity of the biological synapses responsible for memory and learning. Primary functionalities of a biological synapse like short-term memory (STM), long-term memory (LTM), the transition from STM-to-LTM, learning and forgetting, frequency-selective optical filtering, frequency-dependent potentiation and depression, Hebbian learning, and logic gate operations are demonstrated.",2022-04-06T13:11:01Z,2022-04-06T13:11:01Z,http://arxiv.org/abs/2204.02799v1,http://arxiv.org/pdf/2204.02799v1,"cs.ET, physics.app-ph"
Weight Matrix Dimensionality Reduction in Deep Learning via Kronecker   Multi-layer Architectures,"Jarom D. Hogue, Robert M. Kirby, Akil Narayan","Deep learning using neural networks is an effective technique for generating models of complex data. However, training such models can be expensive when networks have large model capacity resulting from a large number of layers and nodes. For training in such a computationally prohibitive regime, dimensionality reduction techniques ease the computational burden, and allow implementations of more robust networks. We propose a novel type of such dimensionality reduction via a new deep learning architecture based on fast matrix multiplication of a Kronecker product decomposition; in particular our network construction can be viewed as a Kronecker product-induced sparsification of an ""extended"" fully connected network. Analysis and practical examples show that this architecture allows a neural network to be trained and implemented with a significant reduction in computational time and resources, while achieving a similar error level compared to a traditional feedforward neural network.",2022-04-08T19:54:52Z,2023-01-17T20:47:19Z,http://arxiv.org/abs/2204.04273v2,http://arxiv.org/pdf/2204.04273v2,"cs.LG, cs.NA, math.NA, 15A23, 15A69, 65F30, 68T05, G.1.3; I.2.6"
A Novel Channel Identification Architecture for mmWave Systems Based on   Eigen Features,"Yibin Zhang, Jinlong Sun, Guan Gui, Haris Gacanin, Fumiyuki Adachi","Millimeter wave (mmWave) communication technique has been developed rapidly because of many advantages of high speed, large bandwidth, and ultra-low delay. However, mmWave communications systems suffer from fast fading and frequent blocking. Hence, the ideal communication environment for mmWave is line of sight (LOS) channel. To improve the efficiency and capacity of mmWave system, and to better build the Internet of Everything (IoE) service network, this paper focuses on the channel identification technique in line-of- sight (LOS) and non-LOS (NLOS) environments. Considering the limited computing ability of user equipments (UEs), this paper proposes a novel channel identification architecture based on eigen features, i.e. eigenmatrix and eigenvector (EMEV) of channel state information (CSI). Furthermore, this paper explores clustered delay line (CDL) channel identification with mmWave, which is defined by the 3rd generation partnership project (3GPP). Ther experimental results show that the EMEV based scheme can achieve identification accuracy of 99.88% assuming perfect CSI. In the robustness test, the maximum noise can be tolerated is SNR= 16 dB, with the threshold acc \geq 95%. What is more, the novel architecture based on EMEV feature will reduce the comprehensive overhead by about 90%.",2022-04-11T12:42:22Z,2022-04-11T12:42:22Z,http://arxiv.org/abs/2204.05052v1,http://arxiv.org/pdf/2204.05052v1,"eess.SP, cs.AI"
End-to-End Speech Translation for Code Switched Speech,"Orion Weller, Matthias Sperber, Telmo Pires, Hendra Setiawan, Christian Gollan, Dominic Telaar, Matthias Paulik","Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -> target) vs bidirectional (source <-> target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.",2022-04-11T13:25:30Z,2022-04-11T13:25:30Z,http://arxiv.org/abs/2204.05076v1,http://arxiv.org/pdf/2204.05076v1,"cs.CL, cs.SD, eess.AS"
Continual Hippocampus Segmentation with Transformers,"Amin Ranem, Camila González, Anirban Mukhopadhyay","In clinical settings, where acquisition conditions and patient populations change over time, continual learning is key for ensuring the safe use of deep neural networks. Yet most existing work focuses on convolutional architectures and image classification. Instead, radiologists prefer to work with segmentation models that outline specific regions-of-interest, for which Transformer-based architectures are gaining traction. The self-attention mechanism of Transformers could potentially mitigate catastrophic forgetting, opening the way for more robust medical image segmentation. In this work, we explore how recently-proposed Transformer mechanisms for semantic segmentation behave in sequential learning scenarios, and analyse how best to adapt continual learning strategies for this setting. Our evaluation on hippocampus segmentation shows that Transformer mechanisms mitigate catastrophic forgetting for medical image segmentation compared to purely convolutional architectures, and demonstrates that regularising ViT modules should be done with caution.",2022-04-17T16:13:04Z,2022-04-17T16:13:04Z,http://arxiv.org/abs/2204.08043v1,http://arxiv.org/pdf/2204.08043v1,"eess.IV, cs.CV"
Automated Audio Captioning using Audio Event Clues,"Ayşegül Özkaya Eren, Mustafa Sert","Audio captioning is an important research area that aims to generate meaningful descriptions for audio clips. Most of the existing research extracts acoustic features of audio clips as input to encoder-decoder and transformer architectures to produce the captions in a sequence-to-sequence manner. Due to data insufficiency and the architecture's inadequate learning capacity, additional information is needed to generate natural language sentences, as well as acoustic features. To address these problems, an encoder-decoder architecture is proposed that learns from both acoustic features and extracted audio event labels as inputs. The proposed model is based on pre-trained acoustic features and audio event detection. Various experiments used different acoustic features, word embedding models, audio event label extraction methods, and implementation configurations to show which combinations have better performance on the audio captioning task. Results of the extensive experiments on multiple datasets show that using audio event labels with the acoustic features improves the recognition performance and the proposed method either outperforms or achieves competitive results with the state-of-the-art models.",2022-04-18T21:30:42Z,2022-04-18T21:30:42Z,http://arxiv.org/abs/2204.08567v1,http://arxiv.org/pdf/2204.08567v1,"cs.SD, eess.AS"
Cross-stitched Multi-modal Encoders,"Karan Singla, Daniel Pressel, Ryan Price, Bhargav Srinivas Chinnari, Yeon-Jun Kim, Srinivas Bangalore","In this paper, we propose a novel architecture for multi-modal speech and text input. We combine pretrained speech and text encoders using multi-headed cross-modal attention and jointly fine-tune on the target problem. The resultant architecture can be used for continuous token-level classification or utterance-level prediction acting on simultaneous text and speech. The resultant encoder efficiently captures both acoustic-prosodic and lexical information. We compare the benefits of multi-headed attention-based fusion for multi-modal utterance-level classification against a simple concatenation of pre-pooled, modality-specific representations. Our model architecture is compact, resource efficient, and can be trained on a single consumer GPU card.",2022-04-20T05:09:36Z,2022-04-20T05:09:36Z,http://arxiv.org/abs/2204.09227v1,http://arxiv.org/pdf/2204.09227v1,"cs.CL, cs.SD, eess.AS"
Reducing Neural Architecture Search Spaces with Training-Free Statistics   and Computational Graph Clustering,"Thorir Mar Ingolfsson, Mark Vero, Xiaying Wang, Lorenzo Lamberti, Luca Benini, Matteo Spallanzani","The computational demands of neural architecture search (NAS) algorithms are usually directly proportional to the size of their target search spaces. Thus, limiting the search to high-quality subsets can greatly reduce the computational load of NAS algorithms. In this paper, we present Clustering-Based REDuction (C-BRED), a new technique to reduce the size of NAS search spaces. C-BRED reduces a NAS space by clustering the computational graphs associated with its architectures and selecting the most promising cluster using proxy statistics correlated with network accuracy. When considering the NAS-Bench-201 (NB201) data set and the CIFAR-100 task, C-BRED selects a subset with 70% average accuracy instead of the whole space's 64% average accuracy.",2022-04-29T13:52:35Z,2022-04-29T13:52:35Z,http://arxiv.org/abs/2204.14103v1,http://arxiv.org/pdf/2204.14103v1,"cs.LG, I.m"
Towards Refactoring of DMARF and GIPSY Case Studies -- A Team 5   SOEN6471-S14 Project Report,"Pavan Kumar Polu, Amjad Al Najjar, Biswajit Banik, Ajay Sujit Kumar, Gustavo Pereira, Prince Japhlet, Bhanu Prakash R., Sabari Krishna Raparla","This paper presents an analysis of the architectural design of two distributed open source systems (OSS) developed in Java: Distributed Modular Audio Recognition Framework (DMARF) and General Intensional Programming System (GIPSY). The research starts with a background study of these frameworks to determine their overall architectures. Afterwards, we identify the actors and stakeholders and draft a domain model for each framework. Next, we evaluated and proposed a fused DMARF over GIPSY Run-time Architecture (DoGRTA) as a domain concept. Later on, the team extracted and studied the actual class diagrams and determined classes of interest. Next, we identified design patterns that were present within the code of each framework. Finally, code smells in the source code were detected using popular tools and a selected number of those identified smells were refactored using established techniques and implemented in the final source code. Tests were written and ran prior and after the refactoring to check for any behavioral changes.",2014-12-23T17:00:41Z,2014-12-23T17:00:41Z,http://arxiv.org/abs/1412.7533v1,http://arxiv.org/pdf/1412.7533v1,"cs.SE, D.2; K.6; H.5.2"
Coding Schemes for Achieving Strong Secrecy at Negligible Cost,"Remi A. Chou, Badri Vellambi, Matthieu Bloch, Joerg Kliewer","We study the problem of achieving strong secrecy over wiretap channels at negligible cost, in the sense of maintaining the overall communication rate of the same channel without secrecy constraints. Specifically, we propose and analyze two source-channel coding architectures, in which secrecy is achieved by multiplexing public and confidential messages. In both cases, our main contribution is to show that secrecy can be achieved without compromising communication rate and by requiring only randomness of asymptotically vanishing rate. Our first source-channel coding architecture relies on a modified wiretap channel code, in which randomization is performed using the output of a source code. In contrast, our second architecture relies on a standard wiretap code combined with a modified source code termed uniform compression code, in which a small shared secret seed is used to enhance the uniformity of the source code output. We carry out a detailed analysis of uniform compression codes and characterize the optimal size of the shared seed.",2015-08-31T17:11:38Z,2016-12-05T18:27:05Z,http://arxiv.org/abs/1508.07920v2,http://arxiv.org/pdf/1508.07920v2,"cs.IT, math.IT"
Linear Memory Networks,"Davide Bacciu, Antonio Carta, Alessandro Sperduti","Recurrent neural networks can learn complex transduction problems that require maintaining and actively exploiting a memory of their inputs. Such models traditionally consider memory and input-output functionalities indissolubly entangled. We introduce a novel recurrent architecture based on the conceptual separation between the functional input-output transformation and the memory mechanism, showing how they can be implemented through different neural components. By building on such conceptualization, we introduce the Linear Memory Network, a recurrent model comprising a feedforward neural network, realizing the non-linear functional transformation, and a linear autoencoder for sequences, implementing the memory component. The resulting architecture can be efficiently trained by building on closed-form solutions to linear optimization problems. Further, by exploiting equivalence results between feedforward and recurrent neural networks we devise a pretraining schema for the proposed architecture. Experiments on polyphonic music datasets show competitive results against gated recurrent networks and other state of the art models.",2018-11-08T11:08:04Z,2018-11-08T11:08:04Z,http://arxiv.org/abs/1811.03356v1,http://arxiv.org/pdf/1811.03356v1,"cs.LG, stat.ML"
ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks,"Iurii Kemaev, Daniil Polykovskiy, Dmitry Vetrov","Neural Network is a powerful Machine Learning tool that shows outstanding performance in Computer Vision, Natural Language Processing, and Artificial Intelligence. In particular, recently proposed ResNet architecture and its modifications produce state-of-the-art results in image classification problems. ResNet and most of the previously proposed architectures have a fixed structure and apply the same transformation to all input images. In this work, we develop a ResNet-based model that dynamically selects Computational Units (CU) for each input object from a learned set of transformations. Dynamic selection allows the network to learn a sequence of useful transformations and apply only required units to predict the image label. We compare our model to ResNet-38 architecture and achieve better results than the original ResNet on CIFAR-10.1 test set. While examining the produced paths, we discovered that the network learned different routes for images from different classes and similar routes for similar images.",2018-11-11T09:45:41Z,2018-11-11T09:45:41Z,http://arxiv.org/abs/1811.04380v1,http://arxiv.org/pdf/1811.04380v1,"stat.ML, cs.LG"
Pitfalls of Graph Neural Network Evaluation,"Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Günnemann","Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.",2018-11-14T15:53:19Z,2019-06-18T13:15:39Z,http://arxiv.org/abs/1811.05868v2,http://arxiv.org/pdf/1811.05868v2,"cs.LG, cs.SI, stat.ML"
Stackelberg GAN: Towards Provable Minimax Equilibrium via   Multi-Generator Architectures,"Hongyang Zhang, Susu Xu, Jiantao Jiao, Pengtao Xie, Ruslan Salakhutdinov, Eric P. Xing","We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design. The discrepancy between the minimax and maximin objective values could serve as a proxy for the difficulties that the alternating gradient descent encounters in the optimization of GANs. In this work, we give new results on the benefits of multi-generator architecture of GANs. We show that the minimax gap shrinks to $\epsilon$ as the number of generators increases with rate $\widetilde{O}(1/\epsilon)$. This improves over the best-known result of $\widetilde{O}(1/\epsilon^2)$. At the core of our techniques is a novel application of Shapley-Folkman lemma to the generic minimax problem, where in the literature the technique was only known to work when the objective function is restricted to the Lagrangian function of a constraint optimization problem. Our proposed Stackelberg GAN performs well experimentally in both synthetic and real-world datasets, improving Fr\'echet Inception Distance by $14.61\%$ over the previous multi-generator GANs on the benchmark datasets.",2018-11-19T22:38:36Z,2018-11-19T22:38:36Z,http://arxiv.org/abs/1811.08010v1,http://arxiv.org/pdf/1811.08010v1,"cs.LG, stat.ML"
ChainGAN: A sequential approach to GANs,"Safwan Hossain, Kiarash Jamali, Yuchen Li, Frank Rudzicz","We propose a new architecture and training methodology for generative adversarial networks. Current approaches attempt to learn the transformation from a noise sample to a generated data sample in one shot. Our proposed generator architecture, called $\textit{ChainGAN}$, uses a two-step process. It first attempts to transform a noise vector into a crude sample, similar to a traditional generator. Next, a chain of networks, called $\textit{editors}$, attempt to sequentially enhance this sample. We train each of these units independently, instead of with end-to-end backpropagation on the entire chain. Our model is robust, efficient, and flexible as we can apply it to various network architectures. We provide rationale for our choices and experimentally evaluate our model, achieving competitive results on several datasets.",2018-11-20T05:30:32Z,2018-11-22T18:56:15Z,http://arxiv.org/abs/1811.08081v2,http://arxiv.org/pdf/1811.08081v2,"cs.LG, cs.CV, stat.ML"
Structure-Based Networks for Drug Validation,"Cătălina Cangea, Arturas Grauslys, Pietro Liò, Francesco Falciani","Classifying chemicals according to putative modes of action (MOAs) is of paramount importance in the context of risk assessment. However, current methods are only able to handle a very small proportion of the existing chemicals. We address this issue by proposing an integrative deep learning architecture that learns a joint representation from molecular structures of drugs and their effects on human cells. Our choice of architecture is motivated by the significant influence of a drug's chemical structure on its MOA. We improve on the strong ability of a unimodal architecture (F1 score of 0.803) to classify drugs by their toxic MOAs (Verhaar scheme) through adding another learning stream that processes transcriptional responses of human cells affected by drugs. Our integrative model achieves an even higher classification performance on the LINCS L1000 dataset - the error is reduced by 4.6%. We believe that our method can be used to extend the current Verhaar scheme and constitute a basis for fast drug validation and risk assessment.",2018-11-21T12:39:19Z,2018-11-21T12:39:19Z,http://arxiv.org/abs/1811.09714v1,http://arxiv.org/pdf/1811.09714v1,"q-bio.QM, cs.AI, cs.LG, stat.ML"
Automatic Induction of Neural Network Decision Tree Algorithms,Chapman Siu,"This work presents an approach to automatically induction for non-greedy decision trees constructed from neural network architecture. This construction can be used to transfer weights when growing or pruning a decision tree, allowing non-greedy decision tree algorithms to automatically learn and adapt to the ideal architecture. In this work, we examine the underpinning ideas within ensemble modelling and Bayesian model averaging which allow our neural network to asymptotically approach the ideal architecture through weights transfer. Experimental results demonstrate that this approach improves models over fixed set of hyperparameters for decision tree models and decision forest models.",2018-11-26T23:06:38Z,2019-04-25T03:21:04Z,http://arxiv.org/abs/1811.10735v4,http://arxiv.org/pdf/1811.10735v4,"stat.ML, cs.LG"
SetGAN: Improving the stability and diversity of generative models   through a permutation invariant architecture,"Alessandro Ferrero, Shireen Elhabian, Ross Whitaker","Generative adversarial networks (GANs) have proven effective in modeling distributions of high-dimensional data. However, their training instability is a well-known hindrance to convergence, which results in practical challenges in their applications to novel data. Furthermore, even when convergence is reached, GANs can be affected by mode collapse, a phenomenon for which the generator learns to model only a small part of the target distribution, disregarding the vast majority of the data manifold or distribution. This paper addresses these challenges by introducing SetGAN, an adversarial architecture that processes sets of generated and real samples, and discriminates between the origins of these sets (i.e., training versus generated data) in a flexible, permutation invariant manner. We also propose a new metric to quantitatively evaluate GANs that does not require previous knowledge of the application, apart from the data itself. Using the new metric, in conjunction with the state-of-the-art evaluation methods, we show that the proposed architecture, when compared with GAN variants stemming from similar strategies, produces more accurate models of the input data in a way that is also less sensitive to hyperparameter settings.",2019-06-28T22:43:02Z,2022-09-27T13:22:29Z,http://arxiv.org/abs/1907.00109v3,http://arxiv.org/pdf/1907.00109v3,"cs.LG, stat.ML"
Joint Functional Splitting and Content Placement for Green Hybrid CRAN,"Ajay Sriram, Meysam Masoudi, Abdulrahman Alabbasi, Cicek Cavdar","A hybrid cloud radio access network (H-CRAN) architecture has been proposed to alleviate the midhaul capacity limitation in C-RAN. In this architecture, functional splitting is utilized to distribute the processing functions between a central cloud and edge clouds. The flexibility of selecting specific split point enables the H-CRAN designer to reduce midhaul bandwidth, or reduce latency, or save energy, or distribute the computation task depending on equipment availability. Meanwhile, techniques for caching are proposed to reduce content delivery latency and the required bandwidth. However, caching imposes new constraints on functional splitting. In this study, considering H-CRAN, a constraint programming problem is formulated to minimize the overall power consumption by selecting the optimal functional split point and content placement, taking into account the content access delay constraint. We also investigate the trade-off between the overall power consumption and occupied midhaul bandwidth in the network. Our results demonstrate that functional splitting together with enabling caching at edge clouds reduces not only content access delays but also fronthaul bandwidth consumption but at the expense of higher power consumption.",2019-06-29T17:19:59Z,2019-06-29T17:19:59Z,http://arxiv.org/abs/1907.00242v1,http://arxiv.org/pdf/1907.00242v1,"cs.NI, eess.SP"
EQuANt (Enhanced Question Answer Network),"François-Xavier Aubet, Dominic Danks, Yuchen Zhu","Machine Reading Comprehension (MRC) is an important topic in the domain of automated question answering and in natural language processing more generally. Since the release of the SQuAD 1.1 and SQuAD 2 datasets, progress in the field has been particularly significant, with current state-of-the-art models now exhibiting near-human performance at both answering well-posed questions and detecting questions which are unanswerable given a corresponding context. In this work, we present Enhanced Question Answer Network (EQuANt), an MRC model which extends the successful QANet architecture of Yu et al. to cope with unanswerable questions. By training and evaluating EQuANt on SQuAD 2, we show that it is indeed possible to extend QANet to the unanswerable domain. We achieve results which are close to 2 times better than our chosen baseline obtained by evaluating a lightweight version of the original QANet architecture on SQuAD 2. In addition, we report that the performance of EQuANt on SQuAD 1.1 after being trained on SQuAD2 exceeds that of our lightweight QANet architecture trained and evaluated on SQuAD 1.1, demonstrating the utility of multi-task learning in the MRC context.",2019-06-24T08:13:45Z,2019-07-03T21:03:37Z,http://arxiv.org/abs/1907.00708v2,http://arxiv.org/pdf/1907.00708v2,"cs.CL, cs.LG, stat.ML"
SkeletonNet: Shape Pixel to Skeleton Pixel,"Sabari Nathan, Priya Kansal","Deep Learning for Geometric Shape Understating has organized a challenge for extracting different kinds of skeletons from the images of different objects. This competition is organized in association with CVPR 2019. There are three different tracks of this competition. The present manuscript describes the method used to train the model for the dataset provided in the first track. The first track aims to extract skeleton pixels from the shape pixels of 89 different objects. For the purpose of extracting the skeleton, a U-net model which is comprised of an encoder-decoder structure has been used. In our proposed architecture, unlike the plain decoder in the traditional Unet, we have designed the decoder in the format of HED architecture, wherein we have introduced 4 side layers and fused them to one dilation convolutional layer to connect the broken links of the skeleton. Our proposed architecture achieved the F1 score of 0.77 on test data.",2019-07-02T23:44:05Z,2019-07-02T23:44:05Z,http://arxiv.org/abs/1907.01683v1,http://arxiv.org/pdf/1907.01683v1,"cs.CV, cs.CG, cs.LG, eess.IV"
Coherency and Online Signal Selection Based Wide Area Control of Wind   Integrated Power Grid,"Abilash Thakallapelli, S J Hossain, Sukumar Kamalasadan","This paper introduces a novel method of designing wide area control (WAC) based on a discrete linear quadratic regulator and Kalman filtering based state-estimation that can be applied for real-time damping of interarea oscillations of wind integrated power grid. The main advantages of the proposed method are that the architecture provides online coherency grouping that properly characterizes real-time changes in the power grid and online wide-area signal selection based on residue method for proper selection of the WAC signals. The proposed architecture can, thus, accurately monitors changes in the power grid and select the appropriate control signal for more effectively damping the interarea oscillation when compared to the conventional local signal based power system stabilizers or offline based WAC designs. The architecture is tested on a wind integrated two-area system and the IEEE 39 bus system in order to show the capability of the proposed method.",2019-07-16T05:28:42Z,2019-07-16T05:28:42Z,http://arxiv.org/abs/1907.06846v1,http://arxiv.org/pdf/1907.06846v1,"eess.SY, cs.SY"
Photonic architecture for reinforcement learning,"Fulvio Flamini, Arne Hamann, Sofiène Jerbi, Lea M. Trenkwalder, Hendrik Poulsen Nautrup, Hans J. Briegel","The last decade has seen an unprecedented growth in artificial intelligence and photonic technologies, both of which drive the limits of modern-day computing devices. In line with these recent developments, this work brings together the state of the art of both fields within the framework of reinforcement learning. We present the blueprint for a photonic implementation of an active learning machine incorporating contemporary algorithms such as SARSA, Q-learning, and projective simulation. We numerically investigate its performance within typical reinforcement learning environments, showing that realistic levels of experimental noise can be tolerated or even be beneficial for the learning process. Remarkably, the architecture itself enables mechanisms of abstraction and generalization, two features which are often considered key ingredients for artificial intelligence. The proposed architecture, based on single-photon evolution on a mesh of tunable beamsplitters, is simple, scalable, and a first integration in portable systems appears to be within the reach of near-term technology.",2019-07-17T13:23:58Z,2019-07-17T13:23:58Z,http://arxiv.org/abs/1907.07503v1,http://arxiv.org/pdf/1907.07503v1,"quant-ph, cs.LG"
Compositional Deep Learning,Bruno Gavranović,"Neural networks have become an increasingly popular tool for solving many real-world problems. They are a general framework for differentiable optimization which includes many other machine learning approaches as special cases. In this thesis we build a category-theoretic formalism around a class of neural networks exemplified by CycleGAN. CycleGAN is a collection of neural networks, closed under composition, whose inductive bias is increased by enforcing composition invariants, i.e. cycle-consistencies. Inspired by Functorial Data Migration, we specify the interconnection of these networks using a categorical schema, and network instances as set-valued functors on this schema. We also frame neural network architectures, datasets, models, and a number of other concepts in a categorical setting and thus show a special class of functors, rather than functions, can be learned using gradient descent. We use the category-theoretic framework to conceive a novel neural network architecture whose goal is to learn the task of object insertion and object deletion in images with unpaired data. We test the architecture on three different datasets and obtain promising results.",2019-07-16T10:21:15Z,2019-07-16T10:21:15Z,http://arxiv.org/abs/1907.08292v1,http://arxiv.org/pdf/1907.08292v1,"cs.LG, cs.AI, math.CT"
Deep Learning for Time Series Forecasting: The Electric Load Case,"Alberto Gasparin, Slobodan Lukovic, Cesare Alippi","Management and efficient operations in critical infrastructure such as Smart Grids take huge advantage of accurate power load forecasting which, due to its nonlinear nature, remains a challenging task. Recently, deep learning has emerged in the machine learning field achieving impressive performance in a vast range of tasks, from image classification to machine translation. Applications of deep learning models to the electric load forecasting problem are gaining interest among researchers as well as the industry, but a comprehensive and sound comparison among different architectures is not yet available in the literature. This work aims at filling the gap by reviewing and experimentally evaluating on two real-world datasets the most recent trends in electric load forecasting, by contrasting deep learning architectures on short term forecast (one day ahead prediction). Specifically, we focus on feedforward and recurrent neural networks, sequence to sequence models and temporal convolutional neural networks along with architectural variants, which are known in the signal processing community but are novel to the load forecasting one.",2019-07-22T10:03:17Z,2019-07-22T10:03:17Z,http://arxiv.org/abs/1907.09207v1,http://arxiv.org/pdf/1907.09207v1,"cs.LG, stat.ML"
Deployment Architectures for Cyber-Physical Control Systems,"Shih-Hao Tseng, James Anderson","We consider the problem of how to deploy a controller to a (networked) cyber-physical system (CPS). Controlling a CPS is an involved task, and synthesizing a controller to respect sensing, actuation, and communication constraints is only part of the challenge. In addition to controller synthesis, one should also consider how the controller will work in the CPS. Put another way, the cyber layer and its interaction with the physical layer need to be taken into account. In this work, we aim to bridge the gap between theoretical controller synthesis and practical CPS deployment. We adopt the system level synthesis (SLS) framework to synthesize a state-feedback controller and provide a deployment architecture for the standard SLS controller. Furthermore, we derive a new controller realization for open-loop stable systems and introduce four different architectures for deployment, ranging from fully centralized to fully distributed. Finally, we compare the trade-offs among them in terms of robustness, memory, computation, and communication overhead.",2019-11-04T22:15:18Z,2020-09-29T02:46:42Z,http://arxiv.org/abs/1911.01510v3,http://arxiv.org/pdf/1911.01510v3,"eess.SY, cs.SY, math.OC"
XACC: A System-Level Software Infrastructure for Heterogeneous   Quantum-Classical Computing,"Alexander J. McCaskey, Dmitry I. Lyakh, Eugene F. Dumitrescu, Sarah S. Powers, Travis S. Humble","Quantum programming techniques and software have advanced significantly over the past five years, with a majority focusing on high-level language frameworks targeting remote REST library APIs. As quantum computing architectures advance and become more widely available, lower-level, system software infrastructures will be needed to enable tighter, co-processor programming and access models. Here we present XACC, a system-level software infrastructure for quantum-classical computing that promotes a service-oriented architecture to expose interfaces for core quantum programming, compilation, and execution tasks. We detail XACC's interfaces, their interactions, and its implementation as a hardware-agnostic framework for both near-term and future quantum-classical architectures. We provide concrete examples demonstrating the utility of this framework with paradigmatic tasks. Our approach lays the foundation for the development of compilers, associated runtimes, and low-level system tools tightly integrating quantum and classical workflows.",2019-11-06T16:02:35Z,2019-11-06T16:02:35Z,http://arxiv.org/abs/1911.02452v1,http://arxiv.org/pdf/1911.02452v1,"quant-ph, cs.PL"
Physics-Guided Architecture (PGA) of Neural Networks for Quantifying   Uncertainty in Lake Temperature Modeling,"Arka Daw, R. Quinn Thomas, Cayelan C. Carey, Jordan S. Read, Alison P. Appling, Anuj Karpatne","To simultaneously address the rising need of expressing uncertainties in deep learning models along with producing model outputs which are consistent with the known scientific knowledge, we propose a novel physics-guided architecture (PGA) of neural networks in the context of lake temperature modeling where the physical constraints are hard coded in the neural network architecture. This allows us to integrate such models with state of the art uncertainty estimation approaches such as Monte Carlo (MC) Dropout without sacrificing the physical consistency of our results. We demonstrate the effectiveness of our approach in ensuring better generalizability as well as physical consistency in MC estimates over data collected from Lake Mendota in Wisconsin and Falling Creek Reservoir in Virginia, even with limited training data. We further show that our MC estimates correctly match the distribution of ground-truth observations, thus making the PGA paradigm amenable to physically grounded uncertainty quantification.",2019-11-06T23:47:14Z,2019-11-06T23:47:14Z,http://arxiv.org/abs/1911.02682v1,http://arxiv.org/pdf/1911.02682v1,"cs.LG, physics.comp-ph, stat.ML"
Understanding Graph Neural Networks with Generalized Geometric   Scattering Transforms,"Michael Perlmutter, Alexander Tong, Feng Gao, Guy Wolf, Matthew Hirn","The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed geometric scattering transforms for graphs based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. As a result, the proposed construction unifies and extends known theoretical results for many of the existing graph scattering architectures. In doing so, this work helps bridge the gap between geometric scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. These results lay the groundwork for future deep learning architectures for graph-structured data that have learned filters and also provably have desirable theoretical properties.",2019-11-14T17:23:06Z,2023-06-29T01:28:23Z,http://arxiv.org/abs/1911.06253v5,http://arxiv.org/pdf/1911.06253v5,"stat.ML, cs.LG"
RSM-GAN: A Convolutional Recurrent GAN for Anomaly Detection in   Contaminated Seasonal Multivariate Time Series,"Farzaneh Khoshnevisan, Zhewen Fan","Robust anomaly detection is a requirement for monitoring complex modern systems with applications such as cyber-security, fraud prevention, and maintenance. These systems generate multiple correlated time series that are highly seasonal and noisy. This paper presents a novel unsupervised deep learning architecture for multivariate time series anomaly detection, called Robust Seasonal Multivariate Generative Adversarial Network (RSM-GAN). It extends recent advancements in GANs with adoption of convolutional-LSTM layers and an attention mechanism to produce state-of-the-art performance. We conduct extensive experiments to demonstrate the strength of our architecture in adjusting for complex seasonality patterns and handling severe levels of training data contamination. We also propose a novel anomaly score assignment and causal inference framework. We compare RSM-GAN with existing classical and deep-learning based anomaly detection models, and the results show that our architecture is associated with the lowest false positive rate and improves precision by 30% and 16% in real-world and synthetic data, respectively. Furthermore, we report the superiority of RSM-GAN regarding accurate root cause identification and NAB scores in all data settings.",2019-11-16T21:45:38Z,2019-11-16T21:45:38Z,http://arxiv.org/abs/1911.07104v1,http://arxiv.org/pdf/1911.07104v1,"cs.LG, stat.ML"
Interstellar: Searching Recurrent Architecture for Knowledge Graph   Embedding,"Yongqi Zhang, Quanming Yao, Lei Chen","Knowledge graph (KG) embedding is well-known in learning representations of KGs. Many models have been proposed to learn the interactions between entities and relations of the triplets. However, long-term information among multiple triplets is also important to KG. In this work, based on the relational paths, which are composed of a sequence of triplets, we define the Interstellar as a recurrent neural architecture search problem for the short-term and long-term information along the paths. First, we analyze the difficulty of using a unified model to work as the Interstellar. Then, we propose to search for recurrent architecture as the Interstellar for different KG tasks. A case study on synthetic data illustrates the importance of the defined search problem. Experiments on real datasets demonstrate the effectiveness of the searched models and the efficiency of the proposed hybrid-search algorithm.",2019-11-17T02:16:24Z,2021-04-28T07:16:19Z,http://arxiv.org/abs/1911.07132v3,http://arxiv.org/pdf/1911.07132v3,"cs.LG, stat.ML"
Segmentation of Defective Skulls from CT Data for Tissue Modelling,"Oldřich Kodym, Michal Španěl, Adam Herout","In this work we present a method of automatic segmentation of defective skulls for custom cranial implant design and 3D printing purposes. Since such tissue models are usually required in patient cases with complex anatomical defects and variety of external objects present in the acquired data, most deep learning-based approaches fall short because it is not possible to create a sufficient training dataset that would encompass the spectrum of all possible structures. Because CNN segmentation experiments in this application domain have been so far limited to simple patch-based CNN architectures, we first show how the usage of the encoder-decoder architecture can substantially improve the segmentation accuracy. Then, we show how the number of segmentation artifacts, which usually require manual corrections, can be further reduced by adding a boundary term to CNN training and by globally optimizing the segmentation with graph-cut. Finally, we show that using the proposed method, 3D segmentation accurate enough for clinical application can be achieved with 2D CNN architectures as well as their 3D counterparts.",2019-11-20T10:31:38Z,2020-11-04T10:02:53Z,http://arxiv.org/abs/1911.08805v2,http://arxiv.org/pdf/1911.08805v2,"eess.IV, cs.CV"
Fast and Deep Graph Neural Networks,"Claudio Gallicchio, Alessio Micheli","We address the efficiency issue for the construction of a deep graph neural network (GNN). The approach exploits the idea of representing each input graph as a fixed point of a dynamical system (implemented through a recurrent neural network), and leverages a deep architectural organization of the recurrent units. Efficiency is gained by many aspects, including the use of small and very sparse networks, where the weights of the recurrent units are left untrained under the stability condition introduced in this work. This can be viewed as a way to study the intrinsic power of the architecture of a deep GNN, and also to provide insights for the set-up of more complex fully-trained models. Through experimental results, we show that even without training of the recurrent connections, the architecture of small deep GNN is surprisingly able to achieve or improve the state-of-the-art performance on a significant set of tasks in the field of graphs classification.",2019-11-20T14:46:54Z,2019-11-20T14:46:54Z,http://arxiv.org/abs/1911.08941v1,http://arxiv.org/pdf/1911.08941v1,"cs.LG, math.DS, stat.ML"
Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking,"Eric Crawford, Joelle Pineau","The ability to detect and track objects in the visual world is a crucial skill for any intelligent agent, as it is a necessary precursor to any object-level reasoning process. Moreover, it is important that agents learn to track objects without supervision (i.e. without access to annotated training videos) since this will allow agents to begin operating in new environments with minimal human assistance. The task of learning to discover and track objects in videos, which we call \textit{unsupervised object tracking}, has grown in prominence in recent years; however, most architectures that address it still struggle to deal with large scenes containing many objects. In the current work, we propose an architecture that scales well to the large-scene, many-object setting by employing spatially invariant computations (convolutions and spatial attention) and representations (a spatially local object specification scheme). In a series of experiments, we demonstrate a number of attractive features of our architecture; most notably, that it outperforms competing methods at tracking objects in cluttered scenes with many objects, and that it can generalize well to videos that are larger and/or contain more objects than videos encountered during training.",2019-11-20T17:03:51Z,2019-11-20T17:03:51Z,http://arxiv.org/abs/1911.09033v1,http://arxiv.org/pdf/1911.09033v1,"cs.LG, cs.CV, stat.ML"
Data Proxy Generation for Fast and Efficient Neural Architecture Search,Minje Park,"Due to the recent advances on Neural Architecture Search (NAS), it gains popularity in designing best networks for specific tasks. Although it shows promising results on many benchmarks and competitions, NAS still suffers from its demanding computation cost for searching high dimensional architectural design space, and this problem becomes even worse when we want to use a large-scale dataset. If we can make a reliable data proxy for NAS, the efficiency of NAS approaches increase accordingly. Our basic observation for making a data proxy is that each example in a specific dataset has a different impact on NAS process and most of examples are redundant from a relative accuracy ranking perspective, which we should preserve when making a data proxy. We propose a systematic approach to measure the importance of each example from this relative accuracy ranking point of view, and make a reliable data proxy based on the statistics of training and testing examples. Our experiment shows that we can preserve the almost same relative accuracy ranking between all possible network configurations even with 10-20$\times$ smaller data proxy.",2019-11-21T07:39:57Z,2019-11-21T07:39:57Z,http://arxiv.org/abs/1911.09322v1,http://arxiv.org/pdf/1911.09322v1,"cs.LG, cs.CV, stat.ML"
Culture-Based Explainable Human-Agent Deconfliction,"Alex Raymond, Hatice Gunes, Amanda Prorok","Law codes and regulations help organise societies for centuries, and as AI systems gain more autonomy, we question how human-agent systems can operate as peers under the same norms, especially when resources are contended. We posit that agents must be accountable and explainable by referring to which rules justify their decisions. The need for explanations is associated with user acceptance and trust. This paper's contribution is twofold: i) we propose an argumentation-based human-agent architecture to map human regulations into a culture for artificial agents with explainable behaviour. Our architecture leans on the notion of argumentative dialogues and generates explanations from the history of such dialogues; and ii) we validate our architecture with a user study in the context of human-agent path deconfliction. Our results show that explanations provide a significantly higher improvement in human performance when systems are more complex. Consequently, we argue that the criteria defining the need of explanations should also consider the complexity of a system. Qualitative findings show that when rules are more complex, explanations significantly reduce the perception of challenge for humans.",2019-11-22T15:51:18Z,2019-11-22T15:51:18Z,http://arxiv.org/abs/1911.10098v1,http://arxiv.org/pdf/1911.10098v1,"cs.MA, cs.HC, cs.LO, cs.RO, I.2.11; I.2.3; H.1.2; I.2.9"
An Iterative Interference Cancellation Algorithm for Large Intelligent   Surfaces,"Jesus Rodriguez Sanchez, Fredrik Rusek, Ove Edfors, Liang Liu","The Large Intelligent Surface (LIS) concept is a promising technology aiming to revolutionize wireless communication by exploiting spatial multiplexing at its fullest. Despite of its potential, due to the size of the LIS and the large number of antenna elements involved there is a need of decentralized architectures together with distributed algorithms which can reduce the inter-connection data-rate and computational requirement in the Central Processing Unit (CPU). In this article we address the uplink detection problem in the LIS system and propose a decentralize architecture based on panels, which perform local linear processing. We also provide the sum-rate capacity for such architecture and derive an algorithm to obtain the equalizer, which aims to maximize the sum-rate capacity. A performance analysis is also presented, including a comparison to a naive approach based on a reduced form of the matched filter (MF) method. The results shows the superiority of the proposed algorithm.",2019-11-25T10:16:24Z,2019-11-25T10:16:24Z,http://arxiv.org/abs/1911.10804v1,http://arxiv.org/pdf/1911.10804v1,"eess.SP, cs.DC, cs.IT, math.IT"
Crypto-Oriented Neural Architecture Design,"Avital Shafran, Gil Segev, Shmuel Peleg, Yedid Hoshen","As neural networks revolutionize many applications, significant privacy conflicts between model users and providers emerge. The cryptography community developed a variety of techniques for secure computation to address such privacy issues. As generic techniques for secure computation are typically prohibitively ineffective, many efforts focus on optimizing their underlying cryptographic tools. Differently, we propose to optimize the initial design of crypto-oriented neural architectures and provide a novel Partial Activation layer. The proposed layer is much faster for secure computation. Evaluating our method on three state-of-the-art architectures (SqueezeNet, ShuffleNetV2, and MobileNetV2) demonstrates significant improvement to the efficiency of secure inference on common evaluation metrics.",2019-11-27T17:57:42Z,2021-02-16T06:42:31Z,http://arxiv.org/abs/1911.12322v3,http://arxiv.org/pdf/1911.12322v3,"cs.LG, cs.CR, stat.ML"
Towards Oracle Knowledge Distillation with Neural Architecture Search,"Minsoo Kang, Jonghwan Mun, Bohyung Han","We present a novel framework of knowledge distillation that is capable of learning powerful and efficient student models from ensemble teacher networks. Our approach addresses the inherent model capacity issue between teacher and student and aims to maximize benefit from teacher models during distillation by reducing their capacity gap. Specifically, we employ a neural architecture search technique to augment useful structures and operations, where the searched network is appropriate for knowledge distillation towards student models and free from sacrificing its performance by fixing the network capacity. We also introduce an oracle knowledge distillation loss to facilitate model search and distillation using an ensemble-based teacher model, where a student network is learned to imitate oracle performance of the teacher. We perform extensive experiments on the image classification datasets---CIFAR-100 and TinyImageNet---using various networks. We also show that searching for a new student model is effective in both accuracy and memory size and that the searched models often outperform their teacher models thanks to neural architecture search with oracle knowledge distillation.",2019-11-29T09:42:15Z,2019-11-29T09:42:15Z,http://arxiv.org/abs/1911.13019v1,http://arxiv.org/pdf/1911.13019v1,"cs.LG, stat.ML"
Architecture and Security of SCADA Systems: A Review,"Geeta Yadav, Kolin Paul","Pipeline bursting, production lines shut down, frenzy traffic, trains confrontation, nuclear reactor shut down, disrupted electric supply, interrupted oxygen supply in ICU - these catastrophic events could result because of an erroneous SCADA system/ Industrial Control System(ICS). SCADA systems have become an essential part of automated control and monitoring of many of the Critical Infrastructures (CI). Modern SCADA systems have evolved from standalone systems into sophisticated complex, open systems, connected to the Internet. This geographically distributed modern SCADA system is vulnerable to threats and cyber attacks. In this paper, we first review the SCADA system architectures that have been proposed/implemented followed by attacks on such systems to understand and highlight the evolving security needs for SCADA systems. A short investigation of the current state of intrusion detection techniques in SCADA systems is done , followed by a brief study of testbeds for SCADA systems. The cloud and Internet of things (IoT) based SCADA systems are studied by analysing the architecture of modern SCADA systems. This review paper ends by highlighting the critical research problems that need to be resolved to close the gaps in the security of SCADA systems.",2020-01-09T11:30:50Z,2020-01-09T11:30:50Z,http://arxiv.org/abs/2001.02925v1,http://arxiv.org/pdf/2001.02925v1,"cs.CR, cs.DC, A.1; J.6"
Lattice QCD on a novel vector architecture,"Benjamin Huth, Nils Meyer, Tilo Wettig","The SX-Aurora TSUBASA PCIe accelerator card is the newest model of NEC's SX architecture family. Its multi-core vector processor features a vector length of 16 kbits and interfaces with up to 48 GB of HBM2 memory in the current models, available since 2018. The compute performance is up to 2.45 TFlop/s peak in double precision, and the memory throughput is up to 1.2 TB/s peak. New models with improved performance characteristics are announced for the near future. In this contribution we discuss key aspects of the SX-Aurora and describe how we enabled the architecture in the Grid Lattice QCD framework.",2020-01-21T14:18:07Z,2020-02-01T22:12:52Z,http://arxiv.org/abs/2001.07557v2,http://arxiv.org/pdf/2001.07557v2,"cs.DC, hep-lat"
High coherence superconducting microwave cavities with indium bump   bonding,"Chan U Lei, Lev Krayzman, Suhas Ganjam, Luigi Frunzio, Robert J. Schoelkopf","Low-loss cavities are important in building high-coherence superconducting quantum computers. Generating high quality joints between parts is crucial to the realization of a scalable quantum computer using the circuit quantum electrodynamics (cQED) framework. In this paper, we adapt the technique of indium bump bonding to the cQED architecture to realize high quality superconducting microwave joints between chips. We use this technique to fabricate compact superconducting cavities in the multilayer microwave integrated quantum circuits (MMIQC) architecture and achieve single photon quality factor over 300 million or single-photon lifetimes approaching 5 ms. To quantify the performance of the resulting seam, we fabricate microwave stripline resonators in multiple sections connected by different numbers of bonds, resulting in a wide range of seam admittances. The measured quality factors combined with the designed seam admittances allow us to bound the conductance of the seam at $g_\text{seam} \ge 2\times 10^{10} /(\Omega \text{m})$. Such a conductance should enable construction of micromachined superconducting cavities with quality factor of at least a billion. These results demonstrate the capability to construct very high quality microwave structures within the MMIQC architecture.",2020-01-24T22:31:59Z,2020-01-24T22:31:59Z,http://arxiv.org/abs/2001.09216v1,http://arxiv.org/pdf/2001.09216v1,"physics.app-ph, cond-mat.mes-hall"
RandomNet: Towards Fully Automatic Neural Architecture Design for   Multimodal Learning,"Stefano Alletto, Shenyang Huang, Vincent Francois-Lavet, Yohei Nakata, Guillaume Rabusseau","Almost all neural architecture search methods are evaluated in terms of performance (i.e. test accuracy) of the model structures that it finds. Should it be the only metric for a good autoML approach? To examine aspects beyond performance, we propose a set of criteria aimed at evaluating the core of autoML problem: the amount of human intervention required to deploy these methods into real world scenarios. Based on our proposed evaluation checklist, we study the effectiveness of a random search strategy for fully automated multimodal neural architecture search. Compared to traditional methods that rely on manually crafted feature extractors, our method selects each modality from a large search space with minimal human supervision. We show that our proposed random search strategy performs close to the state of the art on the AV-MNIST dataset while meeting the desirable characteristics for a fully automated design process.",2020-03-02T20:41:57Z,2020-03-02T20:41:57Z,http://arxiv.org/abs/2003.01181v1,http://arxiv.org/pdf/2003.01181v1,"cs.LG, cs.CV, stat.ML"
Trade-offs In Quasi-Decentralized Massive MIMO,"Juan Vidal Alegría, Fredrik Rusek, Jesús Rodríguez Sánchez, Ove Edfors","Typical massive multiple-input multiple-output (MIMO) architectures consider a centralized approach, in which all baseband data received by each antenna has to be sent to a central processing unit (CPU) to be processed. Due to the enormous amount of antennas expected in massive MIMO base stations (BSs), the number of connections to the CPU required in centralized massive MIMO is not scalable. In recent literature decentralized approaches have been proposed to reduce the number of connections between the antennas and the CPU. However, the reduction in the connections to the CPU requires more outputs per antenna to be generated. We study the trade-off between number of connections to the CPU and number of outputs per antenna. We propose a generalized architecture that allows exploitation of this trade-off, and we define a novel matrix decomposition that allows lossless linear equalization within our proposed architecture.",2020-03-04T09:14:50Z,2020-03-04T09:14:50Z,http://arxiv.org/abs/2003.01961v1,http://arxiv.org/pdf/2003.01961v1,"eess.SP, cs.IT, math.IT"
"TIME: A Transparent, Interpretable, Model-Adaptive and Explainable   Neural Network for Dynamic Physical Processes","Gurpreet Singh, Soumyajit Gupta, Matt Lease, Clint N. Dawson","Partial Differential Equations are infinite dimensional encoded representations of physical processes. However, imbibing multiple observation data towards a coupled representation presents significant challenges. We present a fully convolutional architecture that captures the invariant structure of the domain to reconstruct the observable system. The proposed architecture is significantly low-weight compared to other networks for such problems. Our intent is to learn coupled dynamic processes interpreted as deviations from true kernels representing isolated processes for model-adaptivity. Experimental analysis shows that our architecture is robust and transparent in capturing process kernels and system anomalies. We also show that high weights representation is not only redundant but also impacts network interpretability. Our design is guided by domain knowledge, with isolated process representations serving as ground truths for verification. These allow us to identify redundant kernels and their manifestations in activation maps to guide better designs that are both interpretable and explainable unlike traditional deep-nets.",2020-03-05T04:19:59Z,2020-03-06T04:45:20Z,http://arxiv.org/abs/2003.02426v2,http://arxiv.org/pdf/2003.02426v2,"cs.LG, stat.ML"
Finding Input Characterizations for Output Properties in ReLU Neural   Networks,"Saket Dingliwal, Divyansh Pareek, Jatin Arora","Deep Neural Networks (DNNs) have emerged as a powerful mechanism and are being increasingly deployed in real-world safety-critical domains. Despite the widespread success, their complex architecture makes proving any formal guarantees about them difficult. Identifying how logical notions of high-level correctness relate to the complex low-level network architecture is a significant challenge. In this project, we extend the ideas presented in and introduce a way to bridge the gap between the architecture and the high-level specifications. Our key insight is that instead of directly proving the safety properties that are required, we first prove properties that relate closely to the structure of the neural net and use them to reason about the safety properties. We build theoretical foundations for our approach, and empirically evaluate the performance through various experiments, achieving promising results than the existing approach by identifying a larger region of input space that guarantees a certain property on the output.",2020-03-09T17:29:39Z,2020-03-09T17:29:39Z,http://arxiv.org/abs/2003.04273v1,http://arxiv.org/pdf/2003.04273v1,"cs.LG, cs.NE, stat.ML"
UPR: A Model-Driven Architecture for Deep Phase Retrieval,"Naveed Naimipour, Shahin Khobahi, Mojtaba Soltanalian","The problem of phase retrieval has been intriguing researchers for decades due to its appearance in a wide range of applications. The task of a phase retrieval algorithm is typically to recover a signal from linear phase-less measurements. In this paper, we approach the problem by proposing a hybrid model-based data-driven deep architecture, referred to as the Unfolded Phase Retrieval (UPR), that shows potential in improving the performance of the state-of-the-art phase retrieval algorithms. Specifically, the proposed method benefits from versatility and interpretability of well established model-based algorithms, while simultaneously benefiting from the expressive power of deep neural networks. Our numerical results illustrate the effectiveness of such hybrid deep architectures and showcase the untapped potential of data-aided methodologies to enhance the existing phase retrieval algorithms.",2020-03-09T20:22:40Z,2020-03-09T20:22:40Z,http://arxiv.org/abs/2003.04396v1,http://arxiv.org/pdf/2003.04396v1,"eess.SP, cs.LG, math.OC, stat.ML"
A Survey of End-to-End Driving: Architectures and Training Methods,"Ardi Tampuu, Maksym Semikin, Naveed Muhammad, Dmytro Fishman, Tambet Matiisen","Autonomous driving is of great interest to industry and academia alike. The use of machine learning approaches for autonomous driving has long been studied, but mostly in the context of perception. In this paper we take a deeper look on the so called end-to-end approaches for autonomous driving, where the entire driving pipeline is replaced with a single neural network. We review the learning methods, input and output modalities, network architectures and evaluation schemes in end-to-end driving literature. Interpretability and safety are discussed separately, as they remain challenging for this approach. Beyond providing a comprehensive overview of existing methods, we conclude the review with an architecture that combines the most promising elements of the end-to-end autonomous driving systems.",2020-03-13T17:42:58Z,2021-03-02T13:55:45Z,http://arxiv.org/abs/2003.06404v2,http://arxiv.org/pdf/2003.06404v2,"cs.AI, cs.RO, 68T40, I.2.9"
Segmentation of brain tumor on magnetic resonance imaging using a   convolutional architecture,"Miriam Zulema Jacobo, Jose Mejia","The brain is a complex organ controlling cognitive process and physical functions. Tumors in the brain are accelerated cell growths affecting the normal function and processes in the brain. MRI scans provides detailed images of the body being one of the most common tests to diagnose brain tumors. The process of segmentation of brain tumors from magnetic resonance imaging can provide a valuable guide for diagnosis, treatment planning and prediction of results. Here we consider the problem brain tumor segmentation using a Deep learning architecture for use in tumor segmentation. Although the proposed architecture is simple and computationally easy to train, it is capable of reaching $IoU$ levels of 0.95.",2020-03-17T20:55:48Z,2020-03-17T20:55:48Z,http://arxiv.org/abs/2003.07934v1,http://arxiv.org/pdf/2003.07934v1,"eess.IV, cs.CV"
HyNNA: Improved Performance for Neuromorphic Vision Sensor based   Surveillance using Hybrid Neural Network Architecture,"Deepak Singla, Soham Chatterjee, Lavanya Ramapantulu, Andres Ussa, Bharath Ramesh, Arindam Basu","Applications in the Internet of Video Things (IoVT) domain have very tight constraints with respect to power and area. While neuromorphic vision sensors (NVS) may offer advantages over traditional imagers in this domain, the existing NVS systems either do not meet the power constraints or have not demonstrated end-to-end system performance. To address this, we improve on a recently proposed hybrid event-frame approach by using morphological image processing algorithms for region proposal and address the low-power requirement for object detection and classification by exploring various convolutional neural network (CNN) architectures. Specifically, we compare the results obtained from our object detection framework against the state-of-the-art low-power NVS surveillance system and show an improved accuracy of 82.16% from 63.1%. Moreover, we show that using multiple bits does not improve accuracy, and thus, system designers can save power and area by using only single bit event polarity information. In addition, we explore the CNN architecture space for object classification and show useful insights to trade-off accuracy for lower power using lesser memory and arithmetic operations.",2020-03-19T07:18:33Z,2020-03-19T07:18:33Z,http://arxiv.org/abs/2003.08603v1,http://arxiv.org/pdf/2003.08603v1,"eess.IV, cs.CV, cs.LG"
U-Det: A Modified U-Net architecture with bidirectional feature network   for lung nodule segmentation,"Nikhil Varma Keetha, Samson Anosh Babu P, Chandra Sekhara Rao Annavarapu","Early diagnosis and analysis of lung cancer involve a precise and efficient lung nodule segmentation in computed tomography (CT) images. However, the anonymous shapes, visual features, and surroundings of the nodule in the CT image pose a challenging problem to the robust segmentation of the lung nodules. This article proposes U-Det, a resource-efficient model architecture, which is an end to end deep learning approach to solve the task at hand. It incorporates a Bi-FPN (bidirectional feature network) between the encoder and decoder. Furthermore, it uses Mish activation function and class weights of masks to enhance segmentation efficiency. The proposed model is extensively trained and evaluated on the publicly available LUNA-16 dataset consisting of 1186 lung nodules. The U-Det architecture outperforms the existing U-Net model with the Dice similarity coefficient (DSC) of 82.82% and achieves results comparable to human experts.",2020-03-20T14:25:22Z,2020-03-20T14:25:22Z,http://arxiv.org/abs/2003.09293v1,http://arxiv.org/pdf/2003.09293v1,"eess.IV, cs.CV, cs.LG, stat.ML"
Deep Sets for Generalization in RL,"Tristan Karch, Cédric Colas, Laetitia Teodorescu, Clément Moulin-Frier, Pierre-Yves Oudeyer","This paper investigates the idea of encoding object-centered representations in the design of the reward function and policy architectures of a language-guided reinforcement learning agent. This is done using a combination of object-wise permutation invariant networks inspired from Deep Sets and gated-attention mechanisms. In a 2D procedurally-generated world where agents targeting goals in natural language navigate and interact with objects, we show that these architectures demonstrate strong generalization capacities to out-of-distribution goals. We study the generalization to varying numbers of objects at test time and further extend the object-centered architectures to goals involving relational reasoning.",2020-03-20T18:22:40Z,2020-03-20T18:22:40Z,http://arxiv.org/abs/2003.09443v1,http://arxiv.org/pdf/2003.09443v1,"cs.LG, cs.AI, stat.ML"
A generic ontology and recovery protocols for Human-Robot Collaboration   (HRC) systems,"Kamil Skarzynski, Marcin Stepniak, Waldemar Bartyna, Stanislaw Ambroszkiewicz","Humans are considered as integral components of Human-Robot Collaboration (HRC) systems, not only as object (e.g. in health care), but also as operators and service providers in manufacturing. Sophisticated and complex tasks are to be collaboratively executed by devices (robots) and humans. We introduce a generic ontology for HRC systems. Description of humans is a part of the ontology. Critical and hazardous (for humans) situations, as well as corresponding safeguards are defined on the basis of the ontology. The ontology is an extension of the ontology introduced in Skarzynski et al. (2018) arXiv:1709.03300. The architecture of SO-MRS (see arXiv:1709.03300), a software platform for automatic task accomplishment, is extended to HRC systems. Ongoing experiments, carried out in a simulated HRC system, are to verify the ontology and the architecture.",2020-03-20T20:13:06Z,2020-03-20T20:13:06Z,http://arxiv.org/abs/2003.09485v1,http://arxiv.org/pdf/2003.09485v1,"cs.RO, cs.MA, 68T40, I.2.9"
UAVs as a Service: Boosting Edge Intelligence for Air-Ground Integrated   Networks,"Chao Dong, Yun Shen, Yuben Qu, Qihui Wu, Fan Wu, Guihai Chen","The air-ground integrated network is a key component of future sixth generation (6G) networks to support seamless and near-instant super-connectivity. There is a pressing need to intelligently provision various services in 6G networks, which however is challenging. To meet this need, in this article, we propose a novel architecture called UaaS (UAVs as a Service) for the air-ground integrated network, featuring UAV as a key enabler to boost edge intelligence with the help of machine learning (ML) techniques. We envision that the proposed UaaS architecture could intelligently provision wireless communication service, edge computing service, and edge caching service by a network of UAVs, making full use of UAVs' flexible deployment and diverse ML techniques. We also conduct a case study where UAVs participate in the model training of distributed ML among multiple terrestrial users, whose result shows that the model training is efficient with a negligible energy consumption of UAVs, compared to the flight energy consumption. Finally, we discuss the challenges and open research issues in the UaaS.",2020-03-24T09:54:21Z,2020-03-24T09:54:21Z,http://arxiv.org/abs/2003.10737v1,http://arxiv.org/pdf/2003.10737v1,"cs.NI, eess.SP"
Deep Graph Matching via Blackbox Differentiation of Combinatorial   Solvers,"Michal Rolínek, Paul Swoboda, Dominik Zietlow, Anselm Paulus, Vít Musil, Georg Martius","Building on recent progress at the intersection of combinatorial optimization and deep learning, we propose an end-to-end trainable architecture for deep graph matching that contains unmodified combinatorial solvers. Using the presence of heavily optimized combinatorial solvers together with some improvements in architecture design, we advance state-of-the-art on deep graph matching benchmarks for keypoint correspondence. In addition, we highlight the conceptual advantages of incorporating solvers into deep learning architectures, such as the possibility of post-processing with a strong multi-graph matching solver or the indifference to changes in the training setting. Finally, we propose two new challenging experimental setups. The code is available at https://github.com/martius-lab/blackbox-deep-graph-matching",2020-03-25T21:53:12Z,2020-08-05T10:15:33Z,http://arxiv.org/abs/2003.11657v2,http://arxiv.org/pdf/2003.11657v2,"cs.LG, stat.ML"
BVI-DVC: A Training Database for Deep Video Compression,"Di Ma, Fan Zhang, David R. Bull","Deep learning methods are increasingly being applied in the optimisation of video compression algorithms and can achieve significantly enhanced coding gains, compared to conventional approaches. Such approaches often employ Convolutional Neural Networks (CNNs) which are trained on databases with relatively limited content coverage. In this paper, a new extensive and representative video database, BVI-DVC, is presented for training CNN-based video compression systems, with specific emphasis on machine learning tools that enhance conventional coding architectures, including spatial resolution and bit depth up-sampling, post-processing and in-loop filtering. BVI-DVC contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools. Experimental results show that this database produces significant improvements in terms of coding gains over three existing (commonly used) image/video training databases under the same training and evaluation configurations. The overall additional coding improvements by using the proposed database for all tested coding modules and CNN architectures are up to 10.3% based on the assessment of PSNR and 8.1% based on VMAF.",2020-03-30T15:26:16Z,2020-10-08T10:24:30Z,http://arxiv.org/abs/2003.13552v2,http://arxiv.org/pdf/2003.13552v2,"eess.IV, cs.CV"
Robotic Table Tennis with Model-Free Reinforcement Learning,"Wenbo Gao, Laura Graesser, Krzysztof Choromanski, Xingyou Song, Nevena Lazic, Pannag Sanketi, Vikas Sindhwani, Navdeep Jaitly","We propose a model-free algorithm for learning efficient policies capable of returning table tennis balls by controlling robot joints at a rate of 100Hz. We demonstrate that evolutionary search (ES) methods acting on CNN-based policy architectures for non-visual inputs and convolving across time learn compact controllers leading to smooth motions. Furthermore, we show that with appropriately tuned curriculum learning on the task and rewards, policies are capable of developing multi-modal styles, specifically forehand and backhand stroke, whilst achieving 80\% return rate on a wide range of ball throws. We observe that multi-modality does not require any architectural priors, such as multi-head architectures or hierarchical policies.",2020-03-31T17:46:43Z,2020-05-27T20:18:07Z,http://arxiv.org/abs/2003.14398v2,http://arxiv.org/pdf/2003.14398v2,"cs.LG, cs.RO, stat.ML, I.2.6; I.2.9"
Singer Identification Using Convolutional Acoustic Motif Embeddings,"Aitor Arronte Alvarez, Francisco Gomez-Martin","Flamenco singing is characterized by pitch instability, micro-tonal ornamentations, large vibrato ranges, and a high degree of melodic variability. These musical features make the automatic identification of flamenco singers a difficult computational task. In this article we present an end-to-end pipeline for flamenco singer identification based on acoustic motif embeddings. In the approach taken, the fundamental frequency obtained directly from the raw audio signal is approximated. This approximation reduces the high variability of the audio signal and allows for small melodic patterns to be discovered using a sequential pattern mining technique, thus creating a dictionary of motifs. Several acoustic features are then used to extract fixed length embeddings of variable length motifs by using convolutional architectures. We test the quality of the embeddings in a flamenco singer identification task, comparing our approach with previous deep learning architectures, and study the effect of motivic patterns and acoustic features in the identification task. Results indicate that motivic patterns play a crucial role in identifying flamenco singers by minimizing the size of the signal to be learned, discarding information that is not relevant in the identification task. The deep learning architecture presented outperforms denser models used in large-scale audio classification problems.",2020-08-01T07:27:18Z,2020-08-01T07:27:18Z,http://arxiv.org/abs/2008.00198v1,http://arxiv.org/pdf/2008.00198v1,"eess.AS, cs.SD"
"Graph Neural Networks: Architectures, Stability and Transferability","Luana Ruiz, Fernando Gama, Alejandro Ribeiro","Graph Neural Networks (GNNs) are information processing architectures for signals supported on graphs. They are presented here as generalizations of convolutional neural networks (CNNs) in which individual layers contain banks of graph convolutional filters instead of banks of classical convolutional filters. Otherwise, GNNs operate as CNNs. Filters are composed with pointwise nonlinearities and stacked in layers. It is shown that GNN architectures exhibit equivariance to permutation and stability to graph deformations. These properties help explain the good performance of GNNs that can be observed empirically. It is also shown that if graphs converge to a limit object, a graphon, GNNs converge to a corresponding limit object, a graphon neural network. This convergence justifies the transferability of GNNs across networks with different number of nodes. Concepts are illustrated by the application of GNNs to recommendation systems, decentralized collaborative control, and wireless communication networks.",2020-08-04T18:57:36Z,2021-01-29T18:52:54Z,http://arxiv.org/abs/2008.01767v3,http://arxiv.org/pdf/2008.01767v3,"cs.LG, stat.ML"
Compact Graph Architecture for Speech Emotion Recognition,"A. Shirian, T. Guha","We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architecture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters (~30K) indicating its applicability in resource-constrained devices.",2020-08-05T12:09:09Z,2021-02-02T10:34:47Z,http://arxiv.org/abs/2008.02063v4,http://arxiv.org/pdf/2008.02063v4,"cs.CV, cs.LG, eess.AS"
Effect of Architectures and Training Methods on the Performance of   Learned Video Frame Prediction,"M. Akin Yilmaz, A. Murat Tekalp","We analyze the performance of feedforward vs. recurrent neural network (RNN) architectures and associated training methods for learned frame prediction. To this effect, we trained a residual fully convolutional neural network (FCNN), a convolutional RNN (CRNN), and a convolutional long short-term memory (CLSTM) network for next frame prediction using the mean square loss. We performed both stateless and stateful training for recurrent networks. Experimental results show that the residual FCNN architecture performs the best in terms of peak signal to noise ratio (PSNR) at the expense of higher training and test (inference) computational complexity. The CRNN can be trained stably and very efficiently using the stateful truncated backpropagation through time procedure, and it requires an order of magnitude less inference runtime to achieve near real-time frame prediction with an acceptable performance.",2020-08-13T20:45:28Z,2020-08-13T20:45:28Z,http://arxiv.org/abs/2008.06106v1,http://arxiv.org/pdf/2008.06106v1,"cs.CV, eess.IV"
A Dynamic Deep Neural Network For Multimodal Clinical Data Analysis,"Maria Hügle, Gabriel Kalweit, Thomas Huegle, Joschka Boedecker","Clinical data from electronic medical records, registries or trials provide a large source of information to apply machine learning methods in order to foster precision medicine, e.g. by finding new disease phenotypes or performing individual disease prediction. However, to take full advantage of deep learning methods on clinical data, architectures are necessary that 1) are robust with respect to missing and wrong values, and 2) can deal with highly variable-sized lists and long-term dependencies of individual diagnosis, procedures, measurements and medication prescriptions. In this work, we elaborate limitations of fully-connected neural networks and classical machine learning methods in this context and propose AdaptiveNet, a novel recurrent neural network architecture, which can deal with multiple lists of different events, alleviating the aforementioned limitations. We employ the architecture to the problem of disease progression prediction in rheumatoid arthritis using the Swiss Clinical Quality Management registry, which contains over 10.000 patients and more than 65.000 patient visits. Our proposed approach leads to more compact representations and outperforms the classical baselines.",2020-08-14T11:19:32Z,2020-08-14T11:19:32Z,http://arxiv.org/abs/2008.06294v1,http://arxiv.org/pdf/2008.06294v1,"cs.LG, stat.ML"
"Jointly Fine-Tuning ""BERT-like"" Self Supervised Models to Improve   Multimodal Speech Emotion Recognition","Shamane Siriwardhana, Andrew Reis, Rivindu Weerasekera, Suranga Nanayakkara","Multimodal emotion recognition from speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality-specific ""BERT-like"" pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning ""BERT-like"" SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT.",2020-08-15T08:54:48Z,2020-08-15T08:54:48Z,http://arxiv.org/abs/2008.06682v1,http://arxiv.org/pdf/2008.06682v1,"eess.AS, cs.AI, cs.CL, cs.SD"
Adaptive Signal Variances: CNN Initialization Through Modern   Architectures,"Takahiko Henmi, Esmeraldo Ronnie Rey Zara, Yoshihiro Hirohashi, Tsuyoshi Kato","Deep convolutional neural networks (CNN) have achieved the unwavering confidence in its performance on image processing tasks. The CNN architecture constitutes a variety of different types of layers including the convolution layer and the max-pooling layer. CNN practitioners widely understand the fact that the stability of learning depends on how to initialize the model parameters in each layer. Nowadays, no one doubts that the de facto standard scheme for initialization is the so-called Kaiming initialization that has been developed by He et al. The Kaiming scheme was derived from a much simpler model than the currently used CNN structure having evolved since the emergence of the Kaiming scheme. The Kaiming model consists only of the convolution and fully connected layers, ignoring the max-pooling layer and the global average pooling layer. In this study, we derived the initialization scheme again not from the simplified Kaiming model, but precisely from the modern CNN architectures, and empirically investigated how the new initialization method performs compared to the de facto standard ones that are widely used today.",2020-08-16T11:26:29Z,2020-08-29T06:11:44Z,http://arxiv.org/abs/2008.06885v2,http://arxiv.org/pdf/2008.06885v2,"cs.LG, cs.NE, stat.ML"
Ptolemy: Architecture Support for Robust Deep Learning,"Yiming Gan, Yuxian Qiu, Jingwen Leng, Minyi Guo, Yuhao Zhu","Deep learning is vulnerable to adversarial attacks, where carefully-crafted input perturbations could mislead a well-trained Deep Neural Network to produce incorrect results. Today's countermeasures to adversarial attacks either do not have capability to detect adversarial samples at inference time, or introduce prohibitively high overhead to be practical at inference time.   We propose Ptolemy, an algorithm-architecture co-designed system that detects adversarial attacks at inference time with low overhead and high accuracy.We exploit the synergies between DNN inference and imperative program execution: an input to a DNN uniquely activates a set of neurons that contribute significantly to the inference output, analogous to the sequence of basic blocks exercised by an input in a conventional program. Critically, we observe that adversarial samples tend to activate distinctive paths from those of benign inputs. Leveraging this insight, we propose an adversarial sample detection framework, which uses canary paths generated from offline profiling to detect adversarial samples at runtime. The Ptolemy compiler along with the co-designed hardware enable efficient execution by exploiting the unique algorithmic characteristics. Extensive evaluations show that Ptolemy achieves higher or similar adversarial example detection accuracy than today's mechanisms with a much lower runtime (as low as 2%) overhead.",2020-08-23T04:04:01Z,2020-08-23T04:04:01Z,http://arxiv.org/abs/2008.09954v1,http://arxiv.org/pdf/2008.09954v1,"cs.AR, eess.SP"
CRAC: Checkpoint-Restart Architecture for CUDA with Streams and UVM,"Twinkle Jain, Gene Cooperman","The share of the top 500 supercomputers with NVIDIA GPUs is now over 25% and continues to grow. While fault tolerance is a critical issue for supercomputing, there does not currently exist an efficient, scalable solution for CUDA applications on NVIDIA GPUs. CRAC (Checkpoint-Restart Architecture for CUDA) is new checkpoint-restart solution for fault tolerance that supports the full range of CUDA applications. CRAC combines: low runtime overhead (approximately 1% or less); fast checkpoint-restart; support for scalable CUDA streams (for efficient usage of all of the thousands of GPU cores); and support for the full features of Unified Virtual Memory (eliminating the programmer's burden of migrating memory between device and host). CRAC achieves its flexible architecture by segregating application code (checkpointed) and its external GPU communication via non-reentrant CUDA libraries (not checkpointed) within a single process's memory. This eliminates the high overhead of inter-process communication in earlier approaches, and has fewer limitations.",2020-08-24T17:57:55Z,2020-08-24T17:57:55Z,http://arxiv.org/abs/2008.10596v1,http://arxiv.org/pdf/2008.10596v1,"cs.DC, D.4.5"
Efficient Blind-Spot Neural Network Architecture for Image Denoising,"David Honzátko, Siavash A. Bigdeli, Engin Türetken, L. Andrea Dunbar","Image denoising is an essential tool in computational photography. Standard denoising techniques, which use deep neural networks at their core, require pairs of clean and noisy images for its training. If we do not possess the clean samples, we can use blind-spot neural network architectures, which estimate the pixel value based on the neighbouring pixels only. These networks thus allow training on noisy images directly, as they by-design avoid trivial solutions. Nowadays, the blind-spot is mostly achieved using shifted convolutions or serialization. We propose a novel fully convolutional network architecture that uses dilations to achieve the blind-spot property. Our network improves the performance over the prior work and achieves state-of-the-art results on established datasets.",2020-08-25T13:48:40Z,2020-08-25T13:48:40Z,http://arxiv.org/abs/2008.11010v1,http://arxiv.org/pdf/2008.11010v1,"eess.IV, cs.CV, cs.LG, I.4.3; I.2.10"
Neural Architecture Search For Keyword Spotting,"Tong Mo, Yakun Yu, Mohammad Salameh, Di Niu, Shangling Jui","Deep neural networks have recently become a popular solution to keyword spotting systems, which enable the control of smart devices via voice. In this paper, we apply neural architecture search to search for convolutional neural network models that can help boost the performance of keyword spotting based on features extracted from acoustic signals while maintaining an acceptable memory footprint. Specifically, we use differentiable architecture search techniques to search for operators and their connections in a predefined cell search space. The found cells are then scaled up in both depth and width to achieve competitive performance. We evaluated the proposed method on Google's Speech Commands Dataset and achieved a state-of-the-art accuracy of over 97% on the setting of 12-class utterance classification commonly reported in the literature.",2020-09-01T01:11:41Z,2020-09-02T04:10:58Z,http://arxiv.org/abs/2009.00165v2,http://arxiv.org/pdf/2009.00165v2,"eess.AS, cs.LG, cs.SD"
Graph-based Model of Smart Grid Architectures,"Benedikt Klaer, Ömer Sen, Dennis van der Velde, Immanuel Hacker, Michael Andres, Martin Henze","The rising use of information and communication technology in smart grids likewise increases the risk of failures that endanger the security of power supply, e.g., due to errors in the communication configuration, faulty control algorithms, or cyber-attacks. Co-simulations can be used to investigate such effects, but require precise modeling of the energy, communication, and information domain within an integrated smart grid infrastructure model. Given the complexity and lack of detailed publicly available communication network models for smart grid scenarios, there is a need for an automated and systematic approach to creating such coupled models. In this paper, we present an approach to automatically generate smart grid infrastructure models based on an arbitrary electrical distribution grid model using a generic architectural template. We demonstrate the applicability and unique features of our approach alongside examples concerning network planning, co-simulation setup, and specification of domain-specific intrusion detection systems.",2020-09-01T07:32:19Z,2020-09-01T07:32:19Z,http://arxiv.org/abs/2009.00273v1,http://arxiv.org/pdf/2009.00273v1,"cs.SE, cs.CR, cs.NI, cs.SY, eess.SY"
Error protected qubits in a silicon photonic chip,"Caterina Vigliar, Stefano Paesani, Yunhong Ding, Jeremy C. Adcock, Jianwei Wang, Sam Morley-Short, Davide Bacco, Leif K. Oxenløwe, Mark G. Thompson, John G. Rarity, Anthony Laing","General purpose quantum computers can, in principle, entangle a number of noisy physical qubits to realise composite qubits protected against errors. Architectures for measurement-based quantum computing intrinsically support error-protected qubits and are the most viable approach for constructing an all-photonic quantum computer. Here we propose and demonstrate an integrated silicon photonic architecture that both entangles multiple photons, and encodes multiple physical qubits on individual photons, to produce error-protected qubits. We realise reconfigurable graph states to compare several schemes with and without error-correction encodings and implement a range of quantum information processing tasks. We observe a success rate increase from 62.5% to 95.8% when running a phase estimation algorithm without and with error protection, respectively. Finally, we realise hypergraph states, which are a generalised class of resource states that offer protection against correlated errors. Our results show how quantum error-correction encodings can be implemented with resource-efficient photonic architectures to improve the performance of quantum algorithms.",2020-09-17T14:37:12Z,2020-09-17T14:37:12Z,http://arxiv.org/abs/2009.08339v1,http://arxiv.org/pdf/2009.08339v1,"quant-ph, physics.optics"
GrateTile: Efficient Sparse Tensor Tiling for CNN Processing,"Yu-Sheng Lin, Hung Chang Lu, Yang-Bin Tsao, Yi-Min Chih, Wei-Chao Chen, Shao-Yi Chien","We propose GrateTile, an efficient, hardwarefriendly data storage scheme for sparse CNN feature maps (activations). It divides data into uneven-sized subtensors and, with small indexing overhead, stores them in a compressed yet randomly accessible format. This design enables modern CNN accelerators to fetch and decompressed sub-tensors on-the-fly in a tiled processing manner. GrateTile is suitable for architectures that favor aligned, coalesced data access, and only requires minimal changes to the overall architectural design. We simulate GrateTile with state-of-the-art CNNs and show an average of 55% DRAM bandwidth reduction while using only 0.6% of feature map size for indexing storage.",2020-09-18T08:31:41Z,2020-09-18T08:31:41Z,http://arxiv.org/abs/2009.08685v1,http://arxiv.org/pdf/2009.08685v1,"cs.LG, cs.AR, stat.ML"
A Comprehensive Survey of the Tactile Internet: State of the art and   Research Directions,"N. Promwongsa, A. Ebrahimzadeh, D. Naboulsi, S. Kianpisheh, F. Belqasmi, R. Glitho, N. Crespi, O. Alfandi","The Internet has made several giant leaps over the years, from a fixed to a mobile Internet, then to the Internet of Things, and now to a Tactile Internet. The Tactile Internet goes far beyond data, audio and video delivery over fixed and mobile networks, and even beyond allowing communication and collaboration among things. It is expected to enable haptic communication and allow skill set delivery over networks. Some examples of potential applications are tele-surgery, vehicle fleets, augmented reality and industrial process automation. Several papers already cover many of the Tactile Internet-related concepts and technologies, such as haptic codecs, applications, and supporting technologies. However, none of them offers a comprehensive survey of the Tactile Internet, including its architectures and algorithms. Furthermore, none of them provides a systematic and critical review of the existing solutions. To address these lacunae, we provide a comprehensive survey of the architectures and algorithms proposed to date for the Tactile Internet. In addition, we critically review them using a well-defined set of requirements and discuss some of the lessons learned as well as the most promising research directions.",2020-09-22T14:45:37Z,2020-09-22T14:45:37Z,http://arxiv.org/abs/2009.12164v1,http://arxiv.org/pdf/2009.12164v1,"eess.SP, cs.NI"
Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and   (gradient) stable architecture for learning long time dependencies,"T. Konstantin Rusch, Siddhartha Mishra","Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data.",2020-10-02T12:35:04Z,2021-03-14T19:12:57Z,http://arxiv.org/abs/2010.00951v2,http://arxiv.org/pdf/2010.00951v2,"cs.LG, cs.NE, stat.ML"
Polyphonic Piano Transcription Using Autoregressive Multi-State Note   Model,"Taegyun Kwon, Dasaem Jeong, Juhan Nam","Recent advances in polyphonic piano transcription have been made primarily by a deliberate design of neural network architectures that detect different note states such as onset or sustain and model the temporal evolution of the states. The majority of them, however, use separate neural networks for each note state, thereby optimizing multiple loss functions, and also they handle the temporal evolution of note states by abstract connections between the state-wise neural networks or using a post-processing module. In this paper, we propose a unified neural network architecture where multiple note states are predicted as a softmax output with a single loss function and the temporal order is learned by an auto-regressive connection within the single neural network. This compact model allows to increase note states without architectural complexity. Using the MAESTRO dataset, we examine various combinations of multiple note states including on, onset, sustain, re-onset, offset, and off. We also show that the autoregressive module effectively learns inter-state dependency of notes. Finally, we show that our proposed model achieves performance comparable to state-of-the-arts with fewer parameters.",2020-10-02T17:03:19Z,2020-10-02T17:03:19Z,http://arxiv.org/abs/2010.01104v1,http://arxiv.org/pdf/2010.01104v1,"eess.AS, cs.LG, cs.SD"
AFN: Attentional Feedback Network based 3D Terrain Super-Resolution,"Ashish Kubade, Diptiben Patel, Avinash Sharma, K. S. Rajan","Terrain, representing features of an earth surface, plays a crucial role in many applications such as simulations, route planning, analysis of surface dynamics, computer graphics-based games, entertainment, films, to name a few. With recent advancements in digital technology, these applications demand the presence of high-resolution details in the terrain. In this paper, we propose a novel fully convolutional neural network-based super-resolution architecture to increase the resolution of low-resolution Digital Elevation Model (LRDEM) with the help of information extracted from the corresponding aerial image as a complementary modality. We perform the super-resolution of LRDEM using an attention-based feedback mechanism named 'Attentional Feedback Network' (AFN), which selectively fuses the information from LRDEM and aerial image to enhance and infuse the high-frequency features and to produce the terrain realistically. We compare the proposed architecture with existing state-of-the-art DEM super-resolution methods and show that the proposed architecture outperforms enhancing the resolution of input LRDEM accurately and in a realistic manner.",2020-10-04T16:51:39Z,2020-10-04T16:51:39Z,http://arxiv.org/abs/2010.01626v1,http://arxiv.org/pdf/2010.01626v1,"eess.IV, cs.CV"
Range Expansion for Wireless Power Transfer using Joint Beamforming and   Waveform Architecture: An Experimental Study in Indoor Environment,"Junghoon Kim, Bruno Clerckx","Far-field Wireless Power Transfer (WPT) has emerged as a potential power source for the Internet of Things (IoT) and Wireless Sensor Network (WSN).The expansion of the power transfer range is one of the key challenges to make the technology viable. In this paper, we experimentally study a channel-adaptive joint beamforming and waveform architecture to expand the power transfer range. WPT experiments have been conducted in a variety of wireless channels at various distances in a realistic indoor environment. The measurement data have been fitted using a simple analytical model to analyze the output DC power and achievable range improvement depending on the signal design schemes and the number of tones and antennas. The model shows a clear relationship between signal design versus output DC power and achievable range, and highlight the significant benefit of the proposed architecture to expand the power transfer range.",2020-10-04T20:54:02Z,2021-03-11T16:18:49Z,http://arxiv.org/abs/2010.01680v2,http://arxiv.org/pdf/2010.01680v2,"cs.IT, math.IT"
Learned Hardware/Software Co-Design of Neural Accelerators,"Zhan Shi, Chirag Sakhuja, Milad Hashemi, Kevin Swersky, Calvin Lin","The use of deep learning has grown at an exponential rate, giving rise to numerous specialized hardware and software systems for deep learning. Because the design space of deep learning software stacks and hardware accelerators is diverse and vast, prior work considers software optimizations separately from hardware architectures, effectively reducing the search space. Unfortunately, this bifurcated approach means that many profitable design points are never explored. This paper instead casts the problem as hardware/software co-design, with the goal of automatically identifying desirable points in the joint design space. The key to our solution is a new constrained Bayesian optimization framework that avoids invalid solutions by exploiting the highly constrained features of this design space, which are semi-continuous/semi-discrete. We evaluate our optimization framework by applying it to a variety of neural models, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over hand-tuned state-of-the-art systems, as well as demonstrating strong results on other neural network architectures, such as MLPs and Transformers.",2020-10-05T15:12:52Z,2020-10-05T15:12:52Z,http://arxiv.org/abs/2010.02075v1,http://arxiv.org/pdf/2010.02075v1,"cs.LG, cs.AI, cs.AR, stat.ML"
Cardiac Arrhythmia Detection from ECG with Convolutional Recurrent   Neural Networks,"Jérôme Van Zaen, Ricard Delgado-Gonzalo, Damien Ferrario Mathieu Lemay","Except for a few specific types, cardiac arrhythmias are not immediately life-threatening. However, if not treated appropriately, they can cause serious complications. In particular, atrial fibrillation, which is characterized by fast and irregular heart beats, increases the risk of stroke. We propose three neural network architectures to detect abnormal rhythms from single-lead ECG signals. These architectures combine convolutional layers to extract high-level features pertinent for arrhythmia detection from sliding windows and recurrent layers to aggregate these features over signals of varying durations. We applied the neural networks to the dataset used for the challenge of Computing in Cardiology 2017 and a dataset built by joining three databases available on PhysioNet. Our architectures achieved an accuracy of 86.23% on the first dataset, similar to the winning entries of the challenge, and an accuracy of 92.02% on the second dataset.",2020-10-07T06:24:15Z,2020-10-07T06:24:15Z,http://arxiv.org/abs/2010.03204v1,http://arxiv.org/pdf/2010.03204v1,"cs.LG, eess.SP"
Uncertainty in Neural Processes,"Saeid Naderiparizi, Kenny Chiu, Benjamin Bloem-Reddy, Frank Wood",We explore the effects of architecture and training objective choice on amortized posterior predictive inference in probabilistic conditional generative models. We aim this work to be a counterpoint to a recent trend in the literature that stresses achieving good samples when the amount of conditioning data is large. We instead focus our attention on the case where the amount of conditioning data is small. We highlight specific architecture and objective choices that we find lead to qualitative and quantitative improvement to posterior inference in this low data regime. Specifically we explore the effects of choices of pooling operator and variational family on posterior quality in neural processes. Superior posterior predictive samples drawn from our novel neural process architectures are demonstrated via image completion/in-painting experiments.,2020-10-08T04:10:05Z,2020-10-08T04:10:05Z,http://arxiv.org/abs/2010.03753v1,http://arxiv.org/pdf/2010.03753v1,"cs.LG, stat.ML"
Information Extraction from Swedish Medical Prescriptions with   Sig-Transformer Encoder,"John Pougue Biyong, Bo Wang, Terry Lyons, Alejo J Nevado-Holgado","Relying on large pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) for encoding and adding a simple prediction layer has led to impressive performance in many clinical natural language processing (NLP) tasks. In this work, we present a novel extension to the Transformer architecture, by incorporating signature transform with the self-attention model. This architecture is added between embedding and prediction layers. Experiments on a new Swedish prescription data show the proposed architecture to be superior in two of the three information extraction tasks, comparing to baseline models. Finally, we evaluate two different embedding approaches between applying Multilingual BERT and translating the Swedish text to English then encode with a BERT model pretrained on clinical notes.",2020-10-10T04:22:07Z,2020-10-10T04:22:07Z,http://arxiv.org/abs/2010.04897v1,http://arxiv.org/pdf/2010.04897v1,"cs.CL, 60L10, I.2.7"
Adaptive Aggregation Networks for Class-Incremental Learning,"Yaoyao Liu, Bernt Schiele, Qianru Sun","Class-Incremental Learning (CIL) aims to learn a classification model with the number of classes increasing phase-by-phase. An inherent problem in CIL is the stability-plasticity dilemma between the learning of old and new classes, i.e., high-plasticity models easily forget old classes, but high-stability models are weak to learn new classes. We alleviate this issue by proposing a novel network architecture called Adaptive Aggregation Networks (AANets), in which we explicitly build two types of residual blocks at each residual level (taking ResNet as the baseline architecture): a stable block and a plastic block. We aggregate the output feature maps from these two blocks and then feed the results to the next-level blocks. We adapt the aggregation weights in order to balance these two types of blocks, i.e., to balance stability and plasticity, dynamically. We conduct extensive experiments on three CIL benchmarks: CIFAR-100, ImageNet-Subset, and ImageNet, and show that many existing CIL methods can be straightforwardly incorporated into the architecture of AANets to boost their performances.",2020-10-10T18:24:24Z,2021-03-29T22:09:07Z,http://arxiv.org/abs/2010.05063v3,http://arxiv.org/pdf/2010.05063v3,"cs.CV, stat.ML"
High Area/Energy Efficiency RRAM CNN Accelerator with Kernel-Reordering   Weight Mapping Scheme Based on Pattern Pruning,"Songming Yu, Yongpan Liu, Lu Zhang, Jingyu Wang, Jinshan Yue, Zhuqing Yuan, Xueqing Li, Huazhong Yang","Resistive Random Access Memory (RRAM) is an emerging device for processing-in-memory (PIM) architecture to accelerate convolutional neural network (CNN). However, due to the highly coupled crossbar structure in the RRAM array, it is difficult to exploit the sparsity of the network in RRAM-based CNN accelerator. To optimize the weight mapping of sparse network in the RRAM array and achieve high area and energy efficiency, we propose a novel weight mapping scheme and corresponding RRAM-based CNN accelerator architecture based on pattern pruning and Operation Unit(OU) mechanism. Experimental results show that our work can achieve 4.16x-5.20x crossbar area efficiency, 1.98x-2.15x energy efficiency, and 1.15x-1.35x performance speedup in comparison with the traditional weight mapping method.",2020-10-13T04:08:33Z,2020-10-13T04:08:33Z,http://arxiv.org/abs/2010.06156v1,http://arxiv.org/pdf/2010.06156v1,"cs.AR, C.1.3; C.3"
Throughput-Optimal Topology Design for Cross-Silo Federated Learning,"Othmane Marfoq, Chuan Xu, Giovanni Neglia, Richard Vidal","Federated learning usually employs a client-server architecture where an orchestrator iteratively aggregates model updates from remote clients and pushes them back a refined model. This approach may be inefficient in cross-silo settings, as close-by data silos with high-speed access links may exchange information faster than with the orchestrator, and the orchestrator may become a communication bottleneck. In this paper we define the problem of topology design for cross-silo federated learning using the theory of max-plus linear systems to compute the system throughput---number of communication rounds per time unit. We also propose practical algorithms that, under the knowledge of measurable network characteristics, find a topology with the largest throughput or with provable throughput guarantees. In realistic Internet networks with 10 Gbps access links for silos, our algorithms speed up training by a factor 9 and 1.5 in comparison to the master-slave architecture and to state-of-the-art MATCHA, respectively. Speedups are even larger with slower access links.",2020-10-23T08:28:29Z,2020-11-17T19:04:14Z,http://arxiv.org/abs/2010.12229v2,http://arxiv.org/pdf/2010.12229v2,"cs.LG, cs.DC, cs.NI, math.OC"
FPGA Implementation of Stair Matrix based Massive MIMO Detection,"Shahriar Shahabuddin, Mahmoud A. Albreem, Mohammad Shahanewaz Shahabuddin, Zaheer Khan, Markku Juntti","Approximate matrix inversion based methods is widely used for linear massive multiple-input multiple-output (MIMO) received symbol vector detection. Such detectors typically utilize the diagonally dominant channel matrix of a massive MIMO system. Instead of diagonal matrix, a stair matrix can be utilized to improve the error-rate performance of a massive MIMO detector. In this paper, we present very large-scale integration (VLSI) architecture and field programmable gate array (FPGA) implementation of a stair matrix based iterative detection algorithm. The architecture supports a base station with 128 antennas, 8 users with single antenna, and 256 quadrature amplitude modulation (QAM). The stair matrix based detector can deliver a 142.34 Mbps data rate and reach a clock frequency of 258 MHz in a Xilinx Virtex-7 FPGA. The detector provides superior error-rate performance and higher scaled throughput than most contemporary massive MIMO detectors.",2020-10-29T22:00:00Z,2020-10-29T22:00:00Z,http://arxiv.org/abs/2010.15964v1,http://arxiv.org/pdf/2010.15964v1,"cs.IT, cs.AR, math.IT"
Enabling Zero-shot Multilingual Spoken Language Translation with   Language-Specific Encoders and Decoders,"Carlos Escolano, Marta R. Costa-jussà, José A. R. Fonollosa, Carlos Segura","Current end-to-end approaches to Spoken Language Translation (SLT) rely on limited training resources, especially for multilingual settings. On the other hand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on higher-quality and more massive data sets. Our proposed method extends a MultiNMT architecture based on language-specific encoders-decoders to the task of Multilingual SLT (MultiSLT). Our method entirely eliminates the dependency from MultiSLT data and it is able to translate while training only on ASR and MultiNMT data.   Our experiments on four different languages show that coupling the speech encoder to the MultiNMT architecture produces similar quality translations compared to a bilingual baseline ($\pm 0.2$ BLEU) while effectively allowing for zero-shot MultiSLT. Additionally, we propose using an Adapter module for coupling the speech inputs. This Adapter module produces consistent improvements up to +6 BLEU points on the proposed architecture and +1 BLEU point on the end-to-end baseline.",2020-11-02T16:31:14Z,2021-09-15T18:42:21Z,http://arxiv.org/abs/2011.01097v2,http://arxiv.org/pdf/2011.01097v2,"cs.CL, I.2.7"
Deep learning architectures for inference of AC-OPF solutions,"Thomas Falconer, Letif Mones","We present a systematic comparison between neural network (NN) architectures for inference of AC-OPF solutions. Using fully connected NNs as a baseline we demonstrate the efficacy of leveraging network topology in the models by constructing abstract representations of electrical grids in the graph domain, for both convolutional and graph NNs. The performance of the NN architectures is compared for regression (predicting optimal generator set-points) and classification (predicting the active set of constraints) settings. Computational gains for obtaining optimal solutions are also presented.",2020-11-06T13:33:18Z,2020-12-01T10:03:18Z,http://arxiv.org/abs/2011.03352v2,http://arxiv.org/pdf/2011.03352v2,"cs.LG, cs.SY, eess.SP, eess.SY, physics.data-an"
Similarity-Based Clustering for Enhancing Image Classification   Architectures,Dishant Parikh,"Convolutional networks are at the center of best-in-class computer vision applications for a wide assortment of undertakings. Since 2014, a profound amount of work began to make better convolutional architectures, yielding generous additions in different benchmarks. Albeit expanded model size and computational cost will, in general, mean prompt quality increases for most undertakings but, the architectures now need to have some additional information to increase the performance. I show evidence that with the amalgamation of content-based image similarity and deep learning models, we can provide the flow of information which can be used in making clustered learning possible. The paper shows how training of sub-dataset clusters not only reduces the cost of computation but also increases the speed of evaluating and tuning a model on the given dataset.",2020-11-03T17:03:28Z,2021-10-06T04:48:18Z,http://arxiv.org/abs/2011.04728v3,http://arxiv.org/pdf/2011.04728v3,"cs.CV, cs.GR, cs.LG, eess.IV"
Improvement of plant performance using Closed loop Reference Model   Simple Adaptive Control for Micro Air Vehicle,"Shuvrangshu Jana, M. Seetharama Bhat","In this paper, we present a novel idea to improve the transient performance of the existing Simple Adaptive Control architecture, without requiring high adaptation gains. Improvement in performance is achieved by incorporating the closed loop reference model based on the output feedback to the Simple Adaptive Control architecture. In this proposed scheme, the reference model dynamics is driven by the desired command as well as the error signal between the plant output and the reference model output. It is shown that the modified control architecture improves the system performance without any additional control efforts, which is then validated through simulations of the lateral model dynamics of Micro Air Vehicle.",2020-11-11T17:30:26Z,2020-11-11T17:30:26Z,http://arxiv.org/abs/2011.05924v1,http://arxiv.org/pdf/2011.05924v1,"eess.SY, cs.RO, cs.SY"
A Study on MIMO Channel Estimation by 2D and 3D Convolutional Neural   Networks,"Ben Marinberg, Ariel Cohen, Eilam Ben-Dror, Haim Permuter","In this paper, we study the usage of Convolutional Neural Network (CNN) estimators for the task of Multiple-Input-Multiple-Output Orthogonal Frequency Division Multiplexing (MIMO-OFDM) Channel Estimation (CE). Specifically, the CNN estimators interpolate the channel values of reference signals for estimating the channel of the full OFDM resource element (RE) matrix. We have designed a 2D CNN architecture based on U-net, and a 3D CNN architecture for handling spatial correlation. We investigate the performance of various CNN architectures fora diverse data set generated according to the 5G NR standard and in particular, we investigate the influence of spatial correlation, Doppler, and reference signal resource allocation. The CE CNN estimators are then integrated with MIMO detection algorithms for testing their influence on the system level Bit Error Rate(BER) performance.",2020-11-12T12:24:14Z,2020-11-12T12:24:14Z,http://arxiv.org/abs/2011.08970v1,http://arxiv.org/pdf/2011.08970v1,"eess.SP, cs.LG, cs.SY, eess.SY"
Convolutional Autoencoder for Blind Hyperspectral Image Unmixing,"Yasiru Ranasinghe, Sanjaya Herath, Kavinga Weerasooriya, Mevan Ekanayake, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath","In the remote sensing context spectral unmixing is a technique to decompose a mixed pixel into two fundamental representatives: endmembers and abundances. In this paper, a novel architecture is proposed to perform blind unmixing on hyperspectral images. The proposed architecture consists of convolutional layers followed by an autoencoder. The encoder transforms the feature space produced through convolutional layers to a latent space representation. Then, from these latent characteristics the decoder reconstructs the roll-out image of the monochrome image which is at the input of the architecture; and each single-band image is fed sequentially. Experimental results on real hyperspectral data concludes that the proposed algorithm outperforms existing unmixing methods at abundance estimation and generates competitive results for endmember extraction with RMSE and SAD as the metrics, respectively.",2020-11-18T17:41:31Z,2020-11-18T17:41:31Z,http://arxiv.org/abs/2011.09420v1,http://arxiv.org/pdf/2011.09420v1,"cs.CV, eess.IV"
True-data Testbed for 5G/B5G Intelligent Network,"Yongming Huang, Shengheng Liu, Cheng Zhang, Xiaohu You, Hequan Wu","Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting Internet of Everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and providing high-quality service. As a rapid evolving paradigm, the AI-empowered mobile communications demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased.",2020-11-26T06:42:36Z,2021-01-04T08:51:23Z,http://arxiv.org/abs/2011.13152v2,http://arxiv.org/pdf/2011.13152v2,"cs.NI, cs.AI, cs.AR, eess.SP"
Detecting expressions with multimodal transformers,"Srinivas Parthasarathy, Shiva Sundaram","Developing machine learning algorithms to understand person-to-person engagement can result in natural user experiences for communal devices such as Amazon Alexa. Among other cues such as voice activity and gaze, a person's audio-visual expression that includes tone of the voice and facial expression serves as an implicit signal of engagement between parties in a dialog. This study investigates deep-learning algorithms for audio-visual detection of user's expression. We first implement an audio-visual baseline model with recurrent layers that shows competitive results compared to current state of the art. Next, we propose the transformer architecture with encoder layers that better integrate audio-visual features for expressions tracking. Performance on the Aff-Wild2 database shows that the proposed methods perform better than baseline architecture with recurrent layers with absolute gains approximately 2% for arousal and valence descriptors. Further, multimodal architectures show significant improvements over models trained on single modalities with gains of up to 3.6%. Ablation studies show the significance of the visual modality for the expression detection on the Aff-Wild2 database.",2020-11-30T19:31:03Z,2020-11-30T19:31:03Z,http://arxiv.org/abs/2012.00063v1,http://arxiv.org/pdf/2012.00063v1,"eess.AS, cs.SD, eess.IV"
Deploying deep learning in OpenFOAM with TensorFlow,"Romit Maulik, Himanshu Sharma, Saumil Patel, Bethany Lusch, Elise Jennings","We outline the development of a data science module within OpenFOAM which allows for the in-situ deployment of trained deep learning architectures for general-purpose predictive tasks. This module is constructed with the TensorFlow C API and is integrated into OpenFOAM as an application that may be linked at run time. Notably, our formulation precludes any restrictions related to the type of neural network architecture (i.e., convolutional, fully-connected, etc.). This allows for potential studies of complicated neural architectures for practical CFD problems. In addition, the proposed module outlines a path towards an open-source, unified and transparent framework for computational fluid dynamics and machine learning.",2020-12-01T23:59:30Z,2020-12-01T23:59:30Z,http://arxiv.org/abs/2012.00900v1,http://arxiv.org/pdf/2012.00900v1,"physics.comp-ph, cs.LG, physics.data-an, physics.flu-dyn"
Synthesis to Deployment: Cyber-Physical Control Architectures,"Shih-Hao Tseng, James Anderson","We consider the problem of how to deploy a controller to a (networked) cyber-physical system (CPS). Controlling a CPS is an involved task, and synthesizing a controller to respect sensing, actuation, and communication constraints is only part of the challenge. In addition to controller synthesis, one should also consider how the controller will be incorporated within the CPS. Put another way, the cyber layer and its interaction with the physical layer need to be taken into account.   In this work, we aim to bridge the gap between theoretical controller synthesis and practical CPS deployment. We adopt the system level synthesis (SLS) framework to synthesize a controller, either state-feedback or output-feedback, and provide deployment architectures for the standard SLS controllers. Furthermore, we derive new controller realizations for open-loop stable systems and introduce different state-feedback and output-feedback architectures for deployment, ranging from fully centralized to fully distributed. Finally, we compare the trade-offs among them in terms of robustness, memory, computation, and communication overhead.",2020-12-09T18:09:52Z,2021-06-01T03:47:08Z,http://arxiv.org/abs/2012.05211v2,http://arxiv.org/pdf/2012.05211v2,"math.OC, cs.SY, eess.SY"
Muscle-inspired flexible mechanical logic architecture for colloidal   robotics,"Mayank Agrawal, Sharon C. Glotzer","Materials that respond to external stimuli by expanding or contracting provide a transduction route that integrates sensing and actuation powered directly by the stimuli. This motivates us to build colloidal scale robots using these materials that can morph into arbitrary configurations. For intelligent use of global stimuli in robotic systems, computation ability needs to be incorporated within them. The challenge is to design an architecture that is compact, material agnostic, stable under stochastic forces and can employ stimuli-responsive materials. We present an architecture that computes combinatorial logic using mechanical gates that use muscle-like response - expansion and contraction - as circuit signal with additional benefits of logic circuitry being physically flexible and able to be retrofit to arbitrary robot bodies. We mathematically analyze gate geometry and discuss tuning it for the given requirements of signal dimension and magnitude. We validate the function and stability of the design at the colloidal scale using Brownian dynamics simulations. We also demonstrate the gate design using a 3D printed model. Finally, we simulate a complete robot that folds into Tetris shapes.",2020-12-17T01:14:18Z,2020-12-17T01:14:18Z,http://arxiv.org/abs/2012.09345v1,http://arxiv.org/pdf/2012.09345v1,"cs.RO, physics.app-ph"
Coupled oscillator networks for von Neumann and non von Neumann   computing,"Michele Bonnin, Fabio Lorenzo Traversa, Fabrizio Bonani","The frenetic growth of the need for computation performance and efficiency, along with the intrinsic limitations of the current main solutions, is pushing the scientific community towards unconventional, and sometimes even exotic, alternatives to the standard computing architectures. In this work we provide a panorama of the most relevant alternatives, both according and not the von Neumann architecture, highlighting which of the classical challenges, such as energy efficiency and/or computational complexity, they are trying to tackle. We focus on the alternatives based on networks of weakly coupled oscillators. This unconventional approach, already introduced by Goto and Von Neumann in the 50s, is recently regaining interest with potential applications to both von Neumann and non von Neumann type of computing. In this contribution, we present a general framework based on the phase equation we derive from the description of nonlinear weakly coupled oscillators especially useful for computing applications. We then use this formalism to design and prove the working principle and stability assessment of Boolean gates such as NOT and MAJORITY, that can be potentially employed as building blocks for both von Neumann and non-von Neumann architectures.",2020-12-22T22:06:36Z,2020-12-22T22:06:36Z,http://arxiv.org/abs/2012.12386v1,http://arxiv.org/pdf/2012.12386v1,"cs.ET, math.DS"
Distributed Adaptive Control: An ideal Cognitive Architecture candidate   for managing a robotic recycling plant,"Oscar Guerrero-Rosado, Paul Verschure","In the past decade, society has experienced notable growth in a variety of technological areas. However, the Fourth Industrial Revolution has not been embraced yet. Industry 4.0 imposes several challenges which include the necessity of new architectural models to tackle the uncertainty that open environments represent to cyber-physical systems (CPS). Waste Electrical and Electronic Equipment (WEEE) recycling plants stand for one of such open environments. Here, CPSs must work harmoniously in a changing environment, interacting with similar and not so similar CPSs, and adaptively collaborating with human workers. In this paper, we support the Distributed Adaptive Control (DAC) theory as a suitable Cognitive Architecture for managing a recycling plant. Specifically, a recursive implementation of DAC (between both single-agent and large-scale levels) is proposed to meet the expected demands of the European Project HR-Recycler. Additionally, with the aim of having a realistic benchmark for future implementations of the recursive DAC, a micro-recycling plant prototype is presented.",2020-12-23T10:33:22Z,2020-12-23T10:33:22Z,http://arxiv.org/abs/2012.12586v1,http://arxiv.org/pdf/2012.12586v1,"cs.MA, cs.AI, cs.NE, cs.RO, cs.SY, eess.SY"
Memory-Gated Recurrent Networks,"Yaquan Zhang, Qi Wu, Nanbo Peng, Min Dai, Jing Zhang, Hu Wang","The essence of multivariate sequential learning is all about how to extract dependencies in data. These data sets, such as hourly medical records in intensive care units and multi-frequency phonetic time series, often time exhibit not only strong serial dependencies in the individual components (the ""marginal"" memory) but also non-negligible memories in the cross-sectional dependencies (the ""joint"" memory). Because of the multivariate complexity in the evolution of the joint distribution that underlies the data generating process, we take a data-driven approach and construct a novel recurrent network architecture, termed Memory-Gated Recurrent Networks (mGRN), with gates explicitly regulating two distinct types of memories: the marginal memory and the joint memory. Through a combination of comprehensive simulation studies and empirical experiments on a range of public datasets, we show that our proposed mGRN architecture consistently outperforms state-of-the-art architectures targeting multivariate time series.",2020-12-24T06:04:14Z,2020-12-30T08:48:50Z,http://arxiv.org/abs/2012.13121v2,http://arxiv.org/pdf/2012.13121v2,"cs.LG, q-fin.ST"
RegNet: Self-Regulated Network for Image Classification,"Jing Xu, Yu Pan, Xinglin Pan, Steven Hoi, Zhang Yi, Zenglin Xu","The ResNet and its variants have achieved remarkable successes in various computer vision tasks. Despite its success in making gradient flow through building blocks, the simple shortcut connection mechanism limits the ability of re-exploring new potentially complementary features due to the additive function. To address this issue, in this paper, we propose to introduce a regulator module as a memory mechanism to extract complementary features, which are further fed to the ResNet. In particular, the regulator module is composed of convolutional RNNs (e.g., Convolutional LSTMs or Convolutional GRUs), which are shown to be good at extracting Spatio-temporal information. We named the new regulated networks as RegNet. The regulator module can be easily implemented and appended to any ResNet architecture. We also apply the regulator module for improving the Squeeze-and-Excitation ResNet to show the generalization ability of our method. Experimental results on three image classification datasets have demonstrated the promising performance of the proposed architecture compared with the standard ResNet, SE-ResNet, and other state-of-the-art architectures.",2021-01-03T09:06:25Z,2021-01-03T09:06:25Z,http://arxiv.org/abs/2101.00590v1,http://arxiv.org/pdf/2101.00590v1,"eess.IV, cs.CV"
Transformer for Image Quality Assessment,"Junyong You, Jari Korhonen","Transformer has become the new standard method in natural language processing (NLP), and it also attracts research interests in computer vision area. In this paper we investigate the application of Transformer in Image Quality (TRIQ) assessment. Following the original Transformer encoder employed in Vision Transformer (ViT), we propose an architecture of using a shallow Transformer encoder on the top of a feature map extracted by convolution neural networks (CNN). Adaptive positional embedding is employed in the Transformer encoder to handle images with arbitrary resolutions. Different settings of Transformer architectures have been investigated on publicly available image quality databases. We have found that the proposed TRIQ architecture achieves outstanding performance. The implementation of TRIQ is published on Github (https://github.com/junyongyou/triq).",2020-12-30T18:43:11Z,2021-01-08T12:12:32Z,http://arxiv.org/abs/2101.01097v2,http://arxiv.org/pdf/2101.01097v2,"cs.CV, cs.LG, eess.IV"
Training Deep Architectures Without End-to-End Backpropagation: A Survey   on the Provably Optimal Methods,"Shiyu Duan, Jose C. Principe","This tutorial paper surveys provably optimal alternatives to end-to-end backpropagation (E2EBP) -- the de facto standard for training deep architectures. Modular training refers to strictly local training without both the forward and the backward pass, i.e., dividing a deep architecture into several nonoverlapping modules and training them separately without any end-to-end operation. Between the fully global E2EBP and the strictly local modular training, there are weakly modular hybrids performing training without the backward pass only. These alternatives can match or surpass the performance of E2EBP on challenging datasets such as ImageNet, and are gaining increasing attention primarily because they offer practical advantages over E2EBP, which will be enumerated herein. In particular, they allow for greater modularity and transparency in deep learning workflows, aligning deep learning with the mainstream computer science engineering that heavily exploits modularization for scalability. Modular training has also revealed novel insights about learning and has further implications on other important research domains. Specifically, it induces natural and effective solutions to some important practical problems such as data efficiency and transferability estimation.",2021-01-09T19:56:22Z,2022-08-09T06:07:04Z,http://arxiv.org/abs/2101.03419v3,http://arxiv.org/pdf/2101.03419v3,"cs.LG, cs.NE, stat.ML"
Improving entanglement generation rates in trapped ion quantum networks   using nondestructive photon measurement and storage,"John Hannegan, James D. Siverns, Jake Cassell, Qudsia Quraishi","Long range quantum information processing will require the integration of different technologies to form hybrid architectures combining the strengths of multiple quantum systems. In this work, we propose a hybrid networking architecture designed to improve entanglement rates in quantum networks based on trapped ions. Trapped ions are excellent candidates as network nodes but photon losses make long-distance networking difficult. To overcome some losses and extend the range of trapped-ion-based networks, we propose including neutral-atom-based non-destructive single-photon detection and single photon storage in between networking nodes, forming a hybrid network. This work builds on recently demonstrated optical frequency conversion of single photons emitted by trapped ions. We derive the average two-node entanglement rate for this proposed network architecture as a function of distance. Using reasonable experimental parameters, we show this proposed quantum network can generate remote entanglement rates up to a factor of 100 larger than that of an equivalent homogeneous network at both near-IR and C-band wavelengths for distances up to 50 km.",2021-01-11T23:24:26Z,2021-03-24T21:38:41Z,http://arxiv.org/abs/2101.04236v2,http://arxiv.org/pdf/2101.04236v2,"quant-ph, physics.atom-ph"
End-to-End Speaker-Attributed ASR with Transformer,"Naoyuki Kanda, Guoli Ye, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, Takuya Yoshioka","This paper presents our recent effort on end-to-end speaker-attributed automatic speech recognition, which jointly performs speaker counting, speech recognition and speaker identification for monaural multi-talker audio. Firstly, we thoroughly update the model architecture that was previously designed based on a long short-term memory (LSTM)-based attention encoder decoder by applying transformer architectures. Secondly, we propose a speaker deduplication mechanism to reduce speaker identification errors in highly overlapped regions. Experimental results on the LibriSpeechMix dataset shows that the transformer-based architecture is especially good at counting the speakers and that the proposed model reduces the speaker-attributed word error rate by 47% over the LSTM-based baseline. Furthermore, for the LibriCSS dataset, which consists of real recordings of overlapped speech, the proposed model achieves concatenated minimum-permutation word error rates of 11.9% and 16.3% with and without target speaker profiles, respectively, both of which are the state-of-the-art results for LibriCSS with the monaural setting.",2021-04-05T19:54:15Z,2021-04-05T19:54:15Z,http://arxiv.org/abs/2104.02128v1,http://arxiv.org/pdf/2104.02128v1,"eess.AS, cs.CL, cs.SD"
Techno-Economic Assessment Models for 5G,Carlos Bendicho,"This paper proposes the characteristics a techno-economic model for 5G should have considering both mobile network operators perspective and end users needs. It also presents a review and classification of models in the literature based on the characteristics of such theoretical techno-economic reference model. The performed analysis identifies current gaps in the techno-economic modeling literature for 5G architectures and shows it can be enhanced using agile techno-economic models like the Universal Techno-Economic Model (UTEM) created and developed by the author to industrialize assessment of different technological solutions, considering all market players perspectives and applicable to decision-making in multiple domains. This model can be used for an effective and agile 5G techno-economic assessment, including not only network deployment perspective but also customers and end users requirements as well as other stakeholders to select the most adequate 5G architectural solution considering both technical and economic feasibility. UTEM model is currently available for all industry stakeholders under specific license of use.",2021-04-09T16:56:26Z,2021-04-09T16:56:26Z,http://arxiv.org/abs/2104.04479v1,http://arxiv.org/pdf/2104.04479v1,"cs.NI, C.2.0; C.2.1; C.2.3; C.2.6; C.2.m"
Sifting out the features by pruning: Are convolutional networks the   winning lottery ticket of fully connected ones?,"Franco Pellegrini, Giulio Biroli","Pruning methods can considerably reduce the size of artificial neural networks without harming their performance. In some cases, they can even uncover sub-networks that, when trained in isolation, match or surpass the test accuracy of their dense counterparts. Here we study the inductive bias that pruning imprints in such ""winning lottery tickets"". Focusing on visual tasks, we analyze the architecture resulting from iterative magnitude pruning of a simple fully connected network (FCN). We show that the surviving node connectivity is local in input space, and organized in patterns reminiscent of the ones found in convolutional networks (CNN). We investigate the role played by data and tasks in shaping the architecture of pruned sub-networks. Our results show that the winning lottery tickets of FCNs display the key features of CNNs. The ability of such automatic network-simplifying procedure to recover the key features ""hand-crafted"" in the design of CNNs suggests interesting applications to other datasets and tasks, in order to discover new and efficient architectural inductive biases.",2021-04-27T17:25:54Z,2021-05-14T10:52:49Z,http://arxiv.org/abs/2104.13343v2,http://arxiv.org/pdf/2104.13343v2,"cs.LG, cs.CV, stat.ML"
Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel   Training Dynamics,"Greg Yang, Etai Littwin","Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at initialization has an infinite-width limit for a large class of architectures including modern staples such as ResNet and Transformers. However, their analysis does not apply to training. Here, we show the same neural networks (in the so-called NTK parametrization) during training follow a kernel gradient descent dynamics in function space, where the kernel is the infinite-width NTK. This completes the proof of the *architectural universality* of NTK behavior. To achieve this result, we apply the Tensor Programs technique: Write the entire SGD dynamics inside a Tensor Program and analyze it via the Master Theorem. To facilitate this proof, we develop a graphical notation for Tensor Programs.",2021-05-08T14:05:01Z,2021-05-08T14:05:01Z,http://arxiv.org/abs/2105.03703v1,http://arxiv.org/pdf/2105.03703v1,"cs.LG, cs.NE, math.PR"
The Modern Mathematics of Deep Learning,"Julius Berner, Philipp Grohs, Gitta Kutyniok, Philipp Petersen","We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.",2021-05-09T21:30:42Z,2023-02-08T16:48:49Z,http://arxiv.org/abs/2105.04026v2,http://arxiv.org/pdf/2105.04026v2,"cs.LG, stat.ML"
High-Throughput VLSI architecture for Soft-Decision decoding with   ORBGRAND,"Syed Mohsin Abbas, Thibaud Tonnellier, Furkan Ercan, Marwan Jalaleddine, Warren J. Gross","Guessing Random Additive Noise Decoding (GRAND) is a recently proposed approximate Maximum Likelihood (ML) decoding technique that can decode any linear error-correcting block code. Ordered Reliability Bits GRAND (ORBGRAND) is a powerful variant of GRAND, which outperforms the original GRAND technique by generating error patterns in a specific order. Moreover, their simplicity at the algorithm level renders GRAND family a desirable candidate for applications that demand very high throughput. This work reports the first-ever hardware architecture for ORBGRAND, which achieves an average throughput of up to $42.5$ Gbps for a code length of $128$ at an SNR of $10$ dB. Moreover, the proposed hardware can be used to decode any code provided the length and rate constraints. Compared to the state-of-the-art fast dynamic successive cancellation flip decoder (Fast-DSCF) using a 5G polar $(128,105)$ code, the proposed VLSI implementation has $49\times$ more average throughput while maintaining similar decoding performance.",2021-05-15T01:38:52Z,2021-05-15T01:38:52Z,http://arxiv.org/abs/2105.07115v1,http://arxiv.org/pdf/2105.07115v1,"cs.IT, cs.AR, math.IT"
Physics Validation of Novel Convolutional 2D Architectures for Speeding   Up High Energy Physics Simulations,"Florian Rehm, Sofia Vallecorsa, Kerstin Borras, Dirk Krücker","The precise simulation of particle transport through detectors remains a key element for the successful interpretation of high energy physics results. However, Monte Carlo based simulation is extremely demanding in terms of computing resources. This challenge motivates investigations of faster, alternative approaches for replacing the standard Monte Carlo approach.   We apply Generative Adversarial Networks (GANs), a deep learning technique, to replace the calorimeter detector simulations and speeding up the simulation time by orders of magnitude. We follow a previous approach which used three-dimensional convolutional neural networks and develop new two-dimensional convolutional networks to solve the same 3D image generation problem faster. Additionally, we increased the number of parameters and the neural networks representational power, obtaining a higher accuracy. We compare our best convolutional 2D neural network architecture and evaluate it versus the previous 3D architecture and Geant4 data. Our results demonstrate a high physics accuracy and further consolidate the use of GANs for fast detector simulations.",2021-05-19T07:24:23Z,2021-05-19T07:24:23Z,http://arxiv.org/abs/2105.08960v1,http://arxiv.org/pdf/2105.08960v1,"hep-ex, cs.LG"
Anchor-based Plain Net for Mobile Image Super-Resolution,"Zongcai Du, Jie Liu, Jie Tang, Gangshan Wu","Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand plenty of computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training strategy to further boost the performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in terms of PSNR, while satisfying realistic needs at the same time. Code is avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.",2021-05-20T13:52:53Z,2021-09-25T02:19:22Z,http://arxiv.org/abs/2105.09750v2,http://arxiv.org/pdf/2105.09750v2,"eess.IV, cs.CV"
Functionals in the Clouds: An abstract architecture of serverless   Cloud-Native Apps,"Stanislaw Ambroszkiewicz, Waldemar Bartyna, Stanislaw Bylka","Cloud Native Application CNApp (as a distributed system) is a collection of independent components (micro-services) interacting via communication protocols. This gives rise to present an abstract architecture of CNApp as dynamically re-configurable acyclic directed multi graph where vertices are microservices, and edges are the protocols. Generic mechanisms for such reconfigurations evidently correspond to higher-level functions (functionals). This implies also internal abstract architecture of microservice as a collection of event-triggered serverless functions (including functions implementing the protocols) that are dynamically composed into event-dependent data-flow graphs. Again, generic mechanisms for such compositions correspond to calculus of functionals and relations.",2021-05-21T15:28:49Z,2022-08-25T15:58:50Z,http://arxiv.org/abs/2105.10362v5,http://arxiv.org/pdf/2105.10362v5,"cs.CL, cs.LO, 68M14, F.4.3; F.1.1"
Hyper-Convolution Networks for Biomedical Image Segmentation,"Tianyu Ma, Adrian V. Dalca, Mert R. Sabuncu","The convolution operation is a central building block of neural network architectures widely used in computer vision. The size of the convolution kernels determines both the expressiveness of convolutional neural networks (CNN), as well as the number of learnable parameters. Increasing the network capacity to capture rich pixel relationships requires increasing the number of learnable parameters, often leading to overfitting and/or lack of robustness. In this paper, we propose a powerful novel building block, the hyper-convolution, which implicitly represents the convolution kernel as a function of kernel coordinates. Hyper-convolutions enable decoupling the kernel size, and hence its receptive field, from the number of learnable parameters. In our experiments, focused on challenging biomedical image segmentation tasks, we demonstrate that replacing regular convolutions with hyper-convolutions leads to more efficient architectures that achieve improved accuracy. Our analysis also shows that learned hyper-convolutions are naturally regularized, which can offer better generalization performance. We believe that hyper-convolutions can be a powerful building block in future neural network architectures for computer vision tasks. We provide all of our code here: https://github.com/tym002/Hyper-Convolution",2021-05-21T20:31:08Z,2022-10-06T18:48:42Z,http://arxiv.org/abs/2105.10559v2,http://arxiv.org/pdf/2105.10559v2,"eess.IV, cs.CV"
AutoReCon: Neural Architecture Search-based Reconstruction for Data-free   Compression,"Baozhou Zhu, Peter Hofstee, Johan Peltenburg, Jinho Lee, Zaid Alars","Data-free compression raises a new challenge because the original training dataset for a pre-trained model to be compressed is not available due to privacy or transmission issues. Thus, a common approach is to compute a reconstructed training dataset before compression. The current reconstruction methods compute the reconstructed training dataset with a generator by exploiting information from the pre-trained model. However, current reconstruction methods focus on extracting more information from the pre-trained model but do not leverage network engineering. This work is the first to consider network engineering as an approach to design the reconstruction method. Specifically, we propose the AutoReCon method, which is a neural architecture search-based reconstruction method. In the proposed AutoReCon method, the generator architecture is designed automatically given the pre-trained model for reconstruction. Experimental results show that using generators discovered by the AutoRecon method always improve the performance of data-free compression.",2021-05-25T18:03:25Z,2021-05-25T18:03:25Z,http://arxiv.org/abs/2105.12151v1,http://arxiv.org/pdf/2105.12151v1,"cs.CV, eess.IV"
Least Redundant Gated Recurrent Neural Network,"Łukasz Neumann, Łukasz Lepak, Paweł Wawrzyński","Recurrent neural networks are important tools for sequential data processing. However, they are notorious for problems regarding their training. Challenges include capturing complex relations between consecutive states and stability and efficiency of training. In this paper, we introduce a recurrent neural architecture called Deep Memory Update (DMU). It is based on updating the previous memory state with a deep transformation of the lagged state and the network input. The architecture is able to learn to transform its internal state using any nonlinear function. Its training is stable and fast due to relating its learning rate to the size of the module. Even though DMU is based on standard components, experimental results presented here confirm that it can compete with and often outperform state-of-the-art architectures such as Long Short-Term Memory, Gated Recurrent Units, and Recurrent Highway Networks.",2021-05-28T20:24:00Z,2023-04-17T13:29:37Z,http://arxiv.org/abs/2105.14092v6,http://arxiv.org/pdf/2105.14092v6,"cs.NE, I.2.6"
Reinforcement Learning for on-line Sequence Transformation,"Grzegorz Rypeść, Łukasz Lepak, Paweł Wawrzyński","A number of problems in the processing of sound and natural language, as well as in other areas, can be reduced to simultaneously reading an input sequence and writing an output sequence of generally different length. There are well developed methods that produce the output sequence based on the entirely known input. However, efficient methods that enable such transformations on-line do not exist. In this paper we introduce an architecture that learns with reinforcement to make decisions about whether to read a token or write another token. This architecture is able to transform potentially infinite sequences on-line. In an experimental study we compare it with state-of-the-art methods for neural machine translation. While it produces slightly worse translations than Transformer, it outperforms the autoencoder with attention, even though our architecture translates texts on-line thereby solving a more difficult problem than both reference methods.",2021-05-28T20:31:25Z,2022-02-16T15:06:51Z,http://arxiv.org/abs/2105.14097v2,http://arxiv.org/pdf/2105.14097v2,"cs.LG, cs.CL, I.2.6"
How to Design a Three-Stage Architecture for Audio-Visual Active Speaker   Detection in the Wild,"Okan Köpüklü, Maja Taseska, Gerhard Rigoll","Successful active speaker detection requires a three-stage pipeline: (i) audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation modeling between a reference speaker and the background speakers within each frame, and (iii) temporal modeling for the reference speaker. Each stage of this pipeline plays an important role for the final performance of the created architecture. Based on a series of controlled experiments, this work presents several practical guidelines for audio-visual active speaker detection. Correspondingly, we present a new architecture called ASDNet, which achieves a new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5% outperforming the second best with a large margin of 4.7%. Our code and pretrained models are publicly available.",2021-06-07T19:44:56Z,2021-09-07T06:22:37Z,http://arxiv.org/abs/2106.03932v2,http://arxiv.org/pdf/2106.03932v2,"cs.CV, cs.LG, cs.SD, eess.AS"
Converged Reconfigurable Intelligent Surface and Mobile Edge Computing   for Space Information Networks,"Xuelin Cao, Bo Yang, Chongwen Huang, Chau Yuen, Yan Zhang, Dusit Niyato, Zhu Han","Space information networks (SIN) are facing an ever-increasing thirst for high-speed and high-capacity seamless data transmission due to the integration of ground, air, and space communications. However, this imposes a new paradigm on the architecture design of the integrated SIN. Recently, reconfigurable intelligent surfaces (RISs) and mobile edge computing (MEC) are the most promising techniques, conceived to improve communication and computation capability by reconfiguring the wireless propagation environment and offloading. Hence, converging RISs and MEC in SIN is becoming an effort to reap the double benefits of computation and communication. In this article, we propose an RIS-assisted collaborative MEC architecture for SIN and discuss its implementation. Then we present its potential benefits, major challenges, and feasible applications. Subsequently, we study different cases to evaluate the system data rate and latency. Finally, we conclude with a list of open issues in this research area.",2021-06-08T10:52:51Z,2021-06-08T10:52:51Z,http://arxiv.org/abs/2106.04248v1,http://arxiv.org/pdf/2106.04248v1,"cs.NI, eess.SP"
KIGLIS: Smart Networks for Smart Cities,"Daniel Bogdoll, Patrick Matalla, Christoph Füllner, Christian Raack, Shi Li, Tobias Käfer, Stefan Orf, Marc René Zofka, Finn Sartoris, Christoph Schweikert, Thomas Pfeiffer, André Richter, Sebastian Randel, Rene Bonk","Smart cities will be characterized by a variety of intelligent and networked services, each with specific requirements for the underlying network infrastructure. While smart city architectures and services have been studied extensively, little attention has been paid to the network technology. The KIGLIS research project, consisting of a consortium of companies, universities and research institutions, focuses on artificial intelligence for optimizing fiber-optic networks of a smart city, with a special focus on future mobility applications, such as automated driving. In this paper, we present early results on our process of collecting smart city requirements for communication networks, which will lead towards reference infrastructure and architecture solutions. Finally, we suggest directions in which artificial intelligence will improve smart city networks.",2021-05-14T15:06:08Z,2021-09-28T12:58:40Z,http://arxiv.org/abs/2106.04549v3,http://arxiv.org/pdf/2106.04549v3,"cs.NI, eess.SP"
CODA: Constructivism Learning for Instance-Dependent Dropout   Architecture Construction,Xiaoli Li,"Dropout is attracting intensive research interest in deep learning as an efficient approach to prevent overfitting. Recently incorporating structural information when deciding which units to drop out produced promising results comparing to methods that ignore the structural information. However, a major issue of the existing work is that it failed to differentiate among instances when constructing the dropout architecture. This can be a significant deficiency for many applications. To solve this issue, we propose Constructivism learning for instance-dependent Dropout Architecture (CODA), which is inspired from a philosophical theory, constructivism learning. Specially, based on the theory we have designed a better drop out technique, Uniform Process Mixture Models, using a Bayesian nonparametric method Uniform process. We have evaluated our proposed method on 5 real-world datasets and compared the performance with other state-of-the-art dropout techniques. The experimental results demonstrated the effectiveness of CODA.",2021-06-15T21:32:28Z,2021-06-15T21:32:28Z,http://arxiv.org/abs/2106.08444v1,http://arxiv.org/pdf/2106.08444v1,"cs.LG, stat.ML"
Techniques for Symbol Grounding with SATNet,"Sever Topan, David Rolnick, Xujie Si","Many experts argue that the future of artificial intelligence is limited by the field's ability to integrate symbolic logical reasoning into deep learning architectures. The recently proposed differentiable MAXSAT solver, SATNet, was a breakthrough in its capacity to integrate with a traditional neural network and solve visual reasoning problems. For instance, it can learn the rules of Sudoku purely from image examples. Despite its success, SATNet was shown to succumb to a key challenge in neurosymbolic systems known as the Symbol Grounding Problem: the inability to map visual inputs to symbolic variables without explicit supervision (""label leakage""). In this work, we present a self-supervised pre-training pipeline that enables SATNet to overcome this limitation, thus broadening the class of problems that SATNet architectures can solve to include datasets where no intermediary labels are available at all. We demonstrate that our method allows SATNet to attain full accuracy even with a harder problem setup that prevents any label leakage. We additionally introduce a proofreading method that further improves the performance of SATNet architectures, beating the state-of-the-art on Visual Sudoku.",2021-06-16T18:42:12Z,2021-06-16T18:42:12Z,http://arxiv.org/abs/2106.11072v1,http://arxiv.org/pdf/2106.11072v1,"cs.AI, cs.LG, stat.ML"
Should You Go Deeper? Optimizing Convolutional Neural Network   Architectures without Training by Receptive Field Analysis,"Mats L. Richter, Julius Schöning, Anna Wiedenroth, Ulf Krumnack","When optimizing convolutional neural networks (CNN) for a specific image-based task, specialists commonly overshoot the number of convolutional layers in their designs. By implication, these CNNs are unnecessarily resource intensive to train and deploy, with diminishing beneficial effects on the predictive performance.   The features a convolutional layer can process are strictly limited by its receptive field. By layer-wise analyzing the size of the receptive fields, we can reliably predict sequences of layers that will not contribute qualitatively to the test accuracy in the given CNN architecture. Based on this analysis, we propose design strategies based on a so-called border layer. This layer allows to identify unproductive convolutional layers and hence to resolve these inefficiencies, optimize the explainability and the computational performance of CNNs. Since neither the strategies nor the analysis requires training of the actual model, these insights allow for a very efficient design process of CNN architectures, which might be automated in the future.",2021-06-23T11:04:16Z,2021-10-05T19:19:25Z,http://arxiv.org/abs/2106.12307v2,http://arxiv.org/pdf/2106.12307v2,"cs.LG, cs.AI, cs.NE, stat.ML"
Evaluating Deep Neural Networks for Image Document Enhancement,"Lucas N. Kirsten, Ricardo Piccoli, Ricardo Ribani","This work evaluates six state-of-the-art deep neural network (DNN) architectures applied to the problem of enhancing camera-captured document images. The results from each network were evaluated both qualitatively and quantitatively using Image Quality Assessment (IQA) metrics, and also compared with an existing approach based on traditional computer vision techniques. The best performing architectures generally produced good enhancement compared to the existing algorithm, showing that it is possible to use DNNs for document image enhancement. Furthermore, the best performing architectures could work as a baseline for future investigations on document enhancement using deep learning techniques. The main contributions of this paper are: a baseline of deep learning techniques that can be further improved to provide better results, and a evaluation methodology using IQA metrics for quantitatively comparing the produced images from the neural networks to a ground truth.",2021-06-11T19:48:28Z,2021-06-11T19:48:28Z,http://arxiv.org/abs/2106.15286v1,http://arxiv.org/pdf/2106.15286v1,"cs.CV, cs.LG, eess.IV, I.4.3; I.2.10"
DF-Conformer: Integrated architecture of Conv-TasNet and Conformer using   linear complexity self-attention for speech enhancement,"Yuma Koizumi, Shigeki Karita, Scott Wisdom, Hakan Erdogan, John R. Hershey, Llion Jones, Michiel Bacchiani","Single-channel speech enhancement (SE) is an important task in speech processing. A widely used framework combines an analysis/synthesis filterbank with a mask prediction network, such as the Conv-TasNet architecture. In such systems, the denoising performance and computational efficiency are mainly affected by the structure of the mask prediction network. In this study, we aim to improve the sequential modeling ability of Conv-TasNet architectures by integrating Conformer layers into a new mask prediction network. To make the model computationally feasible, we extend the Conformer using linear complexity attention and stacked 1-D dilated depthwise convolution layers. We trained the model on 3,396 hours of noisy speech data, and show that (i) the use of linear complexity attention avoids high computational complexity, and (ii) our model achieves higher scale-invariant signal-to-noise ratio than the improved time-dilated convolution network (TDCN++), an extended version of Conv-TasNet.",2021-06-30T05:11:29Z,2021-08-05T08:26:02Z,http://arxiv.org/abs/2106.15813v2,http://arxiv.org/pdf/2106.15813v2,"eess.AS, cs.SD"
ESPnet-ST IWSLT 2021 Offline Speech Translation System,"Hirofumi Inaguma, Brian Yan, Siddharth Dalmia, Pengcheng Guo, Jiatong Shi, Kevin Duh, Shinji Watanabe","This paper describes the ESPnet-ST group's IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-Decoder architecture, which equips dedicated decoders for speech recognition and translation tasks in a unified encoder-decoder model and enables search in both source and target language spaces during inference. We also significantly improved audio segmentation by using the pyannote.audio toolkit and merging multiple short segments for long context modeling. Experimental evaluations showed that each of them contributed to large improvements in translation performance. Our best E2E system combined all the above techniques with model ensembling and achieved 31.4 BLEU on the 2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of tst2021.",2021-07-01T17:49:43Z,2021-07-06T15:43:01Z,http://arxiv.org/abs/2107.00636v2,http://arxiv.org/pdf/2107.00636v2,"eess.AS, cs.CL, cs.SD"
Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech   Recognition,"Niko Moritz, Takaaki Hori, Jonathan Le Roux","Attention-based end-to-end automatic speech recognition (ASR) systems have recently demonstrated state-of-the-art results for numerous tasks. However, the application of self-attention and attention-based encoder-decoder models remains challenging for streaming ASR, where each word must be recognized shortly after it was spoken. In this work, we present the dual causal/non-causal self-attention (DCN) architecture, which in contrast to restricted self-attention prevents the overall context to grow beyond the look-ahead of a single layer when used in a deep architecture. DCN is compared to chunk-based and restricted self-attention using streaming transformer and conformer architectures, showing improved ASR performance over restricted self-attention and competitive ASR results compared to chunk-based self-attention, while providing the advantage of frame-synchronous processing. Combined with triggered attention, the proposed streaming end-to-end ASR systems obtained state-of-the-art results on the LibriSpeech, HKUST, and Switchboard ASR tasks.",2021-07-02T20:56:13Z,2021-07-02T20:56:13Z,http://arxiv.org/abs/2107.01269v1,http://arxiv.org/pdf/2107.01269v1,"eess.AS, cs.LG, cs.SD"
Generalization by design: Shortcuts to Generalization in Deep Learning,"Petr Taborsky, Lars Kai Hansen","We take a geometrical viewpoint and present a unifying view on supervised deep learning with the Bregman divergence loss function - this entails frequent classification and prediction tasks. Motivated by simulations we suggest that there is principally no implicit bias of vanilla stochastic gradient descent training of deep models towards ""simpler"" functions. Instead, we show that good generalization may be instigated by bounded spectral products over layers leading to a novel geometric regularizer. It is revealed that in deep enough models such a regularizer enables both, extreme accuracy and generalization, to be reached. We associate popular regularization techniques like weight decay, drop out, batch normalization, and early stopping with this perspective. Backed up by theory we further demonstrate that ""generalization by design"" is practically possible and that good generalization may be encoded into the structure of the network. We design two such easy-to-use structural regularizers that insert an additional \textit{generalization layer} into a model architecture, one with a skip connection and another one with drop-out. We verify our theoretical results in experiments on various feedforward and convolutional architectures, including ResNets, and datasets (MNIST, CIFAR10, synthetic data). We believe this work opens up new avenues of research towards better generalizing architectures.",2021-07-05T20:01:23Z,2021-07-05T20:01:23Z,http://arxiv.org/abs/2107.02253v1,http://arxiv.org/pdf/2107.02253v1,"cs.LG, math.DG, math.PR"
Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of   Absolute Difference,"Saeideh Nabipour, Masoume Gholizade, Nima Nabipour","Euclids algorithm is widely used in calculating of GCD (Greatest Common Divisor) of two positive numbers. There are various fields where this division is used such as channel coding, cryptography, and error correction codes. This makes the GCD a fundamental algorithm in number theory, so a number of methods have been discovered to efficiently compute it. The main contribution of this paper is to investigate a method that computes the GCD of two 32-bit numbers based on Euclidean algorithm which targets six different Xilinx chips. The complexity of this method that we call Optimized_GCDSAD is achieved by utilizing Sum of Absolute Difference (SAD) block which is based on a fast carry-out generation function. The efficiency of the proposed architecture is evaluated based on criteria such as time (latency), area delay product (ADP) and space (slice number) complexity. The VHDL codes of these architectures have been implemented and synthesized through ISE 14.7. A detailed comparative analysis indicates that the proposed Optimized_GCDSAD method based on SAD block outperforms previously known results.",2021-06-06T10:40:42Z,2022-11-17T15:11:34Z,http://arxiv.org/abs/2107.02762v2,http://arxiv.org/pdf/2107.02762v2,"cs.AR, cs.CR, eess.SP"
Deep Autoregressive Models with Spectral Attention,"Fernando Moreno-Pino, Pablo M. Olmos, Antonio Artés-Rodríguez","Time series forecasting is an important problem across many domains, playing a crucial role in multiple real-world applications. In this paper, we propose a forecasting architecture that combines deep autoregressive models with a Spectral Attention (SA) module, which merges global and local frequency domain information in the model's embedded space. By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns. Two spectral attention models, global and local to the time series, integrate this information within the forecast and perform spectral filtering to remove time series's noise. The proposed architecture has a number of useful properties: it can be effectively incorporated into well-know forecast architectures, requiring a low number of parameters and producing interpretable results that improve forecasting accuracy. We test the Spectral Attention Autoregressive Model (SAAM) on several well-know forecast datasets, consistently demonstrating that our model compares favorably to state-of-the-art approaches.",2021-07-13T11:08:47Z,2021-12-26T15:39:51Z,http://arxiv.org/abs/2107.05984v2,http://arxiv.org/pdf/2107.05984v2,"stat.ML, cs.LG"
Generalisation in Neural Networks Does not Require Feature Overlap,"Jeff Mitchell, Jeffrey S. Bowers","That shared features between train and test data are required for generalisation in artificial neural networks has been a common assumption of both proponents and critics of these models. Here, we show that convolutional architectures avoid this limitation by applying them to two well known challenges, based on learning the identity function and learning rules governing sequences of words. In each case, successful performance on the test set requires generalising to features that were not present in the training data, which is typically not feasible for standard connectionist models. However, our experiments demonstrate that neural networks can succeed on such problems when they incorporate the weight sharing employed by convolutional architectures. In the image processing domain, such architectures are intended to reflect the symmetry under spatial translations of the natural world that such images depict. We discuss the role of symmetry in the two tasks and its connection to generalisation.",2021-07-04T09:23:49Z,2021-07-04T09:23:49Z,http://arxiv.org/abs/2107.06872v1,http://arxiv.org/pdf/2107.06872v1,"cs.NE, cs.AI, cs.LG, 68T07, I.2.6; J.4"
A method for decompilation of AMD GCN kernels to OpenCL,"K. I. Mihajlenko, M. A. Lukin, A. S. Stankevich","Introduction: Decompilers are useful tools for software analysis and support in the absence of source code. They are available for many hardware architectures and programming languages. However, none of the existing decompilers support modern AMD GPU architectures such as AMD GCN and RDNA. Purpose: We aim at developing the first assembly decompiler tool for a modern AMD GPU architecture that generates code in the OpenCL language, which is widely used for programming GPGPUs. Results: We developed the algorithms for the following operations: preprocessing assembly code, searching data accesses, extracting system values, decompiling arithmetic operations and recovering data types. We also developed templates for decompilation of branching operations. Practical relevance: We implemented the presented algorithms in Python as a tool called OpenCLDecompiler, which supports a large subset of AMD GCN instructions. This tool automatically converts disassembled GPGPU code into the equivalent OpenCL code, which reduces the effort required to analyze assembly code.",2021-07-16T10:32:54Z,2021-07-16T10:32:54Z,http://arxiv.org/abs/2107.07809v1,http://arxiv.org/pdf/2107.07809v1,"cs.PL, cs.DC, 68N20, D.3.m"
A unified polar decoder platform for low-power and low-cost devices,"Jiajie Tong, Qifan Zhang, Huazi Zhang, Rong Li, Jun Wang, Wen Tong","In this paper, we design a polar decoding platform for diverse application scenarios that require low-cost and low-power communications. Specifically, prevalent polar decoders such as successive cancellation (SC), SC-list (SCL) and Fano decoders are all supported under the same architecture. Unlike high-throughput or low-latency decoders that promote parallelism, this architecture promotes serialization by repeatedly calling a ``sub-process'' that is executed by a core module. The resulting serial SCL-8 decoder is only 3 times as big as an SC decoder. Cost and power are minimized through resource sharing and adaptive decoding techniques, etc. We carried out performance simulation and hardware implementation to evaluate the actual chip area and energy consumption.",2021-07-19T03:43:30Z,2021-07-19T03:43:30Z,http://arxiv.org/abs/2107.08607v1,http://arxiv.org/pdf/2107.08607v1,"cs.IT, cs.AR, math.IT"
An induction proof of the backpropagation algorithm in matrix notation,"Dirk Ostwald, Franziska Usée","Backpropagation (BP) is a core component of the contemporary deep learning incarnation of neural networks. Briefly, BP is an algorithm that exploits the computational architecture of neural networks to efficiently evaluate the gradient of a cost function during neural network parameter optimization. The validity of BP rests on the application of a multivariate chain rule to the computational architecture of neural networks and their associated objective functions. Introductions to deep learning theory commonly present the computational architecture of neural networks in matrix form, but eschew a parallel formulation and justification of BP in the framework of matrix differential calculus. This entails several drawbacks for the theory and didactics of deep learning. In this work, we overcome these limitations by providing a full induction proof of the BP algorithm in matrix notation. Specifically, we situate the BP algorithm in the framework of matrix differential calculus, encompass affine-linear potential functions, prove the validity of the BP algorithm in inductive form, and exemplify the implementation of the matrix form BP algorithm in computer code.",2021-07-20T10:02:17Z,2021-07-20T10:02:17Z,http://arxiv.org/abs/2107.09384v1,http://arxiv.org/pdf/2107.09384v1,"stat.ML, cs.LG, math.ST, q-bio.NC, stat.TH"
Architecture-Guided Test Resource Allocation Via Logic,"Clovis Eberhart, Akihisa Yamada, Stefan Klikovits, Shin-ya Katsumata, Tsutomu Kobayashi, Ichiro Hasuo, Fuyuki Ishikawa","We introduce a new logic named Quantitative Confidence Logic (QCL) that quantifies the level of confidence one has in the conclusion of a proof. By translating a fault tree representing a system's architecture to a proof, we show how to use QCL to give a solution to the test resource allocation problem that takes the given architecture into account. We implemented a tool called Astrahl and compared our results to other testing resource allocation strategies.",2021-07-22T22:31:49Z,2021-07-22T22:31:49Z,http://arxiv.org/abs/2107.10948v1,http://arxiv.org/pdf/2107.10948v1,"cs.SE, D.2.5"
Beyond Voice Identity Conversion: Manipulating Voice Attributes by   Adversarial Learning of Structured Disentangled Representations,"Laurent Benaroya, Nicolas Obin, Axel Roebel","Voice conversion (VC) consists of digitally altering the voice of an individual to manipulate part of its content, primarily its identity, while maintaining the rest unchanged. Research in neural VC has accomplished considerable breakthroughs with the capacity to falsify a voice identity using a small amount of data with a highly realistic rendering. This paper goes beyond voice identity and presents a neural architecture that allows the manipulation of voice attributes (e.g., gender and age). Leveraging the latest advances on adversarial learning of structured speech representation, a novel structured neural network is proposed in which multiple auto-encoders are used to encode speech as a set of idealistically independent linguistic and extra-linguistic representations, which are learned adversariarly and can be manipulated during VC. Moreover, the proposed architecture is time-synchronized so that the original voice timing is preserved during conversion which allows lip-sync applications. Applied to voice gender conversion on the real-world VCTK dataset, our proposed architecture can learn successfully gender-independent representation and convert the voice gender with a very high efficiency and naturalness.",2021-07-26T17:40:43Z,2021-07-27T16:49:15Z,http://arxiv.org/abs/2107.12346v2,http://arxiv.org/pdf/2107.12346v2,"cs.SD, cs.LG, eess.AS"
Utilizing Shannon's Entropy to Create Privacy Aware Architectures,"Abhinav Palia, Rajat Tandon, Carl Mathis","Privacy is an individual choice to determine which personal details can be collected, used and shared. Individual consent and transparency are the core tenets for earning customers trust and this motivates the organizations to adopt privacy enhancing practices while creating the systems. The goal of a privacy-aware design is to protect information in a way that does not increase an adversary's existing knowledge about an individual beyond what is permissible. This becomes critical when these data elements can be linked with the wealth of auxiliary information available outside the system to identify an individual. Privacy regulations around the world provide directives to protect individual privacy but are generally complex and vague, making their translation into actionable and technical privacy-friendly architectures challenging. In this paper, we utilize Shannon's Entropy to create an objective metric that can help simplify the state-of-the-art Privacy Design Strategies proposed in the literature and aid our key technical design decisions to create privacy aware architectures.",2021-09-10T03:31:45Z,2021-09-13T04:32:23Z,http://arxiv.org/abs/2109.04649v2,http://arxiv.org/pdf/2109.04649v2,"cs.CR, cs.DB, cs.IT, math.IT"
MSGDD-cGAN: Multi-Scale Gradients Dual Discriminator Conditional   Generative Adversarial Network,"Mohammadreza Naderi, Zahra Nabizadeh, Nader Karimi, Shahram Shirani, Shadrokh Samavi","Conditional Generative Adversarial Networks (cGANs) have been used in many image processing tasks. However, they still have serious problems maintaining the balance between conditioning the output on the input and creating the output with the desired distribution based on the corresponding ground truth. The traditional cGANs, similar to most conventional GANs, suffer from vanishing gradients, which backpropagate from the discriminator to the generator. Moreover, the traditional cGANs are sensitive to architectural changes due to previously mentioned gradient problems. Therefore, balancing the architecture of the cGANs is almost impossible. Recently MSG-GAN has been proposed to stabilize the performance of the GANs by applying multiple connections between the generator and discriminator. In this work, we propose a method called MSGDD-cGAN, which first stabilizes the performance of the cGANs using multi-connections gradients flow. Secondly, the proposed network architecture balances the correlation of the output to input and the fitness of the output on the target distribution. This balance is generated by using the proposed dual discrimination procedure. We tested our model by segmentation of fetal ultrasound images. Our model shows a 3.18% increase in the F1 score comparing to the pix2pix version of cGANs.",2021-09-12T21:08:37Z,2021-09-12T21:08:37Z,http://arxiv.org/abs/2109.05614v1,http://arxiv.org/pdf/2109.05614v1,"cs.CV, eess.IV"
Real-Time EMG Signal Classification via Recurrent Neural Networks,"Reza Bagherian Azhiri, Mohammad Esmaeili, Mehrdad Nourani","Real-time classification of Electromyography signals is the most challenging part of controlling a prosthetic hand. Achieving a high classification accuracy of EMG signals in a short delay time is still challenging. Recurrent neural networks (RNNs) are artificial neural network architectures that are appropriate for sequential data such as EMG. In this paper, after extracting features from a hybrid time-frequency domain (discrete Wavelet transform), we utilize a set of recurrent neural network-based architectures to increase the classification accuracy and reduce the prediction delay time. The performances of these architectures are compared and in general outperform other state-of-the-art methods by achieving 96% classification accuracy in 600 msec.",2021-09-13T02:36:44Z,2021-09-13T02:36:44Z,http://arxiv.org/abs/2109.05674v1,http://arxiv.org/pdf/2109.05674v1,"eess.SP, cs.CV, cs.LG, cs.RO"
Eformer: Edge Enhancement based Transformer for Medical Image Denoising,"Achleshwar Luthra, Harsh Sulakhe, Tanish Mittal, Abhishek Iyer, Santosh Yadav","In this work, we present Eformer - Edge enhancement based transformer, a novel architecture that builds an encoder-decoder network using transformer blocks for medical image denoising. Non-overlapping window-based self-attention is used in the transformer block that reduces computational requirements. This work further incorporates learnable Sobel-Feldman operators to enhance edges in the image and propose an effective way to concatenate them in the intermediate layers of our architecture. The experimental analysis is conducted by comparing deterministic learning and residual learning for the task of medical image denoising. To defend the effectiveness of our approach, our model is evaluated on the AAPM-Mayo Clinic Low-Dose CT Grand Challenge Dataset and achieves state-of-the-art performance, $i.e.$, 43.487 PSNR, 0.0067 RMSE, and 0.9861 SSIM. We believe that our work will encourage more research in transformer-based architectures for medical image denoising using residual learning.",2021-09-16T15:18:21Z,2021-11-09T11:18:07Z,http://arxiv.org/abs/2109.08044v2,http://arxiv.org/pdf/2109.08044v2,"eess.IV, cs.CV"
Dual-Encoder Architecture with Encoder Selection for Joint Close-Talk   and Far-Talk Speech Recognition,"Felix Weninger, Marco Gaudesi, Ralf Leibold, Roberto Gemello, Puming Zhan","In this paper, we propose a dual-encoder ASR architecture for joint modeling of close-talk (CT) and far-talk (FT) speech, in order to combine the advantages of CT and FT devices for better accuracy. The key idea is to add an encoder selection network to choose the optimal input source (CT or FT) and the corresponding encoder. We use a single-channel encoder for CT speech and a multi-channel encoder with Spatial Filtering neural beamforming for FT speech, which are jointly trained with the encoder selection. We validate our approach on both attention-based and RNN Transducer end-to-end ASR systems. The experiments are done with conversational speech from a medical use case, which is recorded simultaneously with a CT device and a microphone array. Our results show that the proposed dual-encoder architecture obtains up to 9% relative WER reduction when using both CT and FT input, compared to the best single-encoder system trained and tested in matched condition.",2021-09-17T19:52:47Z,2021-09-17T19:52:47Z,http://arxiv.org/abs/2109.08744v1,http://arxiv.org/pdf/2109.08744v1,"eess.AS, cs.LG"
Architecture and Its Vulnerabilities in Smart-Lighting Systems,"Florian Hofer, Barbara Russo","Industry 4.0 embodies one of the significant technological changes of this decade. Cyber-physical systems and the Internet Of Things are two central technologies in this change that embed or connect with sensors and actuators and interact with the physical environment. However, such systems-of-systems undergo additional restrictions in an endeavor to maintain reliability and security when building and interconnecting components to a heterogeneous, multi-domain \textit{Smart-*} systems architecture. This paper presents an application-specific, layer-based approach to an offline security analysis inspired by design science that merges preceding expertise from relevant domains. With the example of a Smart-lighting system, we create a dedicated unified taxonomy for the use case and analyze its distributed Smart-* architecture by multiple layer-based models. We derive potential attacks from the system specifications in an iterative and incremental process and discuss resulting threats and vulnerabilities. Finally, we suggest immediate countermeasures for the latter potential multiple-domain security concerns.",2021-09-19T17:24:22Z,2021-09-19T17:24:22Z,http://arxiv.org/abs/2109.09171v1,http://arxiv.org/pdf/2109.09171v1,"cs.CR, cs.SY, eess.SY"
A Data-Driven Democratized Control Architecture for Regional   Transmission Operators,"Xiaoyuan Fan, Daniel Moscovitz, Liang Du, Walid Saad","As probably the most complicated and critical infrastructure system, U.S. power grids become increasingly vulnerable to extreme events such as cyber-attacks and severe weather, as well as higher DER penetrations and growing information mismatch among system operators, utilities (transmission or generation owners), and end-users. This paper proposes a data-driven democratized control architecture considering two democratization pathways to assist transmission system operators, with a targeted use case of developing online proactive islanding strategies. Detailed discussions on load capability profiling at transmission buses and disaggregation of DER generations are provided and illustrated with real-world utility data. By Combining network and operational constraints, transmission system operators can be equipped with new tools built on top of this architecture, to derive accurate, proactive, and strategic islanding decisions to incorporate the wide range of dynamic portfolios and needs when facing extreme events or unseen grid contingencies.",2021-09-20T19:43:08Z,2021-09-20T19:43:08Z,http://arxiv.org/abs/2109.09813v1,http://arxiv.org/pdf/2109.09813v1,"eess.SY, cs.SY"
Any-to-any connected cavity-mediated architecture for quantum computing   with trapped ions or Rydberg arrays,"Joshua Ramette, Josiah Sinclair, Zachary Vendeiro, Alyssa Rudelis, Marko Cetina, Vladan Vuletić","We propose a hardware architecture and protocol for connecting many local quantum processors contained within an optical cavity. The scheme is compatible with trapped ions or Rydberg arrays, and realizes teleported gates between any two qubits by distributing entanglement via single-photon transfers through a cavity. Heralding enables high-fidelity entanglement even for a cavity of moderate quality. For processors composed of trapped ions in a linear chain, a single cavity with realistic parameters successfully transfers photons every few $\mu$s, enabling the any-to-any entanglement of 20 ion chains containing a total of 500 qubits in 200 $\mu$s, with both fidelities and rates limited only by local operations and ion readout. For processors composed of Rydberg atoms, our method fully connects a large array of thousands of neutral atoms. The connectivity afforded by our architecture is extendable to tens of thousands of qubits using multiple overlapping cavities, expanding capabilities for NISQ era algorithms and Hamiltonian simulations, as well as enabling more robust high-dimensional error correcting schemes.",2021-09-23T18:00:00Z,2021-09-23T18:00:00Z,http://arxiv.org/abs/2109.11551v1,http://arxiv.org/pdf/2109.11551v1,"quant-ph, physics.atom-ph"
Document Automation Architectures and Technologies: A Survey,"Mohammad Ahmadi Achachlouei, Omkar Patil, Tarun Joshi, Vijayan N. Nair","This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in artificial intelligence and deep neural networks.",2021-09-23T19:12:26Z,2021-09-23T19:12:26Z,http://arxiv.org/abs/2109.11603v1,http://arxiv.org/pdf/2109.11603v1,"cs.CL, cs.LG, 68T50, I.7.0; I.2.7; I.2.4"
Graph Representation Learning for Spatial Image Steganalysis,"Qiyun Liu, Hanzhou Wu","In this paper, we introduce a graph representation learning architecture for spatial image steganalysis, which is motivated by the assumption that steganographic modifications unavoidably distort the statistical characteristics of the hidden graph features derived from cover images. In the detailed architecture, we translate each image to a graph, where nodes represent the patches of the image and edges indicate the local relationships between the patches. Each node is associated with a feature vector determined from the corresponding patch by a shallow convolutional neural network (CNN) structure. By feeding the graph to an attention network, the discriminative features can be learned for efficient steganalysis. Experiments indicate that the reported architecture achieves a competitive performance compared to the benchmark CNN model, which has shown the potential of graph learning for steganalysis.",2021-10-03T09:09:08Z,2022-02-15T08:14:43Z,http://arxiv.org/abs/2110.00957v3,http://arxiv.org/pdf/2110.00957v3,"cs.MM, cs.CV, eess.IV"
Feasible Architecture for Quantum Fully Convolutional Networks,"Yusui Chen, Wenhao Hu, Xiang Li","Fully convolutional networks are robust in performing semantic segmentation, with many applications from signal processing to computer vision. From the fundamental principles of variational quantum algorithms, we propose a feasible pure quantum architecture that can be operated on noisy intermediate-scale quantum devices. In this work, a parameterized quantum circuit consisting of three layers, convolutional, pooling, and upsampling, is characterized by generative one-qubit and two-qubit gates and driven by a classical optimizer. This architecture supplies a solution for realizing the dynamical programming on a one-way quantum computer and maximally taking advantage of quantum computing throughout the calculation. Moreover, our algorithm works on many physical platforms, and particularly the upsampling layer can use either conventional qubits or multiple-level systems. Through numerical simulations, our study represents the successful training of a pure quantum fully convolutional network and discusses advantages by comparing it with the hybrid solution.",2021-10-05T01:06:54Z,2021-10-05T01:06:54Z,http://arxiv.org/abs/2110.01771v1,http://arxiv.org/pdf/2110.01771v1,"quant-ph, cs.AI"
SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition,"Jing Pan, Tao Lei, Kwangyoun Kim, Kyu Han, Shinji Watanabe","The Transformer architecture has been well adopted as a dominant architecture in most sequence transduction tasks including automatic speech recognition (ASR), since its attention mechanism excels in capturing long-range dependencies. While models built solely upon attention can be better parallelized than regular RNN, a novel network architecture, SRU++, was recently proposed. By combining the fast recurrence and attention mechanism, SRU++ exhibits strong capability in sequence modeling and achieves near-state-of-the-art results in various language modeling and machine translation tasks with improved compute efficiency. In this work, we present the advantages of applying SRU++ in ASR tasks by comparing with Conformer across multiple ASR benchmarks and study how the benefits can be generalized to long-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model achieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive performances compared with the state-of-the-art Conformer encoder under the same set-up. Specifically, SRU++ can surpass Conformer on long-form speech input with a large margin, based on our analysis.",2021-10-11T19:23:50Z,2021-10-11T19:23:50Z,http://arxiv.org/abs/2110.05571v1,http://arxiv.org/pdf/2110.05571v1,"eess.AS, cs.CL"
Style-based quantum generative adversarial networks for Monte Carlo   events,"Carlos Bravo-Prieto, Julien Baglio, Marco Cè, Anthony Francis, Dorota M. Grabowska, Stefano Carrazza","We propose and assess an alternative quantum generator architecture in the context of generative adversarial learning for Monte Carlo event generation, used to simulate particle physics processes at the Large Hadron Collider (LHC). We validate this methodology by implementing the quantum network on artificial data generated from known underlying distributions. The network is then applied to Monte Carlo-generated datasets of specific LHC scattering processes. The new quantum generator architecture leads to a generalization of the state-of-the-art implementations, achieving smaller Kullback-Leibler divergences even with shallow-depth networks. Moreover, the quantum generator successfully learns the underlying distribution functions even if trained with small training sample sets; this is particularly interesting for data augmentation applications. We deploy this novel methodology on two different quantum hardware architectures, trapped-ion and superconducting technologies, to test its hardware-independent viability.",2021-10-13T18:00:01Z,2022-08-06T06:09:48Z,http://arxiv.org/abs/2110.06933v2,http://arxiv.org/pdf/2110.06933v2,"quant-ph, cs.LG, hep-ph"
Discrete Acoustic Space for an Efficient Sampling in Neural   Text-To-Speech,"Marek Strong, Jonas Rohnke, Antonio Bonafonte, Mateusz Łajszczak, Trevor Wood","We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE) architecture using a split vector quantizer for NTTS, as an enhancement to the well-known Variational Autoencoder (VAE) and Vector Quantized Variational Autoencoder (VQ-VAE) architectures. Compared to these previous architectures, our proposed model retains the benefits of using an utterance-level bottleneck, while keeping significant representation power and a discretized latent space small enough for efficient prediction from text. We train the model on recordings in the expressive task-oriented dialogues domain and show that SVQ-VAE achieves a statistically significant improvement in naturalness over the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent acoustic space is predictable from text, reducing the gap between the standard constant vector synthesis and vocoded recordings by 32%.",2021-10-24T22:15:01Z,2023-09-14T12:34:51Z,http://arxiv.org/abs/2110.12539v3,http://arxiv.org/pdf/2110.12539v3,"cs.SD, cs.LG, eess.AS"
Understanding How Encoder-Decoder Architectures Attend,"Kyle Aitken, Vinay V Ramasesh, Yuan Cao, Niru Maheswaranathan","Encoder-decoder networks with attention have proven to be a powerful way to solve many sequence-to-sequence tasks. In these networks, attention aligns encoder and decoder states and is often used for visualizing network behavior. However, the mechanisms used by networks to generate appropriate attention matrices are still mysterious. Moreover, how these mechanisms vary depending on the particular architecture used for the encoder and decoder (recurrent, feed-forward, etc.) are also not well understood. In this work, we investigate how encoder-decoder networks solve different sequence-to-sequence tasks. We introduce a way of decomposing hidden states over a sequence into temporal (independent of input) and input-driven (independent of sequence position) components. This reveals how attention matrices are formed: depending on the task requirements, networks rely more heavily on either the temporal or input-driven components. These findings hold across both recurrent and feed-forward architectures despite their differences in forming the temporal components. Overall, our results provide new insight into the inner workings of attention-based encoder-decoder networks.",2021-10-28T16:11:27Z,2021-10-28T16:11:27Z,http://arxiv.org/abs/2110.15253v1,http://arxiv.org/pdf/2110.15253v1,"cs.LG, stat.ML"
Scalable Geometric Deep Learning on Molecular Graphs,"Nathan C. Frey, Siddharth Samsi, Joseph McDonald, Lin Li, Connor W. Coley, Vijay Gadepally","Deep learning in molecular and materials sciences is limited by the lack of integration between applied science, artificial intelligence, and high-performance computing. Bottlenecks with respect to the amount of training data, the size and complexity of model architectures, and the scale of the compute infrastructure are all key factors limiting the scaling of deep learning for molecules and materials. Here, we present $\textit{LitMatter}$, a lightweight framework for scaling molecular deep learning methods. We train four graph neural network architectures on over 400 GPUs and investigate the scaling behavior of these methods. Depending on the model architecture, training time speedups up to $60\times$ are seen. Empirical neural scaling relations quantify the model-dependent scaling and enable optimal compute resource allocation and the identification of scalable molecular geometric deep learning model implementations.",2021-12-06T21:29:38Z,2021-12-06T21:29:38Z,http://arxiv.org/abs/2112.03364v1,http://arxiv.org/pdf/2112.03364v1,"cs.LG, cond-mat.mtrl-sci, physics.chem-ph"
Symmetry Perception by Deep Networks: Inadequacy of Feed-Forward   Architectures and Improvements with Recurrent Connections,"Shobhita Sundaram, Darius Sinha, Matthew Groth, Tomotake Sasaki, Xavier Boix","Symmetry is omnipresent in nature and perceived by the visual system of many species, as it facilitates detecting ecologically important classes of objects in our environment. Symmetry perception requires abstraction of long-range spatial dependencies between image regions, and its underlying neural mechanisms remain elusive. In this paper, we evaluate Deep Neural Network (DNN) architectures on the task of learning symmetry perception from examples. We demonstrate that feed-forward DNNs that excel at modelling human performance on object recognition tasks, are unable to acquire a general notion of symmetry. This is the case even when the DNNs are architected to capture long-range spatial dependencies, such as through `dilated' convolutions and the recently introduced `transformers' design. By contrast, we find that recurrent architectures are capable of learning to perceive symmetry by decomposing the long-range spatial dependencies into a sequence of local operations, that are reusable for novel images. These results suggest that recurrent connections likely play an important role in symmetry perception in artificial systems, and possibly, biological ones too.",2021-12-08T08:21:25Z,2022-01-22T04:54:26Z,http://arxiv.org/abs/2112.04162v2,http://arxiv.org/pdf/2112.04162v2,"cs.CV, cs.LG, q-bio.NC"
Learning over All Stabilizing Nonlinear Controllers for a   Partially-Observed Linear System,"Ruigang Wang, Nicholas H. Barbara, Max Revay, Ian R. Manchester","This paper proposes a nonlinear policy architecture for control of partially-observed linear dynamical systems providing built-in closed-loop stability guarantees. The policy is based on a nonlinear version of the Youla parameterization, and augments a known stabilizing linear controller with a nonlinear operator from a recently developed class of dynamic neural network models called the recurrent equilibrium network (REN). We prove that RENs are universal approximators of contracting and Lipschitz nonlinear systems, and subsequently show that the the proposed Youla-REN architecture is a universal approximator of stabilizing nonlinear controllers. The REN architecture simplifies learning since unconstrained optimization can be applied, and we consider both a model-based case where exact gradients are available and reinforcement learning using random search with zeroth-order oracles. In simulation examples our method converges faster to better controllers and is more scalable than existing methods, while guaranteeing stability during learning transients.",2021-12-08T10:43:47Z,2022-06-17T05:43:03Z,http://arxiv.org/abs/2112.04219v3,http://arxiv.org/pdf/2112.04219v3,"eess.SY, cs.LG, cs.SY, math.OC"
Quantum Architecture Search via Continual Reinforcement Learning,"Esther Ye, Samuel Yen-Chi Chen","Quantum computing has promised significant improvement in solving difficult computational tasks over classical computers. Designing quantum circuits for practical use, however, is not a trivial objective and requires expert-level knowledge. To aid this endeavor, this paper proposes a machine learning-based method to construct quantum circuit architectures. Previous works have demonstrated that classical deep reinforcement learning (DRL) algorithms can successfully construct quantum circuit architectures without encoded physics knowledge. However, these DRL-based works are not generalizable to settings with changing device noises, thus requiring considerable amounts of training resources to keep the RL models up-to-date. With this in mind, we incorporated continual learning to enhance the performance of our algorithm. In this paper, we present the Probabilistic Policy Reuse with deep Q-learning (PPR-DQL) framework to tackle this circuit design challenge. By conducting numerical simulations over various noise patterns, we demonstrate that the RL agent with PPR was able to find the quantum gate sequence to generate the two-qubit Bell state faster than the agent that was trained from scratch. The proposed framework is general and can be applied to other quantum gate synthesis or control problems -- including the automatic calibration of quantum devices.",2021-12-10T19:07:56Z,2021-12-10T19:07:56Z,http://arxiv.org/abs/2112.05779v1,http://arxiv.org/pdf/2112.05779v1,"quant-ph, cs.AI, cs.ET, cs.LG, cs.NE"
Public Release and Validation of SPEC CPU2017 PinPoints,"Haiyang Han, Nikos Hardavellas","Phase-based statistical sampling methods such as SimPoints have proven to be effective at dramatically reducing the long time for architectural simulators to run large workloads such as SPEC CPU2017. However, generating and validating them is a long and tenuous process. While checkpoints of program phases, or ""pinballs"", of SPEC CPU2017 have been collected by other researchers and shared with the research community, they are outdated and produce errors when used with the latest versions of the Sniper architectural simulator. To facilitate our own research as well as contribute to the community, we collect and validate our own pinballs for the SPEC CPU2017 SPECspeed suite and release them to the public domain. In this work we document our methodology, the hardware and software details of the collection process, and our validation results. In terms of CPI, our pinballs have an average error rate of 12% when compared with the native whole-program benchmark execution.",2021-12-13T19:31:19Z,2021-12-13T19:31:19Z,http://arxiv.org/abs/2112.06981v1,http://arxiv.org/pdf/2112.06981v1,"cs.PF, cs.AR, B.8.0; C.4; I.6.3"
Efficient differentiable quadratic programming layers: an ADMM approach,"Andrew Butler, Roy Kwon","Recent advances in neural-network architecture allow for seamless integration of convex optimization problems as differentiable layers in an end-to-end trainable neural network. Integrating medium and large scale quadratic programs into a deep neural network architecture, however, is challenging as solving quadratic programs exactly by interior-point methods has worst-case cubic complexity in the number of variables. In this paper, we present an alternative network layer architecture based on the alternating direction method of multipliers (ADMM) that is capable of scaling to problems with a moderately large number of variables. Backward differentiation is performed by implicit differentiation of the residual map of a modified fixed-point iteration. Simulated results demonstrate the computational advantage of the ADMM layer, which for medium scaled problems is approximately an order of magnitude faster than the OptNet quadratic programming layer. Furthermore, our novel backward-pass routine is efficient, from both a memory and computation standpoint, in comparison to the standard approach based on unrolled differentiation or implicit differentiation of the KKT optimality conditions. We conclude with examples from portfolio optimization in the integrated prediction and optimization paradigm.",2021-12-14T15:25:07Z,2021-12-14T15:25:07Z,http://arxiv.org/abs/2112.07464v1,http://arxiv.org/pdf/2112.07464v1,"math.OC, cs.AI, cs.LG, q-fin.PM, stat.ML"
Analog/Mixed-Signal Circuit Synthesis Enabled by the Advancements of   Circuit Architectures and Machine Learning Algorithms,"Shiyu Su, Qiaochu Zhang, Mohsen Hassanpourghadi, Juzheng Liu, Rezwan A Rasul, Mike Shuo-Wei Chen","Analog mixed-signal (AMS) circuit architecture has evolved towards more digital friendly due to technology scaling and demand for higher flexibility/reconfigurability. Meanwhile, the design complexity and cost of AMS circuits has substantially increased due to the necessity of optimizing the circuit sizing, layout, and verification of a complex AMS circuit. On the other hand, machine learning (ML) algorithms have been under exponential growth over the past decade and actively exploited by the electronic design automation (EDA) community. This paper will identify the opportunities and challenges brought about by this trend and overview several emerging AMS design methodologies that are enabled by the recent evolution of AMS circuit architectures and machine learning algorithms. Specifically, we will focus on using neural-network-based surrogate models to expedite the circuit design parameter search and layout iterations. Lastly, we will demonstrate the rapid synthesis of several AMS circuit examples from specification to silicon prototype, with significantly reduced human intervention.",2021-12-15T01:47:08Z,2021-12-15T01:47:08Z,http://arxiv.org/abs/2112.07824v1,http://arxiv.org/pdf/2112.07824v1,"cs.ET, cs.LG, cs.SY, eess.SY"
Enabling NAS with Automated Super-Network Generation,"J. Pablo Muñoz, Nikolay Lyalyushkin, Yash Akhauri, Anastasia Senina, Alexander Kozlov, Nilesh Jain","Recent Neural Architecture Search (NAS) solutions have produced impressive results training super-networks and then deriving subnetworks, a.k.a. child models that outperform expert-crafted models from a pre-defined search space. Efficient and robust subnetworks can be selected for resource-constrained edge devices, allowing them to perform well in the wild. However, constructing super-networks for arbitrary architectures is still a challenge that often prevents the adoption of these approaches. To address this challenge, we present BootstrapNAS, a software framework for automatic generation of super-networks for NAS. BootstrapNAS takes a pre-trained model from a popular architecture, e.g., ResNet- 50, or from a valid custom design, and automatically creates a super-network out of it, then uses state-of-the-art NAS techniques to train the super-network, resulting in subnetworks that significantly outperform the given pre-trained model. We demonstrate the solution by generating super-networks from arbitrary model repositories and make available the resulting super-networks for reproducibility of the results.",2021-12-20T21:45:48Z,2021-12-20T21:45:48Z,http://arxiv.org/abs/2112.10878v1,http://arxiv.org/pdf/2112.10878v1,"cs.LG, I.2; D.0; I.2.2"
Equivariance and generalization in neural networks,"Srinath Bulusu, Matteo Favoni, Andreas Ipp, David I. Müller, Daniel Schuh","The crucial role played by the underlying symmetries of high energy physics and lattice field theories calls for the implementation of such symmetries in the neural network architectures that are applied to the physical system under consideration. In these proceedings, we focus on the consequences of incorporating translational equivariance among the network properties, particularly in terms of performance and generalization. The benefits of equivariant networks are exemplified by studying a complex scalar field theory, on which various regression and classification tasks are examined. For a meaningful comparison, promising equivariant and non-equivariant architectures are identified by means of a systematic search. The results indicate that in most of the tasks our best equivariant architectures can perform and generalize significantly better than their non-equivariant counterparts, which applies not only to physical parameters beyond those represented in the training set, but also to different lattice sizes.",2021-12-23T12:38:32Z,2021-12-23T12:38:32Z,http://arxiv.org/abs/2112.12493v1,http://arxiv.org/pdf/2112.12493v1,"hep-lat, cs.LG, hep-ph, stat.ML"
Distributionally Robust Bootstrap Optimization,"Tyler Summers, Maryam Kamgarpour","Control architectures and autonomy stacks for complex engineering systems are often divided into layers to decompose a complex problem and solution into distinct, manageable sub-problems. To simplify designs, uncertainties are often ignored across layers, an approach with deep roots in classical notions of separation and certainty equivalence. But to develop robust architectures, especially as interactions between data-driven learning layers and model-based decision-making layers grow more intricate, more sophisticated interfaces between layers are required. We propose a basic architecture that couples a statistical parameter estimation layer with a constrained optimization layer. We show how the layers can be tightly integrated by combining bootstrap resampling with distributionally robust optimization. The approach allows a finite-data out-of-sample safety guarantee and an exact reformulation as a tractable finite-dimensional convex optimization problem.",2021-12-27T22:57:54Z,2021-12-27T22:57:54Z,http://arxiv.org/abs/2112.13932v1,http://arxiv.org/pdf/2112.13932v1,"math.OC, stat.AP"
Enhancing Organ at Risk Segmentation with Improved Deep Neural Networks,"Ilkin Isler, Curtis Lisle, Justin Rineer, Patrick Kelly, Damla Turgut, Jacob Ricci, Ulas Bagci","Organ at risk (OAR) segmentation is a crucial step for treatment planning and outcome determination in radiotherapy treatments of cancer patients. Several deep learning based segmentation algorithms have been developed in recent years, however, U-Net remains the de facto algorithm designed specifically for biomedical image segmentation and has spawned many variants with known weaknesses. In this study, our goal is to present simple architectural changes in U-Net to improve its accuracy and generalization properties. Unlike many other available studies evaluating their algorithms on single center data, we thoroughly evaluate several variations of U-Net as well as our proposed enhanced architecture on multiple data sets for an extensive and reliable study of the OAR segmentation problem. Our enhanced segmentation model includes (a)architectural changes in the loss function, (b)optimization framework, and (c)convolution type. Testing on three publicly available multi-object segmentation data sets, we achieved an average of 80% dice score compared to the baseline U-Net performance of 63%.",2022-02-03T21:55:16Z,2022-02-03T21:55:16Z,http://arxiv.org/abs/2202.01866v1,http://arxiv.org/pdf/2202.01866v1,"eess.IV, cs.CV"
Neural Architecture Search for Energy Efficient Always-on Audio Models,"Daniel T. Speckhard, Karolis Misiunas, Sagi Perel, Tenghui Zhu, Simon Carlile, Malcolm Slaney","Mobile and edge computing devices for always-on classification tasks require energy-efficient neural network architectures. In this paper we present several changes to neural architecture searches (NAS) that improve the chance of success in practical situations. Our search simultaneously optimizes for network accuracy, energy efficiency and memory usage. We benchmark the performance of our search on real hardware, but since running thousands of tests with real hardware is difficult we use a random forest model to roughly predict the energy usage of a candidate network. We present a search strategy that uses both Bayesian and regularized evolutionary search with particle swarms, and employs early-stopping to reduce the computational burden. Our search, evaluated on a sound-event classification dataset based upon AudioSet, results in an order of magnitude less energy per inference and a much smaller memory footprint than our baseline MobileNetV1/V2 implementations while slightly improving task accuracy. We also demonstrate how combining a 2D spectrogram with a convolution with many filters causes a computational bottleneck for audio classification and that alternative approaches reduce the computational burden but sacrifice task accuracy.",2022-02-09T06:10:18Z,2023-06-01T12:05:05Z,http://arxiv.org/abs/2202.05397v2,http://arxiv.org/pdf/2202.05397v2,"eess.AS, cs.LG, cs.SD"
Communication and Computation O-RAN Resource Slicing for URLLC Services   Using Deep Reinforcement Learning,"Abderrahime Filali, Boubakr Nour, Soumaya Cherkaoui, Abdellatif Kobbane","The evolution of the future beyond-5G/6G networks towards a service-aware network is based on network slicing technology. With network slicing, communication service providers seek to meet all the requirements imposed by the verticals, including ultra-reliable low-latency communication (URLLC) services. In addition, the open radio access network (O-RAN) architecture paves the way for flexible sharing of network resources by introducing more programmability into the RAN. RAN slicing is an essential part of end-to-end network slicing since it ensures efficient sharing of communication and computation resources. However, due to the stringent requirements of URLLC services and the dynamics of the RAN environment, RAN slicing is challenging. In this article, we propose a two-level RAN slicing approach based on the O-RAN architecture to allocate the communication and computation RAN resources among URLLC end-devices. For each RAN slicing level, we model the resource slicing problem as a single-agent Markov decision process and design a deep reinforcement learning algorithm to solve it. Simulation results demonstrate the efficiency of the proposed approach in meeting the desired quality of service requirements.",2022-02-13T23:49:14Z,2022-02-13T23:49:14Z,http://arxiv.org/abs/2202.06439v1,http://arxiv.org/pdf/2202.06439v1,"cs.NI, eess.SP"
Semi-Equivariant GNN Architectures for Jet Tagging,"Daniel Murnane, Savannah Thais, Jason Wong","Composing Graph Neural Networks (GNNs) of operations that respect physical symmetries has been suggested to give better model performance with a smaller number of learnable parameters. However, real-world applications, such as in high energy physics have not born this out. We present the novel architecture VecNet that combines both symmetry-respecting and unconstrained operations to study and tune the degree of physics-informed GNNs. We introduce a novel metric, the \textit{ant factor}, to quantify the resource-efficiency of each configuration in the search-space. We find that a generalized architecture such as ours can deliver optimal performance in resource-constrained applications.",2022-02-14T18:57:12Z,2022-02-14T18:57:12Z,http://arxiv.org/abs/2202.06941v1,http://arxiv.org/pdf/2202.06941v1,"hep-ph, cs.LG, hep-ex, physics.comp-ph"
An Automated FPGA-based Framework for Rapid Prototyping of Nonbinary   LDPC Codes,"Yaoyu Tao, Qi Wu","Nonbinary LDPC codes have shown superior performance close to the Shannon limit. Compared to binary LDPC codes of similar lengths, they can reach orders of magnitudes lower error rate. However, multitude of design freedoms of nonbinary LDPC codes complicates the practical code and decoder design process. Fast simulations are critically important to evaluate the pros and cons. Rapid prototyping on FPGA is attractive but takes significant design efforts due to its high design complexity. We propose a high-throughput reconfigurable hardware emulation architecture with decoder and peripheral co-design. The architecture enables a library and script-based framework that automates the construction of FPGA emulations. Code and decoder design parameters are programmed either during run time or by script in design time. We demonstrate the capability of the framework in evaluating practical code and decoder design by experimenting with two popular nonbinary LDPC codes, regular (2, dc) codes and quasi-cyclic codes: each emulation model can be auto-constructed within hours and the decoder delivers excellent error-correcting performance on a Xilinx Virtex-5 FPGA with throughput of up to hundreds of Mbps.",2022-02-15T10:22:16Z,2022-03-27T06:05:19Z,http://arxiv.org/abs/2202.07295v2,http://arxiv.org/pdf/2202.07295v2,"cs.IT, cs.AR, math.IT"
Towards AutoQML: A Cloud-Based Automated Circuit Architecture Search   Framework,"Raúl Berganza Gómez, Corey O'Meara, Giorgio Cortiana, Christian B. Mendl, Juan Bernabé-Moreno","The learning process of classical machine learning algorithms is tuned by hyperparameters that need to be customized to best learn and generalize from an input dataset. In recent years, Quantum Machine Learning (QML) has been gaining traction as a possible application of quantum computing which may provide quantum advantage in the future. However, quantum versions of classical machine learning algorithms introduce a plethora of additional parameters and circuit variations that have their own intricacies in being tuned.   In this work, we take the first steps towards Automated Quantum Machine Learning (AutoQML). We propose a concrete description of the problem, and then develop a classical-quantum hybrid cloud architecture that allows for parallelized hyperparameter exploration and model training.   As an application use-case, we train a quantum Generative Adversarial neural Network (qGAN) to generate energy prices that follow a known historic data distribution. Such a QML model can be used for various applications in the energy economics sector.",2022-02-16T12:37:10Z,2022-02-16T12:37:10Z,http://arxiv.org/abs/2202.08024v1,http://arxiv.org/pdf/2202.08024v1,"quant-ph, cs.DC, cs.ET"
The Quarks of Attention,"Pierre Baldi, Roman Vershynin","Attention plays a fundamental role in both natural and artificial intelligence systems. In deep learning, attention-based neural architectures, such as transformer architectures, are widely used to tackle problems in natural language processing and beyond. Here we investigate the fundamental building blocks of attention and their computational properties. Within the standard model of deep learning, we classify all possible fundamental building blocks of attention in terms of their source, target, and computational mechanism. We identify and study three most important mechanisms: additive activation attention, multiplicative output attention (output gating), and multiplicative synaptic attention (synaptic gating). The gating mechanisms correspond to multiplicative extensions of the standard model and are used across all current attention-based deep learning architectures. We study their functional properties and estimate the capacity of several attentional building blocks in the case of linear and polynomial threshold gates. Surprisingly, additive activation attention plays a central role in the proofs of the lower bounds. Attention mechanisms reduce the depth of certain basic circuits and leverage the power of quadratic activations without incurring their full cost.",2022-02-15T18:47:19Z,2022-02-15T18:47:19Z,http://arxiv.org/abs/2202.08371v1,http://arxiv.org/pdf/2202.08371v1,"cs.LG, cs.AI, stat.ML"
WaveMix: Resource-efficient Token Mixing for Images,"Pranav Jeevan, Amit Sethi","Although certain vision transformer (ViT) and CNN architectures generalize well on vision tasks, it is often impractical to use them on green, edge, or desktop computing due to their computational requirements for training and even testing. We present WaveMix as an alternative neural architecture that uses a multi-scale 2D discrete wavelet transform (DWT) for spatial token mixing. Unlike ViTs, WaveMix neither unrolls the image nor requires self-attention of quadratic complexity. Additionally, DWT introduces another inductive bias -- besides convolutional filtering -- to utilize the 2D structure of an image to improve generalization. The multi-scale nature of the DWT also reduces the requirement for a deeper architecture compared to the CNNs, as the latter relies on pooling for partial spatial mixing. WaveMix models show generalization that is competitive with ViTs, CNNs, and token mixers on several datasets while requiring lower GPU RAM (training and testing), number of computations, and storage. WaveMix have achieved State-of-the-art (SOTA) results in EMNIST Byclass and EMNIST Balanced datasets.",2022-03-07T20:15:17Z,2022-03-07T20:15:17Z,http://arxiv.org/abs/2203.03689v1,http://arxiv.org/pdf/2203.03689v1,"cs.CV, cs.AI, cs.LG, I.4.0; I.4.1; I.4.7; I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2;
  I.5.4"
KinyaBERT: a Morphology-aware Kinyarwanda Language Model,"Antoine Nzeyimana, Andre Niyongabo Rubungo","Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality. Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 4.3% in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.",2022-03-16T08:36:14Z,2022-03-17T12:35:21Z,http://arxiv.org/abs/2203.08459v2,http://arxiv.org/pdf/2203.08459v2,"cs.CL, I.2.7; I.2"
Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on   Synthetic Images,"Luca Guarnera, Oliver Giudice, Sebastiano Battiato","Most recent style-transfer techniques based on generative architectures are able to obtain synthetic multimedia contents, or commonly called deepfakes, with almost no artifacts. Researchers already demonstrated that synthetic images contain patterns that can determine not only if it is a deepfake but also the generative architecture employed to create the image data itself. These traces can be exploited to study problems that have never been addressed in the context of deepfakes. To this aim, in this paper a first approach to investigate the image ballistics on deepfake images subject to style-transfer manipulations is proposed. Specifically, this paper describes a study on detecting how many times a digital image has been processed by a generative architecture for style transfer. Moreover, in order to address and study accurately forensic ballistics on deepfake images, some mathematical properties of style-transfer operations were investigated.",2022-03-18T13:11:54Z,2022-03-18T13:11:54Z,http://arxiv.org/abs/2203.09928v1,http://arxiv.org/pdf/2203.09928v1,"cs.CV, cs.LG, eess.IV"
Parametric Scaling of Preprocessing assisted U-net Architecture for   Improvised Retinal Vessel Segmentation,"Kundan Kumar, Sumanshu Agarwal","Extracting blood vessels from retinal fundus images plays a decisive role in diagnosing the progression in pertinent diseases. In medical image analysis, vessel extraction is a semantic binary segmentation problem, where blood vasculature needs to be extracted from the background. Here, we present an image enhancement technique based on the morphological preprocessing coupled with a scaled U-net architecture. Despite a relatively less number of trainable network parameters, the scaled version of U-net architecture provides better performance compare to other methods in the domain. We validated the proposed method on retinal fundus images from the DRIVE database. A significant improvement as compared to the other algorithms in the domain, in terms of the area under ROC curve (>0.9762) and classification accuracy (>95.47%) are evident from the results. Furthermore, the proposed method is resistant to the central vessel reflex while sensitive to detect blood vessels in the presence of background items viz. exudates, optic disc, and fovea.",2022-03-18T15:26:05Z,2022-03-18T15:26:05Z,http://arxiv.org/abs/2203.10014v1,http://arxiv.org/pdf/2203.10014v1,"eess.IV, cs.CV, cs.LG"
SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks,"Christopher Morris, Gaurav Rattan, Sandra Kiefer, Siamak Ravanbakhsh","While (message-passing) graph neural networks have clear limitations in approximating permutation-equivariant functions over graphs or general relational data, more expressive, higher-order graph neural networks do not scale to large graphs. They either operate on $k$-order tensors or consider all $k$-node subgraphs, implying an exponential dependence on $k$ in memory requirements, and do not adapt to the sparsity of the graph. By introducing new heuristics for the graph isomorphism problem, we devise a class of universal, permutation-equivariant graph networks, which, unlike previous architectures, offer a fine-grained control between expressivity and scalability and adapt to the sparsity of the graph. These architectures lead to vastly reduced computation times compared to standard higher-order graph networks in the supervised node- and graph-level classification and regression regime while significantly improving over standard graph neural network and graph kernel architectures in terms of predictive performance.",2022-03-25T21:17:09Z,2022-08-30T09:11:56Z,http://arxiv.org/abs/2203.13913v3,http://arxiv.org/pdf/2203.13913v3,"cs.LG, cs.AI, cs.DS, cs.NE, stat.ML"
Neural Network Optimal Feedback Control with Guaranteed Local Stability,"Tenavi Nakamura-Zimmerer, Qi Gong, Wei Kang","Recent research shows that supervised learning can be an effective tool for designing near-optimal feedback controllers for high-dimensional nonlinear dynamic systems. But the behavior of neural network controllers is still not well understood. In particular, some neural networks with high test accuracy can fail to even locally stabilize the dynamic system. To address this challenge we propose several novel neural network architectures, which we show guarantee local asymptotic stability while retaining the approximation capacity to learn the optimal feedback policy semi-globally. The proposed architectures are compared against standard neural network feedback controllers through numerical simulations of two high-dimensional nonlinear optimal control problems: stabilization of an unstable Burgers-type partial differential equation, and altitude and course tracking for an unmanned aerial vehicle. The simulations demonstrate that standard neural networks can fail to stabilize the dynamics even when trained well, while the proposed architectures are always at least locally stabilizing and can achieve near-optimal performance.",2022-05-01T04:23:24Z,2022-10-07T01:15:52Z,http://arxiv.org/abs/2205.00394v3,http://arxiv.org/pdf/2205.00394v3,"math.OC, cs.LG, cs.SY, eess.SY"
Planning a Cost-Effective Delay-Constrained Passive Optical Network for   5G Fronthaul,"Abdulhalim Fayad, Manish Jha, Tibor Cinkler, Jacek Rak","With the rapid growth in the telecommunications industry moving towards 5G and beyond (5GB) and the emergence of data-hungry and time-sensitive applications, Mobile Network Operators (MNOs) are faced with a considerable challenge to keep up with these new demands. Cloud radio access network (CRAN) has emerged as a cost-effective architecture that improves 5GB performance. The fronthaul segment of the CRAN necessitates a high-capacity and low-latency connection. Optical technologies presented by Passive Optical Networks (PON) have gained attention as a promising technology to meet the fronthaul challenges. In this paper, we proposed an Integer Linear Program (ILP) that optimizes the total cost of ownership (TCO) for 5G using CRAN architecture under different delay thresholds. We considered the Time and Wavelength Division Multiplexing Passive Optical Network (TWDM-PON) as a fronthaul with different splitting ratios.",2022-05-04T13:30:23Z,2022-05-04T13:30:23Z,http://arxiv.org/abs/2205.02055v1,http://arxiv.org/pdf/2205.02055v1,"cs.NI, cs.SY, eess.SY"
Quantum Self-Attention Neural Networks for Text Classification,"Guangxi Li, Xuanqiang Zhao, Xin Wang","An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best existing QNLP model based on syntactic analysis as well as a simple classical self-attention neural network in numerical experiments of text classification tasks on public data sets. We further show that our method exhibits robustness to low-level quantum noises and showcases resilience to quantum neural network architectures.",2022-05-11T16:50:46Z,2023-09-28T00:04:13Z,http://arxiv.org/abs/2205.05625v2,http://arxiv.org/pdf/2205.05625v2,"quant-ph, cs.AI, cs.LG"
Nanoscale Architecture for Frequency-Resolving Single-Photon Detectors,"Steve M. Young, Mohan Sarovar, François Léonard","Single photon detectors play a key role across several basic science and technology applications. While progress has been made in improving performance, single photon detectors that can maintain high performance while also resolving the photon frequency are still lacking. By means of quantum simulations, we show that nanoscale elements cooperatively interacting with the photon field in a photodetector architecture allow to simultaneously achieve high efficiency, low jitter, and high frequency resolution. We discuss how such cooperative interactions are essential to reach this performance regime, analyzing the factors that impact performance and trade-offs between metrics. We illustrate the potential performance for frequency resolution over a 1 eV bandwidth in the visible range, indicating near perfect detection efficiency, jitter of a few hundred femtoseconds, and frequency resolution of tens of meV. Finally, a potential physical realization of such an architecture is presented based on carbon nanotubes functionalized with quantum dots.",2022-05-12T00:48:54Z,2023-08-09T23:49:24Z,http://arxiv.org/abs/2205.05817v2,http://arxiv.org/pdf/2205.05817v2,"quant-ph, cond-mat.mtrl-sci, physics.optics"
All-Photonic Artificial Neural Network Processor Via Non-linear Optics,"Jasvith Raj Basani, Mikkel Heuck, Dirk R. Englund, Stefan Krastanov","Optics and photonics has recently captured interest as a platform to accelerate linear matrix processing, that has been deemed as a bottleneck in traditional digital electronic architectures. In this paper, we propose an all-photonic artificial neural network processor wherein information is encoded in the amplitudes of frequency modes that act as neurons. The weights among connected layers are encoded in the amplitude of controlled frequency modes that act as pumps. Interaction among these modes for information processing is enabled by non-linear optical processes. Both the matrix multiplication and element-wise activation functions are performed through coherent processes, enabling the direct representation of negative and complex numbers without the use of detectors or digital electronics. Via numerical simulations, we show that our design achieves a performance commensurate with present-day state-of-the-art computational networks on image-classification benchmarks. Our architecture is unique in providing a completely unitary, reversible mode of computation. Additionally, the computational speed increases with the power of the pumps to arbitrarily high rates, as long as the circuitry can sustain the higher optical power.",2022-05-17T19:55:30Z,2022-05-17T19:55:30Z,http://arxiv.org/abs/2205.08608v1,http://arxiv.org/pdf/2205.08608v1,"physics.optics, cs.AR, cs.LG"
"Hyperion: A Case for Unified, Self-Hosting, Zero-CPU Data-Processing   Units (DPUs)","Marco Spaziani Brunella, Marco Bonola, Animesh Trivedi","Since the inception of computing, we have been reliant on CPU-powered architectures. However, today this reliance is challenged by manufacturing limitations (CMOS scaling), performance expectations (stalled clocks, Turing tax), and security concerns (microarchitectural attacks). To re-imagine our computing architecture, in this work we take a more radical but pragmatic approach and propose to eliminate the CPU with its design baggage, and integrate three primary pillars of computing, i.e., networking, storage, and computing, into a single, self-hosting, unified CPU-free Data Processing Unit (DPU) called Hyperion. In this paper, we present the case for Hyperion, its design choices, initial work-in-progress details, and seek feedback from the systems community.",2022-05-18T12:12:05Z,2022-08-31T15:42:00Z,http://arxiv.org/abs/2205.08882v2,http://arxiv.org/pdf/2205.08882v2,"cs.AR, cs.DC, cs.OS, cs.PL, C.5; D.3; D.4; B.4"
Equivariant Mesh Attention Networks,"Sourya Basu, Jose Gallego-Posada, Francesco Viganò, James Rowbottom, Taco Cohen","Equivariance to symmetries has proven to be a powerful inductive bias in deep learning research. Recent works on mesh processing have concentrated on various kinds of natural symmetries, including translations, rotations, scaling, node permutations, and gauge transformations. To date, no existing architecture is equivariant to all of these transformations. In this paper, we present an attention-based architecture for mesh data that is provably equivariant to all transformations mentioned above. Our pipeline relies on the use of relative tangential features: a simple, effective, equivariance-friendly alternative to raw node positions as inputs. Experiments on the FAUST and TOSCA datasets confirm that our proposed architecture achieves improved performance on these benchmarks and is indeed equivariant, and therefore robust, to a wide variety of local/global transformations.",2022-05-21T19:53:14Z,2022-08-27T16:43:35Z,http://arxiv.org/abs/2205.10662v2,http://arxiv.org/pdf/2205.10662v2,"cs.LG, cs.CV, stat.ML"
C-AND: Mixed Writing Scheme for Disturb Reduction in 1T Ferroelectric   FET Memory,"Mor M. Dahan, Evelyn T. Breyer, Stefan Slesazeck, Thomas Mikolajick, Shahar Kvatinsky","Ferroelectric field effect transistor (FeFET) memory has shown the potential to meet the requirements of the growing need for fast, dense, low-power, and non-volatile memories. In this paper, we propose a memory architecture named crossed-AND (C-AND), in which each storage cell consists of a single ferroelectric transistor. The write operation is performed using different write schemes and different absolute voltages, to account for the asymmetric switching voltages of the FeFET. It enables writing an entire wordline in two consecutive cycles and prevents current and power through the channel of the transistor. During the read operation, the current and power are mostly sensed at a single selected device in each column. The read scheme additionally enables reading an entire word without read errors, even along long bitlines. Our Simulations demonstrate that, in comparison to the previously proposed AND architecture, the C-AND architecture diminishes read errors, reduces write disturbs, enables the usage of longer bitlines, and saves up to 2.92X in memory cell area.",2022-05-24T13:24:03Z,2022-05-24T13:24:03Z,http://arxiv.org/abs/2205.12061v1,http://arxiv.org/pdf/2205.12061v1,"eess.SY, cs.SY"
On-Demand Redundancy Grouping: Selectable Soft-Error Tolerance for a   Multicore Cluster,"Michael Rogenmoser, Nils Wistoff, Pirmin Vogel, Frank Gürkaynak, Luca Benini","With the shrinking of technology nodes and the use of parallel processor clusters in hostile and critical environments, such as space, run-time faults caused by radiation are a serious cross-cutting concern, also impacting architectural design. This paper introduces an architectural approach to run-time configurable soft-error tolerance at the core level, augmenting a six-core open-source RISC-V cluster with a novel On-Demand Redundancy Grouping (ODRG) scheme. ODRG allows the cluster to operate either as two fault-tolerant cores, or six individual cores for high-performance, with limited overhead to switch between these modes during run-time. The ODRG unit adds less than 11% of a core's area for a three-core group, or a total of 1% of the cluster area, and shows negligible timing increase, which compares favorably to a commercial state-of-the-art implementation, and is 2.5$\times$ faster in fault recovery re-synchronization. Furthermore, when redundancy is not necessary, the ODRG approach allows the redundant cores to be used for independent computation, allowing up to 2.96$\times$ increase in performance for selected applications.",2022-05-25T08:42:34Z,2023-10-03T13:26:28Z,http://arxiv.org/abs/2205.12580v2,http://arxiv.org/pdf/2205.12580v2,"cs.DC, cs.AR, eess.SP"
Large-Scale Crosstalk-Corrected Thermo-Optic Phase Shifter Arrays in   Silicon Photonics,"Volkan Gurses, Reza Fatemi, Aroutin Khachaturian, Ali Hajimiri",We introduce a thermo-optic phase shifter (TOPS) array architecture with independent phase control of each phase shifter for large-scale and high-density photonic integrated circuits with two different control schemes: pulse amplitude modulation (PAM) and pulse width modulation (PWM). We realize a compact spiral TOPS and a 288-element high-density row-column TOPS array with this architecture and drive TOPS with waveforms of both control schemes and of different array sizes. We present a thermal excitation model and a finite difference method-based simulation to simulate large-scale TOPS arrays and compare both schemes experimentally and theoretically. We also analyze the effects of thermal crosstalk in the realized TOPS array and implement a thermal crosstalk correction algorithm with the developed model. The high-density TOPS array architecture and the thermal crosstalk correction algorithm pave the way for high-density TOPS arrays with independent phase control in large-scale photonic integrated circuits interfaced with electronics limited in voltage swing and bandwidth.,2022-06-05T00:49:10Z,2022-10-08T04:01:50Z,http://arxiv.org/abs/2206.04525v2,http://arxiv.org/pdf/2206.04525v2,"physics.optics, cs.SY, eess.SY, physics.app-ph"
Adversarial Audio Synthesis with Complex-valued Polynomial Networks,"Yongtao Wu, Grigorios G Chrysos, Volkan Cevher","Time-frequency (TF) representations in audio synthesis have been increasingly modeled with real-valued networks. However, overlooking the complex-valued nature of TF representations can result in suboptimal performance and require additional modules (e.g., for modeling the phase). To this end, we introduce complex-valued polynomial networks, called APOLLO, that integrate such complex-valued representations in a natural way. Concretely, APOLLO captures high-order correlations of the input elements using high-order tensors as scaling parameters. By leveraging standard tensor decompositions, we derive different architectures and enable modeling richer correlations. We outline such architectures and showcase their performance in audio generation across four benchmarks. As a highlight, APOLLO results in $17.5\%$ improvement over adversarial methods and $8.2\%$ over the state-of-the-art diffusion models on SC09 dataset in audio generation. Our models can encourage the systematic design of other efficient architectures on the complex field.",2022-06-14T12:58:59Z,2022-06-21T13:30:18Z,http://arxiv.org/abs/2206.06811v2,http://arxiv.org/pdf/2206.06811v2,"eess.AS, cs.LG"
ERNAS: An Evolutionary Neural Architecture Search for Magnetic Resonance   Image Reconstructions,"Samira Vafay Eslahi, Jian Tao, Jim Ji","Magnetic resonance imaging (MRI) is one of the noninvasive imaging modalities that can produce high-quality images. However, the scan procedure is relatively slow, which causes patient discomfort and motion artifacts in images. Accelerating MRI hardware is constrained by physical and physiological limitations. A popular alternative approach to accelerated MRI is to undersample the k-space data. While undersampling speeds up the scan procedure, it generates artifacts in the images, and advanced reconstruction algorithms are needed to produce artifact-free images. Recently deep learning has emerged as a promising MRI reconstruction method to address this problem. However, straightforward adoption of the existing deep learning neural network architectures in MRI reconstructions is not usually optimal in terms of efficiency and reconstruction quality. In this work, MRI reconstruction from undersampled data was carried out using an optimized neural network using a novel evolutionary neural architecture search algorithm. Brain and knee MRI datasets show that the proposed algorithm outperforms manually designed neural network-based MR reconstruction models.",2022-06-15T03:42:18Z,2023-01-17T04:38:16Z,http://arxiv.org/abs/2206.07280v2,http://arxiv.org/pdf/2206.07280v2,"eess.IV, cs.CV, cs.NE"
All you need is feedback: Communication with block attention feedback   codes,"Emre Ozfatura, Yulin Shao, Alberto Perotti, Branislav Popovic, Deniz Gunduz","Deep learning based channel code designs have recently gained interest as an alternative to conventional coding algorithms, particularly for channels for which existing codes do not provide effective solutions. Communication over a feedback channel is one such problem, for which promising results have recently been obtained by employing various deep learning architectures. In this paper, we introduce a novel learning-aided code design for feedback channels, called generalized block attention feedback (GBAF) codes, which i) employs a modular architecture that can be implemented using different neural network architectures; ii) provides order-of-magnitude improvements in the probability of error compared to existing designs; and iii) can transmit at desired code rates.",2022-06-19T17:55:04Z,2022-10-05T16:13:17Z,http://arxiv.org/abs/2206.09457v2,http://arxiv.org/pdf/2206.09457v2,"cs.IT, cs.AI, cs.LG, eess.SP, math.IT"
Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic   Graphs,"Yi-Lun Liao, Tess Smidt","Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.",2022-06-23T21:40:37Z,2023-02-28T02:07:30Z,http://arxiv.org/abs/2206.11990v2,http://arxiv.org/pdf/2206.11990v2,"cs.LG, cs.AI, physics.comp-ph"
AutoInit: Automatic Initialization via Jacobian Tuning,"Tianyu He, Darshil Doshi, Andrey Gromov","Good initialization is essential for training Deep Neural Networks (DNNs). Oftentimes such initialization is found through a trial and error approach, which has to be applied anew every time an architecture is substantially modified, or inherited from smaller size networks leading to sub-optimal initialization. In this work we introduce a new and cheap algorithm, that allows one to find a good initialization automatically, for general feed-forward DNNs. The algorithm utilizes the Jacobian between adjacent network blocks to tune the network hyperparameters to criticality. We solve the dynamics of the algorithm for fully connected networks with ReLU and derive conditions for its convergence. We then extend the discussion to more general architectures with BatchNorm and residual connections. Finally, we apply our method to ResMLP and VGG architectures, where the automatic one-shot initialization found by our method shows good performance on vision tasks.",2022-06-27T18:14:51Z,2022-06-27T18:14:51Z,http://arxiv.org/abs/2206.13568v1,http://arxiv.org/pdf/2206.13568v1,"stat.ML, cond-mat.dis-nn, cond-mat.stat-mech, cs.LG"
Deep Neural Networks pruning via the Structured Perspective   Regularization,"Matteo Cacciola, Antonio Frangioni, Xinlin Li, Andrea Lodi","In Machine Learning, Artificial Neural Networks (ANNs) are a very powerful tool, broadly used in many applications. Often, the selected (deep) architectures include many layers, and therefore a large amount of parameters, which makes training, storage and inference expensive. This motivated a stream of research about compressing the original networks into smaller ones without excessively sacrificing performances. Among the many proposed compression approaches, one of the most popular is \emph{pruning}, whereby entire elements of the ANN (links, nodes, channels, \ldots) and the corresponding weights are deleted. Since the nature of the problem is inherently combinatorial (what elements to prune and what not), we propose a new pruning method based on Operational Research tools. We start from a natural Mixed-Integer-Programming model for the problem, and we use the Perspective Reformulation technique to strengthen its continuous relaxation. Projecting away the indicator variables from this reformulation yields a new regularization term, which we call the Structured Perspective Regularization, that leads to structured pruning of the initial architecture. We test our method on some ResNet architectures applied to CIFAR-10, CIFAR-100 and ImageNet datasets, obtaining competitive performances w.r.t.~the state of the art for structured pruning.",2022-06-28T14:58:51Z,2022-06-28T14:58:51Z,http://arxiv.org/abs/2206.14056v1,http://arxiv.org/pdf/2206.14056v1,"cs.LG, cs.CV, math.OC"
Implicit U-Net for volumetric medical image segmentation,"Sergio Naval Marimont, Giacomo Tarroni","U-Net has been the go-to architecture for medical image segmentation tasks, however computational challenges arise when extending the U-Net architecture to 3D images. We propose the Implicit U-Net architecture that adapts the efficient Implicit Representation paradigm to supervised image segmentation tasks. By combining a convolutional feature extractor with an implicit localization network, our implicit U-Net has 40% less parameters than the equivalent U-Net. Moreover, we propose training and inference procedures to capitalize sparse predictions. When comparing to an equivalent fully convolutional U-Net, Implicit U-Net reduces by approximately 30% inference and training time as well as training memory footprint while achieving comparable results in our experiments with two different abdominal CT scan datasets.",2022-06-30T12:00:40Z,2022-06-30T12:00:40Z,http://arxiv.org/abs/2206.15217v1,http://arxiv.org/pdf/2206.15217v1,"eess.IV, cs.CV"
Using a Cognitive Architecture to consider antiblackness in design and   development of AI systems,Christopher L. Dancy,"How might we use cognitive modeling to consider the ways in which antiblackness, and racism more broadly, impact the design and development of AI systems? We provide a discussion and an example towards an answer to this question. We use the ACT-R/{\Phi} cognitive architecture and an existing knowledge graph system, ConceptNet, to consider this question not only from a cognitive and sociocultural perspective, but also from a physiological perspective. In addition to using a cognitive modeling as a means to explore how antiblackness may manifest in the design and development of AI systems (particularly from a software engineering perspective), we also introduce connections between antiblackness, the Human, and computational cognitive modeling. We argue that the typical eschewing of sociocultural processes and knowledge structures in cognitive architectures and cognitive modeling implicitly furthers a colorblind approach to cognitive modeling and hides sociocultural context that is always present in human behavior and affects cognitive processes.",2022-07-01T19:39:13Z,2023-04-24T11:41:58Z,http://arxiv.org/abs/2207.00644v2,http://arxiv.org/pdf/2207.00644v2,"cs.CY, cs.AI, I.2.1; I.2.7; K.4.2"
Deriving Surface Resistivity from Polarimetric SAR Data Using Dual-Input   UNet,"Bibin Wilson, Rajiv Kumar, Narayanarao Bhogapurapu, Anand Singh, Amit Sethi","Traditional survey methods for finding surface resistivity are time-consuming and labor intensive. Very few studies have focused on finding the resistivity/conductivity using remote sensing data and deep learning techniques. In this line of work, we assessed the correlation between surface resistivity and Synthetic Aperture Radar (SAR) by applying various deep learning methods and tested our hypothesis in the Coso Geothermal Area, USA. For detecting the resistivity, L-band full polarimetric SAR data acquired by UAVSAR were used, and MT (Magnetotellurics) inverted resistivity data of the area were used as the ground truth. We conducted experiments to compare various deep learning architectures and suggest the use of Dual Input UNet (DI-UNet) architecture. DI-UNet uses a deep learning architecture to predict the resistivity using full polarimetric SAR data by promising a quick survey addition to the traditional method. Our proposed approach accomplished improved outcomes for the mapping of MT resistivity from SAR data.",2022-07-05T05:10:59Z,2022-07-05T05:10:59Z,http://arxiv.org/abs/2207.01811v1,http://arxiv.org/pdf/2207.01811v1,"physics.geo-ph, cs.CV, cs.LG, eess.SP"
Compute Cost Amortized Transformer for Streaming ASR,"Yi Xie, Jonathan Macoskey, Martin Radfar, Feng-Ju Chang, Brian King, Ariya Rastrow, Athanasios Mouchtaris, Grant P. Strimel","We present a streaming, Transformer-based end-to-end automatic speech recognition (ASR) architecture which achieves efficient neural inference through compute cost amortization. Our architecture creates sparse computation pathways dynamically at inference time, resulting in selective use of compute resources throughout decoding, enabling significant reductions in compute with minimal impact on accuracy. The fully differentiable architecture is trained end-to-end with an accompanying lightweight arbitrator mechanism operating at the frame-level to make dynamic decisions on each input while a tunable loss function is used to regularize the overall level of compute against predictive performance. We report empirical results from experiments using the compute amortized Transformer-Transducer (T-T) model conducted on LibriSpeech data. Our best model can achieve a 60% compute cost reduction with only a 3% relative word error rate (WER) increase.",2022-07-05T03:06:53Z,2022-07-05T03:06:53Z,http://arxiv.org/abs/2207.02393v1,http://arxiv.org/pdf/2207.02393v1,"cs.CL, cs.SD, eess.AS"
Denoising single images by feature ensemble revisited,"Masud An Nur Islam Fahim, Nazmus Saqib, Shafkat Khan Siam, Ho Yub Jung","Image denoising is still a challenging issue in many computer vision sub-domains. Recent studies show that significant improvements are made possible in a supervised setting. However, few challenges, such as spatial fidelity and cartoon-like smoothing remain unresolved or decisively overlooked. Our study proposes a simple yet efficient architecture for the denoising problem that addresses the aforementioned issues. The proposed architecture revisits the concept of modular concatenation instead of long and deeper cascaded connections, to recover a cleaner approximation of the given image. We find that different modules can capture versatile representations, and concatenated representation creates a richer subspace for low-level image restoration. The proposed architecture's number of parameters remains smaller than the number for most of the previous networks and still achieves significant improvements over the current state-of-the-art networks.",2022-07-11T20:23:55Z,2022-07-11T20:23:55Z,http://arxiv.org/abs/2207.05176v1,http://arxiv.org/pdf/2207.05176v1,"cs.CV, cs.LG, eess.IV"
A Data-Efficient Deep Learning Framework for Segmentation and   Classification of Histopathology Images,"Pranav Singh, Jacopo Cirrone","The current study of cell architecture of inflammation in histopathology images commonly performed for diagnosis and research purposes excludes a lot of information available on the biopsy slide. In autoimmune diseases, major outstanding research questions remain regarding which cell types participate in inflammation at the tissue level, and how they interact with each other. While these questions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification provide a much more efficient method to understand the architecture of inflammation in autoimmune disease, holding great promise for novel insights. In this paper, we empirically develop deep learning approaches that use dermatomyositis biopsies of human tissue to detect and identify inflammatory cells. Our approach improves classification performance by 26% and segmentation performance by 5%. We also propose a novel post-processing autoencoder architecture that improves segmentation performance by an additional 3%.",2022-07-13T19:23:49Z,2022-10-22T08:51:17Z,http://arxiv.org/abs/2207.06489v5,http://arxiv.org/pdf/2207.06489v5,"eess.IV, cs.CV, cs.LG"
Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for   COVID-19 Screening With Chest Radiography,"Kai Ma, Pengcheng Xi, Karim Habashy, Ashkan Ebadi, Stéphane Tremblay, Alexander Wong","Building AI models with trustworthiness is important especially in regulated areas such as healthcare. In tackling COVID-19, previous work uses convolutional neural networks as the backbone architecture, which has shown to be prone to over-caution and overconfidence in making decisions, rendering them less trustworthy -- a crucial flaw in the context of medical imaging. In this study, we propose a feature learning approach using Vision Transformers, which use an attention-based mechanism, and examine the representation learning capability of Transformers as a new backbone architecture for medical imaging. Through the task of classifying COVID-19 chest radiographs, we investigate into whether generalization capabilities benefit solely from Vision Transformers' architectural advances. Quantitative and qualitative evaluations are conducted on the trustworthiness of the models, through the use of ""trust score"" computation and a visual explainability technique. We conclude that the attention-based feature learning approach is promising in building trustworthy deep learning models for healthcare.",2022-07-19T14:55:42Z,2022-07-19T14:55:42Z,http://arxiv.org/abs/2207.09312v1,http://arxiv.org/pdf/2207.09312v1,"eess.IV, cs.CV, cs.LG"
Secure and Lightweight Strong PUF Challenge Obfuscation with Keyed   Non-linear FSR,"Kleber Stangherlin, Zhuanhao Wu, Hiren Patel, Manoj Sachdev","We propose a secure and lightweight key based challenge obfuscation for strong PUFs. Our architecture is designed to be resilient against learning attacks. Our obfuscation mechanism uses non-linear feedback shift registers (NLFSRs). Responses are directly provided to the user, without error correction or extra post-processing steps. We also discuss the cost of protecting our architecture against power analysis attacks with clock randomization, and Boolean masking. Security against learning attacks is assessed using avalanche criterion, and deep-neural network attacks. We designed a testchip in 65 nm CMOS. When compared to the baseline arbiter PUF implementation, the cost increase of our proposed architecture is 1.27x, and 2.2x when using clock randomization, and Boolean masking, respectively.",2022-07-22T16:42:02Z,2022-07-22T16:42:02Z,http://arxiv.org/abs/2207.11181v1,http://arxiv.org/pdf/2207.11181v1,"cs.CR, cs.SY, eess.SY"
Time-invariant prefix-free source coding for MIMO LQG control,"Travis C. Cuvelier, Takashi Tanaka, Robert W. Heath Jr","In this work we consider discrete-time multiple-input multiple-output (MIMO) linear-quadratic-Gaussian (LQG) control where the feedback consists of variable length binary codewords. To simplify the decoder architecture, we enforce a strict prefix constraint on the codewords. We develop a data compression architecture that provably achieves a near minimum time-average expected bitrate for a fixed constraint on the LQG performance. The architecture conforms to the strict prefix constraint and does not require time-varying lossless source coding, in contrast to the prior art.",2022-07-26T02:31:49Z,2022-07-26T02:31:49Z,http://arxiv.org/abs/2207.12614v1,http://arxiv.org/pdf/2207.12614v1,"cs.IT, cs.SY, eess.SP, eess.SY, math.IT"
Graph Neural Network with Local Frame for Molecular Potential Energy   Surface,"Xiyuan Wang, Muhan Zhang","Modeling molecular potential energy surface is of pivotal importance in science. Graph Neural Networks have shown great success in this field. However, their message passing schemes need special designs to capture geometric information and fulfill symmetry requirement like rotation equivariance, leading to complicated architectures. To avoid these designs, we introduce a novel local frame method to molecule representation learning and analyze its expressivity. Projected onto a frame, equivariant features like 3D coordinates are converted to invariant features, so that we can capture geometric information with these projections and decouple the symmetry requirement from GNN design. Theoretically, we prove that given non-degenerate frames, even ordinary GNNs can encode molecules injectively and reach maximum expressivity with coordinate projection and frame-frame projection. In experiments, our model uses a simple ordinary GNN architecture yet achieves state-of-the-art accuracy. The simpler architecture also leads to higher scalability. Our model only takes about 30% inference time and 10% GPU memory compared to the most efficient baselines.",2022-08-01T10:01:38Z,2023-04-21T17:58:50Z,http://arxiv.org/abs/2208.00716v2,http://arxiv.org/pdf/2208.00716v2,"cs.LG, physics.chem-ph"
Streaming-capable High-performance Architecture of Learned Image   Compression Codecs,"Fangzheng Lin, Heming Sun, Jiro Katto","Learned image compression allows achieving state-of-the-art accuracy and compression ratios, but their relatively slow runtime performance limits their usage. While previous attempts on optimizing learned image codecs focused more on the neural model and entropy coding, we present an alternative method to improving the runtime performance of various learned image compression models. We introduce multi-threaded pipelining and an optimized memory model to enable GPU and CPU workloads asynchronous execution, fully taking advantage of computational resources. Our architecture alone already produces excellent performance without any change to the neural model itself. We also demonstrate that combining our architecture with previous tweaks to the neural models can further improve runtime performance. We show that our implementations excel in throughput and latency compared to the baseline and demonstrate the performance of our implementations by creating a real-time video streaming encoder-decoder sample application, with the encoder running on an embedded device.",2022-08-02T03:15:48Z,2022-08-02T03:15:48Z,http://arxiv.org/abs/2208.01641v1,http://arxiv.org/pdf/2208.01641v1,"eess.IV, cs.CV"
Neural Optimization Machine: A Neural Network Approach for Optimization,"Jie Chen, Yongming Liu","A novel neural network (NN) approach is proposed for constrained optimization. The proposed method uses a specially designed NN architecture and training/optimization procedure called Neural Optimization Machine (NOM). The objective functions for the NOM are approximated with NN models. The optimization process is conducted by the neural network's built-in backpropagation algorithm. The NOM solves optimization problems by extending the architecture of the NN objective function model. This is achieved by appropriately designing the NOM's structure, activation function, and loss function. The NN objective function can have arbitrary architectures and activation functions. The application of the NOM is not limited to specific optimization problems, e.g., linear and quadratic programming. It is shown that the increase of dimension of design variables does not increase the computational cost significantly. Then, the NOM is extended for multiobjective optimization. Finally, the NOM is tested using numerical optimization problems and applied for the optimal design of processing parameters in additive manufacturing.",2022-08-08T03:34:58Z,2022-08-08T03:34:58Z,http://arxiv.org/abs/2208.03897v1,http://arxiv.org/pdf/2208.03897v1,"stat.ML, cs.LG, math.OC"
Inhale: Enabling High-Performance and Energy-Efficient In-SRAM   Cryptographic Hash for IoT,"Jingyao Zhang, Elaheh Sadredini","In the age of big data, information security has become a major issue of debate, especially with the rise of the Internet of Things (IoT), where attackers can effortlessly obtain physical access to edge devices. The hash algorithm is the current foundation for data integrity and authentication. However, it is challenging to provide a high-performance, high-throughput, and energy-efficient solution on resource-constrained edge devices. In this paper, we propose Inhale, an in-SRAM architecture to effectively compute hash algorithms with innovative data alignment and efficient read/write strategies to implicitly execute data shift operations through the in-situ controller. We present two variations of Inhale: Inhale-Opt, which is optimized for latency, throughput, and area-overhead; and Inhale-Flex, which offers flexibility in repurposing a part of last-level caches for hash computation. We thoroughly evaluate our proposed architectures on both SRAM and ReRAM memories and compare them with the state-of-the-art in-memory and ASIC accelerators. Our performance evaluation confirms that Inhale can achieve 1.4x - 14.5x higher throughput-per-area and about two-orders-of-magnitude higher throughput-per-area-per-energy compared to the state-of-the-art solutions.",2022-08-16T07:29:36Z,2022-08-16T07:29:36Z,http://arxiv.org/abs/2208.07570v1,http://arxiv.org/pdf/2208.07570v1,"cs.CR, cs.AR, 68Mxx"
Leukocyte Classification using Multimodal Architecture Enhanced by   Knowledge Distillation,"Litao Yang, Deval Mehta, Dwarikanath Mahapatra, Zongyuan Ge","Recently, a lot of automated white blood cells (WBC) or leukocyte classification techniques have been developed. However, all of these methods only utilize a single modality microscopic image i.e. either blood smear or fluorescence based, thus missing the potential of a better learning from multimodal images. In this work, we develop an efficient multimodal architecture based on a first of its kind multimodal WBC dataset for the task of WBC classification. Specifically, our proposed idea is developed in two steps - 1) First, we learn modality specific independent subnetworks inside a single network only; 2) We further enhance the learning capability of the independent subnetworks by distilling knowledge from high complexity independent teacher networks. With this, our proposed framework can achieve a high performance while maintaining low complexity for a multimodal dataset. Our unique contribution is two-fold - 1) We present a first of its kind multimodal WBC dataset for WBC classification; 2) We develop a high performing multimodal architecture which is also efficient and low in complexity at the same time.",2022-08-17T14:54:04Z,2022-08-17T14:54:04Z,http://arxiv.org/abs/2208.08331v1,http://arxiv.org/pdf/2208.08331v1,"eess.IV, cs.CV, cs.LG"
Cloud Process Execution Engine: Architecture and Interfaces,"Juergen Mangler, Stefanie Rinderle-Ma","Process Execution Engines are a vital part of Business Process Management (BPM) and Manufacturing Orchestration Management (MOM), as they allow the business or manufacturing logic (expressed in a graphical notation such as BPMN) to be executed. This execution drives and supervises all interactions between humans, machines, software, and the environment. If done right, this will lead to a highly flexible, low-code, and easy to maintain solution, that allows for ad-hoc changes and functional evolution, as well as delivering a wealth of data for data-science applications. The Cloud Process Execution Engine CPEE.org implements a radically distributed scale-out architecture, together with a minimal set of interfaces, to allow for the simplest possible integration with existing services, machines, and existing data-analysis tools. Its open-source components can serve as a blueprint for future development of commercial solutions, and serves as a proven testbed for academic research, teaching, and industrial application since 2008. In this paper we present the architecture, interfaces that make CPEE.org possible, as well as discuss different lifecycle models utilized during execution to provide overarching support for a wide range of data-analysis tasks.",2022-08-25T17:04:10Z,2022-09-18T23:10:40Z,http://arxiv.org/abs/2208.12214v2,http://arxiv.org/pdf/2208.12214v2,"cs.OH, D.2.11; D.2.12; D.2.13; H.4"
Multi-Scale Architectures Matter: On the Adversarial Robustness of   Flow-based Lossless Compression,"Yi-chong Xia, Bin Chen, Yan Feng, Tian-shuo Ge","As a probabilistic modeling technique, the flow-based model has demonstrated remarkable potential in the field of lossless compression \cite{idf,idf++,lbb,ivpf,iflow},. Compared with other deep generative models (eg. Autoregressive, VAEs) \cite{bitswap,hilloc,pixelcnn++,pixelsnail} that explicitly model the data distribution probabilities, flow-based models perform better due to their excellent probability density estimation and satisfactory inference speed. In flow-based models, multi-scale architecture provides a shortcut from the shallow layer to the output layer, which significantly reduces the computational complexity and avoid performance degradation when adding more layers. This is essential for constructing an advanced flow-based learnable bijective mapping. Furthermore, the lightweight requirement of the model design in practical compression tasks suggests that flows with multi-scale architecture achieve the best trade-off between coding complexity and compression efficiency.",2022-08-26T15:17:43Z,2022-08-26T15:17:43Z,http://arxiv.org/abs/2208.12716v1,http://arxiv.org/pdf/2208.12716v1,"cs.CV, cs.IT, math.IT"
An Accurate and Hardware-Efficient Dual Spike Detector for Implantable   Neural Interfaces,"Xiaorang Guo, MohammadAli Shaeri, Mahsa Shoaran","Spike detection plays a central role in neural data processing and brain-machine interfaces (BMIs). A challenge for future-generation implantable BMIs is to build a spike detector that features both low hardware cost and high performance. In this work, we propose a novel hardware-efficient and high-performance spike detector for implantable BMIs. The proposed design is based on a dual-detector architecture with adaptive threshold estimation. The dual-detector comprises two separate TEO-based detectors that distinguish a spike occurrence based on its discriminating features in both high and low noise scenarios. We evaluated the proposed spike detection algorithm on the Wave Clus dataset. It achieved an average detection accuracy of 98.9%, and over 95% in high-noise scenarios, ensuring the reliability of our method. When realized in hardware with a sampling rate of 16kHz and 7-bits resolution, the detection accuracy is 97.4%. Designed in 65nm TSMC process, a 256-channel detector based on this architecture occupies only 682$\mu m^2$ /Channel and consumes 0.07$\mu$W/Channel, improving over the state-of-the-art spike detectors by 39.7% in power consumption and 78.8% in area, while maintaining a high accuracy.",2022-08-29T09:06:10Z,2022-08-29T09:06:10Z,http://arxiv.org/abs/2208.13432v1,http://arxiv.org/pdf/2208.13432v1,"eess.SP, cs.AR, cs.SY, eess.SY"
A Self-Similar Sine-Cosine Fractal Architecture for Multiport   Interferometers,"Jasvith Raj Basani, Sri Krishna Vadlamani, Saumil Bandyopadhyay, Dirk R. Englund, Ryan Hamerly","Multiport interferometers based on integrated beamsplitter meshes have recently captured interest as a platform for many emerging technologies. In this paper, we present a novel architecture for multiport interferometers based on the Sine-Cosine fractal decomposition of a unitary matrix. Our architecture is unique in that it is self-similar, enabling the construction of modular multi-chiplet devices. Due to this modularity, our design enjoys improved resilience to hardware imperfections as compared to conventional multiport interferometers. Additionally, the structure of our circuit enables systematic truncation, which is key in reducing the hardware footprint of the chip as well as compute time in training optical neural networks, while maintaining full connectivity. Numerical simulations show that truncation of these meshes gives robust performance even under large fabrication errors. This design is a step forward in the construction of large-scale programmable photonics, removing a major hurdle in scaling up to practical machine learning and quantum computing applications.",2022-09-07T17:45:52Z,2022-09-07T17:45:52Z,http://arxiv.org/abs/2209.03335v1,http://arxiv.org/pdf/2209.03335v1,"physics.optics, cs.ET"
Vision Transformer with Convolutional Encoder-Decoder for Hand Gesture   Recognition using 24 GHz Doppler Radar,"Kavinda Kehelella, Gayangana Leelarathne, Dhanuka Marasinghe, Nisal Kariyawasam, Viduneth Ariyarathna, Arjuna Madanayake, Ranga Rodrigo, Chamira U. S. Edussooriya","Transformers combined with convolutional encoders have been recently used for hand gesture recognition (HGR) using micro-Doppler signatures. We propose a vision-transformer-based architecture for HGR with multi-antenna continuous-wave Doppler radar receivers. The proposed architecture consists of three modules: a convolutional encoderdecoder, an attention module with three transformer layers, and a multi-layer perceptron. The novel convolutional decoder helps to feed patches with larger sizes to the attention module for improved feature extraction. Experimental results obtained with a dataset corresponding to a two-antenna continuous-wave Doppler radar receiver operating at 24 GHz (published by Skaria et al.) confirm that the proposed architecture achieves an accuracy of 98.3% which substantially surpasses the state-of-the-art on the used dataset.",2022-09-12T05:56:35Z,2022-09-12T05:56:35Z,http://arxiv.org/abs/2209.05032v1,http://arxiv.org/pdf/2209.05032v1,"eess.SP, cs.LG"
A Trio-Method for Retinal Vessel Segmentation using Image Processing,"Mahendra Kumar Gourisaria, Vinayak Singh, Manoj Sahni","Inner Retinal neurons are a most essential part of the retina and they are supplied with blood via retinal vessels. This paper primarily focuses on the segmentation of retinal vessels using a triple preprocessing approach. DRIVE database was taken into consideration and preprocessed by Gabor Filtering, Gaussian Blur, and Edge Detection by Sobel and Pruning. Segmentation was driven out by 2 proposed U-Net architectures. Both the architectures were compared in terms of all the standard performance metrics. Preprocessing generated varied interesting results which impacted the results shown by the UNet architectures for segmentation. This real-time deployment can help in the efficient pre-processing of images with better segmentation and detection.",2022-09-19T22:07:34Z,2022-09-19T22:07:34Z,http://arxiv.org/abs/2209.11230v1,http://arxiv.org/pdf/2209.11230v1,"eess.IV, cs.CV"
A heterogeneous group CNN for image super-resolution,"Chunwei Tian, Yanning Zhang, Wangmeng Zuo, Chia-Wen Lin, David Zhang, Yixuan Yuan","Convolutional neural networks (CNNs) have obtained remarkable performance via deep architectures. However, these CNNs often achieve poor robustness for image super-resolution (SR) under complex scenes. In this paper, we present a heterogeneous group SR CNN (HGSRCNN) via leveraging structure information of different types to obtain a high-quality image. Specifically, each heterogeneous group block (HGB) of HGSRCNN uses a heterogeneous architecture containing a symmetric group convolutional block and a complementary convolutional block in a parallel way to enhance internal and external relations of different channels for facilitating richer low-frequency structure information of different types. To prevent appearance of obtained redundant features, a refinement block with signal enhancements in a serial way is designed to filter useless information. To prevent loss of original information, a multi-level enhancement mechanism guides a CNN to achieve a symmetric architecture for promoting expressive ability of HGSRCNN. Besides, a parallel up-sampling mechanism is developed to train a blind SR model. Extensive experiments illustrate that the proposed HGSRCNN has obtained excellent SR performance in terms of both quantitative and qualitative analysis. Codes can be accessed at https://github.com/hellloxiaotian/HGSRCNN.",2022-09-26T04:14:59Z,2022-09-26T04:14:59Z,http://arxiv.org/abs/2209.12406v1,http://arxiv.org/pdf/2209.12406v1,"eess.IV, cs.CV"
Direct identification of continuous-time linear switched state-space   models,"Manas Mejari, Dario Piga","This paper presents an algorithm for direct continuous-time (CT) identification of linear switched state-space (LSS) models. The key idea for direct CT identification is based on an integral architecture consisting of an LSS model followed by an integral block. This architecture is used to approximate the continuous-time state map of a switched system. A properly constructed objective criterion is proposed based on the integral architecture in order to estimate the unknown parameters and signals of the LSS model. A coordinate descent algorithm is employed to optimize this objective, which alternates between computing the unknown model matrices, switching sequence and estimating the state variables. The effectiveness of the proposed algorithm is shown via a simulation case study.",2022-10-04T09:32:19Z,2022-10-04T09:32:19Z,http://arxiv.org/abs/2210.01488v1,http://arxiv.org/pdf/2210.01488v1,"eess.SY, cs.SY"
A Dynamic Grouping Strategy for Beyond Diagonal Reconfigurable   Intelligent Surfaces with Hybrid Transmitting and Reflecting Mode,"Hongyu Li, Shanpu Shen, Bruno Clerckx","Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a novel branch of RIS which breaks through the limitation of traditional RIS with diagonal scattering matrices. However, the existing research focuses on BD-RIS with fixed architectures regardless of channel state information (CSI), which limit the achievable performance of BD-RIS. To solve this issue, in this paper, we propose a novel dynamically group-connected BD-RIS based on a dynamic grouping strategy. Specifically, RIS antennas are dynamically divided into several subsets adapting to the CSI, yielding a permuted block-diagonal scattering matrix. To verify the effectiveness of the proposed dynamically group-connected BD-RIS, we propose an efficient algorithm to optimize the BD-RIS with dynamic grouping for a BD-RIS-assisted multi-user multiple-input single-output system. Simulation results show that the proposed dynamically group-connected architecture outperforms fixed group-connected architectures.",2022-10-05T18:28:12Z,2023-06-20T14:15:53Z,http://arxiv.org/abs/2210.02499v3,http://arxiv.org/pdf/2210.02499v3,"eess.SP, cs.IT, math.IT"
SpecRNet: Towards Faster and More Accessible Audio DeepFake Detection,"Piotr Kawa, Marcin Plata, Piotr Syga","Audio DeepFakes are utterances generated with the use of deep neural networks. They are highly misleading and pose a threat due to use in fake news, impersonation, or extortion. In this work, we focus on increasing accessibility to the audio DeepFake detection methods by providing SpecRNet, a neural network architecture characterized by a quick inference time and low computational requirements. Our benchmark shows that SpecRNet, requiring up to about 40% less time to process an audio sample, provides performance comparable to LCNN architecture - one of the best audio DeepFake detection models. Such a method can not only be used by online multimedia services to verify a large bulk of content uploaded daily but also, thanks to its low requirements, by average citizens to evaluate materials on their devices. In addition, we provide benchmarks in three unique settings that confirm the correctness of our model. They reflect scenarios of low-resource datasets, detection on short utterances and limited attacks benchmark in which we take a closer look at the influence of particular attacks on given architectures.",2022-10-12T11:36:14Z,2022-10-12T11:36:14Z,http://arxiv.org/abs/2210.06105v1,http://arxiv.org/pdf/2210.06105v1,"cs.SD, cs.LG, eess.AS"
CPSAA: Accelerating Sparse Attention using Crossbar-based   Processing-In-Memory Architecture,"Huize Li, Hai Jin, Long Zheng, Yu Huang, Xiaofei Liao, Dan Chen, Zhuohui Duan, Cong Liu, Jiahong Xu, Chuanyi Gui","The attention mechanism requires huge computational efforts to process unnecessary calculations, significantly limiting the system's performance. Researchers propose sparse attention to convert some DDMM operations to SDDMM and SpMM operations. However, current sparse attention solutions introduce massive off-chip random memory access. We propose CPSAA, a novel crossbar-based PIM-featured sparse attention accelerator. First, we present a novel attention calculation mode. Second, we design a novel PIM-based sparsity pruning architecture. Finally, we present novel crossbar-based methods. Experimental results show that CPSAA has an average of 89.6X, 32.2X, 17.8X, 3.39X, and 3.84X performance improvement and 755.6X, 55.3X, 21.3X, 5.7X, and 4.9X energy-saving when compare with GPU, FPGA, SANGER, ReBERT, and ReTransformer.",2022-10-13T03:20:11Z,2023-10-07T10:27:43Z,http://arxiv.org/abs/2210.06696v2,http://arxiv.org/pdf/2210.06696v2,"cs.AR, cs.SY, eess.SY"
FPGA Hardware Acceleration for Feature-Based Relative Navigation   Applications,"Ramchander Rao Bhaskara, Manoranjan Majji","Estimation of rigid transformation between two point clouds is a computationally challenging problem in vision-based relative navigation. Targeting a real-time navigation solution utilizing point-cloud and image registration algorithms, this paper develops high-performance avionics for power and resource constrained pose estimation framework. A Field-Programmable Gate Array (FPGA) based embedded architecture is developed to accelerate estimation of relative pose between the point-clouds, aided by image features that correspond to the individual point sets. At algorithmic level, the pose estimation method is an adaptation of Optimal Linear Attitude and Translation Estimator (OLTAE) for relative attitude and translation estimation. At the architecture level, the proposed embedded solution is a hardware/software co-design that evaluates the OLTAE computations on the bare-metal hardware for high-speed state estimation. The finite precision FPGA evaluation of the OLTAE algorithm is compared with a double-precision evaluation on MATLAB for performance analysis and error quantification. Implementation results of the proposed finite-precision OLTAE accelerator demonstrate the high-performance compute capabilities of the FPGA-based pose estimation while offering relative numerical errors below 7%.",2022-10-18T00:01:57Z,2022-10-18T00:01:57Z,http://arxiv.org/abs/2210.09481v1,http://arxiv.org/pdf/2210.09481v1,"cs.RO, cs.AR, cs.CV, eess.IV"
Optimisation & Generalisation in Networks of Neurons,Jeremy Bernstein,"The goal of this thesis is to develop the optimisation and generalisation theoretic foundations of learning in artificial neural networks. On optimisation, a new theoretical framework is proposed for deriving architecture-dependent first-order optimisation algorithms. The approach works by combining a ""functional majorisation"" of the loss function with ""architectural perturbation bounds"" that encode an explicit dependence on neural architecture. The framework yields optimisation methods that transfer hyperparameters across learning problems. On generalisation, a new correspondence is proposed between ensembles of networks and individual networks. It is argued that, as network width and normalised margin are taken large, the space of networks that interpolate a particular training set concentrates on an aggregated Bayesian method known as a ""Bayes point machine"". This correspondence provides a route for transferring PAC-Bayesian generalisation theorems over to individual networks. More broadly, the correspondence presents a fresh perspective on the role of regularisation in networks with vastly more parameters than data.",2022-10-18T18:58:40Z,2022-10-18T18:58:40Z,http://arxiv.org/abs/2210.10101v1,http://arxiv.org/pdf/2210.10101v1,"cs.NE, cs.AI, cs.IT, cs.LG, cs.NA, math.IT, math.NA"
Pipelined Architecture for Soft-decision Iterative Projection   Aggregation Decoding for RM Codes,"Marzieh Hashemipour-Nazari, Yuqing Ren, Kees Goossens, Alexios Balatsoukas-Stimming","The recently proposed recursive projection-aggregation (RPA) decoding algorithm for Reed-Muller codes has received significant attention as it provides near-ML decoding performance at reasonable complexity for short codes. However, its complicated structure makes it unsuitable for hardware implementation. Iterative projection-aggregation (IPA) decoding is a modified version of RPA decoding that simplifies the hardware implementation. In this work, we present a flexible hardware architecture for the IPA decoder that can be configured from fully-sequential to fully-parallel, thus making it suitable for a wide range of applications with different constraints and resource budgets. Our simulation and implementation results show that the IPA decoder has 41% lower area consumption, 44% lower latency, four times higher throughput, but currently seven times higher power consumption for a code with block length of 128 and information length of 29 compared to a state-of-the-art polar successive cancellation list (SCL) decoder with comparable decoding performance.",2022-10-20T07:43:49Z,2023-09-06T09:04:04Z,http://arxiv.org/abs/2210.11069v2,http://arxiv.org/pdf/2210.11069v2,"cs.IT, cs.AR, eess.SP, math.IT"
Block subsampled randomized Hadamard transform for low-rank   approximation on distributed architectures,"Oleg Balabanov, Matthias Beaupere, Laura Grigori, Victor Lederer","This article introduces a novel structured random matrix composed blockwise from subsampled randomized Hadamard transforms (SRHTs). The block SRHT is expected to outperform well-known dimension reduction maps, including SRHT and Gaussian matrices, on distributed architectures with not too many cores compared to the dimension. We prove that a block SRHT with enough rows is an oblivious subspace embedding, i.e., an approximate isometry for an arbitrary low-dimensional subspace with high probability. Our estimate of the required number of rows is similar to that of the standard SRHT. This suggests that the two transforms should provide the same accuracy of approximation in the algorithms. The block SRHT can be readily incorporated into randomized methods, for instance to compute a low-rank approximation of a large-scale matrix. For completeness, we revisit some common randomized approaches for this problem such as Randomized Singular Value Decomposition and Nystr\""{o}m approximation, with a discussion of their accuracy and implementation on distributed architectures.",2022-10-20T14:29:33Z,2022-10-20T14:29:33Z,http://arxiv.org/abs/2210.11295v1,http://arxiv.org/pdf/2210.11295v1,"math.NA, cs.DS, cs.NA"
Latency-aware End-to-end Multi-path Data Transmission for URLLC Services,"Liu Cao, Abbas Kiani, Amanda Xiang, Kaippallimalil John, Tony Saboorian","5th Generation Mobile Communication Technology (5G) utilizes the Access Traffic Steering, Switching, and Splitting (ATSSS) rule to enable multi-path data transmission, which is currently being standardized. Recently, the 3rd Generation Partnership Project (3GPP) SA1 and SA2 have been working on the multi-path solution for possible improvement from different perspectives. However, the existing 3GPP multi-path solution has some limitations on ultra-reliable low-latency communication (URLLC) traffic in terms of reliability and latency requirements. In order to capture the potential gains of multi-path architecture in the context of URLLC services, this paper proposes a novel traffic splitting technique that can more efficiently enjoy the benefit of multi-path architecture in reducing user equipment (UE) uplink (UL) end-to-end (E2E) latency. In particular, we formulate an optimization framework that minimizes user's UL E2E latency via the joint optimization on the ratio of traffic assigned to each path and their corresponding transmit power. The performance of the proposed scheme is evaluated via well-designed simulations.",2022-10-25T03:14:19Z,2023-10-21T19:33:23Z,http://arxiv.org/abs/2210.13740v2,http://arxiv.org/pdf/2210.13740v2,"cs.NI, cs.SY, eess.SY"
Does Medical Imaging learn different Convolution Filters?,"Paul Gavrikov, Janis Keuper","Recent work has investigated the distributions of learned convolution filters through a large-scale study containing hundreds of heterogeneous image models. Surprisingly, on average, the distributions only show minor drifts in comparisons of various studied dimensions including the learned task, image domain, or dataset. However, among the studied image domains, medical imaging models appeared to show significant outliers through ""spikey"" distributions, and, therefore, learn clusters of highly specific filters different from other domains. Following this observation, we study the collected medical imaging models in more detail. We show that instead of fundamental differences, the outliers are due to specific processing in some architectures. Quite the contrary, for standardized architectures, we find that models trained on medical data do not significantly differ in their filter distributions from similar architectures trained on data from other domains. Our conclusions reinforce previous hypotheses stating that pre-training of imaging models can be done with any kind of diverse image data.",2022-10-25T07:05:46Z,2022-10-25T07:05:46Z,http://arxiv.org/abs/2210.13799v1,http://arxiv.org/pdf/2210.13799v1,"eess.IV, cs.CV, cs.LG"
Tangent Bundle Filters and Neural Networks: from Manifolds to Cellular   Sheaves and Back,"Claudio Battiloro, Zhiyang Wang, Hans Riess, Paolo Di Lorenzo, Alejandro Ribeiro","In this work we introduce a convolution operation over the tangent bundle of Riemannian manifolds exploiting the Connection Laplacian operator. We use the convolution to define tangent bundle filters and tangent bundle neural networks (TNNs), novel continuous architectures operating on tangent bundle signals, i.e. vector fields over manifolds. We discretize TNNs both in space and time domains, showing that their discrete counterpart is a principled variant of the recently introduced Sheaf Neural Networks. We formally prove that this discrete architecture converges to the underlying continuous TNN. We numerically evaluate the effectiveness of the proposed architecture on a denoising task of a tangent vector field over the unit 2-sphere.",2022-10-26T21:55:45Z,2022-11-18T15:38:30Z,http://arxiv.org/abs/2210.15058v3,http://arxiv.org/pdf/2210.15058v3,"eess.SP, cs.LG"
Light-weighted CNN-Attention based architecture for Hand Gesture   Recognition via ElectroMyography,"Soheil Zabihi, Elahe Rahimian, Amir Asif, Arash Mohammadi","Advancements in Biological Signal Processing (BSP) and Machine-Learning (ML) models have paved the path for development of novel immersive Human-Machine Interfaces (HMI). In this context, there has been a surge of significant interest in Hand Gesture Recognition (HGR) utilizing Surface-Electromyogram (sEMG) signals. This is due to its unique potential for decoding wearable data to interpret human intent for immersion in Mixed Reality (MR) environments. To achieve the highest possible accuracy, complicated and heavy-weighted Deep Neural Networks (DNNs) are typically developed, which restricts their practical application in low-power and resource-constrained wearable systems. In this work, we propose a light-weighted hybrid architecture (HDCAM) based on Convolutional Neural Network (CNN) and attention mechanism to effectively extract local and global representations of the input. The proposed HDCAM model with 58,441 parameters reached a new state-of-the-art (SOTA) performance with 82.91% and 81.28% accuracy on window sizes of 300 ms and 200 ms for classifying 17 hand gestures. The number of parameters to train the proposed HDCAM architecture is 18.87 times less than its previous SOTA counterpart.",2022-10-27T02:12:07Z,2022-10-27T02:12:07Z,http://arxiv.org/abs/2210.15119v1,http://arxiv.org/pdf/2210.15119v1,"cs.LG, eess.SP"
Space-Time Graph Neural Networks with Stochastic Graph Perturbations,"Samar Hadou, Charilaos Kanatsoulis, Alejandro Ribeiro","Space-time graph neural networks (ST-GNNs) are recently developed architectures that learn efficient graph representations of time-varying data. ST-GNNs are particularly useful in multi-agent systems, due to their stability properties and their ability to respect communication delays between the agents. In this paper we revisit the stability properties of ST-GNNs and prove that they are stable to stochastic graph perturbations. Our analysis suggests that ST-GNNs are suitable for transfer learning on time-varying graphs and enables the design of generalized convolutional architectures that jointly process time-varying graphs and time-varying signals. Numerical experiments on decentralized control systems validate our theoretical results and showcase the benefits of traditional and generalized ST-GNN architectures.",2022-10-28T16:59:51Z,2022-10-28T16:59:51Z,http://arxiv.org/abs/2210.16270v1,http://arxiv.org/pdf/2210.16270v1,"cs.LG, eess.SP"
A Law of Data Separation in Deep Learning,"Hangfeng He, Weijie J. Su","While deep learning has enabled significant advances in many areas of science, its black-box nature hinders architecture design for future artificial intelligence applications and interpretation for high-stakes decision makings. We addressed this issue by studying the fundamental question of how deep neural networks process data in the intermediate layers. Our finding is a simple and quantitative law that governs how deep neural networks separate data according to class membership throughout all layers for classification. This law shows that each layer improves data separation at a constant geometric rate, and its emergence is observed in a collection of network architectures and datasets during training. This law offers practical guidelines for designing architectures, improving model robustness and out-of-sample performance, as well as interpreting the predictions.",2022-10-31T02:25:38Z,2023-08-11T00:47:30Z,http://arxiv.org/abs/2210.17020v2,http://arxiv.org/pdf/2210.17020v2,"cs.LG, cs.AI, cs.CV, cs.IT, math.IT, stat.ML"
PELICAN: Permutation Equivariant and Lorentz Invariant or Covariant   Aggregator Network for Particle Physics,"Alexander Bogatskiy, Timothy Hoffman, David W. Miller, Jan T. Offermann","Many current approaches to machine learning in particle physics use generic architectures that require large numbers of parameters and disregard underlying physics principles, limiting their applicability as scientific modeling tools. In this work, we present a machine learning architecture that uses a set of inputs maximally reduced with respect to the full 6-dimensional Lorentz symmetry, and is fully permutation-equivariant throughout. We study the application of this network architecture to the standard task of top quark tagging and show that the resulting network outperforms all existing competitors despite much lower model complexity. In addition, we present a Lorentz-covariant variant of the same network applied to a 4-momentum regression task.",2022-11-01T13:36:50Z,2022-12-23T20:08:14Z,http://arxiv.org/abs/2211.00454v2,http://arxiv.org/pdf/2211.00454v2,"hep-ph, cs.LG, hep-ex"
Conversation-oriented ASR with multi-look-ahead CBS architecture,"Huaibo Zhao, Shinya Fujie, Tetsuji Ogawa, Jin Sakuma, Yusuke Kida, Tetsunori Kobayashi","During conversations, humans are capable of inferring the intention of the speaker at any point of the speech to prepare the following action promptly. Such ability is also the key for conversational systems to achieve rhythmic and natural conversation. To perform this, the automatic speech recognition (ASR) used for transcribing the speech in real-time must achieve high accuracy without delay. In streaming ASR, high accuracy is assured by attending to look-ahead frames, which leads to delay increments. To tackle this trade-off issue, we propose a multiple latency streaming ASR to achieve high accuracy with zero look-ahead. The proposed system contains two encoders that operate in parallel, where a primary encoder generates accurate outputs utilizing look-ahead frames, and the auxiliary encoder recognizes the look-ahead portion of the primary encoder without look-ahead. The proposed system is constructed based on contextual block streaming (CBS) architecture, which leverages block processing and has a high affinity for the multiple latency architecture. Various methods are also studied for architecting the system, including shifting the network to perform as different encoders; as well as generating both encoders' outputs in one encoding pass.",2022-11-02T03:58:56Z,2022-11-02T03:58:56Z,http://arxiv.org/abs/2211.00858v1,http://arxiv.org/pdf/2211.00858v1,"cs.SD, eess.AS"
Executable Models and Instance Tracking for Decentralized Applications   -- Towards an Architecture Based on Blockchains and Cloud Platforms,Felix Härer,"The execution of decentralized applications on blockchains is limited today by technical and organizational barriers, including scalability and the high complexity to specify execution correctly for developers as well as for domain experts in organizations. Overcoming these limitations could allow for decentralized coordination beyond data, where distributed parties rely on higher-level abstractions for coordinating their actions using decentralized applications, not limited to organizations. Towards this goal, the paper at hand proposes executable models as high-level abstraction that can be observed and tracked by distributed parties. In particular, it is investigated how executable models on cloud platforms can be coupled with smart contracts for tracking their execution, concluding with an architecture as exploratory research result towards supporting scalability and decentralized coordination.",2022-11-02T16:40:50Z,2022-11-11T15:30:37Z,http://arxiv.org/abs/2211.01260v3,http://arxiv.org/pdf/2211.01260v3,"cs.DC, cs.NI, C.2.4; D.3.2; E.2; H.2.3"
Predictive Crypto-Asset Automated Market Making Architecture for   Decentralized Finance using Deep Reinforcement Learning,Tristan Lim,"The study proposes a quote-driven predictive automated market maker (AMM) platform with on-chain custody and settlement functions, alongside off-chain predictive reinforcement learning capabilities to improve liquidity provision of real-world AMMs. The proposed AMM architecture is an augmentation to the Uniswap V3, a cryptocurrency AMM protocol, by utilizing a novel market equilibrium pricing for reduced divergence and slippage loss. Further, the proposed architecture involves a predictive AMM capability, utilizing a deep hybrid Long Short-Term Memory (LSTM) and Q-learning reinforcement learning framework that looks to improve market efficiency through better forecasts of liquidity concentration ranges, so liquidity starts moving to expected concentration ranges, prior to asset price movement, so that liquidity utilization is improved. The augmented protocol framework is expected have practical real-world implications, by (i) reducing divergence loss for liquidity providers, (ii) reducing slippage for crypto-asset traders, while (iii) improving capital efficiency for liquidity provision for the AMM protocol. To our best knowledge, there are no known protocol or literature that are proposing similar deep learning-augmented AMM that achieves similar capital efficiency and loss minimization objectives for practical real-world applications.",2022-09-28T01:13:22Z,2023-01-26T11:20:57Z,http://arxiv.org/abs/2211.01346v2,http://arxiv.org/pdf/2211.01346v2,"q-fin.TR, cs.AI, cs.LG, q-fin.CP, 91-08, J.1"
Translated Skip Connections -- Expanding the Receptive Fields of Fully   Convolutional Neural Networks,"Joshua Bruton, Hairong Wang","The effective receptive field of a fully convolutional neural network is an important consideration when designing an architecture, as it defines the portion of the input visible to each convolutional kernel. We propose a neural network module, extending traditional skip connections, called the translated skip connection. Translated skip connections geometrically increase the receptive field of an architecture with negligible impact on both the size of the parameter space and computational complexity. By embedding translated skip connections into a benchmark architecture, we demonstrate that our module matches or outperforms four other approaches to expanding the effective receptive fields of fully convolutional neural networks. We confirm this result across five contemporary image segmentation datasets from disparate domains, including the detection of COVID-19 infection, segmentation of aerial imagery, common object segmentation, and segmentation for self-driving cars.",2022-11-03T19:30:40Z,2022-11-03T19:30:40Z,http://arxiv.org/abs/2211.02111v1,http://arxiv.org/pdf/2211.02111v1,"cs.CV, cs.LG, eess.IV, I.2.10"
Real-Time Target Sound Extraction,"Bandhav Veluri, Justin Chan, Malek Itani, Tuochao Chen, Takuya Yoshioka, Shyamnath Gollakota","We present the first neural network model to achieve real-time and streaming target sound extraction. To accomplish this, we propose Waveformer, an encoder-decoder architecture with a stack of dilated causal convolution layers as the encoder, and a transformer decoder layer as the decoder. This hybrid architecture uses dilated causal convolutions for processing large receptive fields in a computationally efficient manner while also leveraging the generalization performance of transformer-based architectures. Our evaluations show as much as 2.2-3.3 dB improvement in SI-SNRi compared to the prior models for this task while having a 1.2-4x smaller model size and a 1.5-2x lower runtime. We provide code, dataset, and audio samples: https://waveformer.cs.washington.edu/.",2022-11-04T03:51:23Z,2023-04-19T09:43:32Z,http://arxiv.org/abs/2211.02250v3,http://arxiv.org/pdf/2211.02250v3,"cs.SD, cs.LG, eess.AS"
Pit-Pattern Classification of Colorectal Cancer Polyps Using a Hyper   Sensitive Vision-Based Tactile Sensor and Dilated Residual Networks,"Nethra Venkatayogi, Qin Hu, Ozdemir Can Kara, Tarunraj G. Mohanraj, S. Farokh Atashzar, Farshid Alambeigi","In this study, with the goal of reducing the early detection miss rate of colorectal cancer (CRC) polyps, we propose utilizing a novel hyper-sensitive vision-based tactile sensor called HySenSe and a complementary and novel machine learning (ML) architecture that explores the potentials of utilizing dilated convolutions, the beneficial features of the ResNet architecture, and the transfer learning concept applied on a small dataset with the scale of hundreds of images. The proposed tactile sensor provides high-resolution 3D textural images of CRC polyps that will be used for their accurate classification via the proposed dilated residual network. To collect realistic surface patterns of CRC polyps for training the ML models and evaluating their performance, we first designed and additively manufactured 160 unique realistic polyp phantoms consisting of 4 different hardness. Next, the proposed architecture was compared with the state-of-the-art ML models (e.g., AlexNet and DenseNet) and proved to be superior in terms of performance and complexity.",2022-11-13T04:42:10Z,2022-11-13T04:42:10Z,http://arxiv.org/abs/2211.06814v1,http://arxiv.org/pdf/2211.06814v1,"cs.LG, cs.RO, eess.SP"
Learning Stable Graph Neural Networks via Spectral Regularization,"Zhan Gao, Elvin Isufi","Stability of graph neural networks (GNNs) characterizes how GNNs react to graph perturbations and provides guarantees for architecture performance in noisy scenarios. This paper develops a self-regularized graph neural network (SR-GNN) solution that improves the architecture stability by regularizing the filter frequency responses in the graph spectral domain. The SR-GNN considers not only the graph signal as input but also the eigenvectors of the underlying graph, where the signal is processed to generate task-relevant features and the eigenvectors to characterize the frequency responses at each layer. We train the SR-GNN by minimizing the cost function and regularizing the maximal frequency response close to one. The former improves the architecture performance, while the latter tightens the perturbation stability and alleviates the information loss through multi-layer propagation. We further show the SR-GNN preserves the permutation equivariance, which allows to explore the internal symmetries of graph signals and to exhibit transference on similar graph structures. Numerical results with source localization and movie recommendation corroborate our findings and show the SR-GNN yields a comparable performance with the vanilla GNN on the unperturbed graph but improves substantially the stability.",2022-11-13T17:27:21Z,2022-11-13T17:27:21Z,http://arxiv.org/abs/2211.06966v1,http://arxiv.org/pdf/2211.06966v1,"eess.SP, cs.LG"
Privacy and Security in Network Controlled Systems via Dynamic Masking,"Mohamed Abdalmoaty, Sribalaji C. Anand, André M. H. Teixeira","In this paper, we propose a new architecture to enhance the privacy and security of networked control systems against malicious adversaries. We consider an adversary which first learns the system dynamics (privacy) using system identification techniques, and then performs a data injection attack (security). In particular, we consider an adversary conducting zero-dynamics attacks (ZDA) which maximizes the performance cost of the system whilst staying undetected. However, using the proposed architecture, we show that it is possible to (i) introduce significant bias in the system estimates of the adversary: thus providing privacy of the system parameters, and (ii) efficiently detect attacks when the adversary performs a ZDA using the identified system: thus providing security. Through numerical simulations, we illustrate the efficacy of the proposed architecture.",2022-11-14T13:10:51Z,2022-11-14T13:10:51Z,http://arxiv.org/abs/2211.07328v1,http://arxiv.org/pdf/2211.07328v1,"eess.SY, cs.SY"
An Integrated Optical Circuit Architecture for Inverse-Designed Silicon   Photonic Components,"Richard Soref, Dusan Gostimirovic","In this work, we demonstrate a compact toolkit of inverse-designed topologically optimized silicon-photonic devices that are arranged in a plug-and-play fashion to realize many different photonic integrated circuits, both passive and active, each with a small footprint. The silicon-on-insulator 1550-nm toolkit contains a 2x2 3dB splitter-combiner, a 2x2 waveguide crossover and a 2x2 all-forward add-drop resonator. The resonator can become a 2x2 electro-optical crossbar switch by means of the thermo-optical effect or phase-change cladding or free-carrier injection. For each of the ten circuits demonstrated in this work, the toolkit of photonic devices enables the compact circuit to achieve low insertion loss and low crosstalk. By adopting the sophisticated inverse-design approach, the design structure, shape, and sizing of each individual device can be made more flexible to better suit the architecture of the greater circuit. For a compact architecture, we present a unified, parallel waveguide circuit framework into which the devices are designed to fit seamlessly, thus enabling low-complexity circuit design.",2022-11-16T23:07:23Z,2022-11-16T23:07:23Z,http://arxiv.org/abs/2211.09257v1,http://arxiv.org/pdf/2211.09257v1,"cs.ET, physics.optics"
HiveNAS: Neural Architecture Search using Artificial Bee Colony   Optimization,"Mohamed Shahawy, Elhadj Benkhelifa","The traditional Neural Network-development process requires substantial expert knowledge and relies heavily on intuition and trial-and-error. Neural Architecture Search (NAS) frameworks were introduced to robustly search for network topologies, as well as facilitate the automated development of Neural Networks. While some optimization approaches -- such as Genetic Algorithms -- have been extensively explored in the NAS context, other Metaheuristic Optimization algorithms have not yet been investigated. In this study, we evaluate the viability of Artificial Bee Colony optimization for Neural Architecture Search. Our proposed framework, HiveNAS, outperforms existing state-of-the-art Swarm Intelligence-based NAS frameworks in a fraction of the time.",2022-11-18T14:11:47Z,2023-06-15T17:02:09Z,http://arxiv.org/abs/2211.10250v2,http://arxiv.org/pdf/2211.10250v2,"cs.NE, cs.AI, cs.LG, I.2.0; G.1.6; I.2.2; G.3; I.2.11"
System architecture of a four-wheel drive Formula Student vehicle,"Adriano Schommer, Gordana Collier, Robert Norris, Denise Morrey, Ludmila Nesi Maria, Chris Johnston","Formula Student vehicles are becoming increasingly complex, especially with the current shift from internal combustion engines toward electric powertrains. The interaction between software and hardware is complex and imposes additional challenges for systems integration. This paper provides a structured introduction to the OBR22 Oxford Brookes Racing Formula Student electric vehicle. From a system architecture perspective, the four-wheel drive in-hub motors topology is described. Diagrams of the hardware components, the architecture of the high voltage and communication systems are presented. This paper also demonstrates the model-based development process, including an overview of the model-in-the-loop (MiL) and hardware-in-the-loop (HiL) control design phases.",2022-11-18T19:26:30Z,2022-11-18T19:26:30Z,http://arxiv.org/abs/2211.10476v1,http://arxiv.org/pdf/2211.10476v1,"eess.SY, cs.SY"
PMNet: Robust Pathloss Map Prediction via Supervised Learning,"Ju-Hyung Lee, Omer Gokalp Serbetci, Dheeraj Panneer Selvam, Andreas F. Molisch","Pathloss prediction is an essential component of wireless network planning. While ray tracing based methods have been successfully used for many years, they require significant computational effort that may become prohibitive with the increased network densification and/or use of higher frequencies in 5G/B5G (beyond 5G) systems. In this paper, we propose and evaluate a data-driven and model-free pathloss prediction method, dubbed PMNet. This method uses a supervised learning approach: training a neural network (NN) with a limited amount of ray tracing (or channel measurement) data and map data and then predicting the pathloss over location with no ray tracing data with a high level of accuracy. Our proposed pathloss map prediction-oriented NN architecture, which is empowered by state-of-the-art computer vision techniques, outperforms other architectures that have been previously proposed (e.g., UNet, RadioUNet) in terms of accuracy while showing generalization capability. Moreover, PMNet trained on a 4-fold smaller dataset surpasses the other baselines (trained on a 4-fold larger dataset), corroborating the potential of PMNet.",2022-11-18T22:50:01Z,2023-05-16T22:40:56Z,http://arxiv.org/abs/2211.10527v2,http://arxiv.org/pdf/2211.10527v2,"cs.NI, eess.SP"
Enterprise Model Library for Business-IT-Alignment,"Peter Hillmann, Diana Schnell, Harald Hagel, Andreas Karcher","The knowledge of the world is passed on through libraries. Accordingly, domain expertise and experiences should also be transferred within an enterprise by a knowledge base. Therefore, models are an established medium to describe good practices for complex systems, processes, and interconnections. However, there is no structured and detailed approach for a design of an enterprise model library. The objective of this work is the reference architecture of a repository for models with function of reuse. It includes the design of the data structure for filing, the processes for administration and possibilities for usage. Our approach enables consistent mapping of requirements into models via meta-data attributes. Furthermore, the adaptation of reference architectures in specific use cases as well as a reconciliation of interrelationships is enabled. A case study with industry demonstrates the practical benefits of reusing work already done. It provides an organization with systematic access to specifications, standards and guidelines. Thus, further development is accelerated and supported in a structured manner, while complexity remains controllable. The presented approach enriches various enterprise architecture frameworks. It provides benefits for development based on models.",2022-11-21T11:36:54Z,2022-11-21T11:36:54Z,http://arxiv.org/abs/2211.11369v1,http://arxiv.org/pdf/2211.11369v1,"cs.SE, cs.CV, cs.DL, cs.SY, eess.SY"
RAMP: A Flat Nanosecond Optical Network and MPI Operations for   Distributed Deep Learning Systems,"Alessandro Ottino, Joshua Benjamin, Georgios Zervas","Distributed deep learning (DDL) systems strongly depend on network performance. Current electronic packet switched (EPS) network architectures and technologies suffer from variable diameter topologies, low-bisection bandwidth and over-subscription affecting completion time of communication and collective operations.   We introduce a near-exascale, full-bisection bandwidth, all-to-all, single-hop, all-optical network architecture with nanosecond reconfiguration called RAMP, which supports large-scale distributed and parallel computing systems (12.8~Tbps per node for up to 65,536 nodes).   For the first time, a custom RAMP-x MPI strategy and a network transcoder is proposed to run MPI collective operations across the optical circuit switched (OCS) network in a schedule-less and contention-less manner. RAMP achieves 7.6-171$\times$ speed-up in completion time across all MPI operations compared to realistic EPS and OCS counterparts. It can also deliver a 1.3-16$\times$ and 7.8-58$\times$ reduction in Megatron and DLRM training time respectively} while offering 42-53$\times$ and 3.3-12.4$\times$ improvement in energy consumption and cost respectively.",2022-11-28T11:24:51Z,2023-02-24T11:25:22Z,http://arxiv.org/abs/2211.15226v2,http://arxiv.org/pdf/2211.15226v2,"cs.DC, cs.LG, cs.NI, cs.SY, eess.SY"
Neural Speech Phase Prediction based on Parallel Estimation Architecture   and Anti-Wrapping Losses,"Yang Ai, Zhen-Hua Ling","This paper presents a novel speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra by neural networks. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is composed of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. Experimental results show that our proposed neural speech phase prediction model outperforms the iterative Griffin-Lim algorithm and other neural network-based method, in terms of both reconstructed speech quality and generation speed.",2022-11-29T07:16:24Z,2023-02-16T09:15:48Z,http://arxiv.org/abs/2211.15974v2,http://arxiv.org/pdf/2211.15974v2,"cs.SD, eess.AS"
On Solution Functions of Optimization: Universal Approximation and   Covering Number Bounds,"Ming Jin, Vanshaj Khattar, Harshal Kaushik, Bilgehan Sel, Ruoxi Jia","We study the expressibility and learnability of convex optimization solution functions and their multi-layer architectural extension. The main results are: \emph{(1)} the class of solution functions of linear programming (LP) and quadratic programming (QP) is a universal approximant for the $C^k$ smooth model class or some restricted Sobolev space, and we characterize the rate-distortion, \emph{(2)} the approximation power is investigated through a viewpoint of regression error, where information about the target function is provided in terms of data observations, \emph{(3)} compositionality in the form of a deep architecture with optimization as a layer is shown to reconstruct some basic functions used in numerical analysis without error, which implies that \emph{(4)} a substantial reduction in rate-distortion can be achieved with a universal network architecture, and \emph{(5)} we discuss the statistical bounds of empirical covering numbers for LP/QP, as well as a generic optimization problem (possibly nonconvex) by exploiting tame geometry. Our results provide the \emph{first rigorous analysis of the approximation and learning-theoretic properties of solution functions} with implications for algorithmic design and performance guarantees.",2022-12-02T17:16:04Z,2022-12-02T17:16:04Z,http://arxiv.org/abs/2212.01314v1,http://arxiv.org/pdf/2212.01314v1,"cs.LG, math.OC"
Encoder-Decoder Network with Guided Transmission Map: Architecture,"Le-Anh Tran, Dong-Chul Park","An insight into the architecture of the Encoder-Decoder Network with Guided Transmission Map (EDN-GTM), a novel and effective single image dehazing scheme, is presented in this paper. The EDN-GTM takes a conventional RGB hazy image in conjunction with the corresponding transmission map estimated by the dark channel prior (DCP) approach as inputs of the network. The EDN-GTM adopts an enhanced structure of U-Net developed for dehazing tasks and the resulting EDN-GDM has shown state-of-the-art performances on benchmark dehazing datasets in terms of PSNR and SSIM metrics. In order to give an in-depth understanding of the well-designed architecture which largely contributes to the success of the EDN-GTM, extensive experiments and analysis from selecting the core structure of the scheme to investigating advanced network designs are presented in this paper.",2022-12-07T08:16:26Z,2023-03-31T09:16:11Z,http://arxiv.org/abs/2212.05936v2,http://arxiv.org/pdf/2212.05936v2,"cs.CV, eess.IV"
Solving the Wide-band Inverse Scattering Problem via Equivariant Neural   Networks,"Borong Zhang, Leonardo Zepeda-Núñez, Qin Li","This paper introduces a novel deep neural network architecture for solving the inverse scattering problem in frequency domain with wide-band data, by directly approximating the inverse map, thus avoiding the expensive optimization loop of classical methods. The architecture is motivated by the filtered back-projection formula in the full aperture regime and with homogeneous background, and it leverages the underlying equivariance of the problem and compressibility of the integral operator. This drastically reduces the number of training parameters, and therefore the computational and sample complexity of the method. In particular, we obtain an architecture whose number of parameters scale sub-linearly with respect to the dimension of the inputs, while its inference complexity scales super-linearly but with very small constants. We provide several numerical tests that show that the current approach results in better reconstruction than optimization-based techniques such as full-waveform inversion, but at a fraction of the cost while being competitive with state-of-the-art machine learning methods.",2022-12-12T17:36:50Z,2023-10-11T18:40:02Z,http://arxiv.org/abs/2212.06068v3,http://arxiv.org/pdf/2212.06068v3,"math.NA, cs.LG, cs.NA"
"Multi-band Wireless Networks: Architectures, Challenges, and Comparative   Analysis","Mohammad Amin Saeidi, Hina Tabassum, Mohamed-Slim Alouini","This paper presents the vision of multi-band communication networks (MBN) in 6G, where optical and TeraHertz (THz) transmissions will coexist with the conventional radio frequency (RF) spectrum. This paper will first pin-point the fundamental challenges in MBN architectures at the PHYsical (PHY) and Medium Access (MAC) layer, such as unique channel propagation and estimation issues, user offloading and resource allocation, multi-band transceiver design and antenna systems, mobility and handoff management, backhauling, etc. We then perform a quantitative performance assessment of the two fundamental MBN architectures, i.e., {stand-alone MBN} and {integrated MBN} considering critical factors like achievable rate, and capital/operational deployment cost. {Our results show that stand-alone deployment is prone to higher capital and operational expenses for a predefined data rate requirement. Stand-alone deployment, however, offers flexibility and enables controlling the number of access points in different transmission bands.} In addition, we propose a molecular absorption-aware user offloading metric for MBNs and demonstrate its performance gains over conventional user offloading schemes. Finally, open research directions are presented.",2022-12-15T03:46:07Z,2023-06-20T18:53:55Z,http://arxiv.org/abs/2212.07606v2,http://arxiv.org/pdf/2212.07606v2,"cs.IT, math.IT"
Focal-UNet: UNet-like Focal Modulation for Medical Image Segmentation,"MohammadReza Naderi, MohammadHossein Givkashi, Fatemeh Piri, Nader Karimi, Shadrokh Samavi","Recently, many attempts have been made to construct a transformer base U-shaped architecture, and new methods have been proposed that outperformed CNN-based rivals. However, serious problems such as blockiness and cropped edges in predicted masks remain because of transformers' patch partitioning operations. In this work, we propose a new U-shaped architecture for medical image segmentation with the help of the newly introduced focal modulation mechanism. The proposed architecture has asymmetric depths for the encoder and decoder. Due to the ability of the focal module to aggregate local and global features, our model could simultaneously benefit the wide receptive field of transformers and local viewing of CNNs. This helps the proposed method balance the local and global feature usage to outperform one of the most powerful transformer-based U-shaped models called Swin-UNet. We achieved a 1.68% higher DICE score and a 0.89 better HD metric on the Synapse dataset. Also, with extremely limited data, we had a 4.25% higher DICE score on the NeoPolyp dataset. Our implementations are available at: https://github.com/givkashi/Focal-UNet",2022-12-19T06:17:22Z,2022-12-19T06:17:22Z,http://arxiv.org/abs/2212.09263v1,http://arxiv.org/pdf/2212.09263v1,"eess.IV, cs.CV"
Investigation of Network Architecture for Multimodal Head-and-Neck Tumor   Segmentation,"Ye Li, Junyu Chen, Se-in Jang, Kuang Gong, Quanzheng Li","Inspired by the recent success of Transformers for Natural Language Processing and vision Transformer for Computer Vision, many researchers in the medical imaging community have flocked to Transformer-based networks for various main stream medical tasks such as classification, segmentation, and estimation. In this study, we analyze, two recently published Transformer-based network architectures for the task of multimodal head-and-tumor segmentation and compare their performance to the de facto standard 3D segmentation network - the nnU-Net. Our results showed that modeling long-range dependencies may be helpful in cases where large structures are present and/or large field of view is needed. However, for small structures such as head-and-neck tumor, the convolution-based U-Net architecture seemed to perform well, especially when training dataset is small and computational resource is limited.",2022-12-21T02:35:46Z,2022-12-21T02:35:46Z,http://arxiv.org/abs/2212.10724v1,http://arxiv.org/pdf/2212.10724v1,"eess.IV, cs.CV"
xDeepInt: a hybrid architecture for modeling the vector-wise and   bit-wise feature interactions,"YaChen Yan, Liubo Li","Learning feature interactions is the key to success for the large-scale CTR prediction and recommendation. In practice, handcrafted feature engineering usually requires exhaustive searching. In order to reduce the high cost of human efforts in feature engineering, researchers propose several deep neural networks (DNN)-based approaches to learn the feature interactions in an end-to-end fashion. However, existing methods either do not learn both vector-wise interactions and bit-wise interactions simultaneously, or fail to combine them in a controllable manner. In this paper, we propose a new model, xDeepInt, based on a novel network architecture called polynomial interaction network (PIN) which learns higher-order vector-wise interactions recursively. By integrating subspace-crossing mechanism, we enable xDeepInt to balance the mixture of vector-wise and bit-wise feature interactions at a bounded order. Based on the network architecture, we customize a combined optimization strategy to conduct feature selection and interaction selection. We implement the proposed model and evaluate the model performance on three real-world datasets. Our experiment results demonstrate the efficacy and effectiveness of xDeepInt over state-of-the-art models. We open-source the TensorFlow implementation of xDeepInt: https://github.com/yanyachen/xDeepInt.",2023-01-03T13:33:19Z,2023-01-03T13:33:19Z,http://arxiv.org/abs/2301.01089v1,http://arxiv.org/pdf/2301.01089v1,"cs.LG, cs.AI, cs.IR, stat.ML"
Sensor Signal Processing using High-Level Synthesis and Internet of   Things with a Layered Architecture,"CS Reddy, Krishna Anand","Sensor routers play a crucial role in the sector of Internet of Things applications, in which the capacity for transmission of the network signal is limited from cloud systems to sensors and its reversal process. It describes a robust recognized framework with various architected layers to process data at high level synthesis. It is designed to sense the nodes instinctually with the help of Internet of Things where the applications arise in cloud systems. In this paper embedded PEs with four-layer new design framework architecture is proposed to sense the devises of IOT applications with the support of high-level synthesis DBMF (database management function) tool.",2023-01-03T09:22:54Z,2023-01-03T09:22:54Z,http://arxiv.org/abs/2301.03356v1,http://arxiv.org/pdf/2301.03356v1,"cs.NI, cs.AI, nil, F.2.3"
Deep Residual Axial Networks,"Nazmul Shahadat, Anthony S. Maida","While convolutional neural networks (CNNs) demonstrate outstanding performance on computer vision tasks, their computational costs remain high. Several techniques are used to reduce these costs, like reducing channel count, and using separable and depthwise separable convolutions. This paper reduces computational costs by introducing a novel architecture, axial CNNs, which replaces spatial 2D convolution operations with two consecutive depthwise separable 1D operations. The axial CNNs are predicated on the assumption that the dataset supports approximately separable convolution operations with little or no loss of training accuracy. Deep axial separable CNNs still suffer from gradient problems when training deep networks. We modify the construction of axial separable CNNs with residual connections to improve the performance of deep axial architectures and introduce our final novel architecture namely residual axial networks (RANs). Extensive benchmark evaluation shows that RANs achieve at least 1% higher performance with about 77%, 86%, 75%, and 34% fewer parameters and about 75%, 80%, 67%, and 26% fewer flops than ResNets, wide ResNets, MobileNets, and SqueezeNexts on CIFAR benchmarks, SVHN, and Tiny ImageNet image classification datasets. Moreover, our proposed RANs improve deep recursive residual networks performance with 94% fewer parameters on the image super-resolution dataset.",2023-01-11T18:36:54Z,2023-03-18T01:48:48Z,http://arxiv.org/abs/2301.04631v2,http://arxiv.org/pdf/2301.04631v2,"cs.CV, eess.IV"
Optimizing Design Choices for Neural Quantum States,"Moritz Reh, Markus Schmitt, Martin Gärttner","Neural quantum states are a new family of variational ans\""atze for quantum-many body wave functions with advantageous properties in the notoriously challenging case of two spatial dimensions. Since their introduction a wide variety of different network architectures has been employed to study paradigmatic models in quantum many-body physics with a particular focus on quantum spin models. Nonetheless, many questions remain about the effect that the choice of architecture has on the performance on a given task. In this work, we present a unified comparison of a selection of popular network architectures and symmetrization schemes employed for ground state searches of prototypical spin Hamiltonians, namely the two-dimensional transverse-field Ising model and the J1-J2 model. In the presence of a non-trivial sign structure of the ground states, we find that the details of symmetrization crucially influence the performance. We describe this effect in detail and discuss its consequences, especially for autoregressive models, as their direct sampling procedure is not compatible with the symmetrization procedure that we found to be optimal.",2023-01-17T10:30:05Z,2023-02-01T15:11:09Z,http://arxiv.org/abs/2301.06788v2,http://arxiv.org/pdf/2301.06788v2,"cond-mat.str-el, cond-mat.dis-nn, physics.comp-ph, quant-ph"
Quantum HyperNetworks: Training Binary Neural Networks in Quantum   Superposition,"Juan Carrasquilla, Mohamed Hibat-Allah, Estelle Inack, Alireza Makhzani, Kirill Neklyudov, Graham W. Taylor, Giacomo Torlai","Binary neural networks, i.e., neural networks whose parameters and activations are constrained to only two possible values, offer a compelling avenue for the deployment of deep learning models on energy- and memory-limited devices. However, their training, architectural design, and hyperparameter tuning remain challenging as these involve multiple computationally expensive combinatorial optimization problems. Here we introduce quantum hypernetworks as a mechanism to train binary neural networks on quantum computers, which unify the search over parameters, hyperparameters, and architectures in a single optimization loop. Through classical simulations, we demonstrate that of our approach effectively finds optimal parameters, hyperparameters and architectural choices with high probability on classification problems including a two-dimensional Gaussian dataset and a scaled-down version of the MNIST handwritten digits. We represent our quantum hypernetworks as variational quantum circuits, and find that an optimal circuit depth maximizes the probability of finding performant binary neural networks. Our unified approach provides an immense scope for other applications in the field of machine learning.",2023-01-19T20:06:48Z,2023-01-19T20:06:48Z,http://arxiv.org/abs/2301.08292v1,http://arxiv.org/pdf/2301.08292v1,"quant-ph, cs.LG"
Accurate Detection of Paroxysmal Atrial Fibrillation with Certified-GAN   and Neural Architecture Search,"Mehdi Asadi, Fatemeh Poursalim, Mohammad Loni, Masoud Daneshtalab, Mikael Sjödin, Arash Gharehbaghi","This paper presents a novel machine learning framework for detecting Paroxysmal Atrial Fibrillation (PxAF), a pathological characteristic of Electrocardiogram (ECG) that can lead to fatal conditions such as heart attack. To enhance the learning process, the framework involves a Generative Adversarial Network (GAN) along with a Neural Architecture Search (NAS) in the data preparation and classifier optimization phases. The GAN is innovatively invoked to overcome the class imbalance of the training data by producing the synthetic ECG for PxAF class in a certified manner. The effect of the certified GAN is statistically validated. Instead of using a general-purpose classifier, the NAS automatically designs a highly accurate convolutional neural network architecture customized for the PxAF classification task. Experimental results show that the accuracy of the proposed framework exhibits a high value of 99% which not only enhances state-of-the-art by up to 5.1%, but also improves the classification performance of the two widely-accepted baseline methods, ResNet-18, and Auto-Sklearn, by 2.2% and 6.1%.",2023-01-17T14:04:17Z,2023-01-17T14:04:17Z,http://arxiv.org/abs/2301.10173v1,http://arxiv.org/pdf/2301.10173v1,"cs.LG, eess.SP"
Neuronal architecture extracts statistical temporal patterns,"Sandra Nestler, Moritz Helias, Matthieu Gilson","Neuronal systems need to process temporal signals. We here show how higher-order temporal (co-)fluctuations can be employed to represent and process information. Concretely, we demonstrate that a simple biologically inspired feedforward neuronal model is able to extract information from up to the third order cumulant to perform time series classification. This model relies on a weighted linear summation of synaptic inputs followed by a nonlinear gain function. Training both - the synaptic weights and the nonlinear gain function - exposes how the non-linearity allows for the transfer of higher order correlations to the mean, which in turn enables the synergistic use of information encoded in multiple cumulants to maximize the classification accuracy. The approach is demonstrated both on a synthetic and on real world datasets of multivariate time series. Moreover, we show that the biologically inspired architecture makes better use of the number of trainable parameters as compared to a classical machine-learning scheme. Our findings emphasize the benefit of biological neuronal architectures, paired with dedicated learning algorithms, for the processing of information embedded in higher-order statistical cumulants of temporal (co-)fluctuations.",2023-01-24T18:21:33Z,2023-01-24T18:21:33Z,http://arxiv.org/abs/2301.10203v1,http://arxiv.org/pdf/2301.10203v1,"q-bio.NC, cond-mat.dis-nn, cs.LG"
Learning from Mistakes: Self-Regularizing Hierarchical Representations   in Point Cloud Semantic Segmentation,"Elena Camuffo, Umberto Michieli, Simone Milani","Recent advances in autonomous robotic technologies have highlighted the growing need for precise environmental analysis. LiDAR semantic segmentation has gained attention to accomplish fine-grained scene understanding by acting directly on raw content provided by sensors. Recent solutions showed how different learning techniques can be used to improve the performance of the model, without any architectural or dataset change. Following this trend, we present a coarse-to-fine setup that LEArns from classification mistaKes (LEAK) derived from a standard model. First, classes are clustered into macro groups according to mutual prediction errors; then, the learning process is regularized by: (1) aligning class-conditional prototypical feature representation for both fine and coarse classes, (2) weighting instances with a per-class fairness index. Our LEAK approach is very general and can be seamlessly applied on top of any segmentation architecture; indeed, experimental results showed that it enables state-of-the-art performances on different architectures, datasets and tasks, while ensuring more balanced class-wise results and faster convergence.",2023-01-26T14:52:30Z,2023-12-19T17:09:04Z,http://arxiv.org/abs/2301.11145v2,http://arxiv.org/pdf/2301.11145v2,"cs.CV, cs.MM, stat.ML"
Deterministic equivalent and error universality of deep random features   learning,"Dominik Schröder, Hugo Cui, Daniil Dmitriev, Bruno Loureiro","This manuscript considers the problem of learning a random Gaussian network function using a fully connected network with frozen intermediate layers and trainable readout layer. This problem can be seen as a natural generalization of the widely studied random features model to deeper architectures. First, we prove Gaussian universality of the test error in a ridge regression setting where the learner and target networks share the same intermediate layers, and provide a sharp asymptotic formula for it. Establishing this result requires proving a deterministic equivalent for traces of the deep random features sample covariance matrices which can be of independent interest. Second, we conjecture the asymptotic Gaussian universality of the test error in the more general setting of arbitrary convex losses and generic learner/target architectures. We provide extensive numerical evidence for this conjecture, which requires the derivation of closed-form expressions for the layer-wise post-activation population covariances. In light of our results, we investigate the interplay between architecture design and implicit regularization.",2023-02-01T12:37:10Z,2023-02-01T12:37:10Z,http://arxiv.org/abs/2302.00401v1,http://arxiv.org/pdf/2302.00401v1,"stat.ML, cs.LG"
Energy Efficiency of Training Neural Network Architectures: An Empirical   Study,"Yinlena Xu, Silverio Martínez-Fernández, Matias Martinez, Xavier Franch","The evaluation of Deep Learning models has traditionally focused on criteria such as accuracy, F1 score, and related measures. The increasing availability of high computational power environments allows the creation of deeper and more complex models. However, the computations needed to train such models entail a large carbon footprint. In this work, we study the relations between DL model architectures and their environmental impact in terms of energy consumed and CO$_2$ emissions produced during training by means of an empirical study using Deep Convolutional Neural Networks. Concretely, we study: (i) the impact of the architecture and the location where the computations are hosted on the energy consumption and emissions produced; (ii) the trade-off between accuracy and energy efficiency; and (iii) the difference on the method of measurement of the energy consumed using software-based and hardware-based tools.",2023-02-02T09:20:54Z,2023-02-02T09:20:54Z,http://arxiv.org/abs/2302.00967v1,http://arxiv.org/pdf/2302.00967v1,"cs.LG, cs.AI, cs.SE, D.2; I.2"
Ultraviolet superradiance from mega-networks of tryptophan in biological   architectures,"N. S. Babcock, G. Montes-Cabrera, K. E. Oberhofer, M. Chergui, G. L. Celardo, P. Kurian","Networks of tryptophan -- an aromatic amino acid with strong fluorescent response -- are ubiquitous in biological systems, forming diverse architectures in transmembrane proteins, cytoskeletal filaments, sub-neuronal elements, photoreceptor complexes, virion capsids, and other cellular structures. We analyze the cooperative effects induced by ultraviolet (UV) excitation of several biologically relevant tryptophan mega-networks, thus giving insight into novel mechanisms for cellular signalling and control. Our theoretical analysis in the single-excitation manifold predicts the formation of strongly superradiant states due to collective interactions among organized arrangements of up to more than $10^5$ tryptophan UV-excited transition dipoles in microtubule architectures, which leads to an enhancement of the fluorescence quantum yield that is confirmed by our experiments. We demonstrate the observed consequences of this superradiant behavior in the fluorescence quantum yield for hierarchically organized tubulin structures, which increases in different geometric regimes at thermal equilibrium before saturation -- highlighting the effect's persistence in the presence of disorder.",2023-02-03T00:03:59Z,2023-02-03T00:03:59Z,http://arxiv.org/abs/2302.01469v1,http://arxiv.org/pdf/2302.01469v1,"quant-ph, cond-mat.mes-hall, physics.bio-ph, physics.optics"
Software-Defined MIMO OFDM Joint Radar-Communication Platform with Fully   Digital mmWave Architecture,"Ceyhun D. Ozkaptan, Haocheng Zhu, Eylem Ekici, Onur Altintas","Large-scale deployment of connected vehicles with cooperative sensing and maneuvering technologies increases the demand for vehicle-to-everything communication (V2X) band in 5.9 GHz. Besides the V2X spectrum, the under-utilized millimeter-wave (mmWave) bands at 24 and 77 GHz can be leveraged to supplement V2X communication and support high data rates for emerging broadband applications. For this purpose, joint radar-communication (JRC) systems have been proposed in the literature to perform both functions using the same waveform and hardware. In this work, we present a software-defined multiple-input and multiple-output (MIMO) JRC with orthogonal frequency division multiplexing (OFDM) for the 24 GHz mmWave band. We implement a real-time operating full-duplex JRC platform using commercially available software-defined radios and custom-built mmWave front-ends. With fully digital MIMO architecture, we demonstrate simultaneous data transmission and high-resolution radar imaging capabilities of MIMO OFDM JRC in the mmWave band.",2023-02-11T23:19:52Z,2023-02-11T23:19:52Z,http://arxiv.org/abs/2302.05812v1,http://arxiv.org/pdf/2302.05812v1,"eess.SP, cs.AR, B.4.1; B.4.5"
"System identification of neural systems: If we got it right, would we   know?","Yena Han, Tomaso Poggio, Brian Cheung","Artificial neural networks are being proposed as models of parts of the brain. The networks are compared to recordings of biological neurons, and good performance in reproducing neural responses is considered to support the model's validity. A key question is how much this system identification approach tells us about brain computation. Does it validate one model architecture over another? We evaluate the most commonly used comparison techniques, such as a linear encoding model and centered kernel alignment, to correctly identify a model by replacing brain recordings with known ground truth models. System identification performance is quite variable; it also depends significantly on factors independent of the ground truth architecture, such as stimuli images. In addition, we show the limitations of using functional similarity scores in identifying higher-level architectural motifs.",2023-02-13T20:32:37Z,2023-08-30T21:37:02Z,http://arxiv.org/abs/2302.06677v2,http://arxiv.org/pdf/2302.06677v2,"q-bio.NC, cs.AI, cs.LG"
$\mathcal{L}_1$Quad: $\mathcal{L}_1$ Adaptive Augmentation of Geometric   Control for Agile Quadrotors with Performance Guarantees,"Zhuohuan Wu, Sheng Cheng, Pan Zhao, Aditya Gahlawat, Kasey A. Ackerman, Arun Lakshmanan, Chengyu Yang, Jiahao Yu, Naira Hovakimyan","Quadrotors that can operate predictably in the presence of imperfect model knowledge and external disturbances are crucial in safety-critical applications. We present L1Quad, a control architecture that ensures uniformly bounded transient response of the quadrotor's uncertain dynamics on the special Euclidean group SE(3). By leveraging the geometric controller and the L1 adaptive controller, the L1Quad architecture provides a theoretically justified framework for the design and analysis of quadrotor's tracking controller in the presence of nonlinear (time- and state-dependent) uncertainties on both the translational and rotational dynamics. In addition, we validate the performance of the L1Quad architecture through extensive experiments for eleven types of uncertainties across various trajectories. The results demonstrate that the L1Quad can achieve consistently small tracking errors despite the uncertainties and disturbances and significantly outperforms existing state-of-the-art controllers.",2023-02-14T17:31:09Z,2024-12-19T19:48:33Z,http://arxiv.org/abs/2302.07208v2,http://arxiv.org/pdf/2302.07208v2,"eess.SY, cs.RO, cs.SY"
On the Limitations of Physics-informed Deep Learning: Illustrations   Using First Order Hyperbolic Conservation Law-based Traffic Flow Models,"Archie J. Huang, Shaurya Agarwal","Since its introduction in 2017, physics-informed deep learning (PIDL) has garnered growing popularity in understanding the evolution of systems governed by physical laws in terms of partial differential equations (PDEs). However, empirical evidence points to the limitations of PIDL for learning certain types of PDEs. In this paper, we (a) present the challenges in training PIDL architecture, (b) contrast the performance of PIDL architecture in learning a first order scalar hyperbolic conservation law and its parabolic counterpart, (c) investigate the effect of training data sampling, which corresponds to various sensing scenarios in traffic networks, (d) comment on the implications of PIDL limitations for traffic flow estimation and prediction in practice. Detailed in the case study, we present the contradistinction in PIDL results between learning the traffic flow model (LWR PDE) and its variation with diffusion. The outcome indicates that PIDL experiences significant challenges in learning the hyperbolic LWR equation due to the non-smoothness of its solution. On the other hand, the architecture with parabolic PDE, augmented with the diffusion term, leads to the successful reassembly of the density data even with the shockwaves present.",2023-02-23T21:08:02Z,2023-02-23T21:08:02Z,http://arxiv.org/abs/2302.12337v1,http://arxiv.org/pdf/2302.12337v1,"cs.LG, math.AP"
Connectivity Optimized Nested Graph Networks for Crystal Structures,"Robin Ruff, Patrick Reiser, Jan Stühmer, Pascal Friederich","Graph neural networks (GNNs) have been applied to a large variety of applications in materials science and chemistry. Here, we recapitulate the graph construction for crystalline (periodic) materials and investigate its impact on the GNNs model performance. We suggest the asymmetric unit cell as a representation to reduce the number of atoms by using all symmetries of the system. This substantially reduced the computational cost and thus time needed to train large graph neural networks without any loss in accuracy. Furthermore, with a simple but systematically built GNN architecture based on message passing and line graph templates, we introduce a general architecture (Nested Graph Network, NGN) that is applicable to a wide range of tasks. We show that our suggested models systematically improve state-of-the-art results across all tasks within the MatBench benchmark. Further analysis shows that optimized connectivity and deeper message functions are responsible for the improvement. Asymmetric unit cells and connectivity optimization can be generally applied to (crystal) graph networks, while our suggested nested graph framework will open new ways of systematic comparison of GNN architectures.",2023-02-27T19:26:48Z,2023-08-09T15:05:42Z,http://arxiv.org/abs/2302.14102v2,http://arxiv.org/pdf/2302.14102v2,"cs.LG, cond-mat.mtrl-sci, physics.chem-ph, J.2"
End-to-End Speech Recognition: A Survey,"Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, Ralf Schlüter, Shinji Watanabe","In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domain-specific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.",2023-03-03T01:46:41Z,2023-03-03T01:46:41Z,http://arxiv.org/abs/2303.03329v1,http://arxiv.org/pdf/2303.03329v1,"eess.AS, cs.CL, cs.SD"
Addressing the programming challenges of practical interferometric mesh   based optical processors,"Kaveh Rahbardar Mojaver, Bokun Zhao, Edward Leung, S. Mohammad Reza Safaee, Odile Liboiron-Ladouceur","We demonstrate a novel mesh of Mach-Zehnder interferometers (MZIs) for programmable optical processors. The proposed mesh, referred to as Bokun mesh, is an architecture that merges the attributes of the prior topologies Diamond and Clements. Similar to Diamond, Bokun provides diagonal paths passing through every individual MZI enabling direct phase monitoring. However, unlike Diamond and similar to Clements, Bokun maintains a minimum optical depth leading to better scalability. Providing the monitoring option, Bokun's programming is faster improving the total energy efficiency of the processor. The performance of Bokun mesh enabled by an optimal optical depth is also more resilient to the loss and fabrication imperfections compared to architectures with longer depth such as Reck and Diamond. Employing an efficient programming scheme, the proposed architecture improves energy efficiency by 83% maintaining the same computation accuracy for weight matrix changes at 2 kHz.",2023-03-07T03:32:55Z,2023-03-07T03:32:55Z,http://arxiv.org/abs/2303.04151v1,http://arxiv.org/pdf/2303.04151v1,"cs.ET, physics.optics"
Lung segmentation with NASNet-Large-Decoder Net,Youshan Zhang,"Lung cancer has emerged as a severe disease that threatens human life and health. The precise segmentation of lung regions is a crucial prerequisite for localizing tumors, which can provide accurate information for lung image analysis. In this work, we first propose a lung image segmentation model using the NASNet-Large as an encoder and then followed by a decoder architecture, which is one of the most commonly used architectures in deep learning for image segmentation. The proposed NASNet-Large-decoder architecture can extract high-level information and expand the feature map to recover the segmentation map. To further improve the segmentation results, we propose a post-processing layer to remove the irrelevant portion of the segmentation map. Experimental results show that an accurate segmentation model with 0.92 dice scores outperforms state-of-the-art performance.",2023-03-18T02:52:16Z,2023-03-18T02:52:16Z,http://arxiv.org/abs/2303.10315v1,http://arxiv.org/pdf/2303.10315v1,"eess.IV, cs.CV"
Tangent Bundle Convolutional Learning: from Manifolds to Cellular   Sheaves and Back,"Claudio Battiloro, Zhiyang Wang, Hans Riess, Paolo Di Lorenzo, Alejandro Ribeiro","In this work we introduce a convolution operation over the tangent bundle of Riemann manifolds in terms of exponentials of the Connection Laplacian operator. We define tangent bundle filters and tangent bundle neural networks (TNNs) based on this convolution operation, which are novel continuous architectures operating on tangent bundle signals, i.e. vector fields over the manifolds. Tangent bundle filters admit a spectral representation that generalizes the ones of scalar manifold filters, graph filters and standard convolutional filters in continuous time. We then introduce a discretization procedure, both in the space and time domains, to make TNNs implementable, showing that their discrete counterpart is a novel principled variant of the very recently introduced sheaf neural networks. We formally prove that this discretized architecture converges to the underlying continuous TNN. Finally, we numerically evaluate the effectiveness of the proposed architecture on various learning tasks, both on synthetic and real data.",2023-03-20T17:57:15Z,2024-03-15T22:00:45Z,http://arxiv.org/abs/2303.11323v2,http://arxiv.org/pdf/2303.11323v2,"eess.SP, cs.LG"
RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation   Research,"Ching Pui Wan, Tung Li, Jason Min Wang","Reinforcement learning has been applied in operation research and has shown promise in solving large combinatorial optimization problems. However, existing works focus on developing neural network architectures for certain problems. These works lack the flexibility to incorporate recent advances in reinforcement learning, as well as the flexibility of customizing model architectures for operation research problems. In this work, we analyze the end-to-end autoregressive models for vehicle routing problems and show that these models can benefit from the recent advances in reinforcement learning with a careful re-implementation of the model architecture. In particular, we re-implemented the Attention Model and trained it with Proximal Policy Optimization (PPO) in CleanRL, showing at least 8 times speed up in training time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement Learning for Operation Research. We believe that a flexible framework is key to developing deep reinforcement learning models for operation research problems. The code of our work is publicly available at https://github.com/cpwan/RLOR.",2023-03-23T09:07:30Z,2023-03-23T09:07:30Z,http://arxiv.org/abs/2303.13117v1,http://arxiv.org/pdf/2303.13117v1,"math.OC, cs.LG, cs.NE"
Scalable multi-chip quantum architectures enabled by cryogenic hybrid   wireless/quantum-coherent network-in-package,"Eduard Alarcón, Sergi Abadal, Fabio Sebastiano, Masoud Babaie, Edoardo Charbon, Peter Haring Bolívar, Maurizio Palesi, Elena Blokhina, Dirk Leipold, Bogdan Staszewski, Artur Garcia-Sáez, Carmen G. Almudever","The grand challenge of scaling up quantum computers requires a full-stack architectural standpoint. In this position paper, we will present the vision of a new generation of scalable quantum computing architectures featuring distributed quantum cores (Qcores) interconnected via quantum-coherent qubit state transfer links and orchestrated via an integrated wireless interconnect.",2023-03-24T14:03:24Z,2023-04-08T10:57:12Z,http://arxiv.org/abs/2303.14008v3,http://arxiv.org/pdf/2303.14008v3,"quant-ph, cs.ET"
A Heterogeneous Parallel Non-von Neumann Architecture System for   Accurate and Efficient Machine Learning Molecular Dynamics,"Zhuoying Zhao, Ziling Tan, Pinghui Mo, Xiaonan Wang, Dan Zhao, Xin Zhang, Ming Tao, Jie Liu","This paper proposes a special-purpose system to achieve high-accuracy and high-efficiency machine learning (ML) molecular dynamics (MD) calculations. The system consists of field programmable gate array (FPGA) and application specific integrated circuit (ASIC) working in heterogeneous parallelization. To be specific, a multiplication-less neural network (NN) is deployed on the non-von Neumann (NvN)-based ASIC (SilTerra 180 nm process) to evaluate atomic forces, which is the most computationally expensive part of MD. All other calculations of MD are done using FPGA (Xilinx XC7Z100). It is shown that, to achieve similar-level accuracy, the proposed NvN-based system based on low-end fabrication technologies (180 nm) is 1.6x faster and 10^2-10^3x more energy efficiency than state-of-the-art vN based MLMD using graphics processing units (GPUs) based on much more advanced technologies (12 nm), indicating superiority of the proposed NvN-based heterogeneous parallel architecture.",2023-03-26T05:43:49Z,2023-03-26T05:43:49Z,http://arxiv.org/abs/2303.15474v1,http://arxiv.org/pdf/2303.15474v1,"cs.LG, cs.AR, cs.NE, cs.SY, eess.SY"
Dual Cross-Attention for Medical Image Segmentation,"Gorkem Can Ates, Prasoon Mohan, Emrah Celik","We propose Dual Cross-Attention (DCA), a simple yet effective attention module that is able to enhance skip-connections in U-Net-based architectures for medical image segmentation. DCA addresses the semantic gap between encoder and decoder features by sequentially capturing channel and spatial dependencies across multi-scale encoder features. First, the Channel Cross-Attention (CCA) extracts global channel-wise dependencies by utilizing cross-attention across channel tokens of multi-scale encoder features. Then, the Spatial Cross-Attention (SCA) module performs cross-attention to capture spatial dependencies across spatial tokens. Finally, these fine-grained encoder features are up-sampled and connected to their corresponding decoder parts to form the skip-connection scheme. Our proposed DCA module can be integrated into any encoder-decoder architecture with skip-connections such as U-Net and its variants. We test our DCA module by integrating it into six U-Net-based architectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet and MultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS, 2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% on Synapse datasets. Our codes are available at: https://github.com/gorkemcanates/Dual-Cross-Attention",2023-03-30T20:24:57Z,2023-03-30T20:24:57Z,http://arxiv.org/abs/2303.17696v1,http://arxiv.org/pdf/2303.17696v1,"cs.CV, cs.LG, eess.IV"
Learning with augmented target information: An alternative theory of   Feedback Alignment,"Huzi Cheng, Joshua W. Brown","While error backpropagation (BP) has dominated the training of nearly all modern neural networks for a long time, it suffers from several biological plausibility issues such as the symmetric weight requirement and synchronous updates. Feedback Alignment (FA) was proposed as an alternative to BP to address those dilemmas and has been demonstrated to be effective on various tasks and network architectures. Despite its simplicity and effectiveness, a satisfying explanation of how FA works across different architectures is still lacking. Here we propose a novel, architecture-agnostic theory of how FA works through the lens of information theory: Instead of approximating gradients calculated by BP with the same parameter, FA learns effective representations by embedding target information into neural networks to be trained. We show this through the analysis of FA dynamics in idealized settings and then via a series of experiments. Based on the implications of this theory, we designed three variants of FA and show their comparable performance on several tasks. These variants also account for some phenomena and theories in neuroscience such as predictive coding and representational drift.",2023-04-03T22:44:03Z,2023-04-03T22:44:03Z,http://arxiv.org/abs/2304.01406v1,http://arxiv.org/pdf/2304.01406v1,"q-bio.NC, cs.LG"
Dual-Attention Neural Transducers for Efficient Wake Word Spotting in   Speech Recognition,"Saumya Y. Sahai, Jing Liu, Thejaswi Muniyappa, Kanthashree M. Sathyendra, Anastasios Alexandridis, Grant P. Strimel, Ross McGowan, Ariya Rastrow, Feng-Ju Chang, Athanasios Mouchtaris, Siegfried Kunzmann","We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.",2023-04-03T01:19:39Z,2023-04-05T01:22:38Z,http://arxiv.org/abs/2304.01905v2,http://arxiv.org/pdf/2304.01905v2,"cs.SD, cs.CL, cs.LG, eess.AS"
Anisotropic plates with architected tendon network,"Md Shahjahan Hossain, Hossein Ebrahimi, Ranajay Ghosh","We synthesize geometrically tailorable anisotropic plates by combining button shaped fish-scale like features on soft substrates, then lacing them with high-stiffness strings. This creates a new type of biomimetic architectured structure with multiple broken symmetries. First, the tendons and scales together break the symmetry of the bending response between the concave and convex curvature. Next, the weave pattern of the tendons further breaks symmetry along the two directors of plates. The anisotropy is clearly evident in 3-point bending experiments. Motivated by these experiments and the need for design, we formulate the analytical energy-based model to quantify the anisotropic elasticity and tailorable Poisson's ratio. The derived architecture-property relationships can be used to design architected tendon plates with desirable properties.",2023-04-05T16:07:15Z,2023-04-05T16:07:15Z,http://arxiv.org/abs/2304.02547v1,http://arxiv.org/pdf/2304.02547v1,"cond-mat.soft, cond-mat.mtrl-sci, physics.app-ph"
FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For   Medical Imaging Segmentation,"Adrian Celaya, Beatrice Riviere, David Fuentes","Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tumor subcomponent segmentation accuracy and training efficiency. These findings highlight the potential of incorporating the principles of multigrid methods into CNNs to improve the accuracy and efficiency of medical imaging segmentation.",2023-04-05T20:03:08Z,2023-11-10T21:13:09Z,http://arxiv.org/abs/2304.02725v3,http://arxiv.org/pdf/2304.02725v3,"eess.IV, cs.CV, cs.LG"
SDN enabled Mobility Management in Multi Radio Access Technology 5G   networks: A Survey,"Pavan K Mangipudi, Janise McNair","This paper presents a survey of the state of the art in research related to handovers employing software defined networking (SDN) enabled architectures, serving multiple coexisting radio access technologies. As the industrial roll-out of cellular services continues to evolve, it brings with it the coexistence of various IP based networks such as 5G NR, LTE, WiFi, Satellite, and IoT networks. This coexistence of different radio access technologies presents researchers with the opportunity to use these technologies interchangeably to address the longstanding challenges associated with network and traffic management. Advances in software defined technologies including SDN enables handover and interoperability schemes that utilize the principles of SDN to improve the handover performance and achieve interconnection between these heterogeneous networks. This paper explores such advanced SDN enabled architectures for radio access networks, offloading techniques, and implementation approaches. Finally, the challenges and shortcomings of the SDN based handover optimization approaches are discussed, and a few future research paths are laid out.",2023-04-06T19:48:35Z,2023-04-06T19:48:35Z,http://arxiv.org/abs/2304.03346v1,http://arxiv.org/pdf/2304.03346v1,"cs.NI, cs.SY, eess.SY"
CoVE: Towards Confidential Computing on RISC-V Platforms,"Ravi Sahita, Atish Patra, Vedvyas Shanbhogue, Samuel Ortiz, Andrew Bresticker, Dylan Reid, Atul Khare, Rajnesh Kanwal","Multi-tenant computing platforms are typically comprised of several software and hardware components including platform firmware, host operating system kernel, virtualization monitor, and the actual tenant payloads that run on them (typically in a virtual machine, container, or application). This model is well established in large scale commercial deployment, but the downside is that all platform components and operators are in the Trusted Computing Base (TCB) of the tenant. This aspect is ill-suited for privacy-oriented workloads that aim to minimize the TCB footprint. Confidential computing presents a good stepping-stone towards providing a quantifiable TCB for computing. Confidential computing [1] requires the use of a HW-attested Trusted Execution Environments for data-in-use protection. The RISC-V architecture presents a strong foundation for meeting the requirements for Confidential Computing and other security paradigms in a clean slate manner. This paper describes a reference architecture and discusses ISA, non-ISA and system-on-chip (SoC) requirements for confidential computing on RISC-V Platforms. It discusses proposed ISA and non-ISA Extension for Confidential Virtual Machine for RISC-V platforms, referred to as CoVE.",2023-04-12T21:35:44Z,2023-04-12T21:35:44Z,http://arxiv.org/abs/2304.06167v1,http://arxiv.org/pdf/2304.06167v1,"cs.CR, cs.AR, D.4.6"
Cultivated Wildness: Technodiversity and Wildness in Machines,"Zihao Zhang, Bradley Cantrell","This paper investigates the idea of cultivated wildness at the intersection of landscape design and artificial intelligence. The paper posits that contemporary landscape practices should overcome the potentially single understanding on wilderness, and instead explore landscape strategies to cultivate new forms of wild places via ideas and concerns in contemporary Environmental Humanities, Science and Technology Studies, Ecological Sciences, and Landscape Architecture. Drawing cases in environmental engineering, computer science, and landscape architecture research, this paper explores a framework to construct wild places with intelligent machines. In this framework, machines are not understood as a layer of ""digital infrastructure"" that is used to extend localized human intelligence and agency. Rather machines are conceptualized as active agents who can participate in the intelligence of co-production. Recent developments in cybernetic technologies such as sensing networks, artificial intelligence, and cyberphysical systems can also contribute to establishing the framework. At the heart of this framework is ""technodiversity,"" in parallel with biodiversity, since a singular vision on technological development driven by optimization and efficiency reinforces a monocultural approach that eliminates other possible relationships to construct with the environment. Thus, cultivated wildness is also about recognizing ""wildness"" in machines.",2023-05-03T13:25:51Z,2023-05-03T13:25:51Z,http://arxiv.org/abs/2305.02328v1,http://arxiv.org/pdf/2305.02328v1,"cs.AI, cs.RO, cs.SY, eess.SY"
G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar   Tree Transformer,"Kevin Zhang, Vipul Mann, Venkat Venkatasubramanian","Various template-based and template-free approaches have been proposed for single-step retrosynthesis prediction in recent years. While these approaches demonstrate strong performance from a data-driven metrics standpoint, many model architectures do not incorporate underlying chemistry principles. Here, we propose a novel chemistry-aware retrosynthesis prediction framework that combines powerful data-driven models with prior domain knowledge. We present a tree-to-sequence transformer architecture that utilizes hierarchical SMILES grammar-based trees, incorporating crucial chemistry information that is often overlooked by SMILES text-based representations, such as local structures and functional groups. The proposed framework, grammar-based molecular attention tree transformer (G-MATT), achieves significant performance improvements compared to baseline retrosynthesis models. G-MATT achieves a promising top-1 accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, and bioactive similarity rate of 74.8% on the USPTO- 50K dataset. Additional analyses of G-MATT attention maps demonstrate the ability to retain chemistry knowledge without relying on excessively complex model architectures.",2023-05-04T21:04:19Z,2023-08-14T17:38:23Z,http://arxiv.org/abs/2305.03153v2,http://arxiv.org/pdf/2305.03153v2,"cs.LG, cs.AI, cs.FL, cs.SC, q-bio.QM"
ReMark: Receptive Field based Spatial WaterMark Embedding Optimization   using Deep Network,"Natan Semyonov, Rami Puzis, Asaf Shabtai, Gilad Katz","Watermarking is one of the most important copyright protection tools for digital media. The most challenging type of watermarking is the imperceptible one, which embeds identifying information in the data while retaining the latter's original quality. To fulfill its purpose, watermarks need to withstand various distortions whose goal is to damage their integrity. In this study, we investigate a novel deep learning-based architecture for embedding imperceptible watermarks. The key insight guiding our architecture design is the need to correlate the dimensions of our watermarks with the sizes of receptive fields (RF) of modules of our architecture. This adaptation makes our watermarks more robust, while also enabling us to generate them in a way that better maintains image quality. Extensive evaluations on a wide variety of distortions show that the proposed method is robust against most common distortions on watermarks including collusive distortion.",2023-05-11T13:21:29Z,2023-05-11T13:21:29Z,http://arxiv.org/abs/2305.06786v1,http://arxiv.org/pdf/2305.06786v1,"cs.CV, eess.IV"
PANNA 2.0: Efficient neural network interatomic potentials and new   architectures,"Franco Pellegrini, Ruggero Lot, Yusuf Shaidu, Emine Küçükbenli","We present the latest release of PANNA 2.0 (Properties from Artificial Neural Network Architectures), a code for the generation of neural network interatomic potentials based on local atomic descriptors and multilayer perceptrons. Built on a new back end, this new release of PANNA features improved tools for customizing and monitoring network training, better GPU support including a fast descriptor calculator, new plugins for external codes and a new architecture for the inclusion of long-range electrostatic interactions through a variational charge equilibration scheme. We present an overview of the main features of the new code, and several benchmarks comparing the accuracy of PANNA models to the state of the art, on commonly used benchmarks as well as richer datasets.",2023-05-19T16:41:59Z,2023-05-19T16:41:59Z,http://arxiv.org/abs/2305.11805v1,http://arxiv.org/pdf/2305.11805v1,"physics.comp-ph, cond-mat.mtrl-sci, cs.LG, physics.chem-ph"
Benchmarking the human brain against computational architectures,"Céline van Valkenhoef, Catherine Schuman, Philip Walther","The human brain has inspired novel concepts complementary to classical and quantum computing architectures, such as artificial neural networks and neuromorphic computers, but it is not clear how their performances compare. Here we report a new methodological framework for benchmarking cognitive performance based on solving computational problems with increasing problem size. We determine computational efficiencies in experiments with human participants and benchmark these against complexity classes. We show that a neuromorphic architecture with limited field-of-view size and added noise provides a good approximation to our results. The benchmarking also suggests there is no quantum advantage on the scales of human capability compared to the neuromorphic model. Thus, the framework offers unique insights into the computational efficiency of the brain by considering it a black box.",2023-05-15T08:00:26Z,2023-05-15T08:00:26Z,http://arxiv.org/abs/2305.14363v1,http://arxiv.org/pdf/2305.14363v1,"q-bio.NC, cs.NE, quant-ph"
Deep Learning-based Bio-Medical Image Segmentation using UNet   Architecture and Transfer Learning,"Nima Hassanpour, Abouzar Ghavami","Image segmentation is a branch of computer vision that is widely used in real world applications including biomedical image processing. With recent advancement of deep learning, image segmentation has achieved at a very high level performance. Recently, UNet architecture is found as the core of novel deep learning segmentation methods. In this paper we implement UNet architecture from scratch with using basic blocks in Pytorch and evaluate its performance on multiple biomedical image datasets. We also use transfer learning to apply novel modified UNet segmentation packages on the biomedical image datasets. We fine tune the pre-trained transferred model with each specific dataset. We compare its performance with our fundamental UNet implementation. We show that transferred learning model has better performance in image segmentation than UNet model that is implemented from scratch.",2023-05-24T07:45:54Z,2023-05-24T07:45:54Z,http://arxiv.org/abs/2305.14841v1,http://arxiv.org/pdf/2305.14841v1,"eess.IV, cs.CV, cs.LG"
A General Framework for Equivariant Neural Networks on Reductive Lie   Groups,"Ilyes Batatia, Mario Geiger, Jose Munoz, Tess Smidt, Lior Silberman, Christoph Ortner","Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group G. Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general G-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our approach are demonstrated by applying it to the tasks of top quark decay tagging (Lorentz group) and shape recognition (orthogonal group).",2023-05-31T18:09:37Z,2023-05-31T18:09:37Z,http://arxiv.org/abs/2306.00091v1,http://arxiv.org/pdf/2306.00091v1,"stat.ML, cs.LG, hep-th"
Speech Self-Supervised Representation Benchmarking: Are We Doing it   Right?,"Salah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, Mirco Ravanelli","Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.",2023-06-01T08:51:18Z,2023-06-01T08:51:18Z,http://arxiv.org/abs/2306.00452v1,http://arxiv.org/pdf/2306.00452v1,"eess.AS, cs.LG"
Invertible residual networks in the context of regularization theory for   linear inverse problems,"Clemens Arndt, Alexander Denker, Sören Dittmer, Nick Heilenkötter, Meira Iske, Tobias Kluth, Peter Maass, Judith Nickel","Learned inverse problem solvers exhibit remarkable performance in applications like image reconstruction tasks. These data-driven reconstruction methods often follow a two-step scheme. First, one trains the often neural network-based reconstruction scheme via a dataset. Second, one applies the scheme to new measurements to obtain reconstructions. We follow these steps but parameterize the reconstruction scheme with invertible residual networks (iResNets). We demonstrate that the invertibility enables investigating the influence of the training and architecture choices on the resulting reconstruction scheme. For example, assuming local approximation properties of the network, we show that these schemes become convergent regularizations. In addition, the investigations reveal a formal link to the linear regularization theory of linear inverse problems and provide a nonlinear spectral regularization for particular architecture classes. On the numerical side, we investigate the local approximation property of selected trained architectures and present a series of experiments on the MNIST dataset that underpin and extend our theoretical findings.",2023-06-02T07:58:40Z,2023-12-20T11:42:07Z,http://arxiv.org/abs/2306.01335v2,http://arxiv.org/pdf/2306.01335v2,"math.NA, cs.NA"
Deep neural networks architectures from the perspective of manifold   learning,German Magai,"Despite significant advances in the field of deep learning in ap-plications to various areas, an explanation of the learning pro-cess of neural network models remains an important open ques-tion. The purpose of this paper is a comprehensive comparison and description of neural network architectures in terms of ge-ometry and topology. We focus on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of a data manifold on different layers. In this paper, we use the concepts of topological data analysis (TDA) and persistent homological fractal dimension. We present a wide range of experiments with various datasets and configurations of convolutional neural network (CNNs) architectures and Transformers in CV and NLP tasks. Our work is a contribution to the development of the important field of explainable and interpretable AI within the framework of geometrical deep learning.",2023-06-06T04:57:39Z,2023-06-06T04:57:39Z,http://arxiv.org/abs/2306.03406v1,http://arxiv.org/pdf/2306.03406v1,"cs.LG, cs.AI, cs.CV, math.AT"
Matching Latent Encoding for Audio-Text based Keyword Spotting,"Kumari Nishu, Minsik Cho, Devang Naik","Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental results show that our DSP is more effective than other partitioning schemes, and the proposed architecture outperformed the state-of-the-art results on the public dataset in terms of Area Under the ROC Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.",2023-06-08T14:44:23Z,2023-06-08T14:44:23Z,http://arxiv.org/abs/2306.05245v1,http://arxiv.org/pdf/2306.05245v1,"eess.AS, cs.LG, cs.SD"
Hidden symmetries of ReLU networks,"J. Elisenda Grigsby, Kathryn Lindsey, David Rolnick","The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings can determine the same function. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, while increasing towards 1 as width and input dimension increase.",2023-06-09T18:07:06Z,2023-06-09T18:07:06Z,http://arxiv.org/abs/2306.06179v1,http://arxiv.org/pdf/2306.06179v1,"cs.LG, math.CO, math.GT, 57R70, 57Q99, 52B70, 52C35, I.2.6"
deController: A Web3 Native Cyberspace Infrastructure Perspective,"Hao Xu, Yunqing Sun, Zihao Li, Yao Sun, Lei Zhang, Xiaoshuai Zhang","Web3 brings an emerging outlook for the value of decentralization, boosting the decentralized infrastructure. People can benefit from Web3, facilitated by the advances in distributed ledger technology, to read, write and own web content, services and applications more freely without revealing their real identities. Although the features and merits of Web3 have been widely discussed, the network architecture of Web3 and how to achieve complete decentralization considering law compliance in Web3 are still unclear. Here, we propose a perspective of Web3 architecture, deController, consisting of underlay and overlay network as Web3 infrastructures to underpin services and applications. The functions of underlay and overlay and their interactions are illustrated. Meanwhile, the security and privacy of Web3 are analyzed based on a novel design of three-tier identities cooperating with deController. Furthermore, the impacts of laws on privacy and cyber sovereignty to achieve Web3 are discussed.",2023-06-13T06:28:40Z,2023-06-13T06:28:40Z,http://arxiv.org/abs/2306.07565v1,http://arxiv.org/pdf/2306.07565v1,"cs.NI, cs.SY, eess.SY"
Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems,"Bryce Ferenczi, Michael Burke, Tom Drummond","This work introduces a novel and adaptable architecture designed for real-time occupancy forecasting that outperforms existing state-of-the-art models on the Waymo Open Motion Dataset in Soft IOU. The proposed model uses recursive latent state estimation with learned transformer-based functions to effectively update and evolve the state. This enables highly efficient real-time inference on embedded systems, as profiled on an Nvidia Xavier AGX. Our model, MotionPerceiver, achieves this by encoding a scene into a latent state that evolves in time through self-attention mechanisms. Additionally, it incorporates relevant scene observations, such as traffic signals, road topology and agent detections, through cross-attention mechanisms. This forms an efficient data-streaming architecture, that contrasts with the expensive, fixed-sequence input common in existing models. The architecture also offers the distinct advantage of generating occupancy predictions through localized querying based on a point-of-interest, as opposed to generating fixed-size occupancy images that render potentially irrelevant regions.",2023-06-15T06:26:56Z,2024-02-02T02:09:04Z,http://arxiv.org/abs/2306.08879v2,http://arxiv.org/pdf/2306.08879v2,"cs.RO, I.2.9; I.2.10"
Lakat: An open and permissionless architecture for continuous   integration academic publishing,Leonhard Horstmeyer,"In this paper, we present three contributions to the field of academic publishing. Firstly, we introduce Lakat, a novel base layer for a publishing system that fosters collaboration, pluralism and permissionless participation. Drawing inspiration from the philosophy of Imre Lakatos, Lakat is designed as a peer-to-peer process- and conflict-oriented system that supports continuous integration across multiple branches. This architecture provides a robust foundation for the integration of existing reputation systems and incentive structures or the development of new ones. Secondly, we propose a new consensus mechanism, called Proof of Review, which ensures the integrity and quality of the content while promoting active participation from the community. Lastly, we present Lignification, a new finality gadget specifically designed for branched, permissionless systems. Lignification provides a deterministic way to find the consensual state in these systems, ensuring the system's robustness and reliability in handling complex scenarios where multiple contributors may be proposing changes simultaneously. Together, these contributions aim to provide a convenient starting point to tackle some of the issues in traditional paper-formatted publishing of research output. By prioritizing collaboration, process-orientation, and pluralism, Lakat aims to improve the way research is conducted and disseminated and ultimately hopes to contribute to a healthier and more productive academic culture.",2023-06-15T17:27:16Z,2023-06-15T17:27:16Z,http://arxiv.org/abs/2306.09298v1,http://arxiv.org/pdf/2306.09298v1,"cs.NI, H.5.3, H.3.1, C.2.4, H.2.4"
Low-Resource Text-to-Speech Using Specific Data and Noise Augmentation,"Kishor Kayyar Lakshminarayana, Christian Dittmar, Nicola Pia, Emanuël Habets","Many neural text-to-speech architectures can synthesize nearly natural speech from text inputs. These architectures must be trained with tens of hours of annotated and high-quality speech data. Compiling such large databases for every new voice requires a lot of time and effort. In this paper, we describe a method to extend the popular Tacotron-2 architecture and its training with data augmentation to enable single-speaker synthesis using a limited amount of specific training data. In contrast to elaborate augmentation methods proposed in the literature, we use simple stationary noises for data augmentation. Our extension is easy to implement and adds almost no computational overhead during training and inference. Using only two hours of training data, our approach was rated by human listeners to be on par with the baseline Tacotron-2 trained with 23.5 hours of LJSpeech data. In addition, we tested our model with a semantically unpredictable sentences test, which showed that both models exhibit similar intelligibility levels.",2023-06-16T19:42:40Z,2023-06-16T19:42:40Z,http://arxiv.org/abs/2306.10152v1,http://arxiv.org/pdf/2306.10152v1,"eess.AS, cs.SD"
Wireless Data Link at 1Gbps using 256 QAM,David Noel,"This report describes the design and proposal of a wireless link capable of broadcasting at 1 Gbps. For this application, isotropic antennas, 256 QAM modulation, and BER level less than 1e-5, without using error correction coding, were implemented. A frequency of 5GHz was employed to achieve such high data rates. For unlicensed operations in this frequency range, the FCC allocates a 5.15 - 5.35 GHz frequency range with maximum acceptable power levels no greater than 250mW(~24dBm)[2]. Due to its inexpensiveness and simplicity, the transceiver architecture and all its subsystems used the homodyne system. The complete system architecture is described with some of their most significant performance characteristics, including modulation, fundamental and 3rd harmonics, power spectra, and constellation diagrams. To conclude, a Bill of Materials (BOM), costs, and associated specifications were included.",2023-06-17T20:13:35Z,2023-06-17T20:13:35Z,http://arxiv.org/abs/2306.10416v1,http://arxiv.org/pdf/2306.10416v1,"eess.SP, cs.NI"
Complex-valued Adaptive System Identification via Low-Rank Tensor   Decomposition,"Oliver Ploder, Christina Auer, Oliver Lang, Thomas Paireder, Mario Huemer","Machine learning (ML) and tensor-based methods have been of significant interest for the scientific community for the last few decades. In a previous work we presented a novel tensor-based system identification framework to ease the computational burden of tensor-only architectures while still being able to achieve exceptionally good performance. However, the derived approach only allows to process real-valued problems and is therefore not directly applicable on a wide range of signal processing and communications problems, which often deal with complex-valued systems. In this work we therefore derive two new architectures to allow the processing of complex-valued signals, and show that these extensions are able to surpass the trivial, complex-valued extension of the original architecture in terms of performance, while only requiring a slight overhead in computational resources to allow for complex-valued operations.",2023-06-28T07:01:08Z,2023-06-28T07:01:08Z,http://arxiv.org/abs/2306.16428v1,http://arxiv.org/pdf/2306.16428v1,"cs.LG, math.ST, stat.TH"
WavePaint: Resource-efficient Token-mixer for Self-supervised Inpainting,"Pranav Jeevan, Dharshan Sampath Kumar, Amit Sethi","Image inpainting, which refers to the synthesis of missing regions in an image, can help restore occluded or degraded areas and also serve as a precursor task for self-supervision. The current state-of-the-art models for image inpainting are computationally heavy as they are based on transformer or CNN backbones that are trained in adversarial or diffusion settings. This paper diverges from vision transformers by using a computationally-efficient WaveMix-based fully convolutional architecture -- WavePaint. It uses a 2D-discrete wavelet transform (DWT) for spatial and multi-resolution token-mixing along with convolutional layers. The proposed model outperforms the current state-of-the-art models for image inpainting on reconstruction quality while also using less than half the parameter count and considerably lower training and evaluation times. Our model even outperforms current GAN-based architectures in CelebA-HQ dataset without using an adversarially trainable discriminator. Our work suggests that neural architectures that are modeled after natural image priors require fewer parameters and computations to achieve generalization comparable to transformers.",2023-07-01T18:41:34Z,2023-07-01T18:41:34Z,http://arxiv.org/abs/2307.00407v1,http://arxiv.org/pdf/2307.00407v1,"cs.CV, cs.AI, I.2.10; I.4.0; I.4.4; I.4.3; I.4.5; I.4.1; I.4.2; I.4.6; I.4.7;
  I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2; I.5.4"
ScalOTA: Scalable Secure Over-the-Air Software Updates for Vehicles,"Ali Shoker, Fernando Alves, Paulo Esteves-Verissimo","Over-the-Air (OTA) software updates are becoming essential for electric/electronic vehicle architectures in order to reduce recalls amid the increasing software bugs and vulnerabilities. Current OTA update architectures rely heavily on direct cellular repository-to-vehicle links, which makes the repository a communication bottleneck, and increases the cellular bandwidth utilization cost as well as the software download latency. In this paper, we introduce ScalOTA, an end-to-end scalable OTA software update architecture and secure protocol for modern vehicles. For the first time, we propose using a network of update stations, as part of Electric Vehicle charging stations, to boost the download speed through these stations, and reduce the cellular bandwidth overhead significantly. Our formalized OTA update protocol ensures proven end-to-end chain-of-trust including all stakeholders: manufacturer, suppliers, update stations, and all layers of in-vehicle Electric Control Units (ECUs). The empirical evaluation shows that ScalOTA reduces the bandwidth utilization and download latency up to an order of magnitude compared with current OTA update systems.",2023-07-05T05:30:22Z,2023-07-05T05:30:22Z,http://arxiv.org/abs/2307.02032v1,http://arxiv.org/pdf/2307.02032v1,"cs.CR, cs.DC, cs.RO, cs.SY, eess.SY"
Self-Expanding Neural Networks,"Rupert Mitchell, Robin Menzenbach, Kristian Kersting, Martin Mundt","The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only its size, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the ``rate'' at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks with full connectivity and convolutions in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.",2023-07-10T12:49:59Z,2024-02-09T14:02:28Z,http://arxiv.org/abs/2307.04526v3,http://arxiv.org/pdf/2307.04526v3,"cs.LG, I.2.6"
Universal Unitary Photonic Circuits by Interlacing Discrete Fractional   Fourier Transform and Phase Modulation,"Matthew Markowitz, Mohammad-Ali Miri","We introduce a novel parameterization of complex unitary matrices, which allows for the efficient photonic implementation of arbitrary linear discrete unitary operators. The proposed architecture is built on factorizing an $N \times N$ unitary matrix into interlaced discrete fractional Fourier transforms and $N$-parameter diagonal phase shifts. We show that such a configuration can represent arbitrary unitary operators with $N+1$ phase layers. We discuss a gradient-based algorithm for finding the optimal phase parameters for implementing a given unitary matrix. By increasing the number of phase layers beyond the critical value of $N+1$, the optimization consistently converges faster as the system becomes over-determined. We propose an integrated photonic circuit realization of this architecture with coupled waveguide arrays and reconfigurable phase modulators. The proposed architecture can pave the way for developing novel families of programmable photonic circuits for optical classical and quantum information processing.",2023-07-14T00:23:14Z,2023-07-14T00:23:14Z,http://arxiv.org/abs/2307.07101v1,http://arxiv.org/pdf/2307.07101v1,"physics.optics, quant-ph"
randomHAR: Improving Ensemble Deep Learners for Human Activity   Recognition with Sensor Selection and Reinforcement Learning,"Yiran Huang, Yexu Zhou, Till Riedel, Likun Fang, Michael Beigl","Deep learning has proven to be an effective approach in the field of Human activity recognition (HAR), outperforming other architectures that require manual feature engineering. Despite recent advancements, challenges inherent to HAR data, such as noisy data, intra-class variability and inter-class similarity, remain. To address these challenges, we propose an ensemble method, called randomHAR. The general idea behind randomHAR is training a series of deep learning models with the same architecture on randomly selected sensor data from the given dataset. Besides, an agent is trained with the reinforcement learning algorithm to identify the optimal subset of the trained models that are utilized for runtime prediction. In contrast to existing work, this approach optimizes the ensemble process rather than the architecture of the constituent models. To assess the performance of the approach, we compare it against two HAR algorithms, including the current state of the art, on six HAR benchmark datasets. The result of the experiment demonstrates that the proposed approach outperforms the state-of-the-art method, ensembleLSTM.",2023-07-15T10:51:03Z,2023-07-15T10:51:03Z,http://arxiv.org/abs/2307.07770v1,http://arxiv.org/pdf/2307.07770v1,"cs.LG, eess.SP"
NeoSySPArtaN: A Neuro-Symbolic Spin Prediction Architecture for   higher-order multipole waveforms from eccentric Binary Black Hole mergers   using Numerical Relativity,"Amrutaa Vibho, Ali Al Bataineh","The prediction of spin magnitudes in binary black hole and neutron star mergers is crucial for understanding the astrophysical processes and gravitational wave (GW) signals emitted during these cataclysmic events. In this paper, we present a novel Neuro-Symbolic Architecture (NSA) that combines the power of neural networks and symbolic regression to accurately predict spin magnitudes of black hole and neutron star mergers. Our approach utilizes GW waveform data obtained from numerical relativity simulations in the SXS Waveform catalog. By combining these two approaches, we leverage the strengths of both paradigms, enabling a comprehensive and accurate prediction of spin magnitudes. Our experiments demonstrate that the proposed architecture achieves an impressive root-mean-squared-error (RMSE) of 0.05 and mean-squared-error (MSE) of 0.03 for the NSA model and an RMSE of 0.12 for the symbolic regression model alone. We train this model to handle higher-order multipole waveforms, with a specific focus on eccentric candidates, which are known to exhibit unique characteristics. Our results provide a robust and interpretable framework for predicting spin magnitudes in mergers. This has implications for understanding the astrophysical properties of black holes and deciphering the physics underlying the GW signals.",2023-07-20T16:30:51Z,2023-07-20T16:30:51Z,http://arxiv.org/abs/2307.11003v1,http://arxiv.org/pdf/2307.11003v1,"astro-ph.HE, cs.AI"
A signal processing interpretation of noise-reduction convolutional   neural networks,"Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen","Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.",2023-07-25T11:45:28Z,2023-07-25T11:45:28Z,http://arxiv.org/abs/2307.13425v1,http://arxiv.org/pdf/2307.13425v1,"cs.CV, cs.LG, eess.IV, eess.SP"
An Empirical Evaluation of AriDeM using Matrix Multiplication,Patrick Mukala,"For a long time, the Von Neumann has been a successful model of computation for sequential computing .Many models including the dataflow model have been unsuccessfully developed to emulate the same results in parallel computing. It is widely accepted that high performance computation is better-achieved using parallel architectures and is seen as the basis for future computational architectures with the ever-increasing need for high performance computation. We describe a new model of parallel computation known as the Arithmetic Deduction Model (AriDem) which has some similarities with the Von Neumann. A theoretical evaluation conducted on this model in comparison with the predominant von Neumann model indicated AriDeM to be more efficient in resources utilization. In this paper, we conduct an empirical evaluation of the model and the results reflect the output of the theoretical evaluation.",2023-07-16T01:16:19Z,2023-07-16T01:16:19Z,http://arxiv.org/abs/2308.00661v1,http://arxiv.org/pdf/2308.00661v1,"cs.AR, F.2.2; I.2.7"
Embedding Capabilities of Neural ODEs,"Christian Kuehn, Sara-Viola Kuntz","A class of neural networks that gained particular interest in the last years are neural ordinary differential equations (neural ODEs). We study input-output relations of neural ODEs using dynamical systems theory and prove several results about the exact embedding of maps in different neural ODE architectures in low and high dimension. The embedding capability of a neural ODE architecture can be increased by adding, for example, a linear layer, or augmenting the phase space. Yet, there is currently no systematic theory available and our work contributes towards this goal by developing various embedding results as well as identifying situations, where no embedding is possible. The mathematical techniques used include as main components iterative functional equations, Morse functions and suspension flows, as well as several further ideas from analysis. Although practically, mainly universal approximation theorems are used, our geometric dynamical systems viewpoint on universal embedding provides a fundamental understanding, why certain neural ODE architectures perform better than others.",2023-08-02T15:16:34Z,2023-09-28T08:52:11Z,http://arxiv.org/abs/2308.01213v2,http://arxiv.org/pdf/2308.01213v2,"math.DS, cs.NE"
Exploring Different Time-series-Transformer (TST) Architectures: A Case   Study in Battery Life Prediction for Electric Vehicles (EVs),"Niranjan Sitapure, Atharva Kulkarni","In recent years, battery technology for electric vehicles (EVs) has been a major focus, with a significant emphasis on developing new battery materials and chemistries. However, accurately predicting key battery parameters, such as state-of-charge (SOC) and temperature, remains a challenge for constructing advanced battery management systems (BMS). Existing battery models do not comprehensively cover all parameters affecting battery performance, including non-battery-related factors like ambient temperature, cabin temperature, elevation, and regenerative braking during EV operation. Due to the difficulty of incorporating these auxiliary parameters into traditional models, a data-driven approach is suggested. Time-series-transformers (TSTs), leveraging multiheaded attention and parallelization-friendly architecture, are explored alongside LSTM models. Novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM, are also developed and compared against existing models. A dataset comprising 72 driving trips in a BMW i3 (60 Ah) is used to address battery life prediction in EVs, aiming to create accurate TST models that incorporate environmental, battery, vehicle driving, and heating circuit data to predict SOC and battery temperature for future time steps.",2023-08-07T02:42:21Z,2023-08-07T02:42:21Z,http://arxiv.org/abs/2308.03260v1,http://arxiv.org/pdf/2308.03260v1,"cs.LG, cs.SY, eess.SY, J.2; J.6; I.6"
High photon-loss threshold quantum computing using GHZ-state   measurements,"Brendan Pankovich, Angus Kan, Kwok Ho Wan, Maike Ostmann, Alex Neville, Srikrishna Omkar, Adel Sohbi, Kamil Brádler","We propose fault-tolerant architectures based on performing projective measurements in the Greenberger-Horne-Zeilinger (GHZ) basis on constant-sized, entangled resource states. We present linear-optical constructions of the architectures, where the GHZ-state measurements are encoded to suppress the errors induced by photon loss and the probabilistic nature of linear optics. Simulations of our constructions demonstrate high single-photon loss thresholds compared to the state-of-the-art linear-optical architecture realized with encoded two-qubit fusion measurements performed on constant-sized resource states. We believe this result shows a resource-efficient path to achieving photonic fault-tolerant quantum computing.",2023-08-08T11:36:23Z,2023-08-08T11:36:23Z,http://arxiv.org/abs/2308.04192v1,http://arxiv.org/pdf/2308.04192v1,"quant-ph, math-ph, math.MP"
Exploring Multilingual Text Data Distillation,"Shivam Sahni, Harsh Patel","With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power. To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements. However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature. Additionally, existing dataset distillation methods often struggle to generalize to new architectures. In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods. We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization. Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods. Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillation domain.",2023-08-09T14:31:57Z,2023-08-09T14:31:57Z,http://arxiv.org/abs/2308.04982v1,http://arxiv.org/pdf/2308.04982v1,"cs.CL, cs.AI, F.2.2, I.2.7"
"Perpetual Reconfigurable Intelligent Surfaces Through In-Band Energy   Harvesting: Architectures, Protocols, and Challenges","Konstantinos Ntontin, Alexandros-Apostolos A. Boulogeorgos, Sergi Abadal, Agapi Mesodiakaki, Symeon Chatzinotas, Björn Ottersten","Reconfigurable intelligent surfaces (RISs) are considered to be a key enabler of highly energy-efficient 6G and beyond networks. This property arises from the absence of power amplifiers in the structure, in contrast to active nodes, such as small cells and relays. However, still an amount of power is required for their operation. To improve their energy efficiency further, we propose the notion of perpetual RISs, which secure the power needed to supply their functionalities through wireless energy harvesting of the impinging transmitted electromagnetic signals. Towards this, we initially explain the rationale behind such RIS capability and proceed with the presentation of the main RIS controller architecture that can realize this vision under an in-band energy harvesting consideration. Furthermore, we present a typical energy-harvesting architecture followed by two harvesting protocols. Subsequently, we study the performance of the two protocols under a typical communications scenario. Finally, we elaborate on the main research challenges governing the realization of large-scale networks with perpetual RISs.",2023-08-16T10:07:45Z,2023-08-16T10:07:45Z,http://arxiv.org/abs/2308.08267v1,http://arxiv.org/pdf/2308.08267v1,"cs.IT, math.IT"
An Initial Exploration: Learning to Generate Realistic Audio for Silent   Video,"Matthew Martel, Jackson Wagner","Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.",2023-08-23T20:08:56Z,2023-08-23T20:08:56Z,http://arxiv.org/abs/2308.12408v1,http://arxiv.org/pdf/2308.12408v1,"cs.SD, cs.CV, eess.AS"
HNAS-reg: hierarchical neural architecture search for deformable medical   image registration,"Jiong Wu, Yong Fan","Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches.",2023-08-23T21:47:28Z,2023-08-23T21:47:28Z,http://arxiv.org/abs/2308.12440v1,http://arxiv.org/pdf/2308.12440v1,"eess.IV, cs.CV"
"Choice Architecture, Privacy Valuations, and Selection Bias in Consumer   Data","Tesary Lin, Avner Strulov-Shlain","We study how choice architecture that companies deploy during data collection influences consumers' privacy valuations. Further, we explore how this influence affects the quality of data collected, including both volume and representativeness. To this end, we run a large-scale choice experiment to elicit consumers' valuation for their Facebook data while randomizing two common choice frames: default and price anchor. An opt-out default decreases valuations by 14-22% compared to opt-in, while a \$0-50 price anchor decreases valuations by 37-53% compared to a \$50-100 anchor. Moreover, in some consumer segments, the susceptibility to frame influence negatively correlates with consumers' average valuation. We find that conventional frame optimization practices that maximize the volume of data collected can have opposite effects on its representativeness. A bias-exacerbating effect emerges when consumers' privacy valuations and frame effects are negatively correlated. On the other hand, a volume-maximizing frame may also mitigate the bias by getting a high percentage of consumers into the sample data, thereby improving its coverage. We demonstrate the magnitude of the volume-bias trade-off in our data and argue that it should be a decision-making factor in choice architecture design.",2023-08-25T17:11:45Z,2023-08-25T17:11:45Z,http://arxiv.org/abs/2308.13496v1,http://arxiv.org/pdf/2308.13496v1,"econ.GN, q-fin.EC"
Speech Self-Supervised Representations Benchmarking: a Case for Larger   Probing Heads,"Salah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, Mirco Ravanelli","Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference costs, generalization and multi-level feature exploitation.",2023-08-28T09:49:48Z,2024-02-21T16:57:23Z,http://arxiv.org/abs/2308.14456v2,http://arxiv.org/pdf/2308.14456v2,"eess.AS, cs.LG, cs.SD, eess.SP"
Is visual explanation with Grad-CAM more reliable for deeper neural   networks? a case study with automatic pneumothorax diagnosis,"Zirui Qiu, Hassan Rivaz, Yiming Xiao","While deep learning techniques have provided the state-of-the-art performance in various clinical tasks, explainability regarding their decision-making process can greatly enhance the credence of these methods for safer and quicker clinical adoption. With high flexibility, Gradient-weighted Class Activation Mapping (Grad-CAM) has been widely adopted to offer intuitive visual interpretation of various deep learning models' reasoning processes in computer-assisted diagnosis. However, despite the popularity of the technique, there is still a lack of systematic study on Grad-CAM's performance on different deep learning architectures. In this study, we investigate its robustness and effectiveness across different popular deep learning models, with a focus on the impact of the networks' depths and architecture types, by using a case study of automatic pneumothorax diagnosis in X-ray scans. Our results show that deeper neural networks do not necessarily contribute to a strong improvement of pneumothorax diagnosis accuracy, and the effectiveness of GradCAM also varies among different network architectures.",2023-08-29T09:54:30Z,2023-08-29T09:54:30Z,http://arxiv.org/abs/2308.15172v1,http://arxiv.org/pdf/2308.15172v1,"eess.IV, cs.CV, cs.LG"
Exploring Music Genre Classification: Algorithm Analysis and Deployment   Architecture,"Ayan Biswas, Supriya Dhabal, Palaniandavar Venkateswaran","Music genre classification has become increasingly critical with the advent of various streaming applications. Nowadays, we find it impossible to imagine using the artist's name and song title to search for music in a sophisticated music app. It is always difficult to classify music correctly because the information linked to music, such as region, artist, album, or non-album, is so variable. This paper presents a study on music genre classification using a combination of Digital Signal Processing (DSP) and Deep Learning (DL) techniques. A novel algorithm is proposed that utilizes both DSP and DL methods to extract relevant features from audio signals and classify them into various genres. The algorithm was tested on the GTZAN dataset and achieved high accuracy. An end-to-end deployment architecture is also proposed for integration into music-related applications. The performance of the algorithm is analyzed and future directions for improvement are discussed. The proposed DSP and DL-based music genre classification algorithm and deployment architecture demonstrate a promising approach for music genre classification.",2023-09-09T19:01:12Z,2023-09-14T06:05:04Z,http://arxiv.org/abs/2309.04861v2,http://arxiv.org/pdf/2309.04861v2,"cs.SD, cs.IR, eess.AS"
Band-gap regression with architecture-optimized message-passing neural   networks,"Tim Bechtel, Daniel T. Speckhard, Jonathan Godwin, Claudia Draxl","Graph-based neural networks and, specifically, message-passing neural networks (MPNNs) have shown great potential in predicting physical properties of solids. In this work, we train an MPNN to first classify materials through density functional theory data from the AFLOW database as being metallic or semiconducting/insulating. We then perform a neural-architecture search to explore the model architecture and hyperparameter space of MPNNs to predict the band gaps of the materials identified as non-metals. The parameters in the search include the number of message-passing steps, latent size, and activation-function, among others. The top-performing models from the search are pooled into an ensemble that significantly outperforms existing models from the literature. Uncertainty quantification is evaluated with Monte-Carlo Dropout and ensembling, with the ensemble method proving superior. The domain of applicability of the ensemble model is analyzed with respect to the crystal systems, the inclusion of a Hubbard parameter in the density functional calculations, and the atomic species building up the materials.",2023-09-12T16:13:10Z,2023-09-12T16:13:10Z,http://arxiv.org/abs/2309.06348v1,http://arxiv.org/pdf/2309.06348v1,"physics.comp-ph, cs.LG"
deepFDEnet: A Novel Neural Network Architecture for Solving Fractional   Differential Equations,"Ali Nosrati Firoozsalari, Hassan Dana Mazraeh, Alireza Afzal Aghaei, Kourosh Parand","The primary goal of this research is to propose a novel architecture for a deep neural network that can solve fractional differential equations accurately. A Gaussian integration rule and a $L_1$ discretization technique are used in the proposed design. In each equation, a deep neural network is used to approximate the unknown function. Three forms of fractional differential equations have been examined to highlight the method's versatility: a fractional ordinary differential equation, a fractional order integrodifferential equation, and a fractional order partial differential equation. The results show that the proposed architecture solves different forms of fractional differential equations with excellent precision.",2023-09-14T12:58:40Z,2023-09-14T12:58:40Z,http://arxiv.org/abs/2309.07684v1,http://arxiv.org/pdf/2309.07684v1,"cs.LG, cs.AI, cs.NA, math.NA"
Statement: The Metaverse as an Information-Centric Network,"Dirk Kutscher, Jeff Burke, Giuseppe Fioccola, Paulo Mendes","This paper discusses challenges and opportunities of considering the Metaverse as an Information-Centric Network (ICN). The Web today essentially represents a data-centric application layer: data named by URLs is manipulated with REST primitives. However, the semantic gap with the underlying host-oriented transport is significant, typically leading to complexity, centralization, and brittleness. Popular interest in ""the Metaverse"" suggests that the end-user experience of the Web will evolve towards always-on eXtended Reality (XR). With the benefit of a historical perspective, computing advances, and decades of experience with a global network, there is an opportunity to holistically consider the Metaverse not as an application of the current network, but an evolution of the network itself, reducing rather than widening the gap between network architecture and application semantics. An ICN architecture offers the possibility to achieve this with less overhead, low latency, better security, and more disruption tolerance suitable to diverse uses cases, even those facing intermittent connectivity.",2023-09-17T03:37:29Z,2023-09-17T03:37:29Z,http://arxiv.org/abs/2309.09147v1,http://arxiv.org/pdf/2309.09147v1,"cs.NI, cs.MM, C.2; E.1"
Investigating End-to-End ASR Architectures for Long Form Audio   Transcription,"Nithin Rao Koluguri, Samuel Kriman, Georgy Zelenfroind, Somshubra Majumdar, Dima Rekesh, Vahid Noroozi, Jagadeesh Balam, Boris Ginsburg","This paper presents an overview and evaluation of some of the end-to-end ASR models on long-form audios. We study three categories of Automatic Speech Recognition(ASR) models based on their core architecture: (1) convolutional, (2) convolutional with squeeze-and-excitation and (3) convolutional models with attention. We selected one ASR model from each category and evaluated Word Error Rate, maximum audio length and real-time factor for each model on a variety of long audio benchmarks: Earnings-21 and 22, CORAAL, and TED-LIUM3. The model from the category of self-attention with local attention and global token has the best accuracy comparing to other architectures. We also compared models with CTC and RNNT decoders and showed that CTC-based models are more robust and efficient than RNNT on long form audio.",2023-09-18T17:13:50Z,2023-09-20T18:39:20Z,http://arxiv.org/abs/2309.09950v2,http://arxiv.org/pdf/2309.09950v2,"eess.AS, cs.SD"
Beamforming Design for RIS-Aided THz Wideband Communication Systems,"Yihang Jiang, Ziqin Zhou, Xiaoyang Li, Yi Gong","Benefiting from tens of GHz of bandwidth, terahertz (THz) communications has become a promising technology for future 6G networks. However, the conventional hybrid beamforming architecture based on frequency-independent phase-shifters is not able to cope with the beam split effect (BSE) in THz massive multiple-input multiple-output (MIMO) systems. Despite some work introducing the frequency-dependent phase shifts via the time delay network to mitigate the beam splitting in THz wideband communications, the corresponding issue in reconfigurable intelligent surface (RIS)-aided communications has not been well investigated. In this paper, the BSE in THz massive MIMO is quantified by analyzing the array gain loss. A new beamforming architecture has been proposed to mitigate this effect under RIS-aided communications scenarios. Simulations are performed to evaluate the effectiveness of the proposed system architecture in combating the array gain loss.",2023-09-20T09:18:26Z,2023-09-21T09:07:57Z,http://arxiv.org/abs/2309.11161v2,http://arxiv.org/pdf/2309.11161v2,"cs.IT, eess.SP, math.IT"
Understanding Patterns of Deep Learning ModelEvolution in Network   Architecture Search,"Robert Underwood, Meghana Madhastha, Randal Burns, Bogdan Nicolae","Network Architecture Search and specifically Regularized Evolution is a common way to refine the structure of a deep learning model.However, little is known about how models empirically evolve over time which has design implications for designing caching policies, refining the search algorithm for particular applications, and other important use cases.In this work, we algorithmically analyze and quantitatively characterize the patterns of model evolution for a set of models from the Candle project and the Nasbench-201 search space.We show how the evolution of the model structure is influenced by the regularized evolution algorithm. We describe how evolutionary patterns appear in distributed settings and opportunities for caching and improved scheduling. Lastly, we describe the conditions that affect when particular model architectures rise and fall in popularity based on their frequency of acting as a donor in a sliding window.",2023-09-22T02:12:47Z,2023-09-22T02:12:47Z,http://arxiv.org/abs/2309.12576v1,http://arxiv.org/pdf/2309.12576v1,"cs.AI, cs.DC, I.2.6; C.4"
Performance Analysis of UNet and Variants for Medical Image Segmentation,"Walid Ehab, Yongmin Li","Medical imaging plays a crucial role in modern healthcare by providing non-invasive visualisation of internal structures and abnormalities, enabling early disease detection, accurate diagnosis, and treatment planning. This study aims to explore the application of deep learning models, particularly focusing on the UNet architecture and its variants, in medical image segmentation. We seek to evaluate the performance of these models across various challenging medical image segmentation tasks, addressing issues such as image normalization, resizing, architecture choices, loss function design, and hyperparameter tuning. The findings reveal that the standard UNet, when extended with a deep network layer, is a proficient medical image segmentation model, while the Res-UNet and Attention Res-UNet architectures demonstrate smoother convergence and superior performance, particularly when handling fine image details. The study also addresses the challenge of high class imbalance through careful preprocessing and loss function definitions. We anticipate that the results of this study will provide useful insights for researchers seeking to apply these models to new medical imaging problems and offer guidance and best practices for their implementation.",2023-09-22T17:20:40Z,2023-09-22T17:20:40Z,http://arxiv.org/abs/2309.13013v1,http://arxiv.org/pdf/2309.13013v1,"eess.IV, cs.CV"
Protein container disassembly pathways depend on geometric design,"Q. Roussel, S. Benbedra, R Twarock","The majority of viruses are organised according to the structural blueprints of the seminal Caspar-Klug theory. However, there are a number of notable exceptions to this geometric design principle. Prominent examples are the cancer-causing papilloma viridae and the \textit{de novo} designed AaLS cages that exhibit non-quasiequivalent capsid structures with protein numbers excluded by Caspar-Klug theory. The biophysical properties of these geometrically distinct architectures and the fitness advantages driving their evolution are currently unclear. We investigate here the resilience to fragmentation and disassembly behaviour of these capsid geometries by introducing a percolation theory on weighted graphs. We show that these cage architectures follow one of two distinct disassembly pathways, preferring either hole formation or capsid fragmentation. This suggests that preference for specific disassembly scenarios could be a driving force for the evolution of the non Caspar-Klug protein container architectures.",2023-09-27T21:20:17Z,2023-09-27T21:20:17Z,http://arxiv.org/abs/2309.16030v1,http://arxiv.org/pdf/2309.16030v1,"q-bio.BM, 92B05, 92C05"
A Novel U-Net Architecture for Denoising of Real-world Noise Corrupted   Phonocardiogram Signal,"Ayan Mukherjee, Rohan Banerjee, Avik Ghose","The bio-acoustic information contained within heart sound signals are utilized by physicians world-wide for auscultation purpose. However, the heart sounds are inherently susceptible to noise contamination. Various sources of noises like lung sound, coughing, sneezing, and other background noises are involved in such contamination. Such corruption of the heart sound signal often leads to inconclusive or false diagnosis. To address this issue, we have proposed a novel U-Net based deep neural network architecture for denoising of phonocardiogram (PCG) signal in this paper. For the design, development and validation of the proposed architecture, a novel approach of synthesizing real-world noise corrupted PCG signals have been proposed. For the purpose, an open-access real-world noise sample dataset and an open-access PCG dataset has been utilized. The performance of the proposed denoising methodology has been evaluated on the synthesized noisy PCG dataset. The performance of the proposed algorithm has been compared with existing state-of-the-art (SoA) denoising algorithms qualitatively and quantitatively. The proposed denoising technique has shown improvement in performance as comparison to the SoAs.",2023-09-30T01:35:50Z,2023-09-30T01:35:50Z,http://arxiv.org/abs/2310.00216v1,http://arxiv.org/pdf/2310.00216v1,"eess.SP, cs.SD, eess.AS"
Learning State-Augmented Policies for Information Routing in   Communication Networks,"Sourajit Das, Navid NaderiAlizadeh, Alejandro Ribeiro","This paper examines the problem of information routing in a large-scale communication network, which can be formulated as a constrained statistical learning problem having access to only local information. We delineate a novel State Augmentation (SA) strategy to maximize the aggregate information at source nodes using graph neural network (GNN) architectures, by deploying graph convolutions over the topological links of the communication network. The proposed technique leverages only the local information available at each node and efficiently routes desired information to the destination nodes. We leverage an unsupervised learning procedure to convert the output of the GNN architecture to optimal information routing strategies. In the experiments, we perform the evaluation on real-time network topologies to validate our algorithms. Numerical simulations depict the improved performance of the proposed method in training a GNN parameterization as compared to baseline algorithms.",2023-09-30T04:34:25Z,2024-12-06T20:01:21Z,http://arxiv.org/abs/2310.00248v3,http://arxiv.org/pdf/2310.00248v3,"cs.NI, cs.LG, eess.SP"
Optimization or Architecture: How to Hack Kalman Filtering,"Ido Greenberg, Netanel Yannay, Shie Mannor","In non-linear filtering, it is traditional to compare non-linear architectures such as neural networks to the standard linear Kalman Filter (KF). We observe that this mixes the evaluation of two separate components: the non-linear architecture, and the parameters optimization method. In particular, the non-linear model is often optimized, whereas the reference KF model is not. We argue that both should be optimized similarly, and to that end present the Optimized KF (OKF). We demonstrate that the KF may become competitive to neural models - if optimized using OKF. This implies that experimental conclusions of certain previous studies were derived from a flawed process. The advantage of OKF over the standard KF is further studied theoretically and empirically, in a variety of problems. Conveniently, OKF can replace the KF in real-world systems by merely updating the parameters.",2023-10-01T14:00:18Z,2023-10-01T14:00:18Z,http://arxiv.org/abs/2310.00675v1,http://arxiv.org/pdf/2310.00675v1,"cs.LG, eess.SP"
"A MKID-readout based on a heterogeneous, closely coupled architecture","Gerrit Grutzeck, Ingo Krämer, Miroslaw Ciechanowicz, Nicolas Reyes, Carsten König, Andrey Baryshev, Stephen Yates, Bernd Klein","Within this proceeding, we introduce the U-Board platform, a versatile platform for signal generation, acquisition and processing, based on a heterogenous processing architecture. Based on this platform we present a readout for Microwave Kinetic Inductance Detectors (MKIDs) for the A-MKID camera for APEX. In addition to the implementation of the readout on this heterogenous architecture, we also present a first comparison of the performance of the readout compared to the currently used readout of the A-MKID camera. Last but not least, we discuss how we plan to miniaturize the current prototype, which is based on commercial off the shelf components.",2023-10-09T09:12:48Z,2023-10-09T09:12:48Z,http://arxiv.org/abs/2310.05544v1,http://arxiv.org/pdf/2310.05544v1,"astro-ph.IM, physics.ins-det"
Exploiting Language Models as a Source of Knowledge for Cognitive Agents,"James R. Kirk, Robert E. Wray, John E. Laird","Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.",2023-09-05T15:18:04Z,2023-09-05T15:18:04Z,http://arxiv.org/abs/2310.06846v1,http://arxiv.org/pdf/2310.06846v1,"cs.AI, cs.CL, I.2.7; I.2.11"
Equivariant Matrix Function Neural Networks,"Ilyes Batatia, Lars L. Schaaf, Huajie Chen, Gábor Csányi, Christoph Ortner, Felix A. Faber","Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in graphs such as large conjugated molecules, and social networks due to oversmoothing and oversquashing. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack generalizability, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves stateof-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.",2023-10-16T14:17:00Z,2024-01-30T11:10:00Z,http://arxiv.org/abs/2310.10434v2,http://arxiv.org/pdf/2310.10434v2,"stat.ML, cond-mat.mtrl-sci, cs.LG, physics.chem-ph"
Precise influence evaluation in complex networks,"Bingyu Zhu, Qingyun Sun, Jianxin Li, Daqing Li","Evaluating node influence is fundamental for identifying key nodes in complex networks. Existing methods typically rely on generic indicators to rank node influence across diverse networks, thereby ignoring the individualized features of each network itself. Actually, node influence stems not only from general features but the multi-scale individualized information encompassing specific network structure and task. Here we design an active learning architecture to predict node influence quantitively and precisely, which samples representative nodes based on graph entropy correlation matrix integrating multi-scale individualized information. This brings two intuitive advantages: (1) discovering potential high-influence but weak-connected nodes that are usually ignored in existing methods, (2) improving the influence maximization strategy by deducing influence interference. Significantly, our architecture demonstrates exceptional transfer learning capabilities across multiple types of networks, which can identify those key nodes with large disputation across different existing methods. Additionally, our approach, combined with a simple greedy algorithm, exhibits dominant performance in solving the influence maximization problem. This architecture holds great potential for applications in graph mining and prediction tasks.",2023-10-17T17:09:50Z,2024-05-12T17:18:27Z,http://arxiv.org/abs/2310.12181v2,http://arxiv.org/pdf/2310.12181v2,"cs.SI, physics.data-an"
19 Parameters Is All You Need: Tiny Neural Networks for Particle Physics,"Alexander Bogatskiy, Timothy Hoffman, Jan T. Offermann","As particle accelerators increase their collision rates, and deep learning solutions prove their viability, there is a growing need for lightweight and fast neural network architectures for low-latency tasks such as triggering. We examine the potential of one recent Lorentz- and permutation-symmetric architecture, PELICAN, and present its instances with as few as 19 trainable parameters that outperform generic architectures with tens of thousands of parameters when compared on the binary classification task of top quark jet tagging.",2023-10-24T18:51:22Z,2023-12-13T20:56:06Z,http://arxiv.org/abs/2310.16121v3,http://arxiv.org/pdf/2310.16121v3,"hep-ph, cs.LG, hep-ex"
Cascaded Multi-task Adaptive Learning Based on Neural Architecture   Search,"Yingying Gao, Shilei Zhang, Zihao Cui, Chao Deng, Junlan Feng","Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance.",2023-10-23T06:43:50Z,2023-10-23T06:43:50Z,http://arxiv.org/abs/2310.17664v1,http://arxiv.org/pdf/2310.17664v1,"cs.LG, eess.AS, eess.SP"
E(2) Equivariant Neural Networks for Robust Galaxy Morphology   Classification,"Sneh Pandya, Purvik Patel, Franc O, Jonathan Blazek","We propose the use of group convolutional neural network architectures (GCNNs) equivariant to the 2D Euclidean group, $E(2)$, for the task of galaxy morphology classification by utilizing symmetries of the data present in galaxy images as an inductive bias in the architecture. We conduct robustness studies by introducing artificial perturbations via Poisson noise insertion and one-pixel adversarial attacks to simulate the effects of limited observational capabilities. We train, validate, and test GCNNs equivariant to discrete subgroups of $E(2)$ - the cyclic and dihedral groups of order $N$ - on the Galaxy10 DECals dataset and find that GCNNs achieve higher classification accuracy and are consistently more robust than their non-equivariant counterparts, with an architecture equivariant to the group $D_{16}$ achieving a $95.52 \pm 0.18\%$ test-set accuracy. We also find that the model loses $<6\%$ accuracy on a $50\%$-noise dataset and all GCNNs are less susceptible to one-pixel perturbations than an identically constructed CNN. Our code is publicly available at https://github.com/snehjp2/GCNNMorphology.",2023-11-02T18:00:02Z,2023-11-02T18:00:02Z,http://arxiv.org/abs/2311.01500v1,http://arxiv.org/pdf/2311.01500v1,"astro-ph.GA, cs.LG"
Efficient Algorithms for Monte Carlo Particle Transport on AI   Accelerator Hardware,"John Tramm, Bryce Allen, Kazutomo Yoshii, Andrew Siegel, Leighton Wilson","The recent trend toward deep learning has led to the development of a variety of highly innovative AI accelerator architectures. One such architecture, the Cerebras Wafer-Scale Engine 2 (WSE-2), features 40 GB of on-chip SRAM, making it a potentially attractive platform for latency- or bandwidth-bound HPC simulation workloads. In this study, we examine the feasibility of performing continuous energy Monte Carlo (MC) particle transport on the WSE-2 by porting a key kernel from the MC transport algorithm to Cerebras's CSL programming model. New algorithms for minimizing communication costs and for handling load balancing are developed and tested. The WSE-2 is found to run 130 times faster than a highly optimized CUDA version of the kernel run on an NVIDIA A100 GPU -- significantly outpacing the expected performance increase given the difference in transistor counts between the architectures.",2023-11-03T06:27:36Z,2023-11-07T03:22:56Z,http://arxiv.org/abs/2311.01739v2,http://arxiv.org/pdf/2311.01739v2,"cs.DC, cs.PF, D.1.3; J.2"
Understanding the Natural Language of DNA using Encoder-Decoder   Foundation Models with Byte-level Precision,"Aditya Malusare, Harish Kothandaraman, Dipesh Tamboli, Nadia A. Lanman, Vaneet Aggarwal","This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations. In each of these tasks, we demonstrate significant improvement as compared to the existing state-of-the-art results.",2023-11-04T06:00:56Z,2024-08-22T20:18:06Z,http://arxiv.org/abs/2311.02333v3,http://arxiv.org/pdf/2311.02333v3,"cs.LG, q-bio.GN"
End-to-end autoencoding architecture for the simultaneous generation of   medical images and corresponding segmentation masks,"Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Pierre Vera, Su Ruan","Despite the increasing use of deep learning in medical image segmentation, acquiring sufficient training data remains a challenge in the medical field. In response, data augmentation techniques have been proposed; however, the generation of diverse and realistic medical images and their corresponding masks remains a difficult task, especially when working with insufficient training sets. To address these limitations, we present an end-to-end architecture based on the Hamiltonian Variational Autoencoder (HVAE). This approach yields an improved posterior distribution approximation compared to traditional Variational Autoencoders (VAE), resulting in higher image generation quality. Our method outperforms generative adversarial architectures under data-scarce conditions, showcasing enhancements in image quality and precise tumor mask synthesis. We conduct experiments on two publicly available datasets, MICCAI's Brain Tumor Segmentation Challenge (BRATS), and Head and Neck Tumor Segmentation Challenge (HECKTOR), demonstrating the effectiveness of our method on different medical imaging modalities.",2023-11-17T11:56:53Z,2023-11-17T11:56:53Z,http://arxiv.org/abs/2311.10472v1,http://arxiv.org/pdf/2311.10472v1,"eess.IV, cs.CV"
Advancing The Rate-Distortion-Computation Frontier For Neural Image   Compression,"David Minnen, Nick Johnston","The rate-distortion performance of neural image compression models has exceeded the state-of-the-art for non-learned codecs, but neural codecs are still far from widespread deployment and adoption. The largest obstacle is having efficient models that are feasible on a wide variety of consumer hardware. Comparative research and evaluation is difficult due to the lack of standard benchmarking platforms and due to variations in hardware architectures and test environments. Through our rate-distortion-computation (RDC) study we demonstrate that neither floating-point operations (FLOPs) nor runtime are sufficient on their own to accurately rank neural compression methods. We also explore the RDC frontier, which leads to a family of model architectures with the best empirical trade-off between computational requirements and RD performance. Finally, we identify a novel neural compression architecture that yields state-of-the-art RD performance with rate savings of 23.1% over BPG (7.0% over VTM and 3.0% over ELIC) without requiring significantly more FLOPs than other learning-based codecs.",2023-09-26T19:47:31Z,2023-09-26T19:47:31Z,http://arxiv.org/abs/2311.12821v1,http://arxiv.org/pdf/2311.12821v1,"cs.CV, cs.LG, eess.IV"
Neural Crystals,"Sofia Karamintziou, Thanassis Mavropoulos, Dimos Ntioudis, Georgios Meditskos, Stefanos Vrochidis, Ioannis, Kompatsiaris","We face up to the challenge of explainability in Multimodal Artificial Intelligence (MMAI). At the nexus of neuroscience-inspired and quantum computing, interpretable and transparent spin-geometrical neural architectures for early fusion of large-scale, heterogeneous, graph-structured data are envisioned, harnessing recent evidence for relativistic quantum neural coding of (co-)behavioral states in the self-organizing brain, under competitive, multidimensional dynamics. The designs draw on a self-dual classical description - via special Clifford-Lipschitz operations - of spinorial quantum states within registers of at most 16 qubits for efficient encoding of exponentially large neural structures. Formally 'trained', Lorentz neural architectures with precisely one lateral layer of exclusively inhibitory interneurons accounting for anti-modalities, as well as their co-architectures with intra-layer connections are highlighted. The approach accommodates the fusion of up to 16 time-invariant interconnected (anti-)modalities and the crystallization of latent multidimensional patterns. Comprehensive insights are expected to be gained through applications to Multimodal Big Data, under diverse real-world scenarios.",2023-09-05T05:34:57Z,2023-12-08T07:36:16Z,http://arxiv.org/abs/2311.16111v2,http://arxiv.org/pdf/2311.16111v2,"q-bio.NC, cs.AI"
A Data-Driven Safety Preserving Control Architecture for Constrained   Cyber-Physical Systems,"Mehran Attar, Walter Lucia","In this paper, we propose a data-driven networked control architecture for unknown and constrained cyber-physical systems capable of detecting networked false-data-injection attacks and ensuring plant's safety. In particular, on the controller's side, we design a novel robust anomaly detector that can discover the presence of network attacks using a data-driven outer approximation of the expected robust one-step reachable set. On the other hand, on the plant's side, we design a data-driven safety verification module, which resorts to worst-case arguments to determine if the received control input is safe for the plant's evolution. Whenever necessary, the same module is in charge of replacing the networked controller with a local data-driven set-theoretic model predictive controller, whose objective is to keep the plant's trajectory in a pre-established safe configuration until an attack-free condition is recovered. Numerical simulations involving a two-tank water system illustrate the features and capabilities of the proposed control architecture.",2023-12-01T15:35:11Z,2024-02-21T13:11:16Z,http://arxiv.org/abs/2312.00658v2,http://arxiv.org/pdf/2312.00658v2,"eess.SY, cs.SY"
Towards an accurate and generalizable multiple sclerosis lesion   segmentation model using self-ensembled lesion fusion,"Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Dzung L. Pham, Aaron Carass, Jerry L. Prince","Automatic multiple sclerosis (MS) lesion segmentation using multi-contrast magnetic resonance (MR) images provides improved efficiency and reproducibility compared to manual delineation. Current state-of-the-art automatic MS lesion segmentation methods utilize modified U-Net-like architectures. However, in the literature, dedicated architecture modifications were always required to maximize their performance. In addition, the best-performing methods have not proven to be generalizable to diverse test datasets with contrast variations and image artifacts. In this work, we developed an accurate and generalizable MS lesion segmentation model using the well-known U-Net architecture without further modification. A novel test-time self-ensembled lesion fusion strategy is proposed that not only achieved the best performance using the ISBI 2015 MS segmentation challenge data but also demonstrated robustness across various self-ensemble parameter choices. Moreover, equipped with instance normalization rather than batch normalization widely used in literature, the model trained on the ISBI challenge data generalized well on clinical test datasets from different scanners.",2023-12-03T17:08:10Z,2023-12-03T17:08:10Z,http://arxiv.org/abs/2312.01460v1,http://arxiv.org/pdf/2312.01460v1,"eess.IV, cs.CV"
Consistency Models for Scalable and Fast Simulation-Based Inference,"Marvin Schmitt, Valentin Pratz, Ullrich Köthe, Paul-Christian Bürkner, Stefan T Radev","Simulation-based inference (SBI) is constantly in search of more expressive and efficient algorithms to accurately infer the parameters of complex simulation models. In line with this goal, we present consistency models for posterior estimation (CMPE), a new conditional sampler for SBI that inherits the advantages of recent unconstrained architectures and overcomes their sampling inefficiency at inference time. CMPE essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be flexibly tailored to the structure of the estimation problem. We provide hyperparameters and default architectures that support consistency training over a wide range of different dimensions, including low-dimensional ones which are important in SBI workflows but were previously difficult to tackle even with unconditional consistency models. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on hard low-dimensional benchmarks, but also achieves competitive performance with much faster sampling speed on two realistic estimation problems with high data and/or parameter dimensions.",2023-12-09T02:14:12Z,2024-11-04T11:03:30Z,http://arxiv.org/abs/2312.05440v3,http://arxiv.org/pdf/2312.05440v3,"cs.LG, cs.AI, stat.ML"
StemGen: A music generation model that listens,"Julian D. Parker, Janne Spijkervet, Katerina Kosta, Furkan Yesiler, Boris Kuznetsov, Ju-Chiang Wang, Matt Avent, Jitong Chen, Duc Le","End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",2023-12-14T08:09:20Z,2024-01-16T09:15:05Z,http://arxiv.org/abs/2312.08723v2,http://arxiv.org/pdf/2312.08723v2,"cs.SD, cs.LG, eess.AS"
Grammatical information in BERT sentence embeddings as two-dimensional   arrays,"Vivi Nastase, Paola Merlo","Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches.",2023-12-15T15:41:52Z,2023-12-15T15:41:52Z,http://arxiv.org/abs/2312.09890v1,http://arxiv.org/pdf/2312.09890v1,"cs.CL, I.2.7"
Inducing Point Operator Transformer: A Flexible and Scalable   Architecture for Solving PDEs,"Seungjun Lee, Taeil Oh","Solving partial differential equations (PDEs) by learning the solution operators has emerged as an attractive alternative to traditional numerical methods. However, implementing such architectures presents two main challenges: flexibility in handling irregular and arbitrary input and output formats and scalability to large discretizations. Most existing architectures are limited by their desired structure or infeasible to scale large inputs and outputs. To address these issues, we introduce an attention-based model called an inducing-point operator transformer (IPOT). Inspired by inducing points methods, IPOT is designed to handle any input function and output query while capturing global interactions in a computationally efficient way. By detaching the inputs/outputs discretizations from the processor with a smaller latent bottleneck, IPOT offers flexibility in processing arbitrary discretizations and scales linearly with the size of inputs/outputs. Our experimental results demonstrate that IPOT achieves strong performances with manageable computational complexity on an extensive range of PDE benchmarks and real-world weather forecasting scenarios, compared to state-of-the-art methods.",2023-12-18T06:57:31Z,2023-12-18T06:57:31Z,http://arxiv.org/abs/2312.10975v1,http://arxiv.org/pdf/2312.10975v1,"cs.LG, cs.AI, cs.NA, math.NA"
Gophy: Novel Proof-of-Useful-Work blockchain architecture for High   Energy Physics,"Felix Hoffmann, Udo Kebschull","In this publication, a novel architecture for Proof-of-Useful-Work blockchain consensus which aims to replace hash-based block problems with Monte Carlo simulation-based block problems to donate computational power to real-world HEP experiments is described. Design decisions are detailed and challenges are addressed. The architecture is being implemented using Golang and can be run inside the CbmRoot software environment. The goal is to build a bridge between the disciplines HEP and blockchain to build a novel blockchain network in which the network's computational power is not wasted but instead used to support a scientific experiment while at the same time securing the underlying permissioned blockchain. The blockchain features a token-based cryptocurrency that is rewarded to miners that donate computational power and acts as an additional incentive to participate which traditional volunteer computing can not provide. The implementation named gophy is being implemented in Golang and is expected to be open-sourced before the end of 2024.",2024-04-13T22:34:48Z,2024-04-13T22:34:48Z,http://arxiv.org/abs/2404.09093v1,http://arxiv.org/pdf/2404.09093v1,"cs.CR, 94A60, I.m"
A Calibrated and Automated Simulator for Innovations in 5G,"Conrado Boeira, Antor Hasan, Khaleda Papry, Yue Ju, Zhongwen Zhu, Israat Haque","The rise of 5G deployments has created the environment for many emerging technologies to flourish. Self-driving vehicles, Augmented and Virtual Reality, and remote operations are examples of applications that leverage 5G networks' support for extremely low latency, high bandwidth, and increased throughput. However, the complex architecture of 5G hinders innovation due to the lack of accessibility to testbeds or realistic simulators with adequate 5G functionalities. Also, configuring and managing simulators are complex and time consuming. Finally, the lack of adequate representative data hinders the data-driven designs in 5G campaigns. Thus, we calibrated a system-level open-source simulator, Simu5G, following 3GPP guidelines to enable faster innovation in the 5G domain. Furthermore, we developed an API for automatic simulator configuration without knowing the underlying architectural details. Finally, we demonstrate the usage of the calibrated and automated simulator by developing an ML-based anomaly detection in a 5G Radio Access Network (RAN).",2024-04-16T15:17:23Z,2024-04-16T15:17:23Z,http://arxiv.org/abs/2404.10643v1,http://arxiv.org/pdf/2404.10643v1,"cs.NI, eess.SP"
Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural   Ordinary Differential Equations,"Jaesung Park, Sungchul Hong, Yoonseo Cho, Jong-June Jeon","Sea ice at the North Pole is vital to global climate dynamics. However, accurately forecasting sea ice poses a significant challenge due to the intricate interaction among multiple variables. Leveraging the capability to integrate multiple inputs and powerful performances seamlessly, many studies have turned to neural networks for sea ice forecasting. This paper introduces a novel deep architecture named Unicorn, designed to forecast weekly sea ice. Our model integrates multiple time series images within its architecture to enhance its forecasting performance. Moreover, we incorporate a bottleneck layer within the U-Net architecture, serving as neural ordinary differential equations with convolution operations, to capture the spatiotemporal dynamics of latent variables. Through real data analysis with datasets spanning from 1998 to 2021, our proposed model demonstrates significant improvements over state-of-the-art models in the sea ice concentration forecasting task. It achieves an average MAE improvement of 12% compared to benchmark models. Additionally, our method outperforms existing approaches in sea ice extent forecasting, achieving a classification performance improvement of approximately 18%. These experimental results show the superiority of our proposed model.",2024-05-07T01:17:06Z,2024-09-02T03:37:46Z,http://arxiv.org/abs/2405.03929v2,http://arxiv.org/pdf/2405.03929v2,"cs.AI, physics.ao-ph"
Exploring a Cognitive Architecture for Learning Arithmetic Equations,Cole Gawin,"The acquisition and performance of arithmetic skills and basic operations such as addition, subtraction, multiplication, and division are essential for daily functioning, and reflect complex cognitive processes. This paper explores the cognitive mechanisms powering arithmetic learning, presenting a neurobiologically plausible cognitive architecture that simulates the acquisition of these skills. I implement a number vectorization embedding network and an associative memory model to investigate how an intelligent system can learn and recall arithmetic equations in a manner analogous to the human brain. I perform experiments that provide insights into the generalization capabilities of connectionist models, neurological causes of dyscalculia, and the influence of network architecture on cognitive performance. Through this interdisciplinary investigation, I aim to contribute to ongoing research into the neural correlates of mathematical cognition in intelligent systems.",2024-05-05T18:42:00Z,2024-05-05T18:42:00Z,http://arxiv.org/abs/2405.04550v1,http://arxiv.org/pdf/2405.04550v1,"q-bio.NC, cs.AI"
ISR: Invertible Symbolic Regression,"Tony Tohme, Mohammad Javad Khojasteh, Mohsen Sadr, Florian Meyer, Kamal Youcef-Toumi","We introduce an Invertible Symbolic Regression (ISR) method. It is a machine learning technique that generates analytical relationships between inputs and outputs of a given dataset via invertible maps (or architectures). The proposed ISR method naturally combines the principles of Invertible Neural Networks (INNs) and Equation Learner (EQL), a neural network-based symbolic architecture for function learning. In particular, we transform the affine coupling blocks of INNs into a symbolic framework, resulting in an end-to-end differentiable symbolic invertible architecture that allows for efficient gradient-based learning. The proposed ISR framework also relies on sparsity promoting regularization, allowing the discovery of concise and interpretable invertible expressions. We show that ISR can serve as a (symbolic) normalizing flow for density estimation tasks. Furthermore, we highlight its practical applicability in solving inverse problems, including a benchmark inverse kinematics problem, and notably, a geoacoustic inversion problem in oceanography aimed at inferring posterior distributions of underlying seabed parameters from acoustic signals.",2024-05-10T23:20:46Z,2024-05-10T23:20:46Z,http://arxiv.org/abs/2405.06848v1,http://arxiv.org/pdf/2405.06848v1,"cs.LG, cs.AI, cs.IT, math.IT, stat.ML"
Adaptation of Distinct Semantics for Uncertain Areas in Polyp   Segmentation,"Quang Vinh Nguyen, Van Thong Huynh, Soo-Hyung Kim","Colonoscopy is a common and practical method for detecting and treating polyps. Segmenting polyps from colonoscopy image is useful for diagnosis and surgery progress. Nevertheless, achieving excellent segmentation performance is still difficult because of polyp characteristics like shape, color, condition, and obvious non-distinction from the surrounding context. This work presents a new novel architecture namely Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation (ADSNet), which modifies misclassified details and recovers weak features having the ability to vanish and not be detected at the final stage. The architecture consists of a complementary trilateral decoder to produce an early global map. A continuous attention module modifies semantics of high-level features to analyze two separate semantics of the early global map. The suggested method is experienced on polyp benchmarks in learning ability and generalization ability, experimental results demonstrate the great correction and recovery ability leading to better segmentation performance compared to the other state of the art in the polyp image segmentation task. Especially, the proposed architecture could be experimented flexibly for other CNN-based encoders, Transformer-based encoders, and decoder backbones.",2024-05-13T07:41:28Z,2024-05-13T07:41:28Z,http://arxiv.org/abs/2405.07523v1,http://arxiv.org/pdf/2405.07523v1,"cs.CV, cs.AI, I.5.4, I.2.1, I.4.6, J.3"
Learning Coarse-Grained Dynamics on Graph,"Yin Yu, John Harlim, Daning Huang, Yan Li","We consider a Graph Neural Network (GNN) non-Markovian modeling framework to identify coarse-grained dynamical systems on graphs. Our main idea is to systematically determine the GNN architecture by inspecting how the leading term of the Mori-Zwanzig memory term depends on the coarse-grained interaction coefficients that encode the graph topology. Based on this analysis, we found that the appropriate GNN architecture that will account for $K$-hop dynamical interactions has to employ a Message Passing (MP) mechanism with at least $2K$ steps. We also deduce that the memory length required for an accurate closure model decreases as a function of the interaction strength under the assumption that the interaction strength exhibits a power law that decays as a function of the hop distance. Supporting numerical demonstrations on two examples, a heterogeneous Kuramoto oscillator model and a power system, suggest that the proposed GNN architecture can predict the coarse-grained dynamics under fixed and time-varying graph topologies.",2024-05-15T13:25:34Z,2024-05-15T13:25:34Z,http://arxiv.org/abs/2405.09324v1,http://arxiv.org/pdf/2405.09324v1,"math.NA, cond-mat.dis-nn, cs.LG, cs.NA"
Quantum Vision Transformers for Quark-Gluon Classification,"Marçal Comajoan Cara, Gopal Ramesh Dahale, Zhongtian Dong, Roy T. Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu","We introduce a hybrid quantum-classical vision transformer architecture, notable for its integration of variational quantum circuits within both the attention mechanism and the multi-layer perceptrons. The research addresses the critical challenge of computational efficiency and resource constraints in analyzing data from the upcoming High Luminosity Large Hadron Collider, presenting the architecture as a potential solution. In particular, we evaluate our method by applying the model to multi-detector jet images from CMS Open Data. The goal is to distinguish quark-initiated from gluon-initiated jets. We successfully train the quantum model and evaluate it via numerical simulations. Using this approach, we achieve classification performance almost on par with the one obtained with the completely classical architecture, considering a similar number of parameters.",2024-05-16T17:45:54Z,2024-05-16T17:45:54Z,http://arxiv.org/abs/2405.10284v1,http://arxiv.org/pdf/2405.10284v1,"quant-ph, cs.LG, hep-ph, 68Q12 (Primary) 81P68, 68T07 (Secondary)"
EchoPT: A Pretrained Transformer Architecture that Predicts 2D In-Air   Sonar Images for Mobile Robotics,"Jan Steckel, Wouter Jansen, Nico Huebel","The predictive brain hypothesis suggests that perception can be interpreted as the process of minimizing the error between predicted perception tokens generated by an internal world model and actual sensory input tokens. When implementing working examples of this hypothesis in the context of in-air sonar, significant difficulties arise due to the sparse nature of the reflection model that governs ultrasonic sensing. Despite these challenges, creating consistent world models using sonar data is crucial for implementing predictive processing of ultrasound data in robotics. In an effort to enable robust robot behavior using ultrasound as the sole exteroceptive sensor modality, this paper introduces EchoPT, a pretrained transformer architecture designed to predict 2D sonar images from previous sensory data and robot ego-motion information. We detail the transformer architecture that drives EchoPT and compare the performance of our model to several state-of-the-art techniques. In addition to presenting and evaluating our EchoPT model, we demonstrate the effectiveness of this predictive perception approach in two robotic tasks.",2024-05-21T08:18:28Z,2024-05-21T08:18:28Z,http://arxiv.org/abs/2405.12573v1,http://arxiv.org/pdf/2405.12573v1,"cs.RO, cs.LG, cs.SY, eess.SP, eess.SY"
Analog or Digital In-memory Computing? Benchmarking through Quantitative   Modeling,"Jiacong Sun, Pouya Houshmand, Marian Verhelst","In-Memory Computing (IMC) has emerged as a promising paradigm for energy-efficient, throughput-efficient and area-efficient machine learning at the edge. However, the differences in hardware architectures, array dimensions, and fabrication technologies among published IMC realizations have made it difficult to grasp their relative strengths. Moreover, previous studies have primarily focused on exploring and benchmarking the peak performance of a single IMC macro rather than full system performance on real workloads. This paper aims to address the lack of a quantitative comparison of Analog In-Memory Computing (AIMC) and Digital In-Memory Computing (DIMC) processor architectures. We propose an analytical IMC performance model that is validated against published implementations and integrated into a system-level exploration framework for comprehensive performance assessments on different workloads with varying IMC configurations. Our experiments show that while DIMC generally has higher computational density than AIMC, AIMC with large macro sizes may have better energy efficiency than DIMC on convolutional-layers and pointwise-layers, which can exploit high spatial unrolling. On the other hand, DIMC with small macro size outperforms AIMC on depthwise-layers, which feature limited spatial unrolling opportunities inside a macro.",2024-05-23T18:27:19Z,2024-05-23T18:27:19Z,http://arxiv.org/abs/2405.14978v1,http://arxiv.org/pdf/2405.14978v1,"eess.SP, cs.AR, eess.IV"
Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep   Graph Networks,"Simon Heilig, Alessio Gravina, Alessandro Trenta, Claudio Gallicchio, Davide Bacciu","The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.",2024-05-27T13:36:50Z,2025-02-13T16:32:55Z,http://arxiv.org/abs/2405.17163v2,http://arxiv.org/pdf/2405.17163v2,"cs.LG, cs.SY, eess.SY"
Approximately-symmetric neural networks for quantum spin liquids,"Dominik S. Kufel, Jack Kemp, Simon M. Linsel, Chris R. Laumann, Norman Y. Yao","We propose and analyze a family of approximately-symmetric neural networks for quantum spin liquid problems. These tailored architectures are parameter-efficient, scalable, and significantly out-perform existing symmetry-unaware neural network architectures. Utilizing the mixed-field toric code model, we demonstrate that our approach is competitive with the state-of-the-art tensor network and quantum Monte Carlo methods. Moreover, at the largest system sizes (N=480), our method allows us to explore Hamiltonians with sign problems beyond the reach of both quantum Monte Carlo and finite-size matrix-product states. The network comprises an exactly symmetric block following a non-symmetric block, which we argue learns a transformation of the ground state analogous to quasiadiabatic continuation. Our work paves the way toward investigating quantum spin liquid problems within interpretable neural network architectures",2024-05-27T18:00:00Z,2024-05-27T18:00:00Z,http://arxiv.org/abs/2405.17541v1,http://arxiv.org/pdf/2405.17541v1,"quant-ph, cond-mat.dis-nn, cond-mat.str-el, cs.LG"
Length independent generalization bounds for deep SSM architectures,"Dániel Rácz, Mihály Petreczky, Bálint Daróczy","Many state-of-the-art models trained on long-range sequences, for example S4, S5 or LRU, are made of sequential blocks combining State-Space Models (SSMs) with neural networks. In this paper we provide a PAC bound that holds for these kind of architectures with stable SSM blocks and does not depend on the length of the input sequence. Imposing stability of the SSM blocks is a standard practice in the literature, and it is known to help performance. Our results provide a theoretical justification for the use of stable SSM blocks as the proposed PAC bound decreases as the degree of stability of the SSM blocks increases.",2024-05-30T17:32:46Z,2024-07-11T07:55:14Z,http://arxiv.org/abs/2405.20278v2,http://arxiv.org/pdf/2405.20278v2,"cs.LG, cs.AI, stat.ML, 68, I.2.6"
"A Comparative Study of CNN, ResNet, and Vision Transformers for   Multi-Classification of Chest Diseases","Ananya Jain, Aviral Bhardwaj, Kaushik Murali, Isha Surani","Large language models, notably utilizing Transformer architectures, have emerged as powerful tools due to their scalability and ability to process large amounts of data. Dosovitskiy et al. expanded this architecture to introduce Vision Transformers (ViT), extending its applicability to image processing tasks. Motivated by this advancement, we fine-tuned two variants of ViT models, one pre-trained on ImageNet and another trained from scratch, using the NIH Chest X-ray dataset containing over 100,000 frontal-view X-ray images. Our study evaluates the performance of these models in the multi-label classification of 14 distinct diseases, while using Convolutional Neural Networks (CNNs) and ResNet architectures as baseline models for comparison. Through rigorous assessment based on accuracy metrics, we identify that the pre-trained ViT model surpasses CNNs and ResNet in this multilabel classification task, highlighting its potential for accurate diagnosis of various lung conditions from chest X-ray images.",2024-05-31T23:56:42Z,2024-05-31T23:56:42Z,http://arxiv.org/abs/2406.00237v1,http://arxiv.org/pdf/2406.00237v1,"eess.IV, cs.CV, cs.LG"
An Enhanced Encoder-Decoder Network Architecture for Reducing   Information Loss in Image Semantic Segmentation,"Zijun Gao, Qi Wang, Taiyuan Mei, Xiaohan Cheng, Yun Zi, Haowei Yang","The traditional SegNet architecture commonly encounters significant information loss during the sampling process, which detrimentally affects its accuracy in image semantic segmentation tasks. To counter this challenge, we introduce an innovative encoder-decoder network structure enhanced with residual connections. Our approach employs a multi-residual connection strategy designed to preserve the intricate details across various image scales more effectively, thus minimizing the information loss inherent to down-sampling procedures. Additionally, to enhance the convergence rate of network training and mitigate sample imbalance issues, we have devised a modified cross-entropy loss function incorporating a balancing factor. This modification optimizes the distribution between positive and negative samples, thus improving the efficiency of model training. Experimental evaluations of our model demonstrate a substantial reduction in information loss and improved accuracy in semantic segmentation. Notably, our proposed network architecture demonstrates a substantial improvement in the finely annotated mean Intersection over Union (mIoU) on the dataset compared to the conventional SegNet. The proposed network structure not only reduces operational costs by decreasing manual inspection needs but also scales up the deployment of AI-driven image analysis across different sectors.",2024-05-26T05:15:53Z,2024-05-26T05:15:53Z,http://arxiv.org/abs/2406.01605v1,http://arxiv.org/pdf/2406.01605v1,"eess.IV, cs.CV"
A Pipelined Memristive Neural Network Analog-to-Digital Converter,"Loai Danial, Kanishka Sharma, Shahar Kvatinsky","With the advent of high-speed, high-precision, and low-power mixed-signal systems, there is an ever-growing demand for accurate, fast, and energy-efficient analog-to-digital (ADCs) and digital-to-analog converters (DACs). Unfortunately, with the downscaling of CMOS technology, modern ADCs trade off speed, power and accuracy. Recently, memristive neuromorphic architectures of four-bit ADC/DAC have been proposed. Such converters can be trained in real-time using machine learning algorithms, to break through the speedpower-accuracy trade-off while optimizing the conversion performance for different applications. However, scaling such architectures above four bits is challenging. This paper proposes a scalable and modular neural network ADC architecture based on a pipeline of four-bit converters, preserving their inherent advantages in application reconfiguration, mismatch selfcalibration, noise tolerance, and power optimization, while approaching higher resolution and throughput in penalty of latency. SPICE evaluation shows that an 8-bit pipelined ADC achieves 0.18 LSB INL, 0.20 LSB DNL, 7.6 ENOB, and 0.97 fJ/conv FOM. This work presents a significant step towards the realization of large-scale neuromorphic data converters.",2024-06-04T10:51:12Z,2024-06-04T10:51:12Z,http://arxiv.org/abs/2406.02197v1,http://arxiv.org/pdf/2406.02197v1,"eess.SY, cs.NE, cs.SY"
Learning Long Range Dependencies on Graphs via Random Walks,"Dexiong Chen, Till Hendrik Schulz, Karsten Borgwardt","Message-passing graph neural networks (GNNs) excel at capturing local relationships but struggle with long-range dependencies in graphs. In contrast, graph transformers (GTs) enable global information exchange but often oversimplify the graph structure by representing graphs as sets of fixed-length vectors. This work introduces a novel architecture that overcomes the shortcomings of both approaches by combining the long-range information of random walks with local message passing. By treating random walks as sequences, our architecture leverages recent advances in sequence models to effectively capture long-range dependencies within these walks. Based on this concept, we propose a framework that offers (1) more expressive graph representations through random walk sequences, (2) the ability to utilize any sequence model for capturing long-range dependencies, and (3) the flexibility by integrating various GNN and GT architectures. Our experimental evaluations demonstrate that our approach achieves significant performance improvements on 19 graph and node benchmark datasets, notably outperforming existing methods by up to 13\% on the PascalVoc-SP and COCO-SP datasets. The code is available at https://github.com/BorgwardtLab/NeuralWalker.",2024-06-05T15:36:57Z,2024-10-07T14:01:11Z,http://arxiv.org/abs/2406.03386v2,http://arxiv.org/pdf/2406.03386v2,"cs.LG, stat.ML"
Small-E: Small Language Model with Linear Attention for Efficient Speech   Synthesis,"Théodor Lemerle, Nicolas Obin, Axel Roebel","Recent advancements in text-to-speech (TTS) powered by language models have showcased remarkable capabilities in achieving naturalness and zero-shot voice cloning. Notably, the decoder-only transformer is the prominent architecture in this domain. However, transformers face challenges stemming from their quadratic complexity in sequence length, impeding training on lengthy sequences and resource-constrained hardware. Moreover they lack specific inductive bias with regards to the monotonic nature of TTS alignments. In response, we propose to replace transformers with emerging recurrent architectures and introduce specialized cross-attention mechanisms for reducing repeating and skipping issues. Consequently our architecture can be efficiently trained on long samples and achieve state-of-the-art zero-shot voice cloning against baselines of comparable size. Our implementation and demos are available at https://github.com/theodorblackbird/lina-speech.",2024-06-06T19:48:17Z,2024-06-11T12:36:36Z,http://arxiv.org/abs/2406.04467v2,http://arxiv.org/pdf/2406.04467v2,"eess.AS, cs.CL, cs.SD"
What is my quantum computer good for? Quantum capability learning with   physics-aware neural networks,"Daniel Hothem, Ashe Miller, Timothy Proctor","Quantum computers have the potential to revolutionize diverse fields, including quantum chemistry, materials science, and machine learning. However, contemporary quantum computers experience errors that often cause quantum programs run on them to fail. Until quantum computers can reliably execute large quantum programs, stakeholders will need fast and reliable methods for assessing a quantum computer's capability-i.e., the programs it can run and how well it can run them. Previously, off-the-shelf neural network architectures have been used to model quantum computers' capabilities, but with limited success, because these networks fail to learn the complex quantum physics that determines real quantum computers' errors. We address this shortcoming with a new quantum-physics-aware neural network architecture for learning capability models. Our architecture combines aspects of graph neural networks with efficient approximations to the physics of errors in quantum programs. This approach achieves up to $\sim50\%$ reductions in mean absolute error on both experimental and simulated data, over state-of-the-art models based on convolutional neural networks.",2024-06-09T04:11:41Z,2025-02-26T17:31:49Z,http://arxiv.org/abs/2406.05636v2,http://arxiv.org/pdf/2406.05636v2,"quant-ph, cs.LG"
Learning-based cognitive architecture for enhancing coordination in   human groups,"Antonio Grotta, Marco Coraggio, Antonio Spallone, Francesco De Lellis, Mario di Bernardo","As interactions with autonomous agents-ranging from robots in physical settings to avatars in virtual and augmented realities-become more prevalent, developing advanced cognitive architectures is critical for enhancing the dynamics of human-avatar groups. This paper presents a reinforcement-learning-based cognitive architecture, trained via a sim-to-real approach, designed to improve synchronization in periodic motor tasks, crucial for applications in group rehabilitation and sports training. Extensive numerical validation consistently demonstrates improvements in synchronization. Theoretical derivations and numerical investigations are complemented by preliminary experiments with real participants, showing that our avatars can integrate seamlessly into human groups, often being indistinguishable from humans.",2024-06-10T14:17:26Z,2024-06-10T14:17:26Z,http://arxiv.org/abs/2406.06297v1,http://arxiv.org/pdf/2406.06297v1,"eess.SY, cs.SY"
Hardware Implementation of Soft Mapper/Demappers in Iterative EP-based   Receivers,"Ian Fischer Schilling, Serdar Sahin, Camille Leroux, Antonio Maria Cipriano, Christophe Jego","This paper presents a comprehensive study and implementations onto FPGA device of an Expectation Propagation (EP)-based receiver for QPSK, 8-PSK, and 16-QAM. To the best of our knowledge, this is the first for this kind of receiver. The receiver implements a Frequency Domain (FD) Self-Iterated Linear Equalizer (SILE), where EP is used to approximate the true posterior distribution of the transmitted symbols with a simpler distribution. Analytical approximations for the EP feedback generation process and the three constellations are applied to lessen the complexity of the soft mapper/demapper architectures. The simulation results demonstrate that the fixed-point version performs comparably to the floating-point. Moreover, implementation results show the efficiency in terms of FPGA resource usage of the proposed architecture.",2024-06-12T06:58:00Z,2024-06-12T06:58:00Z,http://arxiv.org/abs/2406.07934v1,http://arxiv.org/pdf/2406.07934v1,"cs.AR, eess.SP"
Mycorrhizal Fungi and Plant Symbiosis for Energy Harvesting in the   Internet of Plants,"Fatih E. Bilgen, Ozgur B. Akan","Biological entities in nature have developed sophisticated communication methods over millennia to facilitate cooperation. Among these entities, plants are some of the most intricate communicators. They interact with each other through various communication modalities, creating networks that enable the exchange of information and nutrients. In this paper, we explore this collective behavior and its components. We then introduce the concept of agent plants, outlining their architecture and detailing the tasks of each unit. Additionally, we investigate the mycorrhizal fungi-plant symbiosis to extract glucose for energy harvesting. We propose an architecture that converts the chemical energy stored in these glucose molecules into electrical energy. We conduct comprehensive analyses of the proposed architecture to validate its effectiveness.",2024-06-17T03:26:34Z,2024-06-17T03:26:34Z,http://arxiv.org/abs/2406.11174v1,http://arxiv.org/pdf/2406.11174v1,"cs.ET, eess.SP"
Deep-Learning-Based Channel Estimation for Distributed MIMO with 1-bit   Radio-Over-Fiber Fronthaul,"Alireza Bordbar, Lise Aabel, Christian Häger, Christian Fager, Giuseppe Durisi","We consider the problem of pilot-aided, uplink channel estimation in a distributed massive multiple-input multiple-output (MIMO) architecture, in which the access points are connected to a central processing unit via fiber-optical fronthaul links, carrying a two-level-quantized version of the received analog radio-frequency signal. We adapt to this architecture the deep-learning-based channel-estimation algorithm recently proposed by Nguyen et al. (2023), and explore its robustness to the additional signal distortions (beyond 1-bit quantization) introduced in the considered architecture by the automatic gain controllers (AGCs) and by the comparators. These components are used at the access points to generate the two-level analog waveform from the received signal. Via simulation results, we illustrate that the proposed channel-estimation method outperforms significantly the Bussgang linear minimum mean-square error channel estimator, and it is robust against the additional impairments introduced by the AGCs and the comparators.",2024-06-17T08:38:29Z,2024-07-05T15:51:16Z,http://arxiv.org/abs/2406.11325v2,http://arxiv.org/pdf/2406.11325v2,"eess.SP, cs.LG"
Pivotal Auto-Encoder via Self-Normalizing ReLU,"Nelson Goldenstein, Jeremias Sulam, Yaniv Romano","Sparse auto-encoders are useful for extracting low-dimensional representations from high-dimensional data. However, their performance degrades sharply when the input noise at test time differs from the noise employed during training. This limitation hinders the applicability of auto-encoders in real-world scenarios where the level of noise in the input is unpredictable. In this paper, we formalize single hidden layer sparse auto-encoders as a transform learning problem. Leveraging the transform modeling interpretation, we propose an optimization problem that leads to a predictive model invariant to the noise level at test time. In other words, the same pre-trained model is able to generalize to different noise levels. The proposed optimization algorithm, derived from the square root lasso, is translated into a new, computationally efficient auto-encoding architecture. After proving that our new method is invariant to the noise level, we evaluate our approach by training networks using the proposed architecture for denoising tasks. Our experimental results demonstrate that the trained models yield a significant improvement in stability against varying types of noise compared to commonly used architectures.",2024-06-23T09:06:52Z,2024-06-23T09:06:52Z,http://arxiv.org/abs/2406.16052v1,http://arxiv.org/pdf/2406.16052v1,"cs.LG, eess.SP, stat.ML"
Optimal DC-link Voltage from Weight and Loss Perspective for eVTOLs,"Abhijit Kulkarni, Torbjörn Thiringer, Remus Teodorescu","Electric vertical takeoff and landing (eVTOL) aircraft are emerging as a modern transportation solution aimed at reducing urban traffic congestion and improving the carbon footprint. The power architecture in eVTOLs is defined by the dc bus formed by the battery packs and the power converter used to drive eVTOL motors. A high dc bus voltage is preferred for the power architecture since it can reduce the weight of power cables for a given power rating. However, the impact of high dc bus voltage on the efficiency of the drivetrain power converter must be considered, since reduced efficiency leads to poor battery pack utilization. In this paper, a systematic optimization study is performed considering SiC-based inverter for the drivetrain power converter. Optimal value of dc bus voltage is determined considering the flight profile of eVTOLs. A power converter topology is proposed that can provide optimal performance and enhance the lifetime of the batteries along with providing better monitoring, diagnostics and protection. The optimization strategy is validated experimentally, demonstrating the proposed power architecture's ability to maximize efficiency while enhancing the safety of the battery energy storage system in eVTOLs.",2024-06-25T12:54:11Z,2024-06-25T12:54:11Z,http://arxiv.org/abs/2406.17516v1,http://arxiv.org/pdf/2406.17516v1,"eess.SY, cs.SY"
Operator Learning of Lipschitz Operators: An Information-Theoretic   Perspective,Samuel Lanthaler,"Operator learning based on neural operators has emerged as a promising paradigm for the data-driven approximation of operators, mapping between infinite-dimensional Banach spaces. Despite significant empirical progress, our theoretical understanding regarding the efficiency of these approximations remains incomplete. This work addresses the parametric complexity of neural operator approximations for the general class of Lipschitz continuous operators. Motivated by recent findings on the limitations of specific architectures, termed curse of parametric complexity, we here adopt an information-theoretic perspective. Our main contribution establishes lower bounds on the metric entropy of Lipschitz operators in two approximation settings; uniform approximation over a compact set of input functions, and approximation in expectation, with input functions drawn from a probability measure. It is shown that these entropy bounds imply that, regardless of the activation function used, neural operator architectures attaining an approximation accuracy $\epsilon$ must have a size that is exponentially large in $\epsilon^{-1}$. The size of architectures is here measured by counting the number of encoded bits necessary to store the given model in computational memory. The results of this work elucidate fundamental trade-offs and limitations in operator learning.",2024-06-26T23:36:46Z,2024-07-02T18:13:03Z,http://arxiv.org/abs/2406.18794v2,http://arxiv.org/pdf/2406.18794v2,"cs.LG, cs.NA, math.NA"
Autoencoded Image Compression for Secure and Fast Transmission,"Aryan Kashyap Naveen, Sunil Thunga, Anuhya Murki, Mahati A Kalale, Shriya Anil","With exponential growth in the use of digital image data, the need for efficient transmission methods has become imperative. Traditional image compression techniques often sacrifice image fidelity for reduced file sizes, challenging maintaining quality and efficiency. They also compromise security, leaving images vulnerable to threats such as man-in-the-middle attacks. This paper proposes an autoencoder architecture for image compression to not only help in dimensionality reduction but also inherently encrypt the images. The paper also introduces a composite loss function that combines reconstruction loss and residual loss for improved performance. The autoencoder architecture is designed to achieve optimal dimensionality reduction and regeneration accuracy while safeguarding the compressed data during transmission or storage. Images regenerated by the autoencoder are evaluated against three key metrics: reconstruction quality, compression ratio, and one-way delay during image transfer. The experiments reveal that the proposed architecture achieves an SSIM of 97.5% over the regenerated images and an average latency reduction of 87.5%, indicating its effectiveness as a secure and efficient solution for compressed image transfer.",2024-07-04T15:07:39Z,2024-10-14T12:12:06Z,http://arxiv.org/abs/2407.03990v2,http://arxiv.org/pdf/2407.03990v2,"eess.IV, cs.CV, I.4.2"
Invited: Neuromorphic architectures based on augmented silicon photonics   platforms,"Matěj Hejda, Federico Marchesin, George Papadimitriou, Dimitris Gizopoulos, Benoit Charbonnier, Régis Orobtchouk, Peter Bienstman, Thomas Van Vaerenbergh, Fabio Pavanello","In this work, we discuss our vision for neuromorphic accelerators based on integrated photonics within the framework of the Horizon Europe NEUROPULS project. Augmented integrated photonic architectures that leverage phase-change and III-V materials for optical computing will be presented. A CMOS-compatible platform will be discussed that integrates these materials to fabricate photonic neuromorphic architectures, along with a gem5-based simulation platform to model accelerator operation once it is interfaced with a RISC-V processor. This simulation platform enables accurate system-level accelerator modeling and benchmarking in terms of key metrics such as speed, energy consumption, and footprint.",2024-07-07T22:28:38Z,2024-07-07T22:28:38Z,http://arxiv.org/abs/2407.06240v1,http://arxiv.org/pdf/2407.06240v1,"cs.ET, eess.SP, physics.optics"
Hybrid Classical-Quantum architecture for vectorised image   classification of hand-written sketches,"Y. Cordero, S. Biswas, F. Vilariño, M. Bilkis","Quantum machine learning (QML) investigates how quantum phenomena can be exploited in order to learn data in an alternative way, \textit{e.g.} by means of a quantum computer. While recent results evidence that QML models can potentially surpass their classical counterparts' performance in specific tasks, quantum technology hardware is still unready to reach quantum advantage in tasks of significant relevance to the broad scope of the computer science community. Recent advances indicate that hybrid classical-quantum models can readily attain competitive performances at low architecture complexities. Such investigations are often carried out for image-processing tasks, and are notably constrained to modelling \textit{raster images}, represented as a grid of two-dimensional pixels. Here, we introduce vector-based representation of sketch drawings as a test-bed for QML models. Such a lower-dimensional data structure results handful to benchmark model's performance, particularly in current transition times, where classical simulations of quantum circuits are naturally limited in the number of qubits, and quantum hardware is not readily available to perform large-scale experiments. We report some encouraging results for primitive hybrid classical-quantum architectures, in a canonical sketch recognition problem.",2024-07-08T21:51:20Z,2024-07-08T21:51:20Z,http://arxiv.org/abs/2407.06416v1,http://arxiv.org/pdf/2407.06416v1,"quant-ph, cs.AI, cs.CV"
Enhancing Facial Expression Recognition through Dual-Direction Attention   Mixed Feature Networks: Application to 7th ABAW Challenge,"Josep Cabacas-Maso, Elena Ortega-Beltrán, Ismael Benito-Altamirano, Carles Ventura","We present our contribution to the 7th ABAW challenge at ECCV 2024, by utilizing a Dual-Direction Attention Mixed Feature Network (DDAMFN) for multitask facial expression recognition, we achieve results far beyond the proposed baseline for the Multi-Task ABAW challenge. Our proposal uses the well-known DDAMFN architecture as base to effectively predict valence-arousal, emotion recognition, and facial action units. We demonstrate the architecture ability to handle these tasks simultaneously, providing insights into its architecture and the rationale behind its design. Additionally, we compare our results for a multitask solution with independent single-task performance.",2024-07-17T08:11:37Z,2024-09-05T11:35:21Z,http://arxiv.org/abs/2407.12390v3,http://arxiv.org/pdf/2407.12390v3,"cs.CV, I.4"
On the use of Probabilistic Forecasting for Network Analysis in Open RAN,"Vaishnavi Kasuluru, Luis Blanco, Engin Zeydan","Unlike other single-point Artificial Intelligence (AI)-based prediction techniques, such as Long-Short Term Memory (LSTM), probabilistic forecasting techniques (e.g., DeepAR and Transformer) provide a range of possible outcomes and associated probabilities that enable decision makers to make more informed and robust decisions. At the same time, the architecture of Open RAN has emerged as a revolutionary approach for mobile networks, aiming at openness, interoperability and innovation in the ecosystem of RAN. In this paper, we propose the use of probabilistic forecasting techniques as a radio App (rApp) within the Open RAN architecture. We investigate and compare different probabilistic and single-point forecasting methods and algorithms to estimate the utilization and resource demands of Physical Resource Blocks (PRBs) of cellular base stations. Through our evaluations, we demonstrate the numerical advantages of probabilistic forecasting techniques over traditional single-point forecasting methods and show that they are capable of providing more accurate and reliable estimates. In particular, DeepAR clearly outperforms single-point forecasting techniques such as LSTM and Seasonal-Naive (SN) baselines and other probabilistic forecasting techniques such as Simple-Feed-Forward (SFF) and Transformer neural networks.",2024-07-19T15:03:38Z,2024-07-19T15:03:38Z,http://arxiv.org/abs/2407.14375v1,http://arxiv.org/pdf/2407.14375v1,"cs.NI, cs.AI, cs.DC, cs.IT, cs.LG, math.IT"
A Novel Skiagraphic Method of Casting Shade of a Torus,Tanvir Morshed,"This paper introduces a novel skiagraphic method for shading toroidal forms in architectural illustrations, addressing the challenges of traditional techniques. Skiagraphy projects 3D objects onto 2D surfaces to display geometric properties. Traditional shading of tori involves extensive manual calculations and multiple projections, leading to high complexity and inaccuracies. The proposed method simplifies this by focusing on the elevation view, eliminating the need for multiple projections and complex math. Utilizing descriptive geometry, it reduces labor and complexity. Accuracy was validated through comparisons with SketchUp-generated shading and various torus configurations. This technique streamlines shading toroidal shapes while maintaining the artistic value of traditional illustration. Additionally, it has potential applications in 3D model generation from architectural shade casts, contributing to the evolving field of architectural visualization and representation.",2024-07-18T10:03:31Z,2024-07-18T10:03:31Z,http://arxiv.org/abs/2407.14557v1,http://arxiv.org/pdf/2407.14557v1,"cs.GR, 97G80"
Movable Antenna-Enhanced Wireless Communications: General Architectures   and Implementation Methods,"Boyu Ning, Songjie Yang, Yafei Wu, Peilan Wang, Weidong Mei, Chau Yuen, Emil Björnson","Movable antennas (MAs), traditionally explored in antenna design, have recently garnered significant attention in wireless communications due to their ability to dynamically adjust the antenna positions to changes in the propagation environment. However, previous research has primarily focused on characterizing the performance limits of various MA-assisted wireless communication systems, with less emphasis on their practical implementation. To address this gap, in this article, we propose several general MA architectures that extend existing designs by varying several key aspects to cater to different application scenarios and tradeoffs between cost and performance. Additionally, we draw from fields such as antenna design and mechanical control to provide an overview of candidate implementation methods for the proposed MA architectures, utilizing either direct mechanical or equivalent electronic control. Simulation results are finally presented to support our discussion.",2024-07-22T08:04:46Z,2024-08-08T13:48:04Z,http://arxiv.org/abs/2407.15448v2,http://arxiv.org/pdf/2407.15448v2,"eess.SP, cs.IT, math.IT"
Efficient Replay Memory Architectures in Multi-Agent Reinforcement   Learning for Traffic Congestion Control,"Mukul Chodhary, Kevin Octavian, SooJean Han","Episodic control, inspired by the role of episodic memory in the human brain, has been shown to improve the sample inefficiency of model-free reinforcement learning by reusing high-return past experiences. However, the memory growth of episodic control is undesirable in large-scale multi-agent problems such as vehicle traffic management. This paper proposes a novel replay memory architecture called Dual-Memory Integrated Learning, to augment to multi-agent reinforcement learning methods for congestion control via adaptive light signal scheduling. Our dual-memory architecture mimics two core capabilities of human decision-making. First, it relies on diverse types of memory--semantic and episodic, short-term and long-term--in order to remember high-return states that occur often in the network and filter out states that don't. Second, it employs equivalence classes to group together similar state-action pairs and that can be controlled using the same action (i.e., light signal sequence). Theoretical analyses establish memory growth bounds, and simulation experiments on several intersection networks showcase improved congestion performance (e.g., vehicle throughput) from our method.",2024-07-22T20:20:04Z,2024-07-22T20:20:04Z,http://arxiv.org/abs/2407.16034v1,http://arxiv.org/pdf/2407.16034v1,"eess.SY, cs.SY"
"Deterministic and Reliable Software-Defined Vehicles: key building   blocks, challenges, and vision","Pedro Veloso Teixeira, Duarte Raposo, Rui Lopes, Susana Sargento","As vehicle systems become increasingly complex, with more features, services, sensors, actuators, and processing units, it is important to view vehicles not just as modes of transportation moving toward full autonomy, but also as adaptive systems that respond to the needs of their occupants. Vehicular services can be developed to support these adaptations. However, the increasing complexity of vehicular service development, even with current standardizations, best practices and guidelines, are insufficient to tackle the high complexity of development, with expectations of up to 1 (U.S.) billion lines of code for a fully (level 5) autonomous vehicle. Within this survey, the paradigm of Deterministic Software Defined Vehicles is explored, aiming to enhance the quality and ease of developing automotive services by focusing on service-oriented architectures, virtualization techniques, and the necessary deterministic intra- and inter-vehicular communications. Considering the main open challenges for such verticals, a vision architecture towards improved services development and orchestration is presented, focusing on: a) a deterministic network configurator; b) a data layer configurator; c) a hypervisor configurator; d) the vehicle abstraction layer; and e) a software orchestrator.",2024-07-24T13:56:56Z,2025-01-09T10:13:51Z,http://arxiv.org/abs/2407.17287v2,http://arxiv.org/pdf/2407.17287v2,"cs.DC, cs.NI, C.2.1; C.2.4; C.0; D.2.1; D.2.11; J.7; K.6.4"
NVC-1B: A Large Neural Video Coding Model,"Xihua Sheng, Chuanbo Tang, Li Li, Dong Liu, Feng Wu","The emerging large models have achieved notable progress in the fields of natural language processing and computer vision. However, large models for neural video coding are still unexplored. In this paper, we try to explore how to build a large neural video coding model. Based on a small baseline model, we gradually scale up the model sizes of its different coding parts, including the motion encoder-decoder, motion entropy model, contextual encoder-decoder, contextual entropy model, and temporal context mining module, and analyze the influence of model sizes on video compression performance. Then, we explore to use different architectures, including CNN, mixed CNN-Transformer, and Transformer architectures, to implement the neural video coding model and analyze the influence of model architectures on video compression performance. Based on our exploration results, we design the first neural video coding model with more than 1 billion parameters -- NVC-1B. Experimental results show that our proposed large model achieves a significant video compression performance improvement over the small baseline model, and represents the state-of-the-art compression efficiency. We anticipate large models may bring up the video coding technologies to the next level.",2024-07-28T05:12:22Z,2024-07-28T05:12:22Z,http://arxiv.org/abs/2407.19402v1,http://arxiv.org/pdf/2407.19402v1,"cs.CV, eess.IV"
Anti-Concentration for the Unitary Haar Measure and Applications to   Random Quantum Circuits,"Bill Fefferman, Soumik Ghosh, Wei Zhan","We prove a Carbery-Wright style anti-concentration inequality for the unitary Haar measure, by showing that the probability of a polynomial in the entries of a random unitary falling into an $\varepsilon$ range is at most a polynomial in $\varepsilon$. Using it, we show that the scrambling speed of a random quantum circuit is lower bounded: Namely, every input qubit has an influence that is at least exponentially small in depth, on any output qubit touched by its lightcone.   We give three applications of this new scrambling speed lower bound that apply to random quantum circuits with Haar random gates:   $\bullet$ An optimal $\Omega(\log \varepsilon^{-1})$ depth lower bound for $\varepsilon$-approximate unitary designs;   $\bullet$ A polynomial-time quantum algorithm that computes the depth of a bounded-depth circuit, given oracle access to the circuit;   $\bullet$ A polynomial-time algorithm that learns log-depth circuits up to polynomially small diamond distance, given oracle access to the circuit.   The first depth lower bound works against any architecture. The latter two algorithms apply to architectures defined over any geometric dimension, and can be generalized to a wide class of architectures with good lightcone properties.",2024-07-28T19:10:46Z,2024-07-28T19:10:46Z,http://arxiv.org/abs/2407.19561v1,http://arxiv.org/pdf/2407.19561v1,"quant-ph, cs.CC"
Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool   Libraries,"Felix Ocker, Daniel Tanneberg, Julian Eggert, Michael Gienger","We introduce tulip agent, an architecture for autonomous LLM-based agents with Create, Read, Update, and Delete access to a tool library containing a potentially large number of tools. In contrast to state-of-the-art implementations, tulip agent does not encode the descriptions of all available tools in the system prompt, which counts against the model's context window, or embed the entire prompt for retrieving suitable tools. Instead, the tulip agent can recursively search for suitable tools in its extensible tool library, implemented exemplarily as a vector store. The tulip agent architecture significantly reduces inference costs, allows using even large tool libraries, and enables the agent to adapt and extend its set of tools. We evaluate the architecture with several ablation studies in a mathematics context and demonstrate its generalizability with an application to robotics. A reference implementation and the benchmark are available at github.com/HRI-EU/tulip_agent.",2024-07-31T17:50:54Z,2024-07-31T17:50:54Z,http://arxiv.org/abs/2407.21778v1,http://arxiv.org/pdf/2407.21778v1,"cs.AI, cs.RO, H.3.3; I.2.6; I.2.8; I.2.9"
Scaling and assigning resources on ion trap QCCD architectures,"Anabel Ovide, Daniele Cuomo, Carmen G. Almudever","Ion trap technologies have earned significant attention as potential candidates for quantum information processing due to their long decoherence times and precise manipulation of individual qubits, distinguishing them from other candidates in the field of quantum technologies. However, scalability remains a challenge, as introducing additional qubits into a trap increases noise and heating effects, consequently decreasing operational fidelity. Trapped-ion Quantum Charge-Coupled Device (QCCD) architectures have addressed this limitation by interconnecting multiple traps and employing ion shuttling mechanisms to transfer ions among traps. This new architectural design requires the development of novel compilation techniques for quantum algorithms, which efficiently allocate and route qubits, and schedule operations. The aim of a compiler is to minimize ion movements and, therefore, reduce the execution time of the circuit to achieve a higher fidelity.   In this paper, we propose a novel approach for initial qubit placement, demonstrating enhancements of up to 50\% compared to prior methods. Furthermore, we conduct a scalability analysis on two distinct QCCD topologies: a 1D-linear array and a ring structure. Additionally, we evaluate the impact of the excess capacity -- i.e. the number of free spaces within a trap -- on the algorithm performance.",2024-08-01T01:35:55Z,2024-08-01T01:35:55Z,http://arxiv.org/abs/2408.00225v1,http://arxiv.org/pdf/2408.00225v1,"quant-ph, cs.ET"
"Low-depth, compact and error-tolerant photonic matrix-vector   multiplication beyond the unitary group","S. A. Fldzhyan, M. Yu. Saygin, S. S. Straupe","Large-scale programmable photonic circuits are opening up new possibilities for information processing providing fast and energy-efficient means for matrix-vector multiplication. Here, we introduce a novel architecture of photonic circuits capable of implementing non-unitary transfer matrices, usually required by photonic neural networks, iterative equation solvers or quantum samplers. Our architecture exploits compact low-depth beam-splitter meshes rather than bulky fully connected mixing blocks used in previous designs, making it more compatible with planar integrated photonics technology. We have shown that photonic circuits designed with our architecture have lower depth than their standard counterparts and are extremely tolerant to hardware errors.",2024-08-01T16:06:51Z,2024-08-31T14:25:47Z,http://arxiv.org/abs/2408.00669v2,http://arxiv.org/pdf/2408.00669v2,"physics.optics, quant-ph"
WaveMamba: Spatial-Spectral Wavelet Mamba for Hyperspectral Image   Classification,"Muhammad Ahmad, Muhammad Usama, Manuel Mazzara, Salvatore Distefano","Hyperspectral Imaging (HSI) has proven to be a powerful tool for capturing detailed spectral and spatial information across diverse applications. Despite the advancements in Deep Learning (DL) and Transformer architectures for HSI classification, challenges such as computational efficiency and the need for extensive labeled data persist. This paper introduces WaveMamba, a novel approach that integrates wavelet transformation with the spatial-spectral Mamba architecture to enhance HSI classification. WaveMamba captures both local texture patterns and global contextual relationships in an end-to-end trainable model. The Wavelet-based enhanced features are then processed through the state-space architecture to model spatial-spectral relationships and temporal dependencies. The experimental results indicate that WaveMamba surpasses existing models, achieving an accuracy improvement of 4.5\% on the University of Houston dataset and a 2.0\% increase on the Pavia University dataset.",2024-08-02T12:44:07Z,2024-11-22T12:04:05Z,http://arxiv.org/abs/2408.01231v2,http://arxiv.org/pdf/2408.01231v2,"cs.CV, eess.IV"
Activation degree thresholds and expressiveness of polynomial neural   networks,"Bella Finkel, Jose Israel Rodriguez, Chenxi Wu, Thomas Yahl","We study the expressive power of deep polynomial neural networks through the geometry of their neurovariety. We introduce the notion of the activation degree threshold of a network architecture to express when the dimension of the neurovariety achieves its theoretical maximum. We prove the existence of the activation degree threshold for all polynomial neural networks without width-one bottlenecks and demonstrate a universal upper bound that is quadratic in the width of largest size. In doing so, we prove the high activation degree conjecture of Kileel, Trager, and Bruna. Certain structured architectures have exceptional activation degree thresholds, making them especially expressive in the sense of their neurovariety dimension. In this direction, we prove that polynomial neural networks with equi-width architectures are maximally expressive by showing their activation degree threshold is one.",2024-08-08T16:28:56Z,2025-02-26T03:37:41Z,http://arxiv.org/abs/2408.04569v2,http://arxiv.org/pdf/2408.04569v2,"cs.LG, cs.NE, math.AG, stat.ML"
Enhancing Diabetic Retinopathy Diagnosis: A Lightweight CNN Architecture   for Efficient Exudate Detection in Retinal Fundus Images,Mujadded Al Rabbani Alif,"Retinal fundus imaging plays an essential role in diagnosing various stages of diabetic retinopathy, where exudates are critical markers of early disease onset. Prompt detection of these exudates is pivotal for enabling optometrists to arrest or significantly decelerate the disease progression. This paper introduces a novel, lightweight convolutional neural network architecture tailored for automated exudate detection, designed to identify these markers efficiently and accurately. To address the challenge of limited training data, we have incorporated domain-specific data augmentations to enhance the model's generalizability. Furthermore, we applied a suite of regularization techniques within our custom architecture to boost diagnostic accuracy while optimizing computational efficiency. Remarkably, this streamlined model contains only 4.73 million parameters a reduction of nearly 60% compared to the standard ResNet-18 model, which has 11.69 million parameters. Despite its reduced complexity, our model achieves an impressive F1 score of 90%, demonstrating its efficacy in the early detection of diabetic retinopathy through fundus imaging.",2024-08-13T10:13:33Z,2024-08-13T10:13:33Z,http://arxiv.org/abs/2408.06784v1,http://arxiv.org/pdf/2408.06784v1,"eess.IV, cs.CV, cs.LG"
Attention Please: What Transformer Models Really Learn for Process   Prediction,"Martin Käppel, Lars Ackermann, Stefan Jablonski, Simon Härtl","Predictive process monitoring aims to support the execution of a process during runtime with various predictions about the further evolution of a process instance. In the last years a plethora of deep learning architectures have been established as state-of-the-art for different prediction targets, among others the transformer architecture. The transformer architecture is equipped with a powerful attention mechanism, assigning attention scores to each input part that allows to prioritize most relevant information leading to more accurate and contextual output. However, deep learning models largely represent a black box, i.e., their reasoning or decision-making process cannot be understood in detail. This paper examines whether the attention scores of a transformer based next-activity prediction model can serve as an explanation for its decision-making. We find that attention scores in next-activity prediction models can serve as explainers and exploit this fact in two proposed graph-based explanation approaches. The gained insights could inspire future work on the improvement of predictive business process models as well as enabling a neural network based mining of process models from event logs.",2024-08-12T08:20:38Z,2024-08-12T08:20:38Z,http://arxiv.org/abs/2408.07097v1,http://arxiv.org/pdf/2408.07097v1,"cs.LG, cs.AI, 68T07, 68T01, 68U35, H.4.2; I.2.1; I.2.6"
Perspectives: Comparison of Deep Learning Segmentation Models on   Biophysical and Biomedical Data,"J Shepard Bryan IV, Pedro Pessoa, Meyam Tavakoli, Steve Presse","Deep learning based approaches are now widely used across biophysics to help automate a variety of tasks including image segmentation, feature selection, and deconvolution. However, the presence of multiple competing deep learning architectures, each with its own unique advantages and disadvantages, makes it challenging to select an architecture best suited for a specific application. As such, we present a comprehensive comparison of common models. Here, we focus on the task of segmentation assuming the typically small training dataset sizes available from biophysics experiments and compare the following four commonly used architectures: convolutional neural networks, U-Nets, vision transformers, and vision state space models. In doing so, we establish criteria for determining optimal conditions under which each model excels, thereby offering practical guidelines for researchers and practitioners in the field.",2024-08-14T19:49:19Z,2025-01-30T18:18:26Z,http://arxiv.org/abs/2408.07786v2,http://arxiv.org/pdf/2408.07786v2,"eess.IV, cs.CV, physics.bio-ph"
Towards the Information-Theoretic Limit of Programmable Photonics,"Ryan Hamerly, Jasvith Raj Basani, Alexander Sludds, Sri Krishna Vadlamani, Dirk Englund","The scalability of many programmable photonic circuits is limited by the $2\pi$ tuning range needed for the constituent phase shifters. To address this problem, we introduce the concept of a phase-efficient circuit architecture, where the average phase shift is $\ll 2\pi$. We derive a universal information-theoretic limit to the phase-shift efficiency of universal multiport interferometers, and propose a ""3-MZI"" architecture that approaches this limit to within a factor of $2\times$, approximately a $10\times$ reduction in average phase shift over the prior art, where the average phase shift scales inversely with system size as $O(1/\sqrt{N})$. For non-unitary circuits, we show that the 3-MZI saturates the theoretical bound for Gaussian-distributed target matrices. Using this architecture, we show optical neural network training with all phase shifters constrained to $\lesssim 0.2$ radians without loss of accuracy.",2024-08-19T03:21:24Z,2024-08-19T03:21:24Z,http://arxiv.org/abs/2408.09673v1,http://arxiv.org/pdf/2408.09673v1,"physics.optics, cs.ET"
FPCA: Field-Programmable Pixel Convolutional Array for Extreme-Edge   Intelligence,"Zihan Yin, Akhilesh Jaiswal","The rapid advancement of neural network applications necessitates hardware that not only accelerates computation but also adapts efficiently to dynamic processing requirements. While processing-in-pixel has emerged as a promising solution to overcome the bottlenecks of traditional architectures at the extreme-edge, existing implementations face limitations in reconfigurability and scalability due to their static nature and inefficient area usage. Addressing these challenges, we present a novel architecture that significantly enhances the capabilities of processing-in-pixel for convolutional neural networks. Our design innovatively integrates non-volatile memory (NVM) with novel unit pixel circuit design, enabling dynamic reconfiguration of synaptic weights, kernel size, channel size and stride size. Thus offering unprecedented flexibility and adaptability. With using a separate die for pixel circuit and storing synaptic weights, our circuit achieves a substantial reduction in the required area per pixel thereby increasing the density and scalability of the pixel array. Simulation results demonstrate dot product operations of the circuit, the non-linearity of its analog output and a novel bucket-select curvefit model is proposed to capture it. This work not only addresses the limitations of current in-pixel computing approaches but also opens new avenues for developing more efficient, flexible, and scalable neural network hardware, paving the way for advanced AI applications.",2024-08-03T21:12:41Z,2024-08-03T21:12:41Z,http://arxiv.org/abs/2408.10233v1,http://arxiv.org/pdf/2408.10233v1,"cs.AR, eess.IV"
Multilevel CNNs for Parametric PDEs based on Adaptive Finite Elements,"Janina Enrica Schütte, Martin Eigel","A neural network architecture is presented that exploits the multilevel properties of high-dimensional parameter-dependent partial differential equations, enabling an efficient approximation of parameter-to-solution maps, rivaling best-in-class methods such as low-rank tensor regression in terms of accuracy and complexity. The neural network is trained with data on adaptively refined finite element meshes, thus reducing data complexity significantly. Error control is achieved by using a reliable finite element a posteriori error estimator, which is also provided as input to the neural network.   The proposed U-Net architecture with CNN layers mimics a classical finite element multigrid algorithm. It can be shown that the CNN efficiently approximates all operations required by the solver, including the evaluation of the residual-based error estimator. In the CNN, a culling mask set-up according to the local corrections due to refinement on each mesh level reduces the overall complexity, allowing the network optimization with localized fine-scale finite element data.   A complete convergence and complexity analysis is carried out for the adaptive multilevel scheme, which differs in several aspects from previous non-adaptive multilevel CNN. Moreover, numerical experiments with common benchmark problems from Uncertainty Quantification illustrate the practical performance of the architecture.",2024-08-20T13:32:11Z,2024-08-20T13:32:11Z,http://arxiv.org/abs/2408.10838v1,http://arxiv.org/pdf/2408.10838v1,"cs.LG, cs.NA, math.NA"
Accounts of using the Tustin-Net architecture on a rotary inverted   pendulum,"Stijn van Esch, Fabio Bonassi, Thomas B. Schön","In this report we investigate the use of the Tustin neural network architecture (Tustin-Net) for the identification of a physical rotary inverse pendulum. This physics-based architecture is of particular interest as it builds on the known relationship between velocities and positions. We here aim at discussing the advantages, limitations and performance of Tustin-Nets compared to first-principles grey-box models on a real physical apparatus, showing how, with a standard training procedure, the former can hardly achieve the same accuracy as the latter. To address this limitation, we present a training strategy based on transfer learning that yields Tustin-Nets that are competitive with the first-principles model, without requiring extensive knowledge of the setup as the latter.",2024-08-22T10:04:00Z,2024-08-22T10:04:00Z,http://arxiv.org/abs/2408.12266v1,http://arxiv.org/pdf/2408.12266v1,"eess.SY, cs.LG, cs.SY"
Audio xLSTMs: Learning Self-Supervised Audio Representations with xLSTMs,"Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan","While the transformer has emerged as the eminent neural architecture, several independent lines of research have emerged to address its limitations. Recurrent neural approaches have also observed a lot of renewed interest, including the extended long short-term memory (xLSTM) architecture, which reinvigorates the original LSTM architecture. However, while xLSTMs have shown competitive performance compared to the transformer, their viability for learning self-supervised general-purpose audio representations has not yet been evaluated. This work proposes Audio xLSTM (AxLSTM), an approach to learn audio representations from masked spectrogram patches in a self-supervised setting. Pretrained on the AudioSet dataset, the proposed AxLSTM models outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by up to 20% in relative performance across a set of ten diverse downstream tasks while having up to 45% fewer parameters.",2024-08-29T14:35:56Z,2024-09-02T07:00:41Z,http://arxiv.org/abs/2408.16568v2,http://arxiv.org/pdf/2408.16568v2,"cs.SD, eess.AS"
Classically estimating observables of noiseless quantum circuits,"Armando Angrisani, Alexander Schmidhuber, Manuel S. Rudolph, M. Cerezo, Zoë Holmes, Hsin-Yuan Huang","We present a classical algorithm for estimating expectation values of arbitrary observables on most quantum circuits across all circuit architectures and depths, including those with all-to-all connectivity. We prove that for any architecture where each circuit layer is equipped with a measure invariant under single-qubit rotations, our algorithm achieves a small error $\varepsilon$ on all circuits except for a small fraction $\delta$. The computational time is polynomial in qubit count and circuit depth for any small constant $\varepsilon, \delta$, and quasi-polynomial for inverse-polynomially small $\varepsilon, \delta$. For non-classically-simulable input states or observables, the expectation values can be estimated by augmenting our algorithm with classical shadows of the relevant state or observable. Our approach leverages a Pauli-path method under Heisenberg evolution. While prior works are limited to noisy quantum circuits, we establish classical simulability in noiseless regimes. Given that most quantum circuits in an architecture exhibit chaotic and locally scrambling behavior, our work demonstrates that estimating observables of such quantum dynamics is classically tractable across all geometries.",2024-09-03T08:44:33Z,2024-09-03T08:44:33Z,http://arxiv.org/abs/2409.01706v1,http://arxiv.org/pdf/2409.01706v1,"quant-ph, cs.CC, math-ph, math.MP"
MusicMamba: A Dual-Feature Modeling Approach for Generating Chinese   Traditional Music with Modal Precision,"Jiatao Chen, Tianming Xie, Xing Tang, Jing Wang, Wenjing Dong, Bing Shi","In recent years, deep learning has significantly advanced the MIDI domain, solidifying music generation as a key application of artificial intelligence. However, existing research primarily focuses on Western music and encounters challenges in generating melodies for Chinese traditional music, especially in capturing modal characteristics and emotional expression. To address these issues, we propose a new architecture, the Dual-Feature Modeling Module, which integrates the long-range dependency modeling of the Mamba Block with the global structure capturing capabilities of the Transformer Block. Additionally, we introduce the Bidirectional Mamba Fusion Layer, which integrates local details and global structures through bidirectional scanning, enhancing the modeling of complex sequences. Building on this architecture, we propose the REMI-M representation, which more accurately captures and generates modal information in melodies. To support this research, we developed FolkDB, a high-quality Chinese traditional music dataset encompassing various styles and totaling over 11 hours of music. Experimental results demonstrate that the proposed architecture excels in generating melodies with Chinese traditional music characteristics, offering a new and effective solution for music generation.",2024-09-04T04:00:22Z,2024-09-04T04:00:22Z,http://arxiv.org/abs/2409.02421v1,http://arxiv.org/pdf/2409.02421v1,"cs.SD, eess.AS"
CortexCompile: Harnessing Cortical-Inspired Architectures for Enhanced   Multi-Agent NLP Code Synthesis,"Gautham Ramachandran, Rick Yang","Current approaches to automated code generation often rely on monolithic models that lack real-time adaptability and scalability. This limitation is particularly evident in complex programming tasks that require dynamic adjustment and efficiency. The integration of neuroscience principles into Natural Language Processing (NLP) has the potential to revolutionize automated code generation. This paper presents CortexCompile, a novel modular system inspired by the specialized functions of the human brain's cortical regions. By emulating the distinct roles of the Prefrontal Cortex, Parietal Cortex, Temporal Lobe, and Motor Cortex, CortexCompile achieves significant advancements in scalability, efficiency, and adaptability compared to traditional monolithic models like GPT-4o. The system's architecture features a Task Orchestration Agent that manages dynamic task delegation and parallel processing, facilitating the generation of highly accurate and optimized code across increasingly complex programming tasks. Experimental evaluations demonstrate that CortexCompile consistently outperforms GPT-4o in development time, accuracy, and user satisfaction, particularly in tasks involving real-time strategy games and first-person shooters. These findings underscore the viability of neuroscience-inspired architectures in addressing the limitations of current NLP models, paving the way for more efficient and human-like AI systems.",2024-08-23T18:36:20Z,2024-08-23T18:36:20Z,http://arxiv.org/abs/2409.02938v1,http://arxiv.org/pdf/2409.02938v1,"cs.LG, cs.AI, I.2.2; I.2.7"
On the Optimal Performance of Distributed Cell-Free Massive MIMO with   LoS Propagation,"Noor Ul Ain, Lorenzo Miretti, Sławomir Stańczak","In this study, we revisit the performance analysis of distributed beamforming architectures in dense user-centric cell-free massive multiple-input multiple-output (mMIMO) systems in line-of-sight (LoS) scenarios. By incorporating a recently developed optimal distributed beamforming technique, called the team minimum mean square error (TMMSE) technique, we depart from previous studies that rely on suboptimal distributed beamforming approaches for LoS scenarios. Supported by extensive numerical simulations that follow 3GPP guidelines, we show that such suboptimal approaches may often lead to significant underestimation of the capabilities of distributed architectures, particularly in the presence of strong LoS paths. Considering the anticipated ultra-dense nature of cell-free mMIMO networks and the consequential high likelihood of strong LoS paths, our findings reveal that the team MMSE technique may significantly contribute in narrowing the performance gap between centralized and distributed architectures.",2024-09-05T14:12:51Z,2024-09-05T14:12:51Z,http://arxiv.org/abs/2409.03551v1,http://arxiv.org/pdf/2409.03551v1,"cs.IT, math.IT"
Bottleneck-based Encoder-decoder ARchitecture (BEAR) for Learning   Unbiased Consumer-to-Consumer Image Representations,"Pablo Rivas, Gisela Bichler, Tomas Cerny, Laurie Giddens, Stacie Petter",Unbiased representation learning is still an object of study under specific applications and contexts. Novel architectures are usually crafted to resolve particular problems using mixtures of fundamental pieces. This paper presents different image feature extraction mechanisms that work together with residual connections to encode perceptual image information in an autoencoder configuration. We use image data that aims to support a larger research agenda dealing with issues regarding criminal activity in consumer-to-consumer online platforms. Preliminary results suggest that the proposed architecture can learn rich spaces using ours and other image datasets resolving important challenges that are identified.,2024-09-10T03:31:18Z,2024-09-10T03:31:18Z,http://arxiv.org/abs/2409.06187v1,http://arxiv.org/pdf/2409.06187v1,"cs.CV, cs.LG, I.2.10; I.5.1; K.4.1; H.3.3; I.2.6"
Can Agents Spontaneously Form a Society? Introducing a Novel   Architecture for Generative Multi-Agents to Elicit Social Emergence,"H. Zhang, J. Yin, M. Jiang, C. Su","Generative agents have demonstrated impressive capabilities in specific tasks, but most of these frameworks focus on independent tasks and lack attention to social interactions. We introduce a generative agent architecture called ITCMA-S, which includes a basic framework for individual agents and a framework called LTRHA that supports social interactions among multi-agents. This architecture enables agents to identify and filter out behaviors that are detrimental to social interactions, guiding them to choose more favorable actions. We designed a sandbox environment to simulate the natural evolution of social relationships among multiple identity-less agents for experimental evaluation. The results showed that ITCMA-S performed well on multiple evaluation indicators, demonstrating its ability to actively explore the environment, recognize new agents, and acquire new information through continuous actions and dialogue. Observations show that as agents establish connections with each other, they spontaneously form cliques with internal hierarchies around a selected leader and organize collective activities.",2024-09-10T13:39:29Z,2024-11-19T15:44:30Z,http://arxiv.org/abs/2409.06750v2,http://arxiv.org/pdf/2409.06750v2,"cs.MA, cs.AI, cs.HC, cs.LG, 68T42, I.2.7; J.4"
Adaptive Large Language Models By Layerwise Attention Shortcuts,"Prateek Verma, Mert Pilanci","Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",2024-09-17T03:46:01Z,2024-09-17T03:46:01Z,http://arxiv.org/abs/2409.10870v1,http://arxiv.org/pdf/2409.10870v1,"cs.CL, cs.AI, cs.LG, cs.SD, eess.AS"
"votess: A multi-target, GPU-capable, parallel Voronoi tessellator","Samridh Dev Singh, Chris Byrohl, Dylan Nelson","votess is a library for computing parallel 3D Voronoi tessellations on heterogeneous platforms, from CPUs and GPUs, to future accelerator architectures. To do so, it leverages the SYCL abstraction layer to achieve portability and performance across these architectures. The core library is an implementation of a Voronoi cell-by-cell computation algorithm, producing the geometry of the cells and their neighbor connectivity information, rather than a full combinatorial mesh data structure. This simplifies the Voronoi tessellation and makes it more suitable to data parallel architectures than alternatives such as sequential insertion or the Bowyer-Watson algorithm. The library demonstrates significant performance improvements over established single-threaded programs and serves as a foundational tool for performance-critical applications, such as on-the-fly computations in hydrodynamical codes.",2024-12-05T02:07:32Z,2024-12-11T22:29:41Z,http://arxiv.org/abs/2412.04514v2,http://arxiv.org/pdf/2412.04514v2,"astro-ph.IM, cs.DC"
Whisper-GPT: A Hybrid Representation Audio Large Language Model,Prateek Verma,"We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",2024-12-16T05:03:48Z,2024-12-16T05:03:48Z,http://arxiv.org/abs/2412.11449v1,http://arxiv.org/pdf/2412.11449v1,"cs.SD, cs.AI, cs.CL, cs.LG, eess.AS"
Benchmarking Generative AI Models for Deep Learning Test Input   Generation,"Maryam, Matteo Biagiola, Andrea Stocco, Vincenzo Riccio","Test Input Generators (TIGs) are crucial to assess the ability of Deep Learning (DL) image classifiers to provide correct predictions for inputs beyond their training and test sets. Recent advancements in Generative AI (GenAI) models have made them a powerful tool for creating and manipulating synthetic images, although these advancements also imply increased complexity and resource demands for training.   In this work, we benchmark and combine different GenAI models with TIGs, assessing their effectiveness, efficiency, and quality of the generated test images, in terms of domain validity and label preservation. We conduct an empirical study involving three different GenAI architectures (VAEs, GANs, Diffusion Models), five classification tasks of increasing complexity, and 364 human evaluations. Our results show that simpler architectures, such as VAEs, are sufficient for less complex datasets like MNIST. However, when dealing with feature-rich datasets, such as ImageNet, more sophisticated architectures like Diffusion Models achieve superior performance by generating a higher number of valid, misclassification-inducing inputs.",2024-12-23T15:30:42Z,2024-12-23T15:30:42Z,http://arxiv.org/abs/2412.17652v1,http://arxiv.org/pdf/2412.17652v1,"cs.LG, cs.SE, D.2.5"
Generative Pretrained Embedding and Hierarchical Irregular Time Series   Representation for Daily Living Activity Recognition,"Damien Bouchabou, Sao Mai Nguyen","Within the evolving landscape of smart homes, the precise recognition of daily living activities using ambient sensor data stands paramount. This paper not only aims to bolster existing algorithms by evaluating two distinct pretrained embeddings suited for ambient sensor activations but also introduces a novel hierarchical architecture. We delve into an architecture anchored on Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT design, and contrast it with the previously established state-of-the-art (SOTA) ELMo embeddings for ambient sensors. Our proposed hierarchical structure leverages the strengths of each pre-trained embedding, enabling the discernment of activity dependencies and sequence order, thereby enhancing classification precision. To further refine recognition, we incorporate into our proposed architecture an hour-of-the-day embedding. Empirical evaluations underscore the preeminence of the Transformer Decoder embedding in classification endeavors. Additionally, our innovative hierarchical design significantly bolsters the efficacy of both pre-trained embeddings, notably in capturing inter-activity nuances. The integration of temporal aspects subtly but distinctively augments classification, especially for time-sensitive activities. In conclusion, our GPT-inspired hierarchical approach, infused with temporal insights, outshines the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,http://arxiv.org/pdf/2412.19732v1,"cs.LG, 68T05, I.2.6"
Semialgebraic Neural Networks: From roots to representations,"S. David Mis, Matti Lassas, Maarten V. de Hoop","Many numerical algorithms in scientific computing -- particularly in areas like numerical linear algebra, PDE simulation, and inverse problems -- produce outputs that can be represented by semialgebraic functions; that is, the graph of the computed function can be described by finitely many polynomial equalities and inequalities. In this work, we introduce Semialgebraic Neural Networks (SANNs), a neural network architecture capable of representing any bounded semialgebraic function, and computing such functions up to the accuracy of a numerical ODE solver chosen by the programmer. Conceptually, we encode the graph of the learned function as the kernel of a piecewise polynomial selected from a class of functions whose roots can be evaluated using a particular homotopy continuation method. We show by construction that the SANN architecture is able to execute this continuation method, thus evaluating the learned semialgebraic function. Furthermore, the architecture can exactly represent even discontinuous semialgebraic functions by executing a continuation method on each connected component of the target function. Lastly, we provide example applications of these networks and show they can be trained with traditional deep-learning techniques.",2025-01-02T22:52:07Z,2025-01-02T22:52:07Z,http://arxiv.org/abs/2501.01564v1,http://arxiv.org/pdf/2501.01564v1,"cs.LG, cs.NA, cs.NE, math.NA"
Analysis of mean-field models arising from self-attention dynamics in   transformer architectures with layer normalization,"Martin Burger, Samira Kabri, Yury Korolev, Tim Roith, Lukas Weigand","The aim of this paper is to provide a mathematical analysis of transformer architectures using a self-attention mechanism with layer normalization. In particular, observed patterns in such architectures resembling either clusters or uniform distributions pose a number of challenging mathematical questions. We focus on a special case that admits a gradient flow formulation in the spaces of probability measures on the unit sphere under a special metric, which allows us to give at least partial answers in a rigorous way. The arising mathematical problems resemble those recently studied in aggregation equations, but with additional challenges emerging from restricting the dynamics to the sphere and the particular form of the interaction energy.   We provide a rigorous framework for studying the gradient flow, which also suggests a possible metric geometry to study the general case (i.e. one that is not described by a gradient flow). We further analyze the stationary points of the induced self-attention dynamics. The latter are related to stationary points of the interaction energy in the Wasserstein geometry, and we further discuss energy minimizers and maximizers in different parameter settings.",2025-01-06T15:53:29Z,2025-01-06T15:53:29Z,http://arxiv.org/abs/2501.03096v1,http://arxiv.org/pdf/2501.03096v1,"math.AP, 68Q32, 49Q20, 43A35"
Quantum Data Center Infrastructures: A Scalable Architectural Design   Perspective,"Hassan Shapourian, Eneet Kaur, Troy Sewell, Jiapeng Zhao, Michael Kilzer, Ramana Kompella, Reza Nejabati","This paper presents the design of scalable quantum networks that utilize optical switches to interconnect multiple quantum processors, facilitating large-scale quantum computing. By leveraging these novel architectures, we aim to address the limitations of current quantum processors and explore the potential of quantum data centers. We provide an in-depth analysis of these architectures through the development of simulation tools and performance metrics, offering a detailed comparison of their advantages and trade-offs. We hope this work serves as a foundation for the development of efficient and resilient quantum networks, designed to meet the evolving demands of future quantum computing applications.",2025-01-09T22:12:33Z,2025-01-09T22:12:33Z,http://arxiv.org/abs/2501.05598v1,http://arxiv.org/pdf/2501.05598v1,"quant-ph, physics.optics"
Hierarchical Serpentine-like Organic Crystal Optical Waveguides for   Artificial Neural Networks,"Avulu Vinod Kumar, Mehdi Rohullah, Melchi Chosenyah, Sinduja Gaddam, Rajadurai Chandrasekar","Optical components and circuits that deal with multiple signal generation and processing are quintessential for artificial neural networks. Herein, we present a proof-of-concept four-layered organic optical artificial neural network (ANN)-like architecture, constructed from flexible organic crystals of (E)-1-(((5-methylpyridin-2-yl)imino)methyl)naphthalene-2-ol (MPyIN), employing an atomic force microscopy cantilever tip-based mechanical micromanipulation technique. Initially, the strategic selection of four MPyIN crystal active waveguides of varying lengths, mechanically bending them into serpentine-like forms, followed by their hierarchical integration, creates neuron-like, four-layered interconnected optical waveguides with six optical synapses. The synapses in the ANN-like architecture enable parallel transmissions of passive optical signals via evanescent coupling across multiple paths through various layers of the serpentine-shaped optical waveguides. Notably, the feedforward mechanism allows the synapses to multiply and split the optical signal generated at any input into four diverging signals with varying magnitudes. Here, certain outputs deliver a mixed signal (passive and active) due to diverging and converging optical transmission paths. This hierarchical, ANN-like tiny architecture paves the way for the development of smart optical neural networks utilizing multiple emissive and phase-changing organic crystals.",2025-01-10T10:10:46Z,2025-01-10T10:10:46Z,http://arxiv.org/abs/2501.05831v1,http://arxiv.org/pdf/2501.05831v1,"physics.optics, cond-mat.dis-nn, cond-mat.mtrl-sci"
"Random Sparse Lifts: Construction, Analysis and Convergence of finite   sparse networks","David A. R. Robin, Kevin Scaman, Marc Lelarge","We present a framework to define a large class of neural networks for which, by construction, training by gradient flow provably reaches arbitrarily low loss when the number of parameters grows. Distinct from the fixed-space global optimality of non-convex optimization, this new form of convergence, and the techniques introduced to prove such convergence, pave the way for a usable deep learning convergence theory in the near future, without overparameterization assumptions relating the number of parameters and training samples. We define these architectures from a simple computation graph and a mechanism to lift it, thus increasing the number of parameters, generalizing the idea of increasing the widths of multi-layer perceptrons. We show that architectures similar to most common deep learning models are present in this class, obtained by sparsifying the weight tensors of usual architectures at initialization. Leveraging tools of algebraic topology and random graph theory, we use the computation graph's geometry to propagate properties guaranteeing convergence to any precision for these large sparse models.",2025-01-10T12:52:00Z,2025-01-10T12:52:00Z,http://arxiv.org/abs/2501.05930v1,http://arxiv.org/pdf/2501.05930v1,"math.OC, stat.ML"
Decentralized Multi-Antenna Architectures with Unitary Constraints,"Juan Vidal Alegría, Ove Edfors","The increase in the number of base station (BS) antennas calls for efficient solutions to deal with the increased interconnection bandwidth and processing complexity of traditional centralized approaches. Decentralized approaches are thus gaining momentum, since they achieve important reductions in data/processing volume by preprocessing the received signals before forwarding them to a central node. The WAX framework offers a general description of decentralized architectures with arbitrary interplay between interconnection bandwidth and decentralized processing complexity, but the applicability of this framework has only been studied assuming unrestricted baseband processing. We consider an adaptation of the WAX framework where the decentralized processing has unitary restriction, which allows for energy-efficient implementations based on reconfigurable impedance networks at the cost of some performance loss. Moreover, we propose an effective method to minimize the performance gap with respect to centralized processing. The previous method gives a first step towards characterizing the information-lossless trade-off between interconnection bandwidth and processing complexity in decentralized architectures with unitary constraints.",2025-01-10T15:57:23Z,2025-01-10T15:57:23Z,http://arxiv.org/abs/2501.06067v1,http://arxiv.org/pdf/2501.06067v1,"eess.SP, cs.IT, math.IT"
xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement,"Nikolai Lund Kühne, Jan Østergaard, Jesper Jensen, Zheng-Hua Tan","While attention-based architectures, such as Conformers, excel in speech enhancement, they face challenges such as scalability with respect to input sequence length. In contrast, the recently proposed Extended Long Short-Term Memory (xLSTM) architecture offers linear scalability. However, xLSTM-based models remain unexplored for speech enhancement. This paper introduces xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A comparative analysis reveals that xLSTM-and notably, even LSTM-can match or outperform state-of-the-art Mamba- and Conformer-based systems across various model sizes in speech enhancement on the VoiceBank+Demand dataset. Through ablation studies, we identify key architectural design choices such as exponential gating and bidirectionality contributing to its effectiveness. Our best xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems on the Voicebank+DEMAND dataset.",2025-01-10T18:10:06Z,2025-01-10T18:10:06Z,http://arxiv.org/abs/2501.06146v1,http://arxiv.org/pdf/2501.06146v1,"cs.SD, cs.AI, eess.AS"
Modular Compilation for Quantum Chiplet Architectures,"Mingyoung Jessica Jeng, Nikola Vuk Maruszewski, Connor Selna, Michael Gavrincea, Kaitlin N. Smith, Nikos Hardavellas","As quantum computing technology continues to mature, industry is adopting modular quantum architectures to keep quantum scaling on the projected path and meet performance targets. However, the complexity of chiplet-based quantum devices, coupled with their growing size, presents an imminent scalability challenge for quantum compilation. Contemporary compilation methods are not well-suited to chiplet architectures. In particular, existing qubit allocation methods are often unable to contend with inter-chiplet links, which don't necessary support a universal basis gate set. Furthermore, existing methods of logical-to-physical qubit placement, swap insertion (routing), unitary synthesis, and/or optimization are typically not designed for qubit links of wildly varying levels of duration or fidelity. In this work, we propose SEQC, a complete and parallelized compilation pipeline optimized for chiplet-based quantum computers, including several novel methods for qubit placement, qubit routing, and circuit optimization. SEQC attains up to a 36% increase in circuit fidelity, accompanied by execution time improvements of up to 1.92x. Additionally, owning to its ability to parallelize compilation, SEQC achieves consistent solve time improvements of 2-4x over a chiplet-aware Qiskit baseline.",2025-01-14T22:41:29Z,2025-01-14T22:41:29Z,http://arxiv.org/abs/2501.08478v1,http://arxiv.org/pdf/2501.08478v1,"quant-ph, cs.ET, cs.PL"
QuFeX: Quantum feature extraction module for hybrid quantum-classical   deep neural networks,"Naman Jain, Amir Kalev","We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine learning module. The proposed module enables feature extraction in a reduced-dimensional space, significantly decreasing the number of parallel evaluations required in typical quantum convolutional neural network architectures. Its design allows seamless integration into deep classical neural networks, making it particularly suitable for hybrid quantum-classical models. As an application of QuFeX, we propose Qu-Net -- a hybrid architecture which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is widely used for image segmentation tasks such as medical imaging and autonomous driving. Our numerical analysis indicates that the Qu-Net can achieve superior segmentation performance compared to a U-Net baseline. These results highlight the potential of QuFeX to enhance deep neural networks by leveraging hybrid computational paradigms, providing a path towards a robust framework for real-world applications requiring precise feature extraction.",2025-01-22T19:00:09Z,2025-01-22T19:00:09Z,http://arxiv.org/abs/2501.13165v1,http://arxiv.org/pdf/2501.13165v1,"quant-ph, cs.AI, cs.LG"
Contrast: A Hybrid Architecture of Transformers and State Space Models   for Low-Level Vision,"Aman Urumbekov, Zheng Chen","Transformers have become increasingly popular for image super-resolution (SR) tasks due to their strong global context modeling capabilities. However, their quadratic computational complexity necessitates the use of window-based attention mechanisms, which restricts the receptive field and limits effective context expansion. Recently, the Mamba architecture has emerged as a promising alternative with linear computational complexity, allowing it to avoid window mechanisms and maintain a large receptive field. Nevertheless, Mamba faces challenges in handling long-context dependencies when high pixel-level precision is required, as in SR tasks. This is due to its hidden state mechanism, which can compress and store a substantial amount of context but only in an approximate manner, leading to inaccuracies that transformers do not suffer from. In this paper, we propose \textbf{Contrast}, a hybrid SR model that combines \textbf{Con}volutional, \textbf{Tra}nsformer, and \textbf{St}ate Space components, effectively blending the strengths of transformers and Mamba to address their individual limitations. By integrating transformer and state space mechanisms, \textbf{Contrast} compensates for the shortcomings of each approach, enhancing both global context modeling and pixel-level accuracy. We demonstrate that combining these two architectures allows us to mitigate the problems inherent in each, resulting in improved performance on image super-resolution tasks.",2025-01-23T03:34:14Z,2025-01-23T03:34:14Z,http://arxiv.org/abs/2501.13353v1,http://arxiv.org/pdf/2501.13353v1,"cs.CV, I.5.1"
The Lock Generative Adversarial Network for Medical Waveform Anomaly   Detection,"Wenjie Xu, Scott Dick","Waveform signal analysis is a complex and important task in medical care. For example, mechanical ventilators are critical life-support machines, but they can cause serious injury to patients if they are out of synchronization with the patients' own breathing reflex. This asynchrony is revealed by the waveforms showing flow and pressure histories. Likewise, electrocardiograms record the electrical activity of a patients' heart as a set of waveforms, and anomalous waveforms can reveal important disease states. In both cases, subtle variations in a complex waveform are important information for patient care; signals which may be missed or mis-interpreted by human caregivers.   We report on the design of a novel Lock Generative Adversarial Network architecture for anomaly detection in raw or summarized medical waveform data. The proposed architecture uses alternating optimization of the generator and discriminator networks to solve the convergence dilemma. Furthermore, the fidelity of the generator networks' outputs to the actual distribution of anomalous data is improved via synthetic minority oversampling. We evaluate this new architecture on one ventilator asynchrony dataset, and two electrocardiogram datasets, finding that the performance was either equal or superior to the state-of-the art on all three.",2025-01-23T17:29:50Z,2025-01-23T17:29:50Z,http://arxiv.org/abs/2501.13858v1,http://arxiv.org/pdf/2501.13858v1,"cs.CE, 94C12"
Towards a Cryogenic CMOS-Memristor Neural Decoder for Quantum Error   Correction,"Pierre-Antoine Mouny, Maher Benhouria, Victor Yon, Patrick Dufour, Linxiang Huang, Yann Beilliard, Sophie Rochette, Dominique Drouin, Pooya Ronagh","This paper presents a novel approach utilizing a scalable neural decoder application-specific integrated circuit (ASIC) based on metal oxide memristors in a 180nm CMOS technology. The ASIC architecture employs in-memory computing with memristor crossbars for efficient vector-matrix multiplications (VMM). The ASIC decoder architecture includes an input layer implemented with a VMM and an analog sigmoid activation function, a recurrent layer with analog memory, and an output layer with a VMM and a threshold activation function. Cryogenic characterization of the ASIC is conducted, demonstrating its performance at both room temperature and cryogenic temperatures down to 1.2K. Results indicate stable activation function shapes and pulse responses at cryogenic temperatures. Moreover, power consumption measurements reveal consistent behavior at room and cryogenic temperatures. Overall, this study lays the foundation for developing efficient and scalable neural decoders for quantum error correction in cryogenic environments.",2025-01-24T14:28:14Z,2025-01-24T14:28:14Z,http://arxiv.org/abs/2501.14525v1,http://arxiv.org/pdf/2501.14525v1,"quant-ph, cs.AR"
A novel Trunk Branch-net PINN for flow and heat transfer prediction in   porous medium,"Haoyun Xing, Kaiyan Jin, Guice Yao, Jin Zhao, Dichu Xu, Dongsheng Wen","A novel Trunk-Branch (TB)-net physics-informed neural network (PINN) architecture is developed, which is a PINN-based method incorporating trunk and branch nets to capture both global and local features. The aim is to solve four main classes of problems: forward flow problem, forward heat transfer problem, inverse heat transfer problem, and transfer learning problem within the porous medium, which are notoriously complex that could not be handled by origin PINN. In the proposed TB-net PINN architecture, a Fully-connected Neural Network (FNN) is used as the trunk net, followed by separated FNNs as the branch nets with respect to outputs, and automatic differentiation is performed for partial derivatives of outputs with respect to inputs by considering various physical loss. The effectiveness and flexibility of the novel TB-net PINN architecture is demonstrated through a collection of forward problems, and transfer learning validates the feasibility of resource reuse. Combining with the superiority over traditional numerical methods in solving inverse problems, the proposed TB-net PINN shows its great potential for practical engineering applications.",2025-01-21T05:03:01Z,2025-01-21T05:03:01Z,http://arxiv.org/abs/2501.16362v1,http://arxiv.org/pdf/2501.16362v1,"cs.LG, physics.flu-dyn"
UDiTQC: U-Net-Style Diffusion Transformer for Quantum Circuit Synthesis,"Zhiwei Chen, Hao Tang","Quantum computing is a transformative technology with wide-ranging applications, and efficient quantum circuit generation is crucial for unlocking its full potential. Current diffusion model approaches based on U-Net architectures, while promising, encounter challenges related to computational efficiency and modeling global context. To address these issues, we propose UDiT,a novel U-Net-style Diffusion Transformer architecture, which combines U-Net's strengths in multi-scale feature extraction with the Transformer's ability to model global context. We demonstrate the framework's effectiveness on two tasks: entanglement generation and unitary compilation, where UDiTQC consistently outperforms existing methods. Additionally, our framework supports tasks such as masking and editing circuits to meet specific physical property requirements. This dual advancement, improving quantum circuit synthesis and refining generative model architectures, marks a significant milestone in the convergence of quantum computing and machine learning research.",2025-01-24T15:15:50Z,2025-01-24T15:15:50Z,http://arxiv.org/abs/2501.16380v1,http://arxiv.org/pdf/2501.16380v1,"cs.LG, cs.AI, quant-ph"
Hierarchical Fallback Architecture for High Risk Online Machine Learning   Inference,"Gustavo Polleti, Marlesson Santana, Felipe Sassi Del Sant, Eduardo Fontes","Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios. In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain. We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them. Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios.",2025-01-29T18:30:18Z,2025-01-29T18:30:18Z,http://arxiv.org/abs/2501.17834v1,http://arxiv.org/pdf/2501.17834v1,"cs.LG, cs.CE, cs.SE, I.2.m"
Learning Non-Local Molecular Interactions via Equivariant Local   Representations and Charge Equilibration,"Paul Fuchs, Michał Sanocki, Julija Zavadlav","Graph Neural Network (GNN) potentials relying on chemical locality offer near-quantum mechanical accuracy at significantly reduced computational costs. By propagating local information to distance particles, Message-passing neural networks (MPNNs) extend the locality concept to model interactions beyond their local neighborhood. Still, this locality precludes modeling long-range effects, such as charge transfer, electrostatic interactions, and dispersion effects, which are critical to adequately describe many real-world systems. In this work, we propose the Charge Equilibration Layer for Long-range Interactions (CELLI) to address the challenging modeling of non-local interactions and the high computational cost of MPNNs. This novel architecture generalizes the fourth-generation high-dimensional neural network (4GHDNN) concept, integrating the charge equilibration (Qeq) method into a model-agnostic building block for modern equivariant GNN potentials. A series of benchmarks show that CELLI can extend the strictly local Allegro architecture to model highly non-local interactions and charge transfer. Our architecture generalizes to diverse datasets and large structures, achieving an accuracy comparable to MPNNs at about twice the computational efficiency.",2025-01-31T14:43:22Z,2025-01-31T14:43:22Z,http://arxiv.org/abs/2501.19179v1,http://arxiv.org/pdf/2501.19179v1,"physics.chem-ph, cs.LG, physics.comp-ph"
HEP-JEPA: A foundation model for collider physics using joint embedding   predictive architecture,"Jai Bardhan, Radhikesh Agrawal, Abhiram Tilak, Cyrin Neeraj, Subhadip Mitra",We present a transformer architecture-based foundation model for tasks at high-energy particle colliders such as the Large Hadron Collider. We train the model to classify jets using a self-supervised strategy inspired by the Joint Embedding Predictive Architecture. We use the JetClass dataset containing 100M jets of various known particles to pre-train the model with a data-centric approach -- the model uses a fraction of the jet constituents as the context to predict the embeddings of the unseen target constituents. Our pre-trained model fares well with other datasets for standard classification benchmark tasks. We test our model on two additional downstream tasks: top tagging and differentiating light-quark jets from gluon jets. We also evaluate our model with task-specific metrics and baselines and compare it with state-of-the-art models in high-energy physics. Project site: https://hep-jepa.github.io/,2025-02-06T10:16:27Z,2025-02-06T10:16:27Z,http://arxiv.org/abs/2502.03933v1,http://arxiv.org/pdf/2502.03933v1,"cs.LG, hep-ex, hep-ph"
A Novel Convolutional-Free Method for 3D Medical Imaging Segmentation,Canxuan Gang,"Segmentation of 3D medical images is a critical task for accurate diagnosis and treatment planning. Convolutional neural networks (CNNs) have dominated the field, achieving significant success in 3D medical image segmentation. However, CNNs struggle with capturing long-range dependencies and global context, limiting their performance, particularly for fine and complex structures. Recent transformer-based models, such as TransUNet and nnFormer, have demonstrated promise in addressing these limitations, though they still rely on hybrid CNN-transformer architectures. This paper introduces a novel, fully convolutional-free model based on transformer architecture and self-attention mechanisms for 3D medical image segmentation. Our approach focuses on improving multi-semantic segmentation accuracy and addressing domain adaptation challenges between thick and thin slice CT images. We propose a joint loss function that facilitates effective segmentation of thin slices based on thick slice annotations, overcoming limitations in dataset availability. Furthermore, we present a benchmark dataset for multi-semantic segmentation on thin slices, addressing a gap in current medical imaging research. Our experiments demonstrate the superiority of the proposed model over traditional and hybrid architectures, offering new insights into the future of convolution-free medical image segmentation.",2025-02-08T00:52:45Z,2025-02-08T00:52:45Z,http://arxiv.org/abs/2502.05396v1,http://arxiv.org/pdf/2502.05396v1,"eess.IV, cs.CV"
Comparison of CNN-based deep learning architectures for unsteady CFD   acceleration on small datasets,"Sangam Khanal, Shilaj Baral, Joongoo Jeon","CFD acceleration for virtual nuclear reactors or digital twin technology is a primary goal in the nuclear industry. This study compares advanced convolutional neural network (CNN) architectures for accelerating unsteady computational fluid dynamics (CFD) simulations using small datasets based on a challenging natural convection flow dataset. The advanced architectures such as autoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical conditions to determine their predictive accuracy and robustness in autoregressive time-series predictions. ConvLSTM-UNet consistently outperformed other models, particularly in difference value calculation, achieving lower maximum errors and stable residuals. However, error accumulation remains a challenge, limiting reliable predictions to approximately 10 timesteps. This highlights the need for enhanced strategies to improve long-term prediction stability. The novelty of this work lies in its fair comparison of state-of-the-art CNN models within the RePIT framework, demonstrating their potential for accelerating CFD simulations while identifying limitations under small data conditions. Future research will focus on exploring alternative models, such as graph neural networks and implicit neural representations. These efforts aim to develop a robust hybrid approach for long-term unsteady CFD acceleration, contributing to practical applications in virtual nuclear reactor.",2025-02-06T03:30:49Z,2025-02-06T03:30:49Z,http://arxiv.org/abs/2502.06837v1,http://arxiv.org/pdf/2502.06837v1,"cs.LG, physics.flu-dyn"
Autonomous Deep Agent,"Amy Yu, Erik Lebedev, Lincoln Everett, Xiaoxin Chen, Terry Chen","This technical brief introduces Deep Agent, an advanced autonomous AI system designed to manage complex multi-phase tasks through a novel hierarchical task management architecture. The system's foundation is built on our Hierarchical Task DAG (HTDAG) framework, which dynamically decomposes high-level objectives into manageable sub-tasks while rigorously maintaining dependencies and execution coherence. Deep Agent advances beyond traditional agent systems through three key innovations: First, it implements a recursive two-stage planner-executor architecture that enables continuous task refinement and adaptation as circumstances change. Second, it features an Autonomous API & Tool Creation (AATC) system that automatically generates reusable components from UI interactions, substantially reducing operational costs for similar tasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt Feedback Learning components that optimize Large Language Model prompts for specific scenarios, enhancing both inference accuracy and operational stability. These components are integrated to form a service infrastructure that manages user contexts, handles complex task dependencies, and orchestrates end-to-end agentic workflow execution. Through this sophisticated architecture, Deep Agent establishes a novel paradigm in self-governing AI systems, demonstrating robust capability to independently handle intricate, multi-step tasks while maintaining consistent efficiency and reliability through continuous self-optimization.",2025-02-10T21:46:54Z,2025-02-10T21:46:54Z,http://arxiv.org/abs/2502.07056v1,http://arxiv.org/pdf/2502.07056v1,"cs.AI, cs.LG, I.2.6; I.2.7"
Testbed Development: An Intelligent O-RAN based Cell-Free MIMO Network,"Yi Chu, Mostafa Rahmani, Josh Shackleton, David Grace, Kanapathippillai Cumanan, Hamed Ahmadi, Alister Burr","Cell-free multiple input multiple output (CF-MIMO) systems improve spectral and energy efficiencies using distributed access points (APs) to provide reliable service across an area equivalent to multiple conventional cells. This paper presents a novel design and implementation of a CF-MIMO network leveraging the open radio access network (O-RAN) architecture based testbed to enhance the performance of interference-prone user. The proposed prototype is developed based on open source software components and unlike many other prototypes, our testbed is able to serve commercial 5G user equipment (UE). The RAN intelligent controller (RIC) allows the cell-free (CF) network to access the embedded artificial intelligence and benefit from the network optimisation techniques that O-RAN brings. The testbed includes an intelligent antenna association xApp which determines the antenna group that serves each UE based on the live key performance measurements. The paper demonstrates the deployment and operation of the CF network and the xApp and discusses how the CF networks can benefit from the O-RAN architecture.",2025-02-12T16:07:11Z,2025-02-12T16:07:11Z,http://arxiv.org/abs/2502.08529v1,http://arxiv.org/pdf/2502.08529v1,"cs.NI, cs.SY, eess.SY"
Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid   Topology Control,"Barbera de Mol, Davide Barbieri, Jan Viebahn, Davide Grossi","Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. However, the combinatorial nature of the action space poses a challenge to both conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making into smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally coordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose actions and subsequently a coordinating agent selects the final action. We investigate several implementations of the CCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The CCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The results suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as real-world power grid settings.",2025-02-12T10:16:06Z,2025-02-12T10:16:06Z,http://arxiv.org/abs/2502.08681v1,http://arxiv.org/pdf/2502.08681v1,"cs.MA, cs.AI, cs.LG, I.2.11; I.2.8; I.2.1; I.2.6"
Architecture for Simulating Behavior Mode Changes in Norm-Aware   Autonomous Agents,"Sean Glaze, Daniela Inclezan","This paper presents an architecture for simulating the actions of a norm-aware intelligent agent whose behavior with respect to norm compliance is set, and can later be changed, by a human controller. Updating an agent's behavior mode from a norm-abiding to a riskier one may be relevant when the agent is involved in time-sensitive rescue operations, for example. We base our work on the Authorization and Obligation Policy Language AOPL designed by Gelfond and Lobo for the specification of norms. We introduce an architecture and a prototype software system that can be used to simulate an agent's plans under different behavior modes that can later be changed by the controller. We envision such software to be useful to policy makers, as they can more readily understand how agents may act in certain situations based on the agents' attitudes towards norm-compliance. Policy makers may then refine their policies if simulations show unwanted consequences.",2025-02-13T11:49:02Z,2025-02-13T11:49:02Z,http://arxiv.org/abs/2502.09215v1,http://arxiv.org/pdf/2502.09215v1,"cs.LO, cs.AI, D.1.6; D.3"
SparseZipper: Enhancing Matrix Extensions to Accelerate SpGEMM on CPUs,"Tuan Ta, Joshua Randall, Christopher Batten","The importance of general matrix multiplication (GEMM) is motivating new instruction set extensions for multiplying dense matrices in almost all contemporary ISAs, and these extensions are often implemented using high-performance systolic arrays. However, matrices in emerging workloads are not always dense, and sparse matrices where the vast majority of values are zeros are becoming more common. Existing matrix extensions and micro-architectures cannot efficiently process highly sparse matrices due to two reasons: (1) wasted work when one or both input values are zero; and (2) incompatibility with sparse matrix formats. This work proposes SparseZipper that minimally modifies existing matrix extensions and systolic-array-based micro-architectures specialized for dense-dense GEMM to accelerate sparse-sparse GEMM operating on highly sparse matrices with unstructured sparsity structures. Our performance evaluation shows SparseZipper achieves 5.98x and 2.61x speedup over a scalar hash-based implementation of SpGEMM and a state-of-the-art vectorized SpGEMM version, respectively. Our component-level area evaluation shows SparseZipper increases the area of a baseline 16x16 systolic array by only 12.7% resulting in an area overhead for an entire system-on-chip of just a few percent.",2025-02-17T02:09:46Z,2025-02-17T02:09:46Z,http://arxiv.org/abs/2502.11353v1,http://arxiv.org/pdf/2502.11353v1,"cs.AR, C.1.2"
Reconfigurable Intelligent Surfaces-Assisted Integrated Access and   Backhaul,"Charitha Madapatha, Behrooz Makki, Hao Guo, Tommy Svensson","In this paper, we study the impact of reconfigurable intelligent surfaces (RISs) on the coverage extension of integrated access and backhaul (IAB) networks. Particularly, using a finite stochastic geometry model, with random distributions of user equipments (UEs) in a finite region, and planned hierachical architecture for IAB, we study the service coverage probability defined as the probability of the event that the UEs' minimum rate requirements are satisfied. We present comparisons between different cases including IAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network controlled repeaters (NCRs). Our investigations focus on wide-area IAB assisted with RIS through the lens of different design architectures and deployments, revealing both conflicts and synergies for minimizing the effect of tree foliage over seasonal changes. Our simulation results reveal both opportunities and challenges towards the implementation of RIS in IAB.",2025-02-17T16:46:15Z,2025-02-17T16:46:15Z,http://arxiv.org/abs/2502.12011v1,http://arxiv.org/pdf/2502.12011v1,"cs.IT, cs.LG, cs.NI, math.IT"
MeMo: Towards Language Models with Associative Memory Mechanisms,"Fabio Massimo Zanzotto, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Leonardo Ranaldi, Davide Venditti, Federico Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli","Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that memorization precedes learning. We introduce MeMo, a novel architecture for language modeling that explicitly memorizes sequences of tokens in layered associative memories. By design, MeMo offers transparency and the possibility of model editing, including forgetting texts. We experimented with the MeMo architecture, showing the memorization power of the one-layer and the multi-layer configurations.",2025-02-18T13:39:22Z,2025-02-18T13:39:22Z,http://arxiv.org/abs/2502.12851v1,http://arxiv.org/pdf/2502.12851v1,"cs.CL, cs.AI, I.2.7; I.2.6; I.2.4"
Multi-Target Radar Search and Track Using Sequence-Capable Deep   Reinforcement Learning,"Jan-Hendrik Ewers, David Cormack, Joe Gibbs, David Anderson","The research addresses sensor task management for radar systems, focusing on efficiently searching and tracking multiple targets using reinforcement learning. The approach develops a 3D simulation environment with an active electronically scanned array radar, using a multi-target tracking algorithm to improve observation data quality. Three neural network architectures were compared including an approach using fated recurrent units with multi-headed self-attention. Two pre-training techniques were applied: behavior cloning to approximate a random search strategy and an auto-encoder to pre-train the feature extractor. Experimental results revealed that search performance was relatively consistent across most methods. The real challenge emerged in simultaneously searching and tracking targets. The multi-headed self-attention architecture demonstrated the most promising results, highlighting the potential of sequence-capable architectures in handling dynamic tracking scenarios. The key contribution lies in demonstrating how reinforcement learning can optimize sensor management, potentially improving radar systems' ability to identify and track multiple targets in complex environments.",2025-02-19T09:55:38Z,2025-02-19T09:55:38Z,http://arxiv.org/abs/2502.13584v1,http://arxiv.org/pdf/2502.13584v1,"cs.LG, cs.SY, eess.SY"
A generalized dual potential for inelastic Constitutive Artificial   Neural Networks: A JAX implementation at finite strains,"Hagen Holthusen, Kevin Linka, Ellen Kuhl, Tim Brepols","We present a methodology for designing a generalized dual potential, or pseudo potential, for inelastic Constitutive Artificial Neural Networks (iCANNs). This potential, expressed in terms of stress invariants, inherently satisfies thermodynamic consistency for large deformations. In comparison to our previous work, the new potential captures a broader spectrum of material behaviors, including pressure-sensitive inelasticity.   To this end, we revisit the underlying thermodynamic framework of iCANNs for finite strain inelasticity and derive conditions for constructing a convex, zero-valued, and non-negative dual potential. To embed these principles in a neural network, we detail the architecture's design, ensuring a priori compliance with thermodynamics.   To evaluate the proposed architecture, we study its performance and limitations discovering visco-elastic material behavior, though the method is not limited to visco-elasticity. In this context, we investigate different aspects in the strategy of discovering inelastic materials. Our results indicate that the novel architecture robustly discovers interpretable models and parameters, while autonomously revealing the degree of inelasticity.   The iCANN framework, implemented in JAX, is publicly accessible at https://doi.org/10.5281/zenodo.14894687.",2025-02-19T20:16:45Z,2025-02-19T20:16:45Z,http://arxiv.org/abs/2502.17490v1,http://arxiv.org/pdf/2502.17490v1,"cs.LG, cond-mat.mtrl-sci, cs.AI, cs.CE, 65, 74, I.6; J.2"
A Reverse Mamba Attention Network for Pathological Liver Segmentation,"Jun Zeng, Ulas Bagci, Debesh Jha","We present RMA-Mamba, a novel architecture that advances the capabilities of vision state space models through a specialized reverse mamba attention module (RMA). The key innovation lies in RMA-Mamba's ability to capture long-range dependencies while maintaining precise local feature representation through its hierarchical processing pipeline. By integrating Vision Mamba (VMamba)'s efficient sequence modeling with RMA's targeted feature refinement, our architecture achieves superior feature learning across multiple scales. This dual-mechanism approach enables robust handling of complex morphological patterns while maintaining computational efficiency. We demonstrate RMA-Mamba's effectiveness in the challenging domain of pathological liver segmentation (from both CT and MRI), where traditional segmentation approaches often fail due to tissue variations. When evaluated on a newly introduced cirrhotic liver dataset (CirrMRI600+) of T2-weighted MRI scans, RMA-Mamba achieves the state-of-the-art performance with a Dice coefficient of 92.08%, mean IoU of 87.36%, and recall of 92.96%. The architecture's generalizability is further validated on the cancerous liver segmentation from CT scans (LiTS: Liver Tumor Segmentation dataset), yielding a Dice score of 92.9% and mIoU of 88.99%. The source code of the proposed RMA-Mamba is available at https://github.com/JunZengz/RMAMamba.",2025-02-23T20:41:25Z,2025-02-23T20:41:25Z,http://arxiv.org/abs/2502.18232v1,http://arxiv.org/pdf/2502.18232v1,"eess.IV, cs.AI, cs.CV"
O-RIS-ing: Evaluating RIS-Assisted NextG Open RAN,"Maria Tsampazi, Michele Polese, Falko Dressler, Tommaso Melodia","Reconfigurable Intelligent Surfaces (RISs) pose as a transformative technology to revolutionize the cellular architecture of Next Generation (NextG) Radio Access Networks (RANs). Previous studies have demonstrated the capabilities of RISs in optimizing wireless propagation, achieving high spectral efficiency, and improving resource utilization. At the same time, the transition to softwarized, disaggregated, and virtualized architectures, such as those being standardized by the O-RAN ALLIANCE, enables the vision of a reconfigurable Open RAN. In this work, we aim to integrate these technologies by studying how different resource allocation policies enhance the performance of RIS-assisted Open RANs. We perform a comparative analysis among various network configurations and show how proper network optimization can enhance the performance across the Enhanced Mobile Broadband (eMBB) and Ultra Reliable and Low Latency Communications (URLLC) network slices, achieving up to ~34% throughput improvement. Furthermore, leveraging the capabilities of OpenRAN Gym, we deploy an xApp on Colosseum, the world's largest wireless system emulator with hardware-in-the-loop, to control the Base Station (BS)'s scheduling policy. Experimental results demonstrate that RIS-assisted topologies achieve high resource efficiency and low latency, regardless of the BS's scheduling policy.",2025-02-26T02:09:57Z,2025-02-26T02:09:57Z,http://arxiv.org/abs/2502.18753v1,http://arxiv.org/pdf/2502.18753v1,"cs.NI, eess.SP"
Algorithms in the Ultra-Wide Word Model,"Arash Farzan, Alejandro López-Ortiz, Patrick K. Nicholson, Alejandro Salinger","The effective use of parallel computing resources to speed up algorithms in current multi-core parallel architectures remains a difficult challenge, with ease of programming playing a key role in the eventual success of various parallel architectures. In this paper we consider an alternative view of parallelism in the form of an ultra-wide word processor. We introduce the Ultra-Wide Word architecture and model, an extension of the word-RAM model that allows for constant time operations on thousands of bits in parallel. Word parallelism as exploited by the word-RAM model does not suffer from the more difficult aspects of parallel programming, namely synchronization and concurrency. For the standard word-RAM algorithms, the speedups obtained are moderate, as they are limited by the word size. We argue that a large class of word-RAM algorithms can be implemented in the Ultra-Wide Word model, obtaining speedups comparable to multi-threaded computations while keeping the simplicity of programming of the sequential RAM model. We show that this is the case by describing implementations of Ultra-Wide Word algorithms for dynamic programming and string searching. In addition, we show that the Ultra-Wide Word model can be used to implement a nonstandard memory architecture, which enables the sidestepping of lower bounds of important data structure problems such as priority queues and dynamic prefix sums. While similar ideas about operating on large words have been mentioned before in the context of multimedia processors [Thorup 2003], it is only recently that an architecture like the one we propose has become feasible and that details can be worked out.",2014-11-26T20:25:27Z,2014-12-07T17:36:42Z,http://arxiv.org/abs/1411.7359v2,http://arxiv.org/pdf/1411.7359v2,"cs.DS, cs.DC, F.1.1; F.1.2; F.2.2"
Performance Analysis of Mixed-ADC Massive MIMO Systems over Rician   Fading Channels,"Jiayi Zhang, Linglong Dai, Ziyan He, Shi Jin, Xu Li","The practical deployment of massive multiple-input multiple-output (MIMO) in future fifth generation (5G) wireless communication systems is challenging due to its high hardware cost and power consumption. One promising solution to address this challenge is to adopt the low-resolution analog-to-digital converter (ADC) architecture. However, the practical implementation of such architecture is challenging due to the required complex signal processing to compensate the coarse quantization caused by low-resolution ADCs. Therefore, few high-resolution ADCs are reserved in the recently proposed mixed-ADC architecture to enable low-complexity transceiver algorithms. In contrast to previous works over Rayleigh fading channels, we investigate the performance of mixed-ADC massive MIMO systems over the Rician fading channel, which is more general for the 5G scenarios like Internet of Things (IoT). Specially, novel closed-form approximate expressions for the uplink achievable rate are derived for both cases of perfect and imperfect channel state information (CSI). With the increasing Rician $K$-factor, the derived results show that the achievable rate will converge to a fixed value. We also obtain the power-scaling law that the transmit power of each user can be scaled down proportionally to the inverse of the number of base station (BS) antennas for both perfect and imperfect CSI. Moreover, we reveal the trade-off between the achievable rate and energy efficiency with respect to key system parameters including the quantization bits, number of BS antennas, Rician $K$-factor, user transmit power, and CSI quality. Finally, numerical results are provided to show that the mixed-ADC architecture can achieve a better energy-rate trade-off compared with the ideal infinite-resolution and low-resolution ADC architectures.",2017-03-10T12:01:08Z,2017-03-10T12:01:08Z,http://arxiv.org/abs/1703.03642v1,http://arxiv.org/pdf/1703.03642v1,"cs.IT, math.IT"
Transformations of High-Level Synthesis Codes for High-Performance   Computing,"Johannes de Fine Licht, Maciej Besta, Simon Meierhans, Torsten Hoefler","Spatial computing architectures promise a major stride in performance and energy efficiency over the traditional load/store devices currently employed in large scale computing systems. The adoption of high-level synthesis (HLS) from languages such as C++ and OpenCL has greatly increased programmer productivity when designing for such platforms. While this has enabled a wider audience to target spatial computing architectures, the optimization principles known from traditional software design are no longer sufficient to implement high-performance codes, due to fundamentally distinct aspects of hardware design, such as programming for deep pipelines, distributed memory resources, and scalable routing. To alleviate this, we present a collection of optimizing transformations for HLS, targeting scalable and efficient architectures for high-performance computing (HPC) applications. We systematically identify classes of transformations (pipelining, scalability, and memory), the characteristics of their effect on the HLS code and the resulting hardware (e.g., increasing data reuse or resource consumption), and the objectives that each transformation can target (e.g., resolve interface contention, or increase parallelism). We show how these can be used to efficiently exploit pipelining, on-chip distributed fast memory, and on-chip dataflow, allowing for massively parallel architectures. To quantify the effect of various transformations, we cover the optimization process of a sample set of HPC kernels, provided as open source reference codes. We aim to establish a common toolbox to guide both performance engineers and compiler engineers in tapping into the performance potential offered by spatial computing architectures using HLS.",2018-05-21T20:55:09Z,2020-11-23T14:10:19Z,http://arxiv.org/abs/1805.08288v6,http://arxiv.org/pdf/1805.08288v6,"cs.DC, cs.PL, I.1.3; C.1.4; D.1.3"
5G Massive MIMO Architectures: Self-Backhauled Small Cells versus Direct   Access,"Andrea Bonfante, Lorenzo Galati Giordano, David López-Pérez, Adrian Garcia-Rodriguez, Giovanni Geraci, Paolo Baracca, M. Majid Butt, Nicola Marchetti","In this paper, we focus on one of the key technologies for the fifth-generation wireless communication networks, massive multiple-input-multiple-output (mMIMO), by investigating two of its most relevant architectures: 1) to provide in-band backhaul for the ultra-dense network (UDN) of self-backhauled small cells (SCs), and 2) to provide direct access (DA) to user equipments (UEs). Through comprehensive 3GPP-based system-level simulations and analytical formulations, we show the end-to-end UE rates achievable with these two architectures. Differently from the existing works, we provide results for two strategies of self-backhauled SC deployments, namely random and ad-hoc, where in the latter SCs are purposely positioned close to UEs to achieve line-of-sight (LoS) access links. We also evaluate the optimal backhaul and access time resource partition due to the in-band self-backhauling (s-BH) operations. Our results show that the ad-hoc deployment of self-backhauled SCs closer to the UEs with optimal resource partition and with directive antenna patterns, provides rate improvements for cell-edge UEs that amount to 30% and tenfold gain, as compared to mMIMO DA architecture with pilot reuse 3 and reuse 1, respectively. On the other hand, mMIMO s-BH underperforms mMIMO DA above the median value of the UE rates when the effect of pilot contamination is less severe, and the LoS probability of the DA links improves.",2018-09-11T15:01:48Z,2019-10-29T11:37:37Z,http://arxiv.org/abs/1809.03953v2,http://arxiv.org/pdf/1809.03953v2,"cs.NI, eess.SP"
Deep Neural Networks for Choice Analysis: Architectural Design with   Alternative-Specific Utility Functions,"Shenhao Wang, Baichuan Mo, Jinhua Zhao","Whereas deep neural network (DNN) is increasingly applied to choice analysis, it is challenging to reconcile domain-specific behavioral knowledge with generic-purpose DNN, to improve DNN's interpretability and predictive power, and to identify effective regularization methods for specific tasks. This study designs a particular DNN architecture with alternative-specific utility functions (ASU-DNN) by using prior behavioral knowledge. Unlike a fully connected DNN (F-DNN), which computes the utility value of an alternative k by using the attributes of all the alternatives, ASU-DNN computes it by using only k's own attributes. Theoretically, ASU-DNN can dramatically reduce the estimation error of F-DNN because of its lighter architecture and sparser connectivity. Empirically, ASU-DNN has 2-3% higher prediction accuracy than F-DNN over the whole hyperparameter space in a private dataset that we collected in Singapore and a public dataset in R mlogit package. The alternative-specific connectivity constraint, as a domain-knowledge-based regularization method, is more effective than the most popular generic-purpose explicit and implicit regularization methods and architectural hyperparameters. ASU-DNN is also more interpretable because it provides a more regular substitution pattern of travel mode choices than F-DNN does. The comparison between ASU-DNN and F-DNN can also aid in testing the behavioral knowledge. Our results reveal that individuals are more likely to compute utility by using an alternative's own attributes, supporting the long-standing practice in choice modeling. Overall, this study demonstrates that prior behavioral knowledge could be used to guide the architecture design of DNN, to function as an effective domain-knowledge-based regularization method, and to improve both the interpretability and predictive power of DNN in choice analysis.",2019-09-16T21:01:23Z,2021-04-02T22:38:28Z,http://arxiv.org/abs/1909.07481v2,http://arxiv.org/pdf/1909.07481v2,"cs.LG, cs.AI, econ.GN, q-fin.EC, stat.ML"
Mixed-ADC Massive MIMO Detectors: Performance Analysis and Design   Optimization,"Ti-Cao Zhang, Chao-Kai Wen, Shi Jin, Tao Jiang","Using a very low-resolution analog-to-digital convertor (ADC) unit at each antenna can remarkably reduce the hardware cost and power consumption of a massive multiple-input multiple-output (MIMO) system. However, such a pure low-resolution ADC architecture also complicates parameter estimation problems such as time/frequency synchronization and channel estimation. A mixed-ADC architecture, where most of the antennas are equipped with low-precision ADCs while a few antennas have full-precision ADCs, can solve these issues and actualize the potential of the pure low-resolution ADC architecture. In this paper, we present a unified framework to develop a family of detectors over the massive MIMO uplink system with the mixed-ADC receiver architecture by exploiting probabilistic Bayesian inference. As a basic setup, an optimal detector is developed to provide a minimum mean-squared-error (MMSE) estimate on data symbols. Considering the highly nonlinear steps involved in the quantization process, we also investigate the potential for complexity reduction on the optimal detector by postulating the common \emph{pseudo-quantization noise} (PQN) model. In particular, we provide asymptotic performance expressions including the MSE and bit error rate for the optimal and suboptimal MIMO detectors. The asymptotic performance expressions can be evaluated quickly and efficiently; thus, they are useful in system design optimization. We show that in the low signal-to-noise ratio (SNR) regime, the distortion caused by the PQN model can be ignored, whereas in the high-SNR regime, such distortion may cause 1-bit detection performance loss. The performance gap resulting from the PQN model can be narrowed by a small fraction of high-precision ADCs in the mixed-ADC architecture.",2015-09-26T09:00:42Z,2015-10-01T11:48:13Z,http://arxiv.org/abs/1509.07950v2,http://arxiv.org/pdf/1509.07950v2,"cs.IT, math.IT"
Accuracy Prediction with Non-neural Model for Neural Architecture Search,"Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu","Neural architecture search (NAS) with an accuracy predictor that predicts the accuracy of candidate architectures has drawn increasing attention due to its simplicity and effectiveness. Previous works usually employ neural network-based predictors which require more delicate design and are easy to overfit. Considering that most architectures are represented as sequences of discrete symbols which are more like tabular data and preferred by non-neural predictors, in this paper, we study an alternative approach which uses non-neural model for accuracy prediction. Specifically, as decision tree based models can better handle tabular data, we leverage gradient boosting decision tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor can achieve comparable (if not better) prediction accuracy than neural network based predictors. Moreover, considering that a compact search space can ease the search process, we propose to prune the search space gradually according to important features derived from GBDT. In this way, NAS can be performed by first pruning the search space and then searching a neural architecture, which is more efficient and effective. Experiments on NASBench-101 and ImageNet demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search, regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further achieves 23.4% top-1 error rate on ImageNet when enhanced with search space pruning. Code is provided at https://github.com/renqianluo/GBDT-NAS.",2020-07-09T13:28:49Z,2021-07-19T07:31:57Z,http://arxiv.org/abs/2007.04785v3,http://arxiv.org/pdf/2007.04785v3,"cs.LG, cs.AI, cs.CV, stat.ML"
Tchebichef Transform Domain-based Deep Learning Architecture for Image   Super-resolution,"Ahlad Kumar, Harsh Vardhan Singh","The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer ($TCL$). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters.",2021-02-21T16:39:20Z,2021-02-23T04:21:56Z,http://arxiv.org/abs/2102.10640v2,http://arxiv.org/pdf/2102.10640v2,"eess.IV, cs.CV, cs.LG"
Neural Architecture Search for Compressed Sensing Magnetic Resonance   Image Reconstruction,"Jiangpeng Yan, Shuo Chen, Yongbing Zhang, Xiu Li","Recent works have demonstrated that deep learning (DL) based compressed sensing (CS) implementation can accelerate Magnetic Resonance (MR) Imaging by reconstructing MR images from sub-sampled k-space data. However, network architectures adopted in previous methods are all designed by handcraft. Neural Architecture Search (NAS) algorithms can automatically build neural network architectures which have outperformed human designed ones in several vision tasks. Inspired by this, here we proposed a novel and efficient network for the MR image reconstruction problem via NAS instead of manual attempts. Particularly, a specific cell structure, which was integrated into the model-driven MR reconstruction pipeline, was automatically searched from a flexible pre-defined operation search space in a differentiable manner. Experimental results show that our searched network can produce better reconstruction results compared to previous state-of-the-art methods in terms of PSNR and SSIM with 4-6 times fewer computation resources. Extensive experiments were conducted to analyze how hyper-parameters affect reconstruction performance and the searched structures. The generalizability of the searched architecture was also evaluated on different organ MR datasets. Our proposed method can reach a better trade-off between computation cost and reconstruction performance for MR reconstruction problem with good generalizability and offer insights to design neural networks for other medical image applications. The evaluation code will be available at https://github.com/yjump/NAS-for-CSMRI.",2020-02-22T04:40:16Z,2023-11-02T07:25:23Z,http://arxiv.org/abs/2002.09625v7,http://arxiv.org/pdf/2002.09625v7,"eess.IV, cs.CV"
COVID-19 Antibody Test / Vaccination Certification: There's an app for   that,"Marc Eisenstadt, Manoharan Ramachandran, Niaz Chowdhury, Allan Third, John Domingue","Goal: As the Coronavirus Pandemic of 2019/2020 unfolds, a COVID-19 'Immunity Passport' has been mooted as a way to enable individuals to return back to work. While the quality of antibody testing, the availability of vaccines, and the likelihood of even attaining COVID-19 immunity continue to be researched, we address the issues involved in providing tamper-proof and privacy-preserving certification for test results and vaccinations. Methods: We developed a prototype mobile phone app and requisite decentralized server architecture that facilitates instant verification of tamper-proof test results. Personally identifiable information is only stored at the user's discretion, and the app allows the end-user selectively to present only the specific test result with no other personal information revealed. The architecture, designed for scalability, relies upon (a) the 2019 World Wide Web Consortium standard called 'Verifiable Credentials', (b) Tim Berners-Lee's decentralized personal data platform 'Solid', and (c) a Consortium Ethereum-based blockchain. Results: Our mobile phone app and decentralized server architecture enable the mixture of verifiability and privacy in a manner derived from public/private key pairs and digital signatures, generalized to avoid restrictive ownership of sensitive digital keys and/or data. Benchmark performance tests show it to scale linearly in the worst case, as significant processing is done locally on each app. For the test certificate Holder, Issuer (e.g. healthcare staff, pharmacy) and Verifier (e.g. employer), it is 'just another app' which takes only minutes to use. Conclusions: The app and decentralized server architecture offer a prototype proof of concept that is readily scalable, applicable generically, and in effect 'waiting in the wings' for the biological issues, plus key ethical issues raised in the discussion section, to be resolved.",2020-04-15T22:42:48Z,2020-06-28T18:42:35Z,http://arxiv.org/abs/2004.07376v4,http://arxiv.org/pdf/2004.07376v4,"cs.CR, cs.CY, cs.NI, cs.SI, J.3; K.4.1; E.2; D.4.3; C.5.5; C.2.2; E.3"
edge-SR: Super-Resolution For The Masses,"Pablo Navarrete Michelini, Yunhua Lu, Xingqun Jiang","Classic image scaling (e.g. bicubic) can be seen as one convolutional layer and a single upscaling filter. Its implementation is ubiquitous in all display devices and image processing software. In the last decade deep learning systems have been introduced for the task of image super-resolution (SR), using several convolutional layers and numerous filters. These methods have taken over the benchmarks of image quality for upscaling tasks. Would it be possible to replace classic upscalers with deep learning architectures on edge devices such as display panels, tablets, laptop computers, etc.? On one hand, the current trend in Edge-AI chips shows a promising future in this direction, with rapid development of hardware that can run deep-learning tasks efficiently. On the other hand, in image SR only few architectures have pushed the limit to extreme small sizes that can actually run on edge devices at real-time. We explore possible solutions to this problem with the aim to fill the gap between classic upscalers and small deep learning configurations. As a transition from classic to deep-learning upscaling we propose edge-SR (eSR), a set of one-layer architectures that use interpretable mechanisms to upscale images. Certainly, a one-layer architecture cannot reach the quality of deep learning systems. Nevertheless, we find that for high speed requirements, eSR becomes better at trading-off image quality and runtime performance. Filling the gap between classic and deep-learning architectures for image upscaling is critical for massive adoption of this technology. It is equally important to have an interpretable system that can reveal the inner strategies to solve this problem and guide us to future improvements and better understanding of larger networks.",2021-08-23T18:00:19Z,2021-10-15T15:55:18Z,http://arxiv.org/abs/2108.10335v2,http://arxiv.org/pdf/2108.10335v2,"cs.CV, cs.LG, eess.IV, eess.SP"
What Language Model Architecture and Pretraining Objective Work Best for   Zero-Shot Generalization?,"Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, Colin Raffel","Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",2022-04-12T14:19:49Z,2022-04-12T14:19:49Z,http://arxiv.org/abs/2204.05832v1,http://arxiv.org/pdf/2204.05832v1,"cs.CL, cs.LG, stat.ML"
Uplink HARQ for Distributed and Cloud RAN via Separation of Control and   Data Planes,"Shahrouz Khalili, Osvaldo Simeone","The implementation of uplink HARQ in a Cloud- Radio Access Network RAN (C-RAN) architecture is constrained by the two-way latency on the fronthaul links connecting the Remote Radio Heads (RRHs) with the Baseband Units (BBUs) that perform decoding. To overcome this limitation, this work considers an architecture based on the separation of control and data planes, in which retransmission control decisions are made at the edge of the network, that is, by the RRHs or User Equipments (UEs), while data decoding is carried out remotely at the BBUs. This solution enables low-latency local retransmission decisions to be made at the RRHs or UEs, which are not subject to the fronthaul latency constraints, while at the same time leveraging the decoding capability of the BBUs.   A system with BBU Hoteling system is considered first in which each RRH has a dedicated BBU in the cloud. For this system, the control-data separation leverages low-latency local feedback from an RRH to drive the HARQ process of a given UE. Throughput and probability of error of this solution are analyzed for the three standard HARQ modes of Type-I, Chase Combining and Incremental Redundancy over a general fading MIMO link. Then, novel user-centric low-latency feedback strategies are proposed and analyzed for the C-RAN architecture, with a single centralized BBU, based on limited ""hard"" or ""soft"" local feedback from the RRHs to the UE and on retransmission decisions taken at the UE. The analysis presented in this work allows the optimization of the considered schemes, as well as the investigation of the impact of system parameters such as HARQ protocol type, blocklength and number of antennas on the performance of low-latency local HARQ decisions in BBU Hoteling and C-RAN architectures.",2015-08-26T16:56:04Z,2016-09-05T20:18:08Z,http://arxiv.org/abs/1508.06570v5,http://arxiv.org/pdf/1508.06570v5,"cs.IT, cs.NI, math.IT"
eQASM: An Executable Quantum Instruction Set Architecture,"X. Fu, L. Riesebos, M. A. Rol, J. van Straten, J. van Someren, N. Khammassi, I. Ashraf, R. F. L. Vermeulen, V. Newsum, K. K. L. Loh, J. C. de Sterke, W. J. Vlothuizen, R. N. Schouten, C. G. Almudever, L. DiCarlo, K. Bertels","A widely-used quantum programming paradigm comprises of both the data flow and control flow. Existing quantum hardware cannot well support the control flow, significantly limiting the range of quantum software executable on the hardware. By analyzing the constraints in the control microarchitecture, we found that existing quantum assembly languages are either too high-level or too restricted to support comprehensive flow control on the hardware. Also, as observed with the quantum microinstruction set QuMIS, the quantum instruction set architecture (QISA) design may suffer from limited scalability and flexibility because of microarchitectural constraints. It is an open challenge to design a scalable and flexible QISA which provides a comprehensive abstraction of the quantum hardware.   In this paper, we propose an executable QISA, called eQASM, that can be translated from quantum assembly language (QASM), supports comprehensive quantum program flow control, and is executed on a quantum control microarchitecture. With efficient timing specification, single-operation-multiple-qubit execution, and a very-long-instruction-word architecture, eQASM presents better scalability than QuMIS. The definition of eQASM focuses on the assembly level to be expressive. Quantum operations are configured at compile time instead of being defined at QISA design time. We instantiate eQASM into a 32-bit instruction set targeting a seven-qubit superconducting quantum processor. We validate our design by performing several experiments on a two-qubit quantum processor.",2018-08-07T16:25:20Z,2019-03-09T06:06:01Z,http://arxiv.org/abs/1808.02449v3,http://arxiv.org/pdf/1808.02449v3,"cs.AR, cs.SY, quant-ph"
Networking and processing in optical wireless,"Osama Zwaid Alsulami, Amal A. Alahmadi, Sarah O. M. Saeed, Sanaa Hamid Mohamed, T. E. H. El-Gorashi, Mohammed T. Alresheedi, Jaafar M. H. Elmirghani","Optical wireless communication (OWC) is a promising technology that can provide high data rates while supporting multiple users. The Optical Wireless (OW) physical layer has been researched extensively, however less work was devoted to multiple access and how the OW front end is connected to the network. In this paper, an OWC system which employs a wavelength division multiple access (WDMA) scheme is studied, for the purpose of supporting multiple users. In addition, a cloud/fog architecture is proposed for the first time for OWC to provide processing capabilities. The cloud/fog-integrated architecture uses visible indoor light to create high data rate connections with potential mobile nodes. These optical wireless nodes are further clustered and used as fog mini servers to provide processing services through the optical wireless channel for other users. Additional fog processing units are located in the room, the building, the campus and at the metro level. Further processing capabilities are provided by remote cloud sites. A mixed-integer linear programming (MILP) model was developed and utilised to optimise resource allocation in the indoor OWC system. A second MILP model was developed to optimise the placement of processing tasks in the different fog and cloud nodes available. The optimisation of tasks placement in the cloud-/fog-integrated architecture was analysed using the MILP models. Multiple scenarios were considered where the mobile node locations were varied in the room and the amount of processing and data rate requested by each optical wireless node is varied. The results help identify the optimum colour and access point to use for communication for a given mobile node location and OWC system configuration, the optimum location to place processing and the impact of the network architecture. Areas for future work are identified.",2019-07-22T19:38:35Z,2019-07-22T19:38:35Z,http://arxiv.org/abs/1907.09544v1,http://arxiv.org/pdf/1907.09544v1,"eess.SP, cs.NI"
High-parallelism Inception-like Spiking Neural Networks for Unsupervised   Feature Learning,"Mingyuan Meng, Xingyu Yang, Lei Bi, Jinman Kim, Shanlin Xiao, Zhiyi Yu","Spiking Neural Networks (SNNs) are brain-inspired, event-driven machine learning algorithms that have been widely recognized in producing ultra-high-energy-efficient hardware. Among existing SNNs, unsupervised SNNs based on synaptic plasticity, especially Spike-Timing-Dependent Plasticity (STDP), are considered to have great potential in imitating the learning process of the biological brain. Nevertheless, the existing STDP-based SNNs have limitations in constrained learning capability and/or slow learning speed. Most STDP-based SNNs adopted a slow-learning Fully-Connected (FC) architecture and used a sub-optimal vote-based scheme for spike decoding. In this paper, we overcome these limitations with: 1) a design of high-parallelism network architecture, inspired by the Inception module in Artificial Neural Networks (ANNs); 2) use of a Vote-for-All (VFA) decoding layer as a replacement to the standard vote-based spike decoding scheme, to reduce the information loss in spike decoding and, 3) a proposed adaptive repolarization (resetting) mechanism that accelerates SNNs' learning by enhancing spiking activities. Our experimental results on two established benchmark datasets (MNIST/EMNIST) show that our network architecture resulted in superior performance compared to the widely used FC architecture and a more advanced Locally-Connected (LC) architecture, and that our SNN achieved competitive results with state-of-the-art unsupervised SNNs (95.64%/80.11% accuracy on the MNIST/EMNISE dataset) while having superior learning efficiency and robustness against hardware damage. Our SNN achieved great classification accuracy with only hundreds of training iterations, and random destruction of large numbers of synapses or neurons only led to negligible performance degradation.",2019-12-02T17:19:17Z,2021-03-09T04:00:23Z,http://arxiv.org/abs/2001.01680v5,http://arxiv.org/pdf/2001.01680v5,"cs.NE, cs.LG, q-bio.NC, stat.ML"
Virtualized Logical Qubits: A 2.5D Architecture for Error-Corrected   Quantum Computing,"Casey Duckering, Jonathan M. Baker, David I. Schuster, Frederic T. Chong","Current, near-term quantum devices have shown great progress in recent years culminating with a demonstration of quantum supremacy. In the medium-term, however, quantum machines will need to transition to greater reliability through error correction, likely through promising techniques such as surface codes which are well suited for near-term devices with limited qubit connectivity. We discover quantum memory, particularly resonant cavities with transmon qubits arranged in a 2.5D architecture, can efficiently implement surface codes with substantial hardware savings and performance/fidelity gains. Specifically, we *virtualize logical qubits* by storing them in layers distributed across qubit memories connected to each transmon.   Surprisingly, distributing each logical qubit across many memories has a minimal impact on fault tolerance and results in substantially more efficient operations. Our design permits fast transversal CNOT operations between logical qubits sharing the same physical address which are 6x faster than lattice surgery CNOTs. We develop a novel embedding which saves ~10x in transmons with another 2x from an additional optimization for compactness.   Although Virtualized Logical Qubits (VLQ) pays a 10x penalty in serialization, advantages in the transversal CNOT and area efficiency result in performance comparable to 2D transmon-only architectures. Our simulations show fault tolerance comparable to 2D architectures while saving substantial hardware. Furthermore, VLQ can produce magic states 1.22x faster for a fixed number of transmon qubits. This is a critical benchmark for future fault-tolerant quantum computers. VLQ substantially reduces the hardware requirements for fault tolerance and puts within reach a proof-of-concept experimental demonstration of around 10 logical qubits, requiring only 11 transmons and 9 attached cavities in total.",2020-09-04T02:17:47Z,2020-09-04T02:17:47Z,http://arxiv.org/abs/2009.01982v1,http://arxiv.org/pdf/2009.01982v1,"quant-ph, cs.AR, cs.ET"
Wireless Localisation in WiFi using Novel Deep Architectures,"Peizheng Li, Han Cui, Aftab Khan, Usman Raza, Robert Piechocki, Angela Doufexi, Tim Farnham","This paper studies the indoor localisation of WiFi devices based on a commodity chipset and standard channel sounding. First, we present a novel shallow neural network (SNN) in which features are extracted from the channel state information (CSI) corresponding to WiFi subcarriers received on different antennas and used to train the model. The single-layer architecture of this localisation neural network makes it lightweight and easy-to-deploy on devices with stringent constraints on computational resources. We further investigate for localisation the use of deep learning models and design novel architectures for convolutional neural network (CNN) and long-short term memory (LSTM). We extensively evaluate these localisation algorithms for continuous tracking in indoor environments. Experimental results prove that even an SNN model, after a careful handcrafted feature extraction, can achieve accurate localisation. Meanwhile, using a well-organised architecture, the neural network models can be trained directly with raw data from the CSI and localisation features can be automatically extracted to achieve accurate position estimates. We also found that the performance of neural network-based methods are directly affected by the number of anchor access points (APs) regardless of their structure. With three APs, all neural network models proposed in this paper can obtain localisation accuracy of around 0.5 metres. In addition the proposed deep NN architecture reduces the data pre-processing time by 6.5 hours compared with a shallow NN using the data collected in our testbed. In the deployment phase, the inference time is also significantly reduced to 0.1 ms per sample. We also demonstrate the generalisation capability of the proposed method by evaluating models using different target movement characteristics to the ones in which they were trained.",2020-10-16T22:48:29Z,2020-10-16T22:48:29Z,http://arxiv.org/abs/2010.08658v1,http://arxiv.org/pdf/2010.08658v1,"cs.LG, eess.SP"
pLUTo: Enabling Massively Parallel Computation in DRAM via Lookup Tables,"João Dinis Ferreira, Gabriel Falcao, Juan Gómez-Luna, Mohammed Alser, Lois Orosa, Mohammad Sadrosadati, Jeremie S. Kim, Geraldo F. Oliveira, Taha Shahroodi, Anant Nori, Onur Mutlu","Data movement between the main memory and the processor is a key contributor to execution time and energy consumption in memory-intensive applications. This data movement bottleneck can be alleviated using Processing-in-Memory (PiM). One category of PiM is Processing-using-Memory (PuM), in which computation takes place inside the memory array by exploiting intrinsic analog properties of the memory device. PuM yields high performance and energy efficiency, but existing PuM techniques support a limited range of operations. As a result, current PuM architectures cannot efficiently perform some complex operations (e.g., multiplication, division, exponentiation) without large increases in chip area and design complexity.   To overcome these limitations of existing PuM architectures, we introduce pLUTo (processing-using-memory with lookup table (LUT) operations), a DRAM-based PuM architecture that leverages the high storage density of DRAM to enable the massively parallel storing and querying of lookup tables (LUTs). The key idea of pLUTo is to replace complex operations with low-cost, bulk memory reads (i.e., LUT queries) instead of relying on complex extra logic.   We evaluate pLUTo across 11 real-world workloads that showcase the limitations of prior PuM approaches and show that our solution outperforms optimized CPU and GPU baselines by an average of 713$\times$ and 1.2$\times$, respectively, while simultaneously reducing energy consumption by an average of 1855$\times$ and 39.5$\times$. Across these workloads, pLUTo outperforms state-of-the-art PiM architectures by an average of 18.3$\times$. We also show that different versions of pLUTo provide different levels of flexibility and performance at different additional DRAM area overheads (between 10.2% and 23.1%). pLUTo's source code is openly and fully available at https://github.com/CMU-SAFARI/pLUTo.",2021-04-15T18:10:22Z,2025-01-23T09:15:25Z,http://arxiv.org/abs/2104.07699v6,http://arxiv.org/pdf/2104.07699v6,"cs.AR, cs.DC, B.3.1; C.1.3"
Hybrid-Layers Neural Network Architectures for Modeling the   Self-Interference in Full-Duplex Systems,"Mohamed Elsayed, Ahmad A. Aziz El-Banna, Octavia A. Dobre, Wanyi Shiu, Peiwei Wang","Full-duplex (FD) systems have been introduced to provide high data rates for beyond fifth-generation wireless networks through simultaneous transmission of information over the same frequency resources. However, the operation of FD systems is practically limited by the self-interference (SI), and efficient SI cancelers are sought to make the FD systems realizable. Typically, polynomial-based cancelers are employed to mitigate the SI; nevertheless, they suffer from high complexity. This article proposes two novel hybrid-layers neural network (NN) architectures to cancel the SI with low complexity. The first architecture is referred to as hybrid-convolutional recurrent NN (HCRNN), whereas the second is termed as hybrid-convolutional recurrent dense NN (HCRDNN). In contrast to the state-of-the-art NNs that employ dense or recurrent layers for SI modeling, the proposed NNs exploit, in a novel manner, a combination of different hidden layers (e.g., convolutional, recurrent, and/or dense) in order to model the SI with lower computational complexity than the polynomial and the state-of-the-art NN-based cancelers. The key idea behind using hybrid layers is to build an NN model, which makes use of the characteristics of the different layers employed in its architecture. More specifically, in the HCRNN, a convolutional layer is employed to extract the input data features using a reduced network scale. Moreover, a recurrent layer is then applied to assist in learning the temporal behavior of the input signal from the localized feature map of the convolutional layer. In the HCRDNN, an additional dense layer is exploited to add another degree of freedom for adapting the NN settings in order to achieve the best compromise between the cancellation performance and computational complexity. Complexity analysis and numerical simulations are provided to prove the superiority of the proposed architectures.",2021-10-18T14:18:56Z,2021-10-18T14:18:56Z,http://arxiv.org/abs/2110.09997v1,http://arxiv.org/pdf/2110.09997v1,"eess.SP, cs.IT, cs.LG, math.IT"
"Energy-Proportional Data Center Network Architecture Through OS, Switch   and Laser Co-design","Haiyang Han, Nikos Terzenidis, Dimitris Syrivelis, Arash F. Beldachi, George T. Kanellos, Yigit Demir, Jie Gu, Srikanth Kandula, Nikos Pleros, Fabián Bustamante, Nikos Hardavellas","Optical interconnects are already the dominant technology in large-scale data center networks. However, the high optical loss of many optical components coupled with the low efficiency of laser sources result in high aggregate power requirements for the thousands of optical transceivers used by these networks. As optical interconnects stay always on even as traffic demands ebb and flow, most of this power is wasted. We present LC/DC, a data center network system architecture in which the operating system, the switch, and the optical components are co-designed to achieve energy proportionality.   LC/DC capitalizes on the path divergence of data center networks to turn on and off redundant paths according to traffic demand, while maintaining full connectivity. Turning off redundant paths allows the optical transceivers and their electronic drivers to power down and save energy. Maintaining full connectivity hides the laser turn-on delay. At the node layer, intercepting send requests within the OS allows for the NIC's laser turn-on delay to be fully overlapped with TCP/IP packet processing, and thus egress links can remain powered off until needed with zero performance penalty.   We demonstrate the feasibility of LC/DC by i) implementing the necessary modifications in the Linux kernel and device drivers, ii) implementing a 10Gbit/s FPGA switch, and iii) performing physical experiments with optical devices and circuit simulations. Our results on university data center traces and models of Facebook and Microsoft data center traffic show that LC/DC saves on average 60% of the optical transceivers power (68% max) at the cost of 6% higher packet delay.",2021-12-03T18:47:44Z,2021-12-20T18:08:17Z,http://arxiv.org/abs/2112.02083v2,http://arxiv.org/pdf/2112.02083v2,"cs.NI, cs.AR, C.2"
An Efficient Architecture and High-Throughput Implementation of   CCSDS-123.0-B-2 Hybrid Entropy Coder Targeting Space-Grade SRAM FPGA   Technology,"Panagiotis Chatziantoniou, Antonis Tsigkanos, Dimitris Theodoropoulos, Nektarios Kranitis, Antonis Paschalis","Nowadays, hyperspectral imaging is recognized as cornerstone remote sensing technology. The explosive growth in image data volume and instrument data rates, compete with limited on-board storage resources and downlink bandwidth, making hyperspectral image data compression a mission critical on-board processing task. The Consultative Committee for Space Data Systems (CCSDS) extended the previous issue of the CCSDS-123.0 Recommended Standard for multi- and hyperspectral image compression to provide with Near-Lossless compression functionality. A key feature of the CCSDS-123.0-B-2 is the improved Hybrid Entropy Coder, which at low bit rates, provides substantially better compression performance than the Issue 1 entropy coders. In this paper, we introduce a high-throughput hardware implementation of the CCSDS-123.0-B-2 Hybrid Entropy Coder. The introduced architecture exploits the systolic design pattern to provide modularity and latency insensitivity in a deep and elastic pipeline achieving a constant throughput of 1 sample/cycle with a small FPGA resource footprint. This architecture is described in portable VHDL RTL and is implemented, validated and demonstrated on a commercially available Xilinx KCU105 development board hosting a Xilinx Kintex Ultrascale XCKU040 SRAM FPGA, and thus, is directly transferable to Xilinx Radiation Tolerant Kintex UltraScale XQRKU060 space-grade devices for space deployments. Moreover, state-of-the-art SpaceFibre (ECSS-E-ST-50-11C) serial link interface and test equipment were used in the validation platform to emulate an on-board deployment. The introduced CCSDS-123.0-B-2 Hybrid Entropy Encoder achieves a constant throughput performance of 305 MSamples/s. To the best of our knowledge, this is the first published fully-compliant architecture and high-throughput implementation of the CCSDS-123.0-B-2 Hybrid Entropy Coder, targeting space-grade FPGA technology.",2022-05-09T08:42:03Z,2022-05-09T08:42:03Z,http://arxiv.org/abs/2205.04123v1,http://arxiv.org/pdf/2205.04123v1,"eess.IV, cs.AR"
EVAC+: Multi-scale V-net with Deep Feature CRF Layers for Brain   Extraction,"Jong Sung Park, Shreyas Fadnavis, Eleftherios Garyfallidis","Brain extraction is one of the first steps of pre-processing 3D brain MRI data and a prerequisite for any forthcoming brain imaging analyses. However, it is not a simple segmentation problem due to the complex structure of the brain and human head. Although multiple solutions have been proposed in the literature, we are still far from having truly robust methods. While previous methods have used machine learning with structural/geometric priors, with the development of Deep Learning (DL), there has been an increase in proposed Neural Network architectures. Most models focus on improving the training data and loss functions with little change in the architecture. However, the amount of accessible training data with expert-labelled ground truth vary between groups. Moreover, the labels are created not from scratch but from outputs of non-DL methods. Thus, most DL method's performance depend on the amount and quality of data one has. In this paper, we propose a novel architecture we call EVAC+ to work around this issue. We show that EVAC+ has 3 major advantages compared to other networks: (1) Multi-scale input with limited random augmentation for efficient learning, (2) a unique way of using Conditional Random Fields Recurrent Layer and (3) a loss function specifically created to enhance this architecture. We compare our model to state-of-the-art non-DL and DL methods. Results show that even with little change in the traditional architecture and limited training resources, EVAC+ achieves a high and stable Dice Coefficient and Jaccard Index along with a desirable lower surface distance. Ultimately, our model provides a robust way of accurately reducing segmentation errors in complex multi-tissue interfacing areas of brain.",2022-06-06T18:21:21Z,2023-01-06T02:55:01Z,http://arxiv.org/abs/2206.02837v3,http://arxiv.org/pdf/2206.02837v3,"eess.IV, cs.CV"
"Semantic Communications for Future Internet: Fundamentals, Applications,   and Challenges","Wanting Yang, Hongyang Du, Ziqin Liew, Wei Yang Bryan Lim, Zehui Xiong, Dusit Niyato, Xuefen Chi, Xuemin Sherman Shen, Chunyan Miao","With the increasing demand for intelligent services, the sixth-generation (6G) wireless networks will shift from a traditional architecture that focuses solely on high transmission rate to a new architecture that is based on the intelligent connection of everything. Semantic communication (SemCom), a revolutionary architecture that integrates user as well as application requirements and meaning of information into the data processing and transmission, is predicted to become a new core paradigm in 6G. While SemCom is expected to progress beyond the classical Shannon paradigm, several obstacles need to be overcome on the way to a SemCom-enabled smart wireless Internet. In this paper, we first highlight the motivations and compelling reasons of SemCom in 6G. Then, we outline the major 6G visions and key enabler techniques which lay the foundation of SemCom. Meanwhile, we highlight some benefits of SemCom-empowered 6G and present a SemCom-native 6G network architecture. Next, we show the evolution of SemCom from its introduction to classical SemCom related theory and modern AI-enabled SemCom. Following that, focusing on modern SemCom, we classify SemCom into three categories, i.e., semantic-oriented communication, goal-oriented communication, and semantic-aware communication, and introduce three types of semantic metrics. We then discuss the applications, the challenges and technologies related to semantics and communication. Finally, we introduce future research opportunities. In a nutshell, this paper investigates the fundamentals of SemCom, its applications in 6G networks, and the existing challenges and open issues for further direction.",2022-06-10T12:08:39Z,2022-11-13T14:10:20Z,http://arxiv.org/abs/2207.00427v2,http://arxiv.org/pdf/2207.00427v2,"cs.NI, eess.SP"
A Comparison Study of Deep CNN Architecture in Detecting of Pneumonia,"Al Mohidur Rahman Porag, Md. Mahedi Hasan, Md Taimur Ahad","Pneumonia, a respiratory infection brought on by bacteria or viruses, affects a large number of people, especially in developing and impoverished countries where high levels of pollution, unclean living conditions, and overcrowding are frequently observed, along with insufficient medical infrastructure. Pleural effusion, a condition in which fluids fill the lung and complicate breathing, is brought on by pneumonia. Early detection of pneumonia is essential for ensuring curative care and boosting survival rates. The approach most usually used to diagnose pneumonia is chest X-ray imaging. The purpose of this work is to develop a method for the automatic diagnosis of bacterial and viral pneumonia in digital x-ray pictures. This article first presents the authors' technique, and then gives a comprehensive report on recent developments in the field of reliable diagnosis of pneumonia. In this study, here tuned a state-of-the-art deep convolutional neural network to classify plant diseases based on images and tested its performance. Deep learning architecture is compared empirically. VGG19, ResNet with 152v2, Resnext101, Seresnet152, Mobilenettv2, and DenseNet with 201 layers are among the architectures tested. Experiment data consists of two groups, sick and healthy X-ray pictures. To take appropriate action against plant diseases as soon as possible, rapid disease identification models are preferred. DenseNet201 has shown no overfitting or performance degradation in our experiments, and its accuracy tends to increase as the number of epochs increases. Further, DenseNet201 achieves state-of-the-art performance with a significantly a smaller number of parameters and within a reasonable computing time. This architecture outperforms the competition in terms of testing accuracy, scoring 95%. Each architecture was trained using Keras, using Theano as the backend.",2022-12-30T14:37:32Z,2023-02-14T17:29:50Z,http://arxiv.org/abs/2212.14744v3,http://arxiv.org/pdf/2212.14744v3,"eess.IV, cs.CV, cs.LG"
A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel   Training Applied to Image Recognition Problems,"Axel Klawonn, Martin Lanser, Janine Weber","Deep neural networks (DNNs) and, in particular, convolutional neural networks (CNNs) have brought significant advances in a wide range of modern computer application problems. However, the increasing availability of large amounts of datasets as well as the increasing available computational power of modern computers lead to a steady growth in the complexity and size of DNN and CNN models, respectively, and thus, to longer training times. Hence, various methods and attempts have been developed to accelerate and parallelize the training of complex network architectures. In this work, a novel CNN-DNN architecture is proposed that naturally supports a model parallel training strategy and that is loosely inspired by two-level domain decomposition methods (DDM). First, local CNN models, that is, subnetworks, are defined that operate on overlapping or nonoverlapping parts of the input data, for example, sub-images. The subnetworks can be trained completely in parallel and independently of each other. Each subnetwork then outputs a local decision for the given machine learning problem which is exclusively based on the respective local input data. Subsequently, in a second step, an additional DNN model is trained which evaluates the local decisions of the local subnetworks and generates a final, global decision. In this paper, we apply the proposed architecture to image classification problems using CNNs. Experimental results for different 2D image classification problems are provided as well as a face recognition problem, and a classification problem for 3D computer tomography (CT) scans. Therefore, classical ResNet and VGG architectures are considered. The results show that the proposed approach can significantly accelerate the required training time compared to the global model and, additionally, can also help to improve the accuracy of the underlying classification problem.",2023-02-13T18:06:59Z,2024-07-01T19:12:49Z,http://arxiv.org/abs/2302.06564v2,http://arxiv.org/pdf/2302.06564v2,"cs.LG, cs.CV, 68T07, 68W10, 68W15, 65N55, I.2.6"
Reactive Multi-agent Coordination using Auction-based Task Allocation   and Behavior Trees,"Niklas Dahlquist, Björn Lindqvist, Akshit Saradagi, George Nikolakopoulos","This article presents an architecture for multi-agent task allocation and task execution, through the unification of a market-inspired task-auctioning system with Behavior Trees for managing and executing lower level behaviors. We consider the scenario with multi-stage tasks, such as 'pick and place', whose arrival times are not known a priori. In such a scenario, a coordinating architecture is expected to be reactive to newly arrived tasks and the resulting rerouting of agents should be dependent on the stage of completion of their current multi-stage tasks. In the novel architecture proposed in this article, a central auctioning system gathers bids (cost-estimates for completing currently available tasks) from all agents, and solves a combinatorial problem to optimally assign tasks to agents. For every agent, it's participation in the auctioning system and execution of an assigned multi-stage task is managed using behavior trees, which switch among several well-defined behaviors in response to changing scenarios. The auctioning system is run at a fixed rate, allowing for newly added tasks to be incorporated into the auctioning system, which makes the solution reactive and allows for the rerouting of some agents (subject to the states of the behavior trees). We demonstrate that the proposed architecture is especially well-suited for multi-stage tasks, where high costs are incurred when rerouting agents who have completed one or more stages of their current tasks. The scalability analysis of the proposed architecture reveals that it scales well with the number of agents and number of tasks. The proposed framework is experimentally validated in multiple scenarios in a lab environment. A video of a demonstration can be viewed at: https://youtu.be/ZdEkoOOlB2g}.",2023-04-04T17:32:07Z,2023-04-04T17:32:07Z,http://arxiv.org/abs/2304.01976v1,http://arxiv.org/pdf/2304.01976v1,"cs.RO, cs.SY, eess.SY"
Mapping quantum circuits to modular architectures with QUBO,"Medina Bandic, Luise Prielinger, Jonas Nüßlein, Anabel Ovide, Santiago Rodrigo, Sergi Abadal, Hans van Someren, Gayane Vardoyan, Eduard Alarcon, Carmen G. Almudever, Sebastian Feld","Modular quantum computing architectures are a promising alternative to monolithic QPU (Quantum Processing Unit) designs for scaling up quantum devices. They refer to a set of interconnected QPUs or cores consisting of tightly coupled quantum bits that can communicate via quantum-coherent and classical links. In multi-core architectures, it is crucial to minimize the amount of communication between cores when executing an algorithm. Therefore, mapping a quantum circuit onto a modular architecture involves finding an optimal assignment of logical qubits (qubits in the quantum circuit) to different cores with the aim to minimize the number of expensive inter-core operations while adhering to given hardware constraints. In this paper, we propose for the first time a Quadratic Unconstrained Binary Optimization (QUBO) technique to encode the problem and the solution for both qubit allocation and inter-core communication costs in binary decision variables. To this end, the quantum circuit is split into slices, and qubit assignment is formulated as a graph partitioning problem for each circuit slice. The costly inter-core communication is reduced by penalizing inter-core qubit communications. The final solution is obtained by minimizing the overall cost across all circuit slices. To evaluate the effectiveness of our approach, we conduct a detailed analysis using a representative set of benchmarks having a high number of qubits on two different multi-core architectures. Our method showed promising results and performed exceptionally well with very dense and highly-parallelized circuits that require on average 0.78 inter-core communications per two-qubit gate.",2023-05-11T09:45:47Z,2023-05-11T09:45:47Z,http://arxiv.org/abs/2305.06687v1,http://arxiv.org/pdf/2305.06687v1,"quant-ph, cs.ET"
Darwin: A DRAM-based Multi-level Processing-in-Memory Architecture for   Data Analytics,"Donghyuk Kim, Jae-Young Kim, Wontak Han, Jongsoon Won, Haerang Choi, Yongkee Kwon, Joo-Young Kim","Processing-in-memory (PIM) architecture is an inherent match for data analytics application, but we observe major challenges to address when accelerating it using PIM. In this paper, we propose Darwin, a practical LRDIMM-based multi-level PIM architecture for data analytics, which fully exploits the internal bandwidth of DRAM using the bank-, bank group-, chip-, and rank-level parallelisms. Considering the properties of data analytics operators and DRAM's area constraints, Darwin maximizes the internal data bandwidth by placing the PIM processing units, buffers, and control circuits across the hierarchy of DRAM. More specifically, it introduces the bank processing unit for each bank in which a single instruction multiple data (SIMD) unit handles regular data analytics operators and bank group processing unit for each bank group to handle workload imbalance in the condition-oriented data analytics operators. Furthermore, Darwin supports a novel PIM instruction architecture that concatenates instructions for multiple thread executions on bank group processing entities, addressing the command bottleneck by enabling separate control of up to 512 different in-memory processing units simultaneously. We build a cycle-accurate simulation framework to evaluate Darwin with various DRAM configurations, optimization schemes and workloads. Darwin achieves up to 14.7x speedup over the non-optimized version. Finally, the proposed Darwin architecture achieves 4.0x-43.9x higher throughput and reduces energy consumption by 85.7% than the baseline CPU system (Intel Xeon Gold 6226 + 4 channels of DDR4-2933). Compared to the state-of-the-art PIM, Darwin achieves up to 7.5x and 7.1x in the basic query operators and TPC-H queries, respectively. Darwin is based on the latest GDDR6 and requires only 5.6% area overhead, suggesting a promising PIM solution for the future main memory system.",2023-05-23T11:53:30Z,2023-05-23T11:53:30Z,http://arxiv.org/abs/2305.13970v1,http://arxiv.org/pdf/2305.13970v1,"eess.SY, cs.SY"
"A Scalable, Fast and Programmable Neural Decoder for Fault-Tolerant   Quantum Computation Using Surface Codes","Mengyu Zhang, Xiangyu Ren, Guanglei Xi, Zhenxing Zhang, Qiaonian Yu, Fuming Liu, Hualiang Zhang, Shengyu Zhang, Yi-Cong Zheng","Quantum error-correcting codes (QECCs) can eliminate the negative effects of quantum noise, the major obstacle to the execution of quantum algorithms. However, realizing practical quantum error correction (QEC) requires resolving many challenges to implement a high-performance real-time decoding system. Many decoding algorithms have been proposed and optimized in the past few decades, of which neural network (NNs) based solutions have drawn an increasing amount of attention due to their high efficiency. Unfortunately, previous works on neural decoders are still at an early stage and have only relatively simple architectures, which makes them unsuitable for practical QEC. In this work, we propose a scalable, fast, and programmable neural decoding system to meet the requirements of FTQEC for rotated surface codes (RSC). Firstly, we propose a hardware-efficient NN decoding algorithm with relatively low complexity and high accuracy. Secondly, we develop a customized hardware decoder with architectural optimizations to reduce latency. Thirdly, our proposed programmable architecture boosts the scalability and flexibility of the decoder by maximizing parallelism. Fourthly, we build an FPGA-based decoding system with integrated control hardware for evaluation. Our $L=5$ ($L$ is the code distance) decoder achieves an extremely low decoding latency of 197 ns, and the $L=7$ configuration also requires only 1.136 $\mu$s, both taking $2L$ rounds of syndrome measurements. The accuracy results of our system are close to minimum weight perfect matching (MWPM). Furthermore, our programmable architecture reduces hardware resource consumption by up to $3.0\times$ with only a small latency loss. We validated our approach in real-world scenarios by conducting a proof-of-concept benchmark with practical noise models, including one derived from experimental data gathered from physical hardware.",2023-05-25T06:23:32Z,2023-05-25T06:23:32Z,http://arxiv.org/abs/2305.15767v1,http://arxiv.org/pdf/2305.15767v1,"quant-ph, cs.AR"
OSNet & MNetO: Two Types of General Reconstruction Architectures for   Linear Computed Tomography in Multi-Scenarios,"Zhisheng Wang, Zihan Deng, Fenglin Liu, Yixing Huang, Haijun Yu, Junning Cui","Recently, linear computed tomography (LCT) systems have actively attracted attention. To weaken projection truncation and image the region of interest (ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective solution. However, in BPF for LCT, it is difficult to achieve stable interior reconstruction, and for differentiated backprojection (DBP) images of LCT, multiple rotation-finite inversion of Hilbert transform (Hilbert filtering)-inverse rotation operations will blur the image. To satisfy multiple reconstruction scenarios for LCT, including interior ROI, complete object, and exterior region beyond field-of-view (FOV), and avoid the rotation operations of Hilbert filtering, we propose two types of reconstruction architectures. The first overlays multiple DBP images to obtain a complete DBP image, then uses a network to learn the overlying Hilbert filtering function, referred to as the Overlay-Single Network (OSNet). The second uses multiple networks to train different directional Hilbert filtering models for DBP images of multiple linear scannings, respectively, and then overlays the reconstructed results, i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both local and global features from DBP images at the same time. We investigate two architectures from different networks, FOV sizes, pixel sizes, number of projections, geometric magnification, and processing time. Experimental results show that two architectures can both recover images. OSNet outperforms BPF in various scenarios. For the different networks, ST-pix2pixGAN is superior to pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences among the multiple models, but any one of its models is suitable for imaging the exterior edge in a certain direction.",2023-09-21T07:59:58Z,2023-09-25T14:25:59Z,http://arxiv.org/abs/2309.11858v2,http://arxiv.org/pdf/2309.11858v2,"cs.CV, cs.AI, 68T07(Primary) 68U10, 68T20(Secondary)"
Deep Equilibrium Based Neural Operators for Steady-State PDEs,"Tanya Marwah, Ashwini Pokle, J. Zico Kolter, Zachary C. Lipton, Jianfeng Lu, Andrej Risteski","Data-driven machine learning approaches are being increasingly used to solve partial differential equations (PDEs). They have shown particularly striking successes when training an operator, which takes as input a PDE in some family, and outputs its solution. However, the architectural design space, especially given structural knowledge of the PDE family of interest, is still poorly understood. We seek to remedy this gap by studying the benefits of weight-tied neural network architectures for steady-state PDEs. To achieve this, we first demonstrate that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator. Motivated by this observation, we propose FNO-DEQ, a deep equilibrium variant of the FNO architecture that directly solves for the solution of a steady-state PDE as the infinite-depth fixed point of an implicit operator layer using a black-box root solver and differentiates analytically through this fixed point resulting in $\mathcal{O}(1)$ training memory. Our experiments indicate that FNO-DEQ-based architectures outperform FNO-based baselines with $4\times$ the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when trained with datasets with more noisy observations than the FNO-based baselines, demonstrating the benefits of using appropriate inductive biases in architectural design for different neural network based PDE solvers. Further, we show a universal approximation result that demonstrates that FNO-DEQ can approximate the solution to any steady-state PDE that can be written as a fixed point equation.",2023-11-30T22:34:57Z,2023-11-30T22:34:57Z,http://arxiv.org/abs/2312.00234v1,http://arxiv.org/pdf/2312.00234v1,"cs.LG, cs.NA, math.NA, stat.ML"
Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC   Middleware: Applications in Quantum Simulations,"Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang, Chen-Yu Liu","Achieving high-performance computation on quantum systems presents a formidable challenge that necessitates bridging the capabilities between quantum hardware and classical computing resources. This study introduces an innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture, which integrates cutting-edge quantum software framework works with high-performance classical computing resources to address challenges in quantum simulation for materials and condensed matter physics. At the heart of this architecture is the seamless integration of VQE algorithms running on QPUs for efficient quantum state preparation, Tensor Network states, and QCNNs for classifying quantum states on classical hardware.   For benchmarking quantum simulators, the QCQ architecture utilizes the cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's Lightning plugin, demonstrating up to tenfold increases in computational speed for complex phase transition classification tasks compared to traditional CPU-based methods. This significant acceleration enables models such as the transverse field Ising and XXZ systems to accurately predict phase transitions with a 99.5% accuracy. The architecture's ability to distribute computation between QPUs and classical resources addresses critical bottlenecks in Quantum-HPC, paving the way for scalable quantum simulation.   The QCQ framework embodies a synergistic combination of quantum algorithms, machine learning, and Quantum-HPC capabilities, enhancing its potential to provide transformative insights into the behavior of quantum systems across different scales. As quantum hardware continues to improve, this hybrid distribution-aware framework will play a crucial role in realizing the full potential of quantum computing by seamlessly integrating distributed quantum resources with the state-of-the-art classical computing infrastructure.",2024-03-09T07:38:45Z,2024-03-18T08:54:10Z,http://arxiv.org/abs/2403.05828v2,http://arxiv.org/pdf/2403.05828v2,"quant-ph, cs.AI, cs.AR, cs.DC"
An extrapolation-driven network architecture for physics-informed deep   learning,"Yong Wang, Yanzhong Yao, Zhiming Gao","Current PINN implementations with sequential learning strategies often experience some weaknesses, such as the failure to reproduce the previous training results when using a single network, the difficulty to strictly ensure continuity and smoothness at the time interval nodes when using multiple networks, and the increase in complexity and computational overhead. To overcome these shortcomings, we first investigate the extrapolation capability of the PINN method for time-dependent PDEs. Taking advantage of this extrapolation property, we generalize the training result obtained in a specific time subinterval to larger intervals by adding a correction term to the network parameters of the subinterval. The correction term is determined by further training with the sample points in the added subinterval. Secondly, by designing an extrapolation control function with special characteristics and combining it with a correction term, we construct a new neural network architecture whose network parameters are coupled with the time variable, which we call the extrapolation-driven network architecture. Based on this architecture, using a single neural network, we can obtain the overall PINN solution of the whole domain with the following two characteristics: (1) it completely inherits the local solution of the interval obtained from the previous training, (2) at the interval node, it strictly maintains the continuity and smoothness that the true solution has. The extrapolation-driven network architecture allows us to divide a large time domain into multiple subintervals and solve the time-dependent PDEs one by one in a chronological order. This training scheme respects the causality principle and effectively overcomes the difficulties of the conventional PINN method in solving the evolution equation on a large time domain. Numerical experiments verify the performance of our method.",2024-06-18T10:08:30Z,2024-11-29T13:06:31Z,http://arxiv.org/abs/2406.12460v4,http://arxiv.org/pdf/2406.12460v4,"math.NA, cs.NA"
COAC: Cross-layer Optimization of Accelerator Configurability for   Efficient CNN Processing,"Steven Colleman, Man Shi, Marian Verhelst","To achieve high accuracy, convolutional neural networks (CNNs) are increasingly growing in complexity and diversity in layer types and topologies. This makes it very challenging to efficiently deploy such networks on custom processor architectures for resource-scarce edge devices. Existing mapping exploration frameworks enable searching for the optimal execution schedules or hardware mappings of individual network layers, by optimizing each layer's spatial (dataflow parallelization) and temporal unrolling (execution order). However, these tools fail to take into account the overhead of supporting different unrolling schemes within a common hardware architecture. Using a fixed unrolling scheme across all layers is also not ideal, as this misses significant opportunities for energy and latency savings from optimizing the mapping of diverse layer types. A balanced approach assesses the right amount of mapping flexibility needed across target neural networks, while taking into account the overhead to support multiple unrollings. This paper, therefore, presents COAC, a cross-layer design space exploration and mapping framework to optimize the flexibility of neural processing architectures by balancing configurability overhead against resulting energy and latency savings for end-to-end inference. COAC does not only provide a systematical analysis of the architectural overhead in function of the supported spatial unrollings, but also builds an automated flow to find the best unrolling combination(s) for efficient end-to-end inference with limited hardware overhead. Results demonstrate that architectures with carefully optimized flexibility can achieve up to 38% EDP (energy-delay-product) savings for a set of six neural networks at the expense of a relative area increase of 9.5%.",2024-06-19T18:11:52Z,2024-06-19T18:11:52Z,http://arxiv.org/abs/2406.13752v1,http://arxiv.org/pdf/2406.13752v1,"eess.SY, cs.SY"
"Superior Computer Chess with Model Predictive Control, Reinforcement   Learning, and Rollout","Atharva Gundawar, Yuchao Li, Dimitri Bertsekas","In this paper we apply model predictive control (MPC), rollout, and reinforcement learning (RL) methodologies to computer chess. We introduce a new architecture for move selection, within which available chess engines are used as components. One engine is used to provide position evaluations in an approximation in value space MPC/RL scheme, while a second engine is used as nominal opponent, to emulate or approximate the moves of the true opponent player.   We show that our architecture improves substantially the performance of the position evaluation engine. In other words our architecture provides an additional layer of intelligence, on top of the intelligence of the engines on which it is based. This is true for any engine, regardless of its strength: top engines such as Stockfish and Komodo Dragon (of varying strengths), as well as weaker engines.   Structurally, our basic architecture selects moves by a one-move lookahead search, with an intermediate move generated by a nominal opponent engine, and followed by a position evaluation by another chess engine. Simpler schemes that forego the use of the nominal opponent, also perform better than the position evaluator, but not quite by as much. More complex schemes, involving multistep lookahead, may also be used and generally tend to perform better as the length of the lookahead increases.   Theoretically, our methodology relies on generic cost improvement properties and the superlinear convergence framework of Newton's method, which fundamentally underlies approximation in value space, and related MPC/RL and rollout/policy iteration schemes. A critical requirement of this framework is that the first lookahead step should be executed exactly. This fact has guided our architectural choices, and is apparently an important factor in improving the performance of even the best available chess engines.",2024-09-10T13:05:45Z,2024-09-10T13:05:45Z,http://arxiv.org/abs/2409.06477v1,http://arxiv.org/pdf/2409.06477v1,"cs.AI, cs.LG, cs.SY, eess.SY"
Convergence efficiency of quantum gates and circuits,"Linghang Kong, Zimu Li, Zi-Wen Liu","We consider quantum circuit models where the gates are drawn from arbitrary gate ensembles given by probabilistic distributions over certain gate sets and circuit architectures, which we call stochastic quantum circuits. Of main interest in this work is the speed of convergence of stochastic circuits with different gate ensembles and circuit architectures to unitary t-designs. A key motivation for this theory is the varying preference for different gates and circuit architectures in different practical scenarios. In particular, it provides a versatile framework for devising efficient circuits for implementing $t$-designs and relevant applications including random circuit and scrambling experiments, as well as benchmarking the performance of gates and circuit architectures. We examine various important settings in depth. A key aspect of our study is an ""ironed gadget"" model, which allows us to systematically evaluate and compare the convergence efficiency of entangling gates and circuit architectures. Particularly notable results include i) gadgets of two-qubit gates with KAK coefficients $\left(\frac{\pi}{4}-\frac{1}{8}\arccos(\frac{1}{5}),\frac{\pi}{8},\frac{1}{8}\arccos(\frac{1}{5})\right)$ (which we call $\chi$ gates) directly form exact 2- and 3-designs; ii) the iSWAP gate family achieves the best efficiency for convergence to 2-designs under mild conjectures with numerical evidence, even outperforming the Haar-random gate, for generic many-body circuits; iii) iSWAP + complete graph achieve the best efficiency for convergence to 2-designs among all graph circuits. A variety of numerical results are provided to complement our analysis. We also derive robustness guarantees for our analysis against gate perturbations. Additionally, we provide cursory analysis on gates with higher locality and found that the Margolus gate outperforms various other well-known gates.",2024-11-07T17:40:19Z,2024-11-07T17:40:19Z,http://arxiv.org/abs/2411.04898v1,http://arxiv.org/pdf/2411.04898v1,"quant-ph, cond-mat.str-el, cs.CC, cs.IT, math-ph, math.IT, math.MP"
AI-Native Multi-Access Future Networks -- The REASON Architecture,"Konstantinos Katsaros, Ioannis Mavromatis, Kostantinos Antonakoglou, Saptarshi Ghosh, Dritan Kaleshi, Toktam Mahmoodi, Hamid Asgari, Anastasios Karousos, Iman Tavakkolnia, Hossein Safi, Harald Hass, Constantinos Vrontos, Amin Emami, Juan Parra Ullauri, Shadi Moazzeni, Dimitra Simeonidou","The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions.   This paper presents REASON's architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.",2024-11-11T11:10:39Z,2024-11-25T11:58:44Z,http://arxiv.org/abs/2411.06870v2,http://arxiv.org/pdf/2411.06870v2,"cs.NI, cs.AI, cs.SY, eess.SY"
Comparative Analysis of Resource-Efficient CNN Architectures for Brain   Tumor Classification,"Md Ashik Khan, Rafath Bin Zafar Auvee","Accurate brain tumor classification in MRI images is critical for timely diagnosis and treatment planning. While deep learning models like ResNet-18, VGG-16 have shown high accuracy, they often come with increased complexity and computational demands. This study presents a comparative analysis of effective yet simple Convolutional Neural Network (CNN) architecture and pre-trained ResNet18, and VGG16 model for brain tumor classification using two publicly available datasets: Br35H:: Brain Tumor Detection 2020 and Brain Tumor MRI Dataset. The custom CNN architecture, despite its lower complexity, demonstrates competitive performance with the pre-trained ResNet18 and VGG16 models. In binary classification tasks, the custom CNN achieved an accuracy of 98.67% on the Br35H dataset and 99.62% on the Brain Tumor MRI Dataset. For multi-class classification, the custom CNN, with a slight architectural modification, achieved an accuracy of 98.09%, on the Brain Tumor MRI Dataset. Comparatively, ResNet18 and VGG16 maintained high performance levels, but the custom CNNs provided a more computationally efficient alternative. Additionally,the custom CNNs were evaluated using few-shot learning (0, 5, 10, 15, 20, 40, and 80 shots) to assess their robustness, achieving notable accuracy improvements with increased shots. This study highlights the potential of well-designed, less complex CNN architectures as effective and computationally efficient alternatives to deeper, pre-trained models for medical imaging tasks, including brain tumor classification. This study underscores the potential of custom CNNs in medical imaging tasks and encourages further exploration in this direction.",2024-11-23T16:13:40Z,2024-12-23T16:40:32Z,http://arxiv.org/abs/2411.15596v3,http://arxiv.org/pdf/2411.15596v3,"eess.IV, cs.CV, I.2.10 Vision and Scene Understanding, I.4.8 Scene Analysis, 92C55
  Biomedical imaging and signal processing"
Field theoretic study of bilayer membrane fusion: I. Hemifusion   mechanism,"Kirill Katsov, Marcus Mueller, Michael Schick","Self-consistent field theory is used to determine structural and energetic properties of metastable intermediates and unstable transition states involved in the standard stalk mechanism of bilayer membrane fusion. A microscopic model of flexible amphiphilic chains dissolved in hydrophilic solvent is employed to describe these self-assembled structures. We find that the barrier to formation of the initial stalk is much smaller than previously estimated by phenomenological theories. Therefore its creation it is not the rate limiting process. The barrier which is relevant is associated with the rather limited radial expansion of the stalk into a hemifusion diaphragm. It is strongly affected by the architecture of the amphiphile, decreasing as the effective spontaneous curvature of the amphiphile is made more negative. It is also reduced when the tension is increased. At high tension the fusion pore, created when a hole forms in the hemifusion diaphragm, expands without bound. At very low membrane tension, small fusion pores can be trapped in a flickering metastable state. Successful fusion is severely limited by the architecture of the lipids. If the effective spontaneous curvature is not sufficiently negative, fusion does not occur because metastable stalks, whose existence is a seemingly necessary prerequisite, do not form at all. However if the spontaneous curvature is too negative, stalks are so stable that fusion does not occur because the system is unstable either to a phase of stable radial stalks, or to an inverted-hexagonal phase induced by stable linear stalks. Our results on the architecture and tension needed for successful fusion are summarized in a phase diagram.",2003-12-18T05:12:20Z,2004-08-12T18:59:58Z,http://arxiv.org/abs/cond-mat/0312453v2,http://arxiv.org/pdf/cond-mat/0312453v2,"cond-mat.soft, q-bio.QM"
Analysis and Simulation of Delay and Buffer Requirements of   satellite-ATM Networks for TCP/IP Traffic,"Rohit Goyal, Sastri Kota, Raj Jain, Sonia Fahmy, Bobby Vandalore, Jerry Kallaus","In this paper we present a model to study the end-to-end delay performance of a satellite-ATM netowrk. We describe a satellite-ATM network architecture. The architecture presents a trade-off between the on-board switching/processing features and the complexity of the satellite communication systems. The end-to-end delay of a connection passing through a satellite constellation consists of the transmission delay, the uplink and downlink ground terminal-satellite propagation delay, the inter-satellite link delays, the on-board switching, processing and buffering delays. In a broadband satellite network, the propagation and the buffering delays have the most impact on the overall delay. We present an analysis of the propagation and buffering delay components for GEO and LEO systems. We model LEO constellations as satellites evenly spaced in circular orbits around the earth. A simple routing algorithm for LEO systems calculates locally optimal paths for the end-to-end connection. This is used to calculate the end-to-end propagation delays for LEO networks. We present a simulation model to calculate the buffering delay for TCP/IP traffic over ATM ABR and UBR service categories. We apply this model to calculate total end-to-end delays for TCP/IP over satellite-ATM networks.",1998-09-23T14:03:39Z,1998-09-23T14:03:39Z,http://arxiv.org/abs/cs/9809052v1,http://arxiv.org/pdf/cs/9809052v1,"cs.NI, C.2.1"
An Internet Multicast System for the Stock Market,"N. F. Maxemchuk, D. H. Shur","We are moving toward a distributed, international, twenty-four hour, electronic stock exchange. The exchange will use the global Internet, or internet technology. This system is a natural application of multicast because there are a large number of receivers that should receive the same information simultaneously.   The data requirements for the stock exchange are discussed. The current multicast protocols lack the reliability, fairness, and scalability needed in this application. We describe a distributed architecture together with a reliable multicast protocol, a modification of the RMP protocol, that has characteristics appropriate for this application.   The architecture is used in three applications: In the first, we construct a unified stock ticker of the transactions that are being conducted on the various physical and electronic exchanges. Our objective is to deliver the the same combined ticker reliably and simultaneously to all receivers, anywhere in the world. In the second, we construct a unified sequence of buy and sell offers that are delivered to a single exchange or a collection of exchanges. Our objective is to give all traders the same fair access to an exchange independent of their relative distances to the exchange or the loss characteristics of the international network. In the third, we construct a distributed, electronic trading floor that can replace the current exchanges. This application uses the innovations from the first two applications to combine their fairness attributes.",2000-02-17T21:01:10Z,2000-02-17T21:01:10Z,http://arxiv.org/abs/cs/0002011v1,http://arxiv.org/pdf/cs/0002011v1,"cs.NI, C.2"
State Analysis and Aggregation Study for Multicast-based Micro Mobility,Ahmed Helmy,"IP mobility addresses the problem of changing the network point-of-attachment transparently during movement. Mobile IP is the proposed standard by IETF. Several studies, however, have shown that Mobile IP has several drawbacks, such as triangle routing and poor handoff performance. Multicast-based mobility has been proposed as a promising solution to the above problems, incurring less end-to-end delays and fast smooth handoff. Nonetheless, such architecture suffers from multicast state scalability problems with the growth in number of mobile nodes. This architecture also requires ubiquitous multicast deployment and more complex security measures. To alleviate these problems, we propose an intra-domain multicast-based mobility solution. A mobility proxy allocates a multicast address for each mobile that moves to its domain. The mobile uses this multicast address within a domain for micro mobility. Also, aggregation is considered to reduce the multicast state. We conduct multicast state analysis to study the efficiency of several aggregation techniques. We use extensive simulation to evaluate our protocol's performance over a variety of real and generated topologies. We take aggregation gain as metric for our evaluation.   Our simulation results show that in general leaky aggregation obtains better gains than perfect aggregation. Also, we notice that aggregation gain increases with the increase in number of visiting mobile nodes and with the decrease in number of mobility proxies within a domain.",2001-05-23T18:20:36Z,2001-05-23T18:20:36Z,http://arxiv.org/abs/cs/0105031v1,http://arxiv.org/pdf/cs/0105031v1,"cs.NI, C.2;C.2.1;C.2.2"
Efficient Micro-Mobility using Intra-domain Multicast-based Mechanisms   (M&M),"Ahmed Helmy, Muhammad Jaseemuddin, Ganesha Bhaskara","One of the most important metrics in the design of IP mobility protocols is the handover performance. The current Mobile IP (MIP) standard has been shown to exhibit poor handover performance. Most other work attempts to modify MIP to slightly improve its efficiency, while others propose complex techniques to replace MIP. Rather than taking these approaches, we instead propose a new architecture for providing efficient and smooth handover, while being able to co-exist and inter-operate with other technologies. Specifically, we propose an intra-domain multicast-based mobility architecture, where a visiting mobile is assigned a multicast address to use while moving within a domain. Efficient handover is achieved using standard multicast join/prune mechanisms. Two approaches are proposed and contrasted. The first introduces the concept proxy-based mobility, while the other uses algorithmic mapping to obtain the multicast address of visiting mobiles. We show that the algorithmic mapping approach has several advantages over the proxy approach, and provide mechanisms to support it. Network simulation (using NS-2) is used to evaluate our scheme and compare it to other routing-based micro-mobility schemes - CIP and HAWAII. The proactive handover results show that both M&M and CIP shows low handoff delay and packet reordering depth as compared to HAWAII. The reason for M&M's comparable performance with CIP is that both use bi-cast in proactive handover. The M&M, however, handles multiple border routers in a domain, where CIP fails. We also provide a handover algorithm leveraging the proactive path setup capability of M&M, which is expected to outperform CIP in case of reactive handover.",2002-08-16T18:04:10Z,2002-08-16T18:04:10Z,http://arxiv.org/abs/cs/0208025v1,http://arxiv.org/pdf/cs/0208025v1,"cs.NI, C.2.1; C.2.2"
Mapping the Gnutella Network: Properties of Large-Scale Peer-to-Peer   Systems and Implications for System Design,"Matei Ripeanu, Ian Foster, Adriana Iamnitchi","Despite recent excitement generated by the peer-to-peer (P2P) paradigm and the surprisingly rapid deployment of some P2P applications, there are few quantitative evaluations of P2P systems behavior. The open architecture, achieved scale, and self-organizing structure of the Gnutella network make it an interesting P2P architecture to study. Like most other P2P applications, Gnutella builds, at the application level, a virtual network with its own routing mechanisms. The topology of this virtual network and the routing mechanisms used have a significant influence on application properties such as performance, reliability, and scalability. We have built a ""crawler"" to extract the topology of Gnutella's application level network. In this paper we analyze the topology graph and evaluate generated network traffic. Our two major findings are that: (1) although Gnutella is not a pure power-law network, its current configuration has the benefits and drawbacks of a power-law structure, and (2) the Gnutella virtual network topology does not match well the underlying Internet topology, hence leading to ineffective use of the physical networking infrastructure. These findings guide us to propose changes to the Gnutella protocol and implementations that may bring significant performance and scalability improvements. We believe that our findings as well as our measurement and analysis techniques have broad applicability to P2P systems and provide unique insights into P2P system design tradeoffs.",2002-09-25T08:27:35Z,2002-09-25T08:27:35Z,http://arxiv.org/abs/cs/0209028v1,http://arxiv.org/pdf/cs/0209028v1,"cs.DC, cond-mat.stat-mech, cs.NI, C.2.4"
MonALISA : A Distributed Monitoring Service Architecture,"H. B. Newman, I. C. Legrand, P. Galvez, R. Voicu, C. Cirstoiu","The MonALISA (Monitoring Agents in A Large Integrated Services Architecture) system provides a distributed monitoring service. MonALISA is based on a scalable Dynamic Distributed Services Architecture which is designed to meet the needs of physics collaborations for monitoring global Grid systems, and is implemented using JINI/JAVA and WSDL/SOAP technologies. The scalability of the system derives from the use of multithreaded Station Servers to host a variety of loosely coupled self-describing dynamic services, the ability of each service to register itself and then to be discovered and used by any other services, or clients that require such information, and the ability of all services and clients subscribing to a set of events (state changes) in the system to be notified automatically. The framework integrates several existing monitoring tools and procedures to collect parameters describing computational nodes, applications and network performance. It has built-in SNMP support and network-performance monitoring algorithms that enable it to monitor end-to-end network performance as well as the performance and state of site facilities in a Grid. MonALISA is currently running around the clock on the US CMS test Grid as well as an increasing number of other sites. It is also being used to monitor the performance and optimize the interconnections among the reflectors in the VRVS system.",2003-06-16T08:33:44Z,2003-06-16T08:33:44Z,http://arxiv.org/abs/cs/0306096v1,http://arxiv.org/pdf/cs/0306096v1,"cs.DC, H4.3;H5.2;J2;D2.8"
Source-Channel Diversity for Parallel Channels,"J. Nicholas Laneman, Emin Martinian, Gregory W. Wornell, John G. Apostolopoulos","We consider transmitting a source across a pair of independent, non-ergodic channels with random states (e.g., slow fading channels) so as to minimize the average distortion. The general problem is unsolved. Hence, we focus on comparing two commonly used source and channel encoding systems which correspond to exploiting diversity either at the physical layer through parallel channel coding or at the application layer through multiple description source coding.   For on-off channel models, source coding diversity offers better performance. For channels with a continuous range of reception quality, we show the reverse is true. Specifically, we introduce a new figure of merit called the distortion exponent which measures how fast the average distortion decays with SNR. For continuous-state models such as additive white Gaussian noise channels with multiplicative Rayleigh fading, optimal channel coding diversity at the physical layer is more efficient than source coding diversity at the application layer in that the former achieves a better distortion exponent.   Finally, we consider a third decoding architecture: multiple description encoding with a joint source-channel decoding. We show that this architecture achieves the same distortion exponent as systems with optimal channel coding diversity for continuous-state channels, and maintains the the advantages of multiple description systems for on-off channels. Thus, the multiple description system with joint decoding achieves the best performance, from among the three architectures considered, on both continuous-state and on-off channels.",2004-12-29T01:35:49Z,2004-12-29T01:35:49Z,http://arxiv.org/abs/cs/0412113v1,http://arxiv.org/pdf/cs/0412113v1,"cs.IT, math.IT, c.2.1"
"aDORe: a modular, standards-based Digital Object Repository","Herbert Van de Sompel, Jeroen Bekaert, Xiaoming Liu, Luda Balakireva, Thorsten Schwander","This paper describes the aDORe repository architecture, designed and implemented for ingesting, storing, and accessing a vast collection of Digital Objects at the Research Library of the Los Alamos National Laboratory. The aDORe architecture is highly modular and standards-based. In the architecture, the MPEG-21 Digital Item Declaration Language is used as the XML-based format to represent Digital Objects that can consist of multiple datastreams as Open Archival Information System Archival Information Packages (OAIS AIPs).Through an ingestion process, these OAIS AIPs are stored in a multitude of autonomous repositories. A Repository Index keeps track of the creation and location of all the autonomous repositories, whereas an Identifier Locator registers in which autonomous repository a given Digital Object or OAIS AIP resides. A front-end to the complete environment, the OAI-PMH Federator, is introduced for requesting OAIS Dissemination Information Packages (OAIS DIPs). These OAIS DIPs can be the stored OAIS AIPs themselves, or transformations thereof. This front-end allows OAI-PMH harvesters to recurrently and selectively collect batches of OAIS DIPs from aDORe, and hence to create multiple, parallel services using the collected objects. Another front-end, the OpenURL Resolver, is introduced for requesting OAIS Result Sets. An OAIS Result Set is a dissemination of an individual Digital Object or of its constituent datastreams. Both front-ends make use of an MPEG-21 Digital Item Processing Engine to apply services to OAIS AIPs, Digital Objects, or constituent datastreams that were specified in a dissemination request.",2005-02-04T22:09:14Z,2005-02-04T22:09:14Z,http://arxiv.org/abs/cs/0502028v1,http://arxiv.org/pdf/cs/0502028v1,"cs.DL, H 3.7"
A Model Based Approach to Reachability Routing,"Leland Smith, Muthukumar Thirunavukkarasu, Srinidhi Varadarajan, Naren Ramakrishnan","Current directions in network routing research have not kept pace with the latest developments in network architectures, such as peer-to-peer networks, sensor networks, ad-hoc wireless networks, and overlay networks. A common characteristic among all of these new technologies is the presence of highly dynamic network topologies. Currently deployed single-path routing protocols cannot adequately cope with this dynamism, and existing multi-path algorithms make trade-offs which lead to less than optimal performance on these networks. This drives the need for routing protocols designed with the unique characteristics of these networks in mind.   In this paper we propose the notion of reachability routing as a solution to the challenges posed by routing on such dynamic networks. In particular, our formulation of reachability routing provides cost-sensitive multi-path forwarding along with loop avoidance within the confines of the Internet Protocol (IP) architecture. This is achieved through the application of reinforcement learning within a probabilistic routing framework. Following an explanation of our design decisions and a description of the algorithm, we provide an evaluation of the performance of the algorithm on a variety of network topologies. The results show consistently superior performance compared to other reinforcement learning based routing algorithms.",2005-11-14T16:32:21Z,2005-11-14T16:32:21Z,http://arxiv.org/abs/cs/0511053v1,http://arxiv.org/pdf/cs/0511053v1,"cs.NI, C.2.2; I.2.11"
User-Relative Names for Globally Connected Personal Devices,"Bryan Ford, Jacob Strauss, Chris Lesniewski-Laas, Sean Rhea, Frans Kaashoek, Robert Morris","Nontechnical users who own increasingly ubiquitous network-enabled personal devices such as laptops, digital cameras, and smart phones need a simple, intuitive, and secure way to share information and services between their devices. User Information Architecture, or UIA, is a novel naming and peer-to-peer connectivity architecture addressing this need. Users assign UIA names by ""introducing"" devices to each other on a common local-area network, but these names remain securely bound to their target as devices migrate. Multiple devices owned by the same user, once introduced, automatically merge their namespaces to form a distributed ""personal cluster"" that the owner can access or modify from any of his devices. Instead of requiring users to allocate globally unique names from a central authority, UIA enables users to assign their own ""user-relative"" names both to their own devices and to other users. With UIA, for example, Alice can always access her iPod from any of her own personal devices at any location via the name ""ipod"", and her friend Bob can access her iPod via a relative name like ""ipod.Alice"".",2006-03-18T17:27:22Z,2006-03-18T17:27:22Z,http://arxiv.org/abs/cs/0603076v1,http://arxiv.org/pdf/cs/0603076v1,"cs.NI, cs.DC, cs.OS, C.2.1; C.2.2"
Network Coding for Distributed Storage Systems,"Alexandros G. Dimakis, P. Brighten Godfrey, Martin J. Wainwright, Kannan Ramchandran","Peer-to-peer distributed storage systems provide reliable access to data through redundancy spread over nodes across the Internet. A key goal is to minimize the amount of bandwidth used to maintain that redundancy. Storing a file using an erasure code, in fragments spread across nodes, promises to require less redundancy and hence less maintenance bandwidth than simple replication to provide the same level of reliability. However, since fragments must be periodically replaced as nodes fail, a key question is how to generate a new fragment in a distributed way while transferring as little data as possible across the network.   In this paper, we introduce a general technique to analyze storage architectures that combine any form of coding and replication, as well as presenting two new schemes for maintaining redundancy using erasure codes. First, we show how to optimally generate MDS fragments directly from existing fragments in the system. Second, we introduce a new scheme called Regenerating Codes which use slightly larger fragments than MDS but have lower overall bandwidth use. We also show through simulation that in realistic environments, Regenerating Codes can reduce maintenance bandwidth use by 25 percent or more compared with the best previous design--a hybrid of replication and erasure codes--while simplifying system architecture.",2007-02-02T06:55:37Z,2007-02-02T06:55:37Z,http://arxiv.org/abs/cs/0702015v1,http://arxiv.org/pdf/cs/0702015v1,"cs.IT, cs.NI, math.IT"
The architecture of the protein domain universe,Nikolay V. Dokholyan,"Understanding the design of the universe of protein structures may provide insights into protein evolution. We study the architecture of the protein domain universe, which has been found to poses peculiar scale-free properties (Dokholyan et al., Proc. Natl. Acad. Sci. USA 99: 14132-14136 (2002)). We examine the origin of these scale-free properties of the graph of protein domain structures (PDUG) and determine that that the PDUG is not modular, i.e. it does not consist of modules with uniform properties. Instead, we find the PDUG to be self-similar at all scales. We further characterize the PDUG architecture by studying the properties of the hub nodes that are responsible for the scale-free connectivity of the PDUG. We introduce a measure of the betweenness centrality of protein domains in the PDUG and find a power-law distribution of the betweenness centrality values. The scale-free distribution of hubs in the protein universe suggests that a set of specific statistical mechanics models, such as the self-organized criticality model, can potentially identify the principal driving forces of molecular evolution. We also find a gatekeeper protein domain, removal of which partitions the largest cluster into two large sub-clusters. We suggest that the loss of such gatekeeper protein domains in the course of evolution is responsible for the creation of new fold families.",2004-08-12T00:04:06Z,2004-08-12T00:04:06Z,http://arxiv.org/abs/q-bio/0408006v1,http://arxiv.org/pdf/q-bio/0408006v1,"q-bio.MN, cond-mat.stat-mech, q-bio.BM"
Programmable Quantum Networks with Pure States,Alexander Yu. Vlasov,"Modern classical computing devices, except of simplest calculators, have von Neumann architecture, i.e., a part of the memory is used for the program and a part for the data. It is likely, that analogues of such architecture are also desirable for the future applications in quantum computing, communications and control. It is also interesting for the modern theoretical research in the quantum information science and raises challenging questions about an experimental assessment of such a programmable models. Together with some progress in the given direction, such ideas encounter specific problems arising from the very essence of quantum laws. Currently are known two different ways to overcome such problems, sometime denoted as a stochastic and deterministic approach. The presented paper is devoted to the second one, that is also may be called the programmable quantum networks with pure states.   In the paper are discussed basic principles and theoretical models that can be used for the design of such nano-devices, e.g., the conditional quantum dynamics, the Nielsen-Chuang ""no-programming theorem, the idea of deterministic and stochastic quantum gates arrays. Both programmable quantum networks with finite registers and hybrid models with continuous quantum variables are considered. As a basic model for the universal programmable quantum network with pure states and finite program register is chosen a ""Control-Shift"" quantum processor architecture with three buses introduced in earlier works. It is shown also, that quantum cellular automata approach to the construction of an universal programmable quantum computer often may be considered as the particular case of such design.",2005-03-30T14:48:27Z,2005-03-30T14:48:27Z,http://arxiv.org/abs/quant-ph/0503230v1,http://arxiv.org/pdf/quant-ph/0503230v1,"quant-ph, cs.OH"
Critical phenomena in complex networks,"S. N. Dorogovtsev, A. V. Goltsev, J. F. F. Mendes","The combination of the compactness of networks, featuring small diameters, and their complex architectures results in a variety of critical effects dramatically different from those in cooperative systems on lattices. In the last few years, researchers have made important steps toward understanding the qualitatively new critical phenomena in complex networks. We review the results, concepts, and methods of this rapidly developing field. Here we mostly consider two closely related classes of these critical phenomena, namely structural phase transitions in the network architectures and transitions in cooperative models on networks as substrates. We also discuss systems where a network and interacting agents on it influence each other. We overview a wide range of critical phenomena in equilibrium and growing networks including the birth of the giant connected component, percolation, k-core percolation, phenomena near epidemic thresholds, condensation transitions, critical phenomena in spin models placed on networks, synchronization, and self-organized criticality effects in interacting systems on networks. We also discuss strong finite size effects in these systems and highlight open problems and perspectives.",2007-04-30T20:21:37Z,2007-11-16T21:46:13Z,http://arxiv.org/abs/0705.0010v6,http://arxiv.org/pdf/0705.0010v6,"cond-mat.stat-mech, cs.NI, math-ph, math.MP, physics.soc-ph"
Managing Separation of Concerns in Grid Applications Through   Architectural Model Transformations,"David Manset, Herve Verjus, Richard McClatchey","Grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (VO). Most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. As a consequence, Grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. Not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific Grid configurations, platforms and infra-structures. Having neither guidelines nor rules in the design of a Grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. It is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows Grid develop-ments requirements. Because Grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. This paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of Grids.",2007-07-05T11:10:03Z,2007-07-05T11:10:03Z,http://arxiv.org/abs/0707.0761v1,http://arxiv.org/pdf/0707.0761v1,"cs.SE, cs.DC, D.2.11"
Transactional WaveCache: Towards Speculative and Out-of-Order DataFlow   Execution of Memory Operations,"Leandro A. J. Marzulo, Felipe M. G. França, Vítor Santos Costa","The WaveScalar is the first DataFlow Architecture that can efficiently provide the sequential memory semantics required by imperative languages. This work presents an alternative memory ordering mechanism for this architecture, the Transaction WaveCache. Our mechanism maintains the execution order of memory operations within blocks of code, called Waves, but adds the ability to speculatively execute, out-of-order, operations from different waves. This ordering mechanism is inspired by progress in supporting Transactional Memories. Waves are considered as atomic regions and executed as nested transactions. If a wave has finished the execution of all its memory operations, as soon as the previous waves are committed, it can be committed. If a hazard is detected in a speculative Wave, all the following Waves (children) are aborted and re-executed. We evaluate the WaveCache on a set artificial benchmarks. If the benchmark does not access memory often, we could achieve speedups of around 90%. Speedups of 33.1% and 24% were observed on more memory intensive applications, and slowdowns up to 16% arise if memory bandwidth is a bottleneck. For an application full of WAW, WAR and RAW hazards, a speedup of 139.7% was verified.",2007-12-07T15:59:37Z,2007-12-07T15:59:37Z,http://arxiv.org/abs/0712.1167v1,http://arxiv.org/pdf/0712.1167v1,"cs.AR, cs.DC, C.1.3"
Development of Architectures for Internet Telerobotics Systems,Riyanto Bambang,"This paper presents our experience in developing and implementing Internet telerobotics system. Internet telerobotics system refers to a robot system controlled and monitored remotely through the Internet. A robot manipulator with five degrees of freedom, called Mentor, is employed. Client-server architecture is chosen as a platform for our Internet telerobotics system. Three generations of telerobotics systems have evolved in this research. The first generation was based on CGI and two tiered architecture, where a client presents a Graphical User Interface to the user, and utilizes the user's data entry and actions to perform requests to robot server running on a different machine. The second generation was developed using Java. We also employ Java 3D for creating and manipulating 3D geometry of manipulator links and for constructing the structures used in rendering that geometry, resulting in 3D robot movement simulation presented to the users(clients) through their web browser. Recent development in our Internet telerobotics includes object recognition through image captured by a camera, which poses challenging problem, given the undeterministic latency of the Internet. The third generation is centered around the use of CORBA for development platform of distributed internet telerobotics system, aimed at distributing task of telerobotics system.",2008-04-24T10:43:46Z,2008-04-24T10:43:46Z,http://arxiv.org/abs/0804.3891v1,http://arxiv.org/pdf/0804.3891v1,"cs.RO, I.5.5"
Energy and Time Efficient Scheduling of Tasks with Dependencies on   Asymmetric Multiprocessors,"Ioannis Chatzigiannakis, Georgios Giannoulis, Paul G. Spirakis","In this work we study the problem of scheduling tasks with dependencies in multiprocessor architectures where processors have different speeds. We present the preemptive algorithm ""Save-Energy"" that given a schedule of tasks it post processes it to improve the energy efficiency without any deterioration of the makespan. In terms of time efficiency, we show that preemptive scheduling in an asymmetric system can achieve the same or better optimal makespan than in a symmetric system. Motivited by real multiprocessor systems, we investigate architectures that exhibit limited asymmetry: there are two essentially different speeds. Interestingly, this special case has not been studied in the field of parallel computing and scheduling theory; only the general case was studied where processors have $K$ essentially different speeds. We present the non-preemptive algorithm ``Remnants'' that achieves almost optimal makespan. We provide a refined analysis of a recent scheduling method. Based on this analysis, we specialize the scheduling policy and provide an algorithm of $(3 + o(1))$ expected approximation factor. Note that this improves the previous best factor (6 for two speeds). We believe that our work will convince researchers to revisit this well studied scheduling problem for these simple, yet realistic, asymmetric multiprocessor architectures.",2008-04-25T03:16:21Z,2008-06-06T14:21:18Z,http://arxiv.org/abs/0804.4039v2,http://arxiv.org/pdf/0804.4039v2,"cs.DC, cs.DS, cs.PF, C.1.4; D.1.4"
Architecture and Performance Models for QoS-Driven Effective Peering of   Content Delivery Networks,"Mukaddim Pathan, Rajkumar Buyya","The proprietary nature of existing Content Delivery Networks (CDNs) means they are closed and do not naturally cooperate. A CDN is expected to provide high performance Internet content delivery through global coverage, which might be an obstacle for new CDN providers, as well as affecting commercial viability of existing ones. Finding ways for distinct CDNs to coordinate and cooperate with other CDNs is necessary to achieve better overall service, as perceived by end-users, at lower cost. In this paper, we present an architecture to support peering arrangements between CDNs, based on a Virtual Organization (VO) model. Our approach promotes peering among providers, while upholding user perceived performance. This is achieved through proper policy management of negotiated Service Level Agreements (SLAs) between peers. We also present a Quality of Service (QoS)-driven performance modeling approach for peering CDNs in order to predict the user perceived performance. We show that peering between CDNs upholds user perceived performance by satisfying the target QoS. The methodology presented in this paper provides CDNs a way to dynamically distribute user requests to other peers according to different request-redirection policies. The model-based approach helps an overloaded CDN to return to a normal state by offloading excess requests to the peers. It also assists in making concrete QoS guarantee for a CDN provider. Our approach endeavors to achieve scalability and resource sharing among CDNs through effective peering in a user transparent manner, thus evolving past the current landscape where non-cooperative and distinct CDNs exist.",2009-07-28T10:02:15Z,2009-07-28T10:02:15Z,http://arxiv.org/abs/0907.4876v1,http://arxiv.org/pdf/0907.4876v1,"cs.DC, cs.NI, C.2.4"
Hard Data on Soft Errors: A Large-Scale Assessment of Real-World Error   Rates in GPGPU,"Imran S. Haque, Vijay S. Pande","Graphics processing units (GPUs) are gaining widespread use in computational chemistry and other scientific simulation contexts because of their huge performance advantages relative to conventional CPUs. However, the reliability of GPUs in error-intolerant applications is largely unproven. In particular, a lack of error checking and correcting (ECC) capability in the memory subsystems of graphics cards has been cited as a hindrance to the acceptance of GPUs as high-performance coprocessors, but the impact of this design has not been previously quantified.   In this article we present MemtestG80, our software for assessing memory error rates on NVIDIA G80 and GT200-architecture-based graphics cards. Furthermore, we present the results of a large-scale assessment of GPU error rate, conducted by running MemtestG80 on over 20,000 hosts on the Folding@home distributed computing network. Our control experiments on consumer-grade and dedicated-GPGPU hardware in a controlled environment found no errors. However, our survey over cards on Folding@home finds that, in their installed environments, two-thirds of tested GPUs exhibit a detectable, pattern-sensitive rate of memory soft errors. We demonstrate that these errors persist after controlling for overclocking and environmental proxies for temperature, but depend strongly on board architecture.",2009-10-03T02:04:22Z,2009-11-14T04:14:30Z,http://arxiv.org/abs/0910.0505v2,http://arxiv.org/pdf/0910.0505v2,"cs.AR, cs.GR, B.3.4"
Virtual Machine Support for Many-Core Architectures: Decoupling Abstract   from Concrete Concurrency Models,"Stefan Marr, Michael Haupt, Stijn Timbermont, Bram Adams, Theo D'Hondt, Pascal Costanza, Wolfgang De Meuter","The upcoming many-core architectures require software developers to exploit concurrency to utilize available computational power. Today's high-level language virtual machines (VMs), which are a cornerstone of software development, do not provide sufficient abstraction for concurrency concepts. We analyze concrete and abstract concurrency models and identify the challenges they impose for VMs. To provide sufficient concurrency support in VMs, we propose to integrate concurrency operations into VM instruction sets.   Since there will always be VMs optimized for special purposes, our goal is to develop a methodology to design instruction sets with concurrency support. Therefore, we also propose a list of trade-offs that have to be investigated to advise the design of such instruction sets.   As a first experiment, we implemented one instruction set extension for shared memory and one for non-shared memory concurrency. From our experimental results, we derived a list of requirements for a full-grown experimental environment for further research.",2010-02-04T09:48:53Z,2010-02-04T09:48:53Z,http://arxiv.org/abs/1002.0939v1,http://arxiv.org/pdf/1002.0939v1,"cs.DC, cs.AR, cs.PL, cs.SE, D.3.4; D.1.3"
A flexible architecture for modeling and simulation of diffusional   association,"Fiete Haack, Stefan Leye, Adelinde M. Uhrmacher","Up to now, it is not possible to obtain analytical solutions for complex molecular association processes (e.g. Molecule recognition in Signaling or catalysis). Instead Brownian Dynamics (BD) simulations are commonly used to estimate the rate of diffusional association, e.g. to be later used in mesoscopic simulations. Meanwhile a portfolio of diffusional association (DA) methods have been developed that exploit BD.   However, DA methods do not clearly distinguish between modeling, simulation, and experiment settings. This hampers to classify and compare the existing methods with respect to, for instance model assumptions, simulation approximations or specific optimization strategies for steering the computation of trajectories.   To address this deficiency we propose FADA (Flexible Architecture for Diffusional Association) - an architecture that allows the flexible definition of the experiment comprising a formal description of the model in SpacePi, different simulators, as well as validation and analysis methods. Based on the NAM (Northrup-Allison-McCammon) method, which forms the basis of many existing DA methods, we illustrate the structure and functioning of FADA. A discussion of future validation experiments illuminates how the FADA can be exploited in order to estimate reaction rates and how validation techniques may be applied to validate additional features of the model.",2010-02-22T06:38:58Z,2010-02-22T06:38:58Z,http://arxiv.org/abs/1002.4064v1,http://arxiv.org/pdf/1002.4064v1,"cs.CE, q-bio.QM"
"Energy-Efficient Management of Data Center Resources for Cloud   Computing: A Vision, Architectural Elements, and Open Challenges","Rajkumar Buyya, Anton Beloglazov, Jemal Abawajy","Cloud computing is offering utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only save energy for the environment but also reduce operational costs. This paper presents vision, challenges, and architectural elements for energy-efficient management of Cloud computing environments. We focus on the development of dynamic resource provisioning and allocation algorithms that consider the synergy between various data center infrastructures (i.e., the hardware, power units, cooling and software), and holistically work to boost data center energy efficiency and performance. In particular, this paper proposes (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering quality-of-service expectations, and devices power usage characteristics; and (c) a novel software technology for energy-efficient management of Clouds. We have validated our approach by conducting a set of rigorous performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant performance gains as regards to response time and cost saving under dynamic workload scenarios.",2010-06-02T06:45:07Z,2010-06-02T06:45:07Z,http://arxiv.org/abs/1006.0308v1,http://arxiv.org/pdf/1006.0308v1,"cs.DC, C.2.4"
Capacity of All Nine Models of Channel Output Feedback for the Two-user   Interference Channel,"Achaleshwar Sahai, Vaneet Aggarwal, Melda Yuksel, Ashutosh Sabharwal","In this paper, we study the impact of different channel output feedback architectures on the capacity of the two-user interference channel. For a two-user interference channel, a feedback link can exist between receivers and transmitters in 9 canonical architectures (see Fig. 2), ranging from only one feedback link to four feedback links. We derive the exact capacity region for the symmetric deterministic interference channel and the constant-gap capacity region for the symmetric Gaussian interference channel for all of the 9 architectures. We show that for a linear deterministic symmetric interference channel, in the weak interference regime, all models of feedback, except the one, which has only one of the receivers feeding back to its own transmitter, have the identical capacity region. When only one of the receivers feeds back to its own transmitter, the capacity region is a strict subset of the capacity region of the rest of the feedback models in the weak interference regime. However, the sum-capacity of all feedback models is identical in the weak interference regime. Moreover, in the strong interference regime all models of feedback with at least one of the receivers feeding back to its own transmitter have the identical sum-capacity. For the Gaussian interference channel, the results of the linear deterministic model follow, where capacity is replaced with approximate capacity.",2011-04-25T20:36:06Z,2013-01-25T08:19:19Z,http://arxiv.org/abs/1104.4805v3,http://arxiv.org/pdf/1104.4805v3,"cs.IT, math.IT"
SuperNova: Super-peers Based Architecture for Decentralized Online   Social Networks,"Rajesh Sharma, Anwitaman Datta","Recent years have seen several earnest initiatives from both academic researchers as well as open source communities to implement and deploy decentralized online social networks (DOSNs). The primary motivations for DOSNs are privacy and autonomy from big brotherly service providers. The promise of decentralization is complete freedom for end-users from any service providers both in terms of keeping privacy about content and communication, and also from any form of censorship. However decentralization introduces many challenges. One of the principal problems is to guarantee availability of data even when the data owner is not online, so that others can access the said data even when a node is offline or down. In this paper, we argue that a pragmatic design needs to explicitly allow for and leverage on system heterogeneity, and provide incentives for the resource rich participants in the system to contribute such resources. To that end we introduce SuperNova - a super-peer based DOSN architecture. While proposing the SuperNova architecture, we envision a dynamic system driven by incentives and reputation, however, investigation of such incentives and reputation, and its effect on determining peer behaviors is a subject for our future study. In this paper we instead investigate the efficacy of a super-peer based system at any time point (a snap-shot of the envisioned dynamic system), that is to say, we try to quantify the performance of SuperNova system given any (fixed) mix of peer population and strategies.",2011-04-30T10:49:27Z,2011-05-25T13:56:50Z,http://arxiv.org/abs/1105.0074v2,http://arxiv.org/pdf/1105.0074v2,"cs.SI, cs.DC, physics.soc-ph, 68-02, B.4.4"
Receiver Architectures for MIMO-OFDM Based on a Combined VMP-SP   Algorithm,"Carles Navarro Manchón, Gunvor E. Kirkelund, Erwin Riegler, Lars P. B. Christensen, Bernard H. Fleury","Iterative information processing, either based on heuristics or analytical frameworks, has been shown to be a very powerful tool for the design of efficient, yet feasible, wireless receiver architectures. Within this context, algorithms performing message-passing on a probabilistic graph, such as the sum-product (SP) and variational message passing (VMP) algorithms, have become increasingly popular.   In this contribution, we apply a combined VMP-SP message-passing technique to the design of receivers for MIMO-ODFM systems. The message-passing equations of the combined scheme can be obtained from the equations of the stationary points of a constrained region-based free energy approximation. When applied to a MIMO-OFDM probabilistic model, we obtain a generic receiver architecture performing iterative channel weight and noise precision estimation, equalization and data decoding. We show that this generic scheme can be particularized to a variety of different receiver structures, ranging from high-performance iterative structures to low complexity receivers. This allows for a flexible design of the signal processing specially tailored for the requirements of each specific application. The numerical assessment of our solutions, based on Monte Carlo simulations, corroborates the high performance of the proposed algorithms and their superiority to heuristic approaches.",2011-11-24T20:57:18Z,2011-11-24T20:57:18Z,http://arxiv.org/abs/1111.5848v1,http://arxiv.org/pdf/1111.5848v1,"stat.ML, cs.IT, math.IT"
Randomly Evolving Idiotypic Networks: Structural Properties and   Architecture,"Holger Schmidtchen, Mario Thüne, Ulrich Behn","We consider a minimalistic dynamic model of the idiotypic network of B-lymphocytes. A network node represents a population of B-lymphocytes of the same specificity (idiotype), which is encoded by a bitstring. The links of the network connect nodes with complementary and nearly complementary bitstrings, allowing for a few mismatches. A node is occupied if a lymphocyte clone of the corresponding idiotype exists, otherwise it is empty. There is a continuous influx of new B-lymphocytes of random idiotype from the bone marrow. B-lymphocytes are stimulated by cross-linking their receptors with complementary structures. If there are too many complementary structures, steric hindrance prevents cross-linking. Stimulated cells proliferate and secrete antibodies of the same idiotype as their receptors, unstimulated lymphocytes die.   Depending on few parameters, the autonomous system evolves randomly towards patterns of highly organized architecture, where the nodes can be classified into groups according to their statistical properties. We observe and describe analytically the building principles of these patterns, which allow to calculate number and size of the node groups and the number of links between them. The architecture of all patterns observed so far in simulations can be explained this way. A tool for real-time pattern identification is proposed.",2012-01-17T15:25:18Z,2012-01-17T15:25:18Z,http://arxiv.org/abs/1201.3618v1,http://arxiv.org/pdf/1201.3618v1,"q-bio.CB, cond-mat.dis-nn, physics.bio-ph"
"A low-resolution, GSa/s streaming digitizer for a correlation-based   trigger system","Kurtis Nishimura, Matthew Andrew, Zhe Cao, Michael Cooney, Peter Gorham, Luca Macchiarulo, Lisa Ritter, Andres Romero-Wolf, Gary Varner","Searches for radio signatures of ultra-high energy neutrinos and cosmic rays could benefit from improved efficiency by using real-time beamforming or correlation triggering. For missions with power limitations, such as the ANITA-3 Antarctic balloon experiment, full speed high resolution digitization of incoming signals is not practical. To this end, the University of Hawaii has developed the Realtime Independent Three-bit Converter (RITC), a 3-channel, 3-bit, streaming analog-to-digital converter implemented in the IBM-8RF 0.13 um process. RITC is primarily designed to digitize broadband radio signals produced by the Askaryan effect, and thus targets an analog bandwidth of >1 GHz, with a sample-and-hold architecture capable of storing up to 2.6 gigasamples-per-second. An array of flash analog-to-digital converters perform 3-bit conversion of sets of stored samples while acquisition continues elsewhere in the sampling array. A serial interface is provided to access an array of on chip digital-to-analog converters that control the digitization thresholds for each channel as well as the overall sampling rate. Demultiplexed conversion outputs are read out simultaneously for each channel via a set of 36 LVDS links, each running at 650 Mb/s. We briefly describe the design architecture of RITC. Evaluation of the RITC is currently under way, and we will report testing updates as they become available, including prospects for the use of this architecture as the analog half of a novel triggering system for the ANITA-3 ultra-high energy neutrino experiment.",2012-03-19T17:23:13Z,2012-03-19T17:23:13Z,http://arxiv.org/abs/1203.4178v1,http://arxiv.org/pdf/1203.4178v1,"physics.ins-det, astro-ph.IM"
A lightweight dynamic pseudonym identity based authentication and key   agreement protocol without verification tables for multi-server architecture,"Kaiping Xue, Peilin Hong, Changsha Ma","Traditional password based authentication schemes are mostly considered in single server environments. They are unfitted for the multi-server environments from two aspects. On the one hand, users need to register in each server and to store large sets of data, including identities and passwords. On the other hand, servers are required to store a verification table containing user identities and passwords. Recently, On the base on Sood et al.'s protocol(2011), Li et al. proposed an improved dynamic identity based authentication and key agreement protocol for multi-server architecture(2012). Li et al. claims that the proposed scheme can make up the security weaknesses of Sood et al.'s protocol. Unfortunately, our further research shows that Li et al.'s protocol contains several drawbacks and can not resist some types of known attacks, such as replay attack, Deny-of-Service attack, internal attack, eavesdropping attack, masquerade attack, and so on. In this paper, we further propose a light dynamic pseudonym identity based authentication and key agreement protocol for multi-server architecture. In our scheme, service providing servers don't need to maintain verification tables for users. The proposed protocol provides not only the declared security features in Li et al.'s paper, but also some other security features, such as traceability and identity protection.",2012-04-17T16:38:16Z,2012-04-17T16:38:16Z,http://arxiv.org/abs/1204.3831v1,http://arxiv.org/pdf/1204.3831v1,"cs.CR, D.4.6"
Wireless Information and Power Transfer: Architecture Design and   Rate-Energy Tradeoff,"Xun Zhou, Rui Zhang, Chin Keong Ho","Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT).",2012-05-03T05:40:57Z,2013-09-06T02:29:13Z,http://arxiv.org/abs/1205.0618v3,http://arxiv.org/pdf/1205.0618v3,"cs.IT, math.IT"
An overview to Software Architecture in Intrusion Detection System,"Mehdi Bahrami, Mohammad Bahrami","Today by growing network systems, security is a key feature of each network infrastructure. Network Intrusion Detection Systems (IDS) provide defense model for all security threats which are harmful to any network. The IDS could detect and block attack-related network traffic. The network control is a complex model. Implementation of an IDS could make delay in the network. Several software-based network intrusion detection systems are developed. However, the model has a problem with high speed traffic. This paper reviews of many type of software architecture in intrusion detection systems and describes the design and implementation of a high-performance network intrusion detection system that combines the use of software-based network intrusion detection sensors and a network processor board. The network processor which is a hardware-based model could acts as a customized load balancing splitter. This model cooperates with a set of modified content-based network intrusion detection sensors rather than IDS in processing network traffic and controls the high-speed.",2012-05-20T05:38:36Z,2014-02-21T08:38:02Z,http://arxiv.org/abs/1205.4385v2,http://arxiv.org/pdf/1205.4385v2,"cs.SE, cs.CR, cs.DC, cs.NI, D.2.11"
High Accuracy Gravitational Waveforms from Black Hole Binary Inspirals   Using OpenCL,"Justin McKennon, Gary Forrester, Gaurav Khanna","There is a strong need for high-accuracy and efficient modeling of extreme-mass-ratio binary black hole systems because these are strong sources of gravitational waves that would be detected by future observatories. In this article, we present sample results from our Teukolsky EMRI code: a time-domain Teukolsky equation solver (a linear, hyperbolic, partial differential equation solver using finite-differencing), that takes advantage of several mathematical and computational enhancements to efficiently generate long-duration and high-accuracy EMRI waveforms.   We emphasize here the computational advances made in the context of this code. Currently there is considerable interest in making use of many-core processor architectures, such as Nvidia and AMD graphics processing units (GPUs) for scientific computing. Our code uses the Open Computing Language (OpenCL) for taking advantage of the massive parallelism offered by modern GPU architectures. We present the performance of our Teukolsky EMRI code on multiple modern processor architectures and demonstrate the high level of accuracy and performance it is able to achieve. We also present the code's scaling performance on a large supercomputer i.e. NSF's XSEDE resource: Keeneland (a 201 TeraFLOP/s, 120-node HP SL390 system with 240 Intel Xeon 5660 CPUs and 360 NVIDIA Fermi M2070 graphics processors, with the nodes connected by an InfiniBand QDR network).",2012-06-01T18:34:57Z,2012-06-01T18:34:57Z,http://arxiv.org/abs/1206.0270v1,http://arxiv.org/pdf/1206.0270v1,"gr-qc, physics.comp-ph"
Parallel Discrete Event Simulation with Erlang,"Luca Toscano, Gabriele D'Angelo, Moreno Marzolla","Discrete Event Simulation (DES) is a widely used technique in which the state of the simulator is updated by events happening at discrete points in time (hence the name). DES is used to model and analyze many kinds of systems, including computer architectures, communication networks, street traffic, and others. Parallel and Distributed Simulation (PADS) aims at improving the efficiency of DES by partitioning the simulation model across multiple processing elements, in order to enabling larger and/or more detailed studies to be carried out. The interest on PADS is increasing since the widespread availability of multicore processors and affordable high performance computing clusters. However, designing parallel simulation models requires considerable expertise, the result being that PADS techniques are not as widespread as they could be. In this paper we describe ErlangTW, a parallel simulation middleware based on the Time Warp synchronization protocol. ErlangTW is entirely written in Erlang, a concurrent, functional programming language specifically targeted at building distributed systems. We argue that writing parallel simulation models in Erlang is considerably easier than using conventional programming languages. Moreover, ErlangTW allows simulation models to be executed either on single-core, multicore and distributed computing architectures. We describe the design and prototype implementation of ErlangTW, and report some preliminary performance results on multicore and distributed architectures using the well known PHOLD benchmark.",2012-06-13T12:12:21Z,2014-07-24T08:38:15Z,http://arxiv.org/abs/1206.2775v3,http://arxiv.org/pdf/1206.2775v3,"cs.DC, D.1.3; I.6.8"
Distributed Control of Generation in a Transmission Grid with a High   Penetration of Renewables,"Krishnamurthy Dvijotham, Michael Chertkov, Scott Backhaus","Deviations of grid frequency from the nominal frequency are an indicator of the global imbalance between genera- tion and load. Two types of control, a distributed propor- tional control and a centralized integral control, are cur- rently used to keep frequency deviations small. Although generation-load imbalance can be very localized, both controls primarily rely on frequency deviation as their in- put. The time scales of control require the outputs of the centralized integral control to be communicated to distant generators every few seconds. We reconsider this con- trol/communication architecture and suggest a hybrid ap- proach that utilizes parameterized feedback policies that can be implemented in a fully distributed manner because the inputs to these policies are local observables at each generator. Using an ensemble of forecasts of load and time-intermittent generation representative of possible fu- ture scenarios, we perform a centralized off-line stochas- tic optimization to select the generator-specific feedback parameters. These parameters need only be communi- cated to generators once per control period (60 minutes in our simulations). We show that inclusion of local power flows as feedback inputs is crucial and reduces frequency deviations by a factor of ten. We demonstrate our con- trol on a detailed transmission model of the Bonneville Power Administration (BPA). Our findings suggest that a smart automatic and distributed control, relying on ad- vanced off-line and system-wide computations commu- nicated to controlled generators infrequently, may be a viable control and communication architecture solution. This architecture is suitable for a future situation when generation-load imbalances are expected to grow because of increased penetration of time-intermittent generation.",2012-11-19T20:35:01Z,2012-11-19T20:35:01Z,http://arxiv.org/abs/1211.4555v1,http://arxiv.org/pdf/1211.4555v1,"cs.SY, math.OC"
A Composite Design Pattern for Service Injection and Composition of Web   Services for Peer-To-Peer Computing with Service Oriented Architecture,"Vishnuvardhan Mannava, T. Ramesh","In this paper we present a Service Injection and composition Design Pattern for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented design patterns, and amalgamation of the Strategy, Worker Object, and Check-List Design Patterns used to design the Self-Adaptive Systems. It will apply self reconfiguration planes dynamically without the interruption or intervention of the administrator for handling service failures at the servers. When a client requests for a complex service, Service Composition should be done to fulfil the request. If a service is not available in the memory, it will be injected as Aspectual Feature Module code. We used Service Oriented Architecture (SOA) with Web Services in Java to Implement the composite Design Pattern. As far as we know, there are no studies on composition of design patterns for Peer-to-peer computing domain. The pattern is described using a java-like notation for the classes and interfaces. A simple UML class and Sequence diagrams are depicted.",2012-11-22T08:55:25Z,2012-11-22T08:55:25Z,http://arxiv.org/abs/1211.5596v1,http://arxiv.org/pdf/1211.5596v1,"cs.SE, cs.DC, cs.NI, D.2.11; D.2.10; D.3.3; C.2.4; D.1.3"
Bringing simulation to implementation: Presentation of a global approach   in the design of passive solar buildings under humid tropical climates,"François Garde, Harry Boyer, Robert Celaire","In early 1995, a DSM pilot initiative has been launched in the French islands of Guadeloupe and Reunion through a partnership between several public and private partners (the French Public Utility EDF, the University of Reunion Island, low cost housing companies, architects, energy consultants, etc...) to set up standards to improve thermal design of new residential buildings in tropical climates. This partnership led to defining optimized bio-climatic urban planning and architectural designs featuring the use of passive cooling architectural principles (solar shading, natural ventilation) and components, as well as energy efficient systems and technologies. The design and sizing of each architectural component on internal thermal comfort in building has been assessed with a validated thermal and airflow building simulation software (CODYRUN). These technical specifications have been edited in a reference document which has been used to build over 300 new pilot dwellings through the years 1996-1998 in Reunion Island and in Guadeloupe. An experimental monitoring has been made in these first ECODOM dwellings in 1998 and 1999. It will result in experimental validation of impact of the passive cooling strategies on thermal comfort of occupants leading to modify specifications if necessary. The paper present all the methodology used for the elaboration of ECODOM, from the simulations to the experimental results. This follow up is important, as the setting up of the ECODOM standard will be the first step towards the setting up of thermal regulations in the French overseas territories, by the year 2002.",2012-12-18T07:35:35Z,2013-04-15T06:56:00Z,http://arxiv.org/abs/1212.5252v2,http://arxiv.org/pdf/1212.5252v2,"cs.CE, physics.class-ph"
Relational Access Control with Bivalent Permissions in a Social   Web/Collaboration Architecture,"Todd Davies, Mike D. Mintz","We describe an access control model that has been implemented in the web content management framework ""Deme"" (which rhymes with ""team""). Access control in Deme is an example of what we call ""bivalent relation object access control""(BROAC). This model builds on recent work by Giunchiglia et al. on relation-based access control (RelBAC), as well as other work on relational, flexible, fine-grained, and XML access control models. We describe Deme's architecture and review access control models, motivating our approach. BROAC allows for both positive and negative permissions, which may conflict with each other. We argue for the usefulness of defining access control rules as objects in the target database, and for the necessity of resolving permission conflicts in a social Web/collaboration architecture. After describing how Deme access control works, including the precedence relations between different permission types in Deme, we provide several examples of realistic scenarios in which permission conflicts arise, and show how Deme resolves them. Initial performance tests indicate that permission checking scales linearly in time on a practical Deme website.",2013-02-07T22:47:56Z,2013-02-07T22:47:56Z,http://arxiv.org/abs/1302.1901v1,http://arxiv.org/pdf/1302.1901v1,"cs.SI, cs.CR, K.4.3; D.4.6"
Cooperative localization by dual foot-mounted inertial sensors and   inter-agent ranging,"John-Olof Nilsson, Dave Zachariah, Isaac Skog, Peter Händel","The implementation challenges of cooperative localization by dual foot-mounted inertial sensors and inter-agent ranging are discussed and work on the subject is reviewed. System architecture and sensor fusion are identified as key challenges. A partially decentralized system architecture based on step-wise inertial navigation and step-wise dead reckoning is presented. This architecture is argued to reduce the computational cost and required communication bandwidth by around two orders of magnitude while only giving negligible information loss in comparison with a naive centralized implementation. This makes a joint global state estimation feasible for up to a platoon-sized group of agents. Furthermore, robust and low-cost sensor fusion for the considered setup, based on state space transformation and marginalization, is presented. The transformation and marginalization are used to give the necessary flexibility for presented sampling based updates for the inter-agent ranging and ranging free fusion of the two feet of an individual agent. Finally, characteristics of the suggested implementation are demonstrated with simulations and a real-time system implementation.",2013-04-12T15:55:15Z,2013-11-21T18:17:39Z,http://arxiv.org/abs/1304.3663v4,http://arxiv.org/pdf/1304.3663v4,"cs.RO, cs.MA, cs.SY, I.2.9; I.2.11"
Photo-active collagen systems with controlled triple helix architecture,"Giuseppe Tronci, Stephen J. Russell, David J. Wood","The design of photo-active collagen systems is presented as a basis for establishing biomimetic materials with varied network architecture and programmable macroscopic properties. Following in-house isolation of type I collagen, reaction with vinyl-bearing compounds of varied backbone rigidity, i.e. 4-vinylbenzyl chloride (4VBC) and glycidyl methacrylate (GMA), was carried out. TNBS colorimetric assay, 1H-NMR and ATR-FTIR confirmed covalent and tunable functionalization of collagen lysines. Depending on the type and extent of functionalization, controlled stability and thermal denaturation of triple helices were observed via circular dichroism (CD), whereby the hydrogen-bonding capability of introduced moieties was shown to play a major role. Full gel formation was observed following photo-activation of functionalized collagen solutions. The presence of a covalent network only slightly affected collagen triple helix conformation (as observed by WAXS and ATR-FTIR), confirming the structural organization of functionalized collagen precursors. Photo-activated hydrogels demonstrated an increased denaturation temperature (DSC) with respect to native collagen, suggesting that the formation of the covalent network successfully stabilized collagen triple helices. Moreover, biocompatibility and mechanical competence of obtained hydrogels were successfully demonstrated under physiologically-relevant conditions. These results demonstrate that this novel synthetic approach enabled the formation of biocompatible collagen systems with defined network architecture and programmable macroscopic properties, which can only partially be obtained with current synthetic methods.",2013-06-17T10:21:00Z,2013-06-21T10:39:40Z,http://arxiv.org/abs/1306.3799v2,http://arxiv.org/pdf/1306.3799v2,"physics.chem-ph, physics.bio-ph, q-bio.BM"
Architectural improvements and 28 nm FPGA implementation of the APEnet+   3D Torus network for hybrid HPC systems,"Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Francesca Lo Cicero, Pier Stanislao Paolucci, Alessandro Lonardo, Davide Rossetti, Francesco Simula, Laura Tosoratto, Piero Vicini","Modern Graphics Processing Units (GPUs) are now considered accelerators for general purpose computation. A tight interaction between the GPU and the interconnection network is the strategy to express the full potential on capability computing of a multi-GPU system on large HPC clusters; that is the reason why an efficient and scalable interconnect is a key technology to finally deliver GPUs for scientific HPC. In this paper we show the latest architectural and performance improvement of the APEnet+ network fabric, a FPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps of raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC. The board implements a Remote Direct Memory Access (RDMA) protocol that leverages upon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to obtain real zero-copy, low-latency GPU-to-GPU transfers. Finally, we report on the development activities for 2013 focusing on the adoption of the latest generation 28 nm FPGAs and the preliminary tests performed on this new platform.",2013-11-07T17:00:02Z,2013-11-14T19:48:36Z,http://arxiv.org/abs/1311.1741v2,http://arxiv.org/pdf/1311.1741v2,"cs.AR, cs.DC, physics.comp-ph"
Is Joint Training Better for Deep Auto-Encoders?,"Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju","Traditionally, when generative models of data are developed via deep architectures, greedy layer-wise pre-training is employed. In a well-trained model, the lower layer of the architecture models the data distribution conditional upon the hidden variables, while the higher layers model the hidden distribution prior. But due to the greedy scheme of the layerwise training technique, the parameters of lower layers are fixed when training higher layers. This makes it extremely challenging for the model to learn the hidden distribution prior, which in turn leads to a suboptimal model for the data distribution. We therefore investigate joint training of deep autoencoders, where the architecture is viewed as one stack of two or more single-layer autoencoders. A single global reconstruction objective is jointly optimized, such that the objective for the single autoencoders at each layer acts as a local, layer-level regularizer. We empirically evaluate the performance of this joint training scheme and observe that it not only learns a better data model, but also learns better higher layer representations, which highlights its potential for unsupervised feature learning. In addition, we find that the usage of regularizations in the joint training scheme is crucial in achieving good performance. In the supervised setting, joint training also shows superior performance when training deeper models. The joint training framework can thus provide a platform for investigating more efficient usage of different types of regularizers, especially in light of the growing volumes of available unlabeled data.",2014-05-06T17:41:33Z,2015-06-15T23:52:59Z,http://arxiv.org/abs/1405.1380v4,http://arxiv.org/pdf/1405.1380v4,"stat.ML, cs.LG, cs.NE"
Cluster-level tuning of a shallow water equation solver on the Intel MIC   architecture,"Andrey Vladimirov, Cliff Addison","The paper demonstrates the optimization of the execution environment of a hybrid OpenMP+MPI computational fluid dynamics code (shallow water equation solver) on a cluster enabled with Intel Xeon Phi coprocessors. The discussion includes: (1) Controlling the number and affinity of OpenMP threads to optimize access to memory bandwidth; (2) Tuning the inter-operation of OpenMP and MPI to partition the problem for better data locality; (3) Ordering the MPI ranks in a way that directs some of the traffic into faster communication channels; (4) Using efficient peer-to-peer communication between Xeon Phi coprocessors based on the InfiniBand fabric.   With tuning, the application has 90% percent efficiency of parallel scaling up to 8 Intel Xeon Phi coprocessors in 2 compute nodes. For larger problems, scalability is even better, because of the greater computation to communication ratio. However, problems of that size do not fit in the memory of one coprocessor. The performance of the solver on one Intel Xeon Phi coprocessor 7120P exceeds the performance on a dual-socket Intel Xeon E5-2697 v2 CPU by a factor of 1.6x. In a 2-node cluster with 4 coprocessors per compute node, the MIC architecture yields 5.8x more performance than the CPUs. Only one line of legacy Fortran code had to be changed in order to achieve the reported performance on the MIC architecture (not counting changes to the command-line interface). The methodology discussed in this paper is directly applicable to other bandwidth-bound stencil algorithms utilizing a hybrid OpenMP+MPI approach.",2014-08-07T22:53:51Z,2014-08-07T22:53:51Z,http://arxiv.org/abs/1408.1727v1,http://arxiv.org/pdf/1408.1727v1,"cs.MS, cs.CE, cs.DC, physics.comp-ph, physics.flu-dyn"
Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power   Management,"Yanchao Lu, Donghong Wu, Bingsheng He, Xueyan Tang, Jianliang Xu, Minyi Guo","Modern DRAM architectures allow a number of low-power states on individual memory ranks for advanced power management. Many previous studies have taken advantage of demotions on low-power states for energy saving. However, most of the demotion schemes are statically performed on a limited number of pre-selected low-power states, and are suboptimal for different workloads and memory architectures. Even worse, the idle periods are often too short for effective power state transitions, especially for memory intensive applications. Wrong decisions on power state transition incur significant energy and delay penalties. In this paper, we propose a novel memory system design named RAMZzz with rank-aware energy saving optimizations including dynamic page migrations and adaptive demotions. Specifically, we group the pages with similar access locality into the same rank with dynamic page migrations. Ranks have their hotness: hot ranks are kept busy for high utilization and cold ranks can have more lengthy idle periods for power state transitions. We further develop adaptive state demotions by considering all low-power states for each rank and a prediction model to estimate the power-down timeout among states. We experimentally compare our algorithm with other energy saving policies with cycle-accurate simulation. Experiments with benchmark workloads show that RAMZzz achieves significant improvement on energy-delay2 and energy consumption over other energy saving techniques.",2014-09-19T09:30:49Z,2014-09-19T09:30:49Z,http://arxiv.org/abs/1409.5567v1,http://arxiv.org/pdf/1409.5567v1,"cs.PF, cs.AR, cs.OS, B.3.2"
Unsupervised Domain Adaptation by Backpropagation,"Yaroslav Ganin, Victor Lempitsky","Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).   As the training progresses, the approach promotes the emergence of ""deep"" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation.   Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.",2014-09-26T08:22:21Z,2015-02-27T14:54:37Z,http://arxiv.org/abs/1409.7495v2,http://arxiv.org/pdf/1409.7495v2,"stat.ML, cs.LG, cs.NE"
"Pruned Bit-Reversal Permutations: Mathematical Characterization, Fast   Algorithms and Architectures",Mohammad M. Mansour,"A mathematical characterization of serially-pruned permutations (SPPs) employed in variable-length permuters and their associated fast pruning algorithms and architectures are proposed. Permuters are used in many signal processing systems for shuffling data and in communication systems as an adjunct to coding for error correction. Typically only a small set of discrete permuter lengths are supported. Serial pruning is a simple technique to alter the length of a permutation to support a wider range of lengths, but results in a serial processing bottleneck. In this paper, parallelizing SPPs is formulated in terms of recursively computing sums involving integer floor and related functions using integer operations, in a fashion analogous to evaluating Dedekind sums. A mathematical treatment for bit-reversal permutations (BRPs) is presented, and closed-form expressions for BRP statistics are derived. It is shown that BRP sequences have weak correlation properties. A new statistic called permutation inliers that characterizes the pruning gap of pruned interleavers is proposed. Using this statistic, a recursive algorithm that computes the minimum inliers count of a pruned BR interleaver (PBRI) in logarithmic time complexity is presented. This algorithm enables parallelizing a serial PBRI algorithm by any desired parallelism factor by computing the pruning gap in lookahead rather than a serial fashion, resulting in significant reduction in interleaving latency and memory overhead. Extensions to 2-D block and stream interleavers, as well as applications to pruned fast Fourier transforms and LTE turbo interleavers, are also presented. Moreover, hardware-efficient architectures for the proposed algorithms are developed. Simulation results demonstrate 3 to 4 orders of magnitude improvement in interleaving time compared to existing approaches.",2014-10-18T13:03:19Z,2014-10-18T13:03:19Z,http://arxiv.org/abs/1410.4954v1,http://arxiv.org/pdf/1410.4954v1,"cs.IT, math.IT"
Coordinated Multibeam Satellite Co-location: The Dual Satellite Paradigm,"Dimitrios Christopoulos, Shree Krishna Sharma, Symeon Chatzinotas, Jens Krauseand Bjorn Ottersten","In the present article, a new system architecture for the next generation of satellite communication (SatComs) is presented. The key concept lies in the collaboration between multibeam satellites that share one orbital position. Multi-satellite constellations in unique orbital slots offer gradual deployment to cover unpredictable traffic patterns and redundancy to hardware failure advantages. They are also of high relevance during the satellite replacement phases or necessitated by constraints in the maximum communications payload that a single satellite can bear. In this context, the potential gains of advanced architectures, that is architectures enabled by the general class of cooperative and cognitive techniques, are exhibited via a simple paradigm. More specifically, the scenario presented herein, involves two co-existing multibeam satellites which illuminate overlapping coverage areas. Based on this scenario, specific types of cooperative and cognitive techniques are herein considered as candidate technologies that can boost the performance of multibeam satellite constellations. These techniques are compared to conventional frequency splitting configurations in terms of three different criteria, namely the spectral efficiency, the power efficiency and the fairness. Consequently, insightful guidelines for the design of future high throughput constellations of multibeam satellites are given.",2015-03-24T10:56:13Z,2015-03-24T10:56:13Z,http://arxiv.org/abs/1503.06981v1,http://arxiv.org/pdf/1503.06981v1,"cs.IT, math.IT"
Task-based adaptive multiresolution for time-space multi-scale   reaction-diffusion systems on multi-core architectures,"Stéphane Descombes, Max Duarte, Thierry Dumont, Thomas Guillet, Violaine Louvet, Marc Massot","A new solver featuring time-space adaptation and error control has been recently introduced to tackle the numerical solution of stiff reaction-diffusion systems. Based on operator splitting, finite volume adaptive multiresolution and high order time integrators with specific stability properties for each operator, this strategy yields high computational efficiency for large multidimensional computations on standard architectures such as powerful workstations. However, the data structure of the original implementation, based on trees of pointers, provides limited opportunities for efficiency enhancements, while posing serious challenges in terms of parallel programming and load balancing. The present contribution proposes a new implementation of the whole set of numerical methods including Radau5 and ROCK4, relying on a fully different data structure together with the use of a specific library, TBB, for shared-memory, task-based parallelism with work-stealing. The performance of our implementation is assessed in a series of test-cases of increasing difficulty in two and three dimensions on multi-core and many-core architectures, demonstrating high scalability.",2015-06-09T12:53:48Z,2016-10-18T12:28:12Z,http://arxiv.org/abs/1506.04651v3,http://arxiv.org/pdf/1506.04651v3,"cs.NA, cs.DC, math.AP, math.NA"
Service Provider DevOps network capabilities and tools,"Rebecca Steinert, Wolfgang John, Pontus Sköldström, Bertrand Pechenot, András Gulyás, István Pelle, Tamás Lévai, Felicián Németh, Juhoon Kim, Catalin Meirosu, Xuejun Cai, Chunyan Fu, Kostas Pentikousis, Sachin Sharma, Ioanna Papafili, Guido Marchetto, Riccardo Sisto, Fulvio Risso, Per Kreuger, Jan Ekman, Shaoteng Liu, Antonio Manzalini, Apoorv Shukla, Stefan Schmid","This report provides an understanding of how the UNIFY Service Provider (SP)-DevOps concept can be applied and integrated with a combined cloud and transport network NFV architecture. Specifically, the report contains technical descriptions of a set of novel SP-DevOps tools and support functions that facilitate observability, troubleshooting, verification, and VNF development processes. The tools and support functions are described in detail together with their architectural mapping, giving a wider understanding of the SP-DevOps concept as a whole, and how SP-DevOps tools can be used for supporting orchestration and programmability in the UNIFY NFV framework. The concept is further exemplified in a case study for deployment and scaling of an Elastic Firewall.",2015-10-09T20:32:52Z,2015-10-15T14:59:35Z,http://arxiv.org/abs/1510.02818v2,http://arxiv.org/pdf/1510.02818v2,"cs.NI, C.2.1; C.2.2; C.2.3; C.2.4"
"Hybrid RF and Digital Beamformer for Cellular Networks: Algorithms,   Microwave Architectures and Measurements","Vijay Venkateswaran, Florian Pivit, Lei Guan","Modern wireless communication networks, particularly cellular networks utilize multiple antennas to improve the capacity and signal coverage. In these systems, typically an active transceiver is connected to each antenna. However, this one-to-one mapping between transceivers and antennas will dramatically increase the cost and complexity of a large phased antenna array system.   In this paper, firstly we propose a \emph{partially adaptive} beamformer architecture where a reduced number of transceivers with a digital beamformer (DBF) is connected to an increased number of antennas through an RF beamforming network (RFBN). Then, based on the proposed architecture, we present a methodology to derive the minimum number of transceivers that are required for marco-cell and small-cell base stations, respectively. Subsequently, in order to achieve optimal beampatterns with given cellular standard requirements and RF operational constraints, we propose efficient algorithms to jointly design DBF and RFBN. Starting from the proposed algorithms, we specify generic microwave RFBNs for optimal marco-cell and small-cell networks. In order to verify the proposed approaches, we compare the performance of RFBN using simulations and anechoic chamber measurements. Experimental measurement results confirm the robustness and performance of the proposed hybrid DBF-RFBN concept eventually ensuring that theoretical multi-antenna capacity and coverage are achieved at a little incremental cost.",2015-10-09T21:02:53Z,2015-10-09T21:02:53Z,http://arxiv.org/abs/1510.02822v1,http://arxiv.org/pdf/1510.02822v1,"cs.IT, math.IT"
Reduced Complexity Belief Propagation Decoders for Polar Codes,"Jun Lin, Chenrong Xiong, Zhiyuan Yan","Polar codes are newly discovered capacity-achieving codes, which have attracted lots of research efforts. Polar codes can be efficiently decoded by the low-complexity successive cancelation (SC) algorithm and the SC list (SCL) decoding algorithm. The belief propagation (BP) decoding algorithm not only is an alternative to the SC and SCL decoders, but also provides soft outputs that are necessary for joint detection and decoding. Both the BP decoder and the soft cancelation (SCAN) decoder were proposed for polar codes to output soft information about the coded bits. In this paper, first a belief propagation decoding algorithm, called reduced complexity soft cancelation (RCSC) decoding algorithm, is proposed. Let $N$ denote the block length. Our RCSC decoding algorithm needs to store only $5N-3$ log-likelihood ratios (LLRs), significantly less than $4N-2+\frac{N\log_2N}{2}$ and $N(\log_2N+1)$ LLRs needed by the BP and SCAN decoders, respectively, when $N\geqslant 64$. Besides, compared to the SCAN decoding algorithm, our RCSC decoding algorithm eliminates unnecessary additions over the real field. Then the simplified SC (SSC) principle is applied to our RCSC decoding algorithm, and the resulting SSC-aided RCSC (S-RCSC) decoding algorithm further reduces the computational complexity. Finally, based on the S-RCSC decoding algorithm, we propose a corresponding memory efficient decoder architecture, which has better error performance than existing architectures. Besides, our decoder architecture consumes less energy on updating LLRs.",2015-10-22T06:11:43Z,2015-10-22T06:11:43Z,http://arxiv.org/abs/1510.06495v1,http://arxiv.org/pdf/1510.06495v1,"cs.IT, math.IT"
Some Epistemological Problems with the Knowledge Level in Cognitive   Architectures,Antonio Lieto,"This article addresses an open problem in the area of cognitive systems and architectures: namely the problem of handling (in terms of processing and reasoning capabilities) complex knowledge structures that can be at least plausibly comparable, both in terms of size and of typology of the encoded information, to the knowledge that humans process daily for executing everyday activities. Handling a huge amount of knowledge, and selectively retrieve it ac- cording to the needs emerging in different situational scenarios, is an important aspect of human intelligence. For this task, in fact, humans adopt a wide range of heuristics (Gigerenzer and Todd) due to their bounded rationality (Simon, 1957). In this perspective, one of the re- quirements that should be considered for the design, the realization and the evaluation of intelligent cognitively inspired systems should be represented by their ability of heuristically identify and retrieve, from the general knowledge stored in their artificial Long Term Memory (LTM), that one which is synthetically and contextually relevant. This require- ment, however, is often neglected. Currently, artificial cognitive systems and architectures are not able, de facto, to deal with complex knowledge structures that can be even slightly comparable to the knowledge heuris- tically managed by humans. In this paper I will argue that this is not only a technological problem but also an epistemological one and I will briefly sketch a proposal for a possible solution.",2015-11-26T21:31:20Z,2015-11-26T21:31:20Z,http://arxiv.org/abs/1511.08512v1,http://arxiv.org/pdf/1511.08512v1,"cs.AI, I.2.0"
Towards Enabling Broadband for a Billon Plus Population with TV White   Spaces,"Animesh Kumar, Abhay Karandikar, Gaurang Naik, Meghna Khaturia, Shubham Saha, Mahak Arora, Jaspreet Singh","One of the major impediments to providing broadband connectivity in semi-urban and rural India is the lack of robust and affordable backhaul. Fiber connectivity in terms of backhaul that is being planned (or provided) by the Government of India would reach only till rural offices (named Gram Panchayat) in the Indian rural areas. In this exposition, we articulate how TV white space can address the challenge in providing broadband connectivity to a billion plus population within India. The villages can form local Wi-Fi clusters. The problem of connecting the Wi-Fi clusters to the optical fiber points can be addressed using a TV white space based backhaul (middle-mile) network.   The amount of TV white space present in India is very large when compared with the developed world. Therefore, we discuss a backhaul architecture for rural India, which utilizes TV white spaces. We also showcase results from our TV white space testbed, which support the effectiveness of backhaul by using TV white spaces. Our testbed provides a broadband access network to rural population in thirteen villages. The testbed is deployed over an area of $25$km$^2$, and extends seamless broadband connectivity from optical fiber locations or Internet gateways to remote (difficult to connect) rural regions. We also discuss standards and TV white space regulations, which are pertinent to the backhaul architecture mentioned above.",2016-03-07T10:53:02Z,2016-03-07T10:53:02Z,http://arxiv.org/abs/1603.01999v1,http://arxiv.org/pdf/1603.01999v1,"cs.NI, cs.IT, math.IT"
Joint DOA and Frequency Estimation with Sub-Nyquist Sampling,"Liang Liu, Ping Wei","In this paper, to jointly estimate the frequency and the direction-of-arrival(DOA) of the narrowband far-field signals, a novel array receiver architecture is presented by the concept of the sub-Nyquist sampling techniques. In particular, our contribution is threefold. i) First, we propose a time-space union signal reception model for receiving array signals, where the sub-Nyquist sampling techniques and arbitrary array geometries are employed to decrease the time-domain sampling rate and improve the DOA estimation accuracy. A better joint estimation is obtained in the higher time-space union space. ii) Second, two joint estimation algorithms are proposed for the receiving model. One is based on a trilinear decomposition from the third-order tensor theory and the other is based on subspace decomposition. iii) Third, we derive the corresponding Cram\'er\text{-}Rao Bound (CRB) for frequency and DOA estimates. In the case of the branch number of our architecture is equal to the reduction factor of the sampling rate, it is observed that the CRB is robust in terms of the number of signals, while the CRB based on the Nyquist sampling scheme will increase with respect to the number of signals. In addition, the new steer vectors of the union time-space model are completely uncorrelated under the limited number of sensors, which improves the estimation performance. Furthermore, the simulation results demonstrate that our estimates via the receiver architecture associated with the proposed algorithms closely match the CRB according to the noise levels, the branch number and the source number as well.",2016-04-18T08:41:36Z,2017-02-05T10:05:24Z,http://arxiv.org/abs/1604.05037v2,http://arxiv.org/pdf/1604.05037v2,"cs.IT, math.IT"
Myocardial Architecture and Patient Variability in Clinical Patterns of   Atrial Fibrillation,"Kishan A. Manani, Kim Christensen, Nicholas S. Peters","Atrial fibrillation (AF) increases the risk of stroke by a factor of four to five and is the most common abnormal heart rhythm. The progression of AF with age, from short self-terminating episodes to persistence, varies between individuals and is poorly understood. An inability to understand and predict variation in AF progression has resulted in less patient-specific therapy. Likewise, it has been a challenge to relate the microstructural features of heart muscle tissue (myocardial architecture) with the emergent temporal clinical patterns of AF. We use a simple model of activation wavefront propagation on an anisotropic structure, mimicking heart muscle tissue, to show how variation in AF behaviour arises naturally from microstructural differences between individuals. We show that the stochastic nature of progressive transversal uncoupling of muscle strands (e.g., due to fibrosis or gap junctional remodelling), as occurs with age, results in variability in AF episode onset time, frequency, duration, burden and progression between individuals. This is consistent with clinical observations. The uncoupling of muscle strands can cause critical architectural patterns in the myocardium. These critical patterns anchor micro-re-entrant wavefronts and thereby trigger AF. It is the number of local critical patterns of uncoupling as opposed to global uncoupling that determines AF progression. This insight may eventually lead to patient specific therapy when it becomes possible to observe the cellular structure of a patient's heart.",2016-06-13T12:01:23Z,2016-06-13T12:01:23Z,http://arxiv.org/abs/1606.03910v1,http://arxiv.org/pdf/1606.03910v1,"q-bio.TO, physics.bio-ph, physics.med-ph"
"Wireless Information and Power Transfer: Nonlinearity, Waveform Design   and Rate-Energy Tradeoff",Bruno Clerckx,"The design of Wireless Information and Power Transfer (WIPT) has so far relied on an oversimplified and inaccurate linear model of the energy harvester. In this paper, we depart from this linear model and design WIPT considering the rectifier nonlinearity. We develop a tractable model of the rectifier nonlinearity that is flexible enough to cope with general multicarrier modulated input waveforms. Leveraging that model, we motivate and introduce a novel WIPT architecture relying on the superposition of multi-carrier unmodulated and modulated waveforms at the transmitter. The superposed WIPT waveforms are optimized as a function of the channel state information so as to characterize the rate-energy region of the whole system. Analysis and numerical results illustrate the performance of the derived waveforms and WIPT architecture and highlight that nonlinearity radically changes the design of WIPT. We make key and refreshing observations. First, analysis (confirmed by circuit simulations) shows that modulated and unmodulated waveforms are not equally suitable for wireless power delivery, namely modulation being beneficial in single-carrier transmissions but detrimental in multi-carrier transmissions. Second, a multicarrier unmodulated waveform (superposed to a multi-carrier modulated waveform) is useful to enlarge the rate-energy region of WIPT. Third, a combination of power splitting and time sharing is in general the best strategy. Fourth, a non-zero mean Gaussian input distribution outperforms the conventional capacity-achieving zero-mean Gaussian input distribution in multi-carrier transmissions. Fifth, the rectifier nonlinearity is beneficial to system performance and is essential to efficient WIPT design.",2016-07-19T14:29:34Z,2017-09-20T19:03:36Z,http://arxiv.org/abs/1607.05602v3,http://arxiv.org/pdf/1607.05602v3,"cs.IT, cs.NI, math.IT"
Protein Secondary Structure Prediction Using Deep Multi-scale   Convolutional Neural Networks and Next-Step Conditioning,"Akosua Busia, Jasmine Collins, Navdeep Jaitly","Recently developed deep learning techniques have significantly improved the accuracy of various speech and image recognition systems. In this paper we adapt some of these techniques for protein secondary structure prediction. We first train a series of deep neural networks to predict eight-class secondary structure labels given a protein's amino acid sequence information and find that using recent methods for regularization, such as dropout and weight-norm constraining, leads to measurable gains in accuracy. We then adapt recent convolutional neural network architectures--Inception, ReSNet, and DenseNet with Batch Normalization--to the problem of protein structure prediction. These convolutional architectures make heavy use of multi-scale filter layers that simultaneously compute features on several scales, and use residual connections to prevent underfitting. Using a carefully modified version of these architectures, we achieve state-of-the-art performance of 70.0% per amino acid accuracy on the public CB513 benchmark dataset. Finally, we explore additions from sequence-to-sequence learning, altering the model to make its predictions conditioned on both the protein's amino acid sequence and its past secondary structure labels. We introduce a new method of ensembling such a conditional model with our convolutional model, an approach which reaches 70.6% Q8 accuracy on CB513. We argue that these results can be further refined for larger boosts in prediction accuracy through more sophisticated attempts to control overfitting of conditional models. We aim to release the code for these experiments as part of the TensorFlow repository.",2016-11-04T19:32:15Z,2016-11-04T19:32:15Z,http://arxiv.org/abs/1611.01503v1,http://arxiv.org/pdf/1611.01503v1,"cs.LG, q-bio.BM"
Tunable Efficient Unitary Neural Networks (EUNN) and their application   to RNNs,"Li Jing, Yichen Shen, Tena Dubček, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, Marin Soljačić","Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely $\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.",2016-12-15T20:39:15Z,2017-04-03T17:13:38Z,http://arxiv.org/abs/1612.05231v3,http://arxiv.org/pdf/1612.05231v3,"cs.LG, cs.NE, stat.ML"
Parsimonious Inference on Convolutional Neural Networks: Learning and   applying on-line kernel activation rules,"I. Theodorakopoulos, V. Pothos, D. Kastaniotis, N. Fragoulis","A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.",2017-01-18T20:03:12Z,2017-01-31T12:15:43Z,http://arxiv.org/abs/1701.05221v5,http://arxiv.org/pdf/1701.05221v5,"cs.CV, cs.AI, cs.LG, cs.NE, 68T10, 62H30, 68Q32, 68T05, 68Q32, 91E40, I.5; F.1.1; F.4.1; K.3.2; I.4; I.4.8"
OptNet: Differentiable Optimization as a Layer in Neural Networks,"Brandon Amos, J. Zico Kolter","This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.",2017-03-01T18:58:48Z,2021-12-02T17:34:50Z,http://arxiv.org/abs/1703.00443v5,http://arxiv.org/pdf/1703.00443v5,"cs.LG, cs.AI, math.OC, stat.ML"
On the Expressive Power of Overlapping Architectures of Deep Learning,"Or Sharir, Amnon Shashua","Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field). To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.",2017-03-06T19:07:12Z,2018-02-24T14:47:00Z,http://arxiv.org/abs/1703.02065v4,http://arxiv.org/pdf/1703.02065v4,"cs.LG, cs.NE, stat.ML"
Learned Optimizers that Scale and Generalize,"Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando de Freitas, Jascha Sohl-Dickstein","Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on. We release an open source implementation of the meta-training algorithm.",2017-03-14T23:05:54Z,2017-09-07T23:38:09Z,http://arxiv.org/abs/1703.04813v4,http://arxiv.org/pdf/1703.04813v4,"cs.LG, cs.NE, stat.ML"
A Service-Oriented Architecture for Assisting the Authoring of Semantic   Crowd Maps,"Henrique Santos, Vasco Furtado","Although there are increasingly more initiatives for the generation of semantic knowledge based on user participation, there is still a shortage of platforms for regular users to create applications on which semantic data can be exploited and generated automatically. We propose an architecture, called Semantic Maps (SeMaps), for assisting the authoring and hosting of applications in which the maps combine the aggregation of a Geographic Information System and crowd-generated content (called here crowd maps). In these systems, the digital map works as a blackboard for accommodating stories told by people about events they want to share with others typically participating in their social networks. SeMaps offers an environment for the creation and maintenance of sites based on crowd maps with the possibility for the user to characterize semantically that which s/he intends to mark on the map. The designer of a crowd map, by informing a linguistic expression that designates what has to be marked on the maps, is guided in a process that aims to associate a concept from a common-sense base to this linguistic expression. Thus, the crowd maps start to have dominion over common-sense inferential relations that define the meaning of the marker, and are able to make inferences about the network of linked data. This makes it possible to generate maps that have the power to perform inferences and access external sources (such as DBpedia) that constitute information that is useful and appropriate to the context of the map. In this paper we describe the architecture of SeMaps and how it was applied in a crowd map authoring tool.",2017-04-06T14:22:56Z,2017-04-06T14:22:56Z,http://arxiv.org/abs/1704.01855v1,http://arxiv.org/pdf/1704.01855v1,"cs.AI, cs.CY, I.2.4"
Event Stream-Based Process Discovery using Abstract Representations,"Sebastiaan J. van Zelst, Boudewijn F. van Dongen, Wil M. P. van der Aalst","The aim of process discovery, originating from the area of process mining, is to discover a process model based on business process execution data. A majority of process discovery techniques relies on an event log as an input. An event log is a static source of historical data capturing the execution of a business process. In this paper we focus on process discovery relying on online streams of business process execution events. Learning process models from event streams poses both challenges and opportunities, i.e. we need to handle unlimited amounts of data using finite memory and, preferably, constant time. We propose a generic architecture that allows for adopting several classes of existing process discovery techniques in context of event streams. Moreover, we provide several instantiations of the architecture, accompanied by implementations in the process mining tool-kit ProM (http://promtools.org). Using these instantiations, we evaluate several dimensions of stream-based process discovery. The evaluation shows that the proposed architecture allows us to lift process discovery to the streaming domain.",2017-04-25T12:10:35Z,2017-04-25T12:10:35Z,http://arxiv.org/abs/1704.08101v1,http://arxiv.org/pdf/1704.08101v1,"cs.DB, cs.AI, cs.DS, stat.ML"
Spectral Ergodicity in Deep Learning Architectures via Surrogate Random   Matrices,"Mehmet Süzen, Cornelius Weber, Joan J. Cerdà","In this work a novel method to quantify spectral ergodicity for random matrices is presented. The new methodology combines approaches rooted in the metrics of Thirumalai-Mountain (TM) and Kullbach-Leibler (KL) divergence. The method is applied to a general study of deep and recurrent neural networks via the analysis of random matrix ensembles mimicking typical weight matrices of those systems. In particular, we examine circular random matrix ensembles: circular unitary ensemble (CUE), circular orthogonal ensemble (COE), and circular symplectic ensemble (CSE). Eigenvalue spectra and spectral ergodicity are computed for those ensembles as a function of network size. It is observed that as the matrix size increases the level of spectral ergodicity of the ensemble rises, i.e., the eigenvalue spectra obtained for a single realisation at random from the ensemble is closer to the spectra obtained averaging over the whole ensemble. Based on previous results we conjecture that success of deep learning architectures is strongly bound to the concept of spectral ergodicity. The method to compute spectral ergodicity proposed in this work could be used to optimise the size and architecture of deep as well as recurrent neural networks.",2017-04-25T17:26:08Z,2017-07-11T09:57:03Z,http://arxiv.org/abs/1704.08303v3,http://arxiv.org/pdf/1704.08303v3,"stat.ML, cond-mat.stat-mech, cs.LG"
A Digital Neuromorphic Architecture Efficiently Facilitating Complex   Synaptic Response Functions Applied to Liquid State Machines,"Michael R. Smith, Aaron J. Hill, Kristofor D. Carlson, Craig M. Vineyard, Jonathon Donaldson, David R. Follett, Pamela L. Follett, John H. Naegle, Conrad D. James, James B. Aimone","Information in neural networks is represented as weighted connections, or synapses, between neurons. This poses a problem as the primary computational bottleneck for neural networks is the vector-matrix multiply when inputs are multiplied by the neural network weights. Conventional processing architectures are not well suited for simulating neural networks, often requiring large amounts of energy and time. Additionally, synapses in biological neural networks are not binary connections, but exhibit a nonlinear response function as neurotransmitters are emitted and diffuse between neurons. Inspired by neuroscience principles, we present a digital neuromorphic architecture, the Spiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex synaptic response functions without requiring additional hardware components. We consider the paradigm of spiking neurons with temporally coded information as opposed to non-spiking rate coded neurons used in most neural networks. In this paradigm we examine liquid state machines applied to speech recognition and show how a liquid state machine with temporal dynamics maps onto the STPU-demonstrating the flexibility and efficiency of the STPU for instantiating neural algorithms.",2017-03-21T16:12:31Z,2017-03-21T16:12:31Z,http://arxiv.org/abs/1704.08306v1,http://arxiv.org/pdf/1704.08306v1,"q-bio.NC, cs.NE, stat.ML"
Exponential Capacity in an Autoencoder Neural Network with a Hidden   Layer,"Alireza Alemi, Alia Abbara","A fundamental aspect of limitations in learning any computation in neural architectures is characterizing their optimal capacities.   An important, widely-used neural architecture is known as autoencoders where the network reconstructs the input at the output layer via a representation at a hidden layer.   Even though capacities of several neural architectures have been addressed using statistical physics methods, the capacity of autoencoder neural networks is not well-explored.   Here, we analytically show that an autoencoder network of binary neurons with a hidden layer can achieve a capacity that grows exponentially with network size.   The network has fixed random weights encoding a set of dense input patterns into a dense, expanded (or \emph{overcomplete}) hidden layer representation. A set of learnable weights decodes the input patters at the output layer. We perform a mean-field approximation of the model to reduce the model to a perceptron problem with an input-output dependency. Carrying out Gardner's \emph{replica} calculation, we show that as the expansion ratio, defined as the number of hidden units over the number of input units, increases, the autoencoding capacity grows exponentially even when the sparseness or the coding level of the hidden layer representation is changed. The replica-symmetric solution is locally stable and is in good agreement with simulation results obtained using a local learning rule. In addition, the degree of symmetry between the encoding and decoding weights monotonically increases with the expansion ratio.",2017-05-21T12:13:42Z,2017-05-21T12:13:42Z,http://arxiv.org/abs/1705.07441v1,http://arxiv.org/pdf/1705.07441v1,"q-bio.NC, cond-mat.dis-nn"
Compiling quantum circuits to realistic hardware architectures using   temporal planners,"Davide Venturelli, Minh Do, Eleanor Rieffel, Jeremy Frank","To run quantum algorithms on emerging gate-model quantum hardware, quantum circuits must be compiled to take into account constraints on the hardware. For near-term hardware, with only limited means to mitigate decoherence, it is critical to minimize the duration of the circuit. We investigate the application of temporal planners to the problem of compiling quantum circuits to newly emerging quantum hardware. While our approach is general, we focus on compiling to superconducting hardware architectures with nearest neighbor constraints. Our initial experiments focus on compiling Quantum Alternating Operator Ansatz (QAOA) circuits whose high number of commuting gates allow great flexibility in the order in which the gates can be applied. That freedom makes it more challenging to find optimal compilations but also means there is a greater potential win from more optimized compilation than for less flexible circuits. We map this quantum circuit compilation problem to a temporal planning problem, and generated a test suite of compilation problems for QAOA circuits of various sizes to a realistic hardware architecture. We report compilation results from several state-of-the-art temporal planners on this test set. This early empirical evaluation demonstrates that temporal planning is a viable approach to quantum circuit compilation.",2017-05-24T18:52:43Z,2017-12-21T10:41:42Z,http://arxiv.org/abs/1705.08927v2,http://arxiv.org/pdf/1705.08927v2,"quant-ph, cs.AI, cs.ET, cs.SY"
An Iterative BP-CNN Architecture for Channel Decoding,"Fei Liang, Cong Shen, Feng Wu","Inspired by recent advances in deep learning, we propose a novel iterative BP-CNN architecture for channel decoding under correlated noise. This architecture concatenates a trained convolutional neural network (CNN) with a standard belief-propagation (BP) decoder. The standard BP decoder is used to estimate the coded bits, followed by a CNN to remove the estimation errors of the BP decoder and obtain a more accurate estimation of the channel noise. Iterating between BP and CNN will gradually improve the decoding SNR and hence result in better decoding performance. To train a well-behaved CNN model, we define a new loss function which involves not only the accuracy of the noise estimation but also the normality test for the estimation errors, i.e., to measure how likely the estimation errors follow a Gaussian distribution. The introduction of the normality test to the CNN training shapes the residual noise distribution and further reduces the BER of the iterative decoding, compared to using the standard quadratic loss function. We carry out extensive experiments to analyze and verify the proposed framework. The iterative BP-CNN decoder has better BER performance with lower complexity, is suitable for parallel implementation, does not rely on any specific channel model or encoding method, and is robust against training mismatches. All of these features make it a good candidate for decoding modern channel codes.",2017-07-18T15:41:49Z,2017-07-18T15:41:49Z,http://arxiv.org/abs/1707.05697v1,http://arxiv.org/pdf/1707.05697v1,"stat.ML, cs.IT, math.IT"
Single-Channel Multi-talker Speech Recognition with Permutation   Invariant Training,"Yanmin Qian, Xuankai Chang, Dong Yu","Although great progresses have been made in automatic speech recognition (ASR), significant performance degradation is still observed when recognizing multi-talker mixed speech. In this paper, we propose and evaluate several architectures to address this problem under the assumption that only a single channel of mixed signal is available. Our technique extends permutation invariant training (PIT) by introducing the front-end feature separation module with the minimum mean square error (MSE) criterion and the back-end recognition module with the minimum cross entropy (CE) criterion. More specifically, during training we compute the average MSE or CE over the whole utterance for each possible utterance-level output-target assignment, pick the one with the minimum MSE or CE, and optimize for that assignment. This strategy elegantly solves the label permutation problem observed in the deep learning based multi-talker mixed speech separation and recognition systems. The proposed architectures are evaluated and compared on an artificially mixed AMI dataset with both two- and three-talker mixed speech. The experimental results indicate that our proposed architectures can cut the word error rate (WER) by 45.0% and 25.0% relatively against the state-of-the-art single-talker speech recognition system across all speakers when their energies are comparable, for two- and three-talker mixed speech, respectively. To our knowledge, this is the first work on the multi-talker mixed speech recognition on the challenging speaker-independent spontaneous large vocabulary continuous speech task.",2017-07-19T03:48:54Z,2017-07-19T03:48:54Z,http://arxiv.org/abs/1707.06527v1,http://arxiv.org/pdf/1707.06527v1,"cs.SD, cs.CL, cs.LG, eess.AS, I.2.7"
Nonparametric regression using deep neural networks with ReLU activation   function,Johannes Schmidt-Hieber,"Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to $\log n$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates.",2017-08-22T14:25:55Z,2020-09-13T10:18:51Z,http://arxiv.org/abs/1708.06633v5,http://arxiv.org/pdf/1708.06633v5,"math.ST, cs.LG, stat.ML, stat.TH, 62G08"
Machine Learning Inspired Energy-Efficient Hybrid Precoding for MmWave   Massive MIMO Systems,"Xinyu Gao, Linglong Dai, Ying Sun, Shuangfeng Han, I Chih-Lin","Hybrid precoding is a promising technique for mmWave massive MIMO systems, as it can considerably reduce the number of required radio-frequency (RF) chains without obvious performance loss. However, most of the existing hybrid precoding schemes require a complicated phase shifter network, which still involves high energy consumption. In this paper, we propose an energy-efficient hybrid precoding architecture, where the analog part is realized by a small number of switches and inverters instead of a large number of high-resolution phase shifters. Our analysis proves that the performance gap between the proposed hybrid precoding architecture and the traditional one is small and keeps constant when the number of antennas goes to infinity. Then, inspired by the cross-entropy (CE) optimization developed in machine learning, we propose an adaptive CE (ACE)-based hybrid precoding scheme for this new architecture. It aims to adaptively update the probability distributions of the elements in hybrid precoder by minimizing the CE, which can generate a solution close to the optimal one with a sufficiently high probability. Simulation results verify that our scheme can achieve the near-optimal sum-rate performance and much higher energy efficiency than traditional schemes.",2017-08-23T14:34:49Z,2017-08-24T15:32:49Z,http://arxiv.org/abs/1708.07022v2,http://arxiv.org/pdf/1708.07022v2,"cs.IT, math.IT"
Deep learning with convolutional neural networks for decoding and   visualization of EEG pathology,"Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank Hutter, Tonio Ball","We apply convolutional neural networks (ConvNets) to the task of distinguishing pathological from normal EEG recordings in the Temple University Hospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet architectures recently shown to decode task-related information from EEG at least as well as established algorithms designed for this purpose. In decoding EEG pathology, both ConvNets reached substantially better accuracies (about 6% better, ~85% vs. ~79%) than the only published result for this dataset, and were still better when using only 1 minute of each recording for training and only six seconds of each recording for testing. We used automated methods to optimize architectural hyperparameters and found intriguingly different ConvNet architectures, e.g., with max pooling as the only nonlinearity. Visualizations of the ConvNet decoding behavior showed that they used spectral power changes in the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside other features, consistent with expectations derived from spectral analysis of the EEG data and from the textual medical reports. Analysis of the textual medical reports also highlighted the potential for accuracy increases by integrating contextual information, such as the age of subjects. In summary, the ConvNets and visualization techniques used in this study constitute a next step towards clinically useful automated EEG diagnosis and establish a new baseline for future work on this topic.",2017-08-26T19:14:47Z,2018-01-11T20:11:04Z,http://arxiv.org/abs/1708.08012v3,http://arxiv.org/pdf/1708.08012v3,"cs.LG, cs.NE, stat.ML, I.2.6"
"Exploring Architectures, Data and Units For Streaming End-to-End Speech   Recognition with RNN-Transducer","Kanishka Rao, Haşim Sak, Rohit Prabhavalkar","We investigate training end-to-end speech recognition models with the recurrent neural network transducer (RNN-T): a streaming, all-neural, sequence-to-sequence architecture which jointly learns acoustic and language model components from transcribed acoustic data. We explore various model architectures and demonstrate how the model can be improved further if additional text or pronunciation data are available. The model consists of an `encoder', which is initialized from a connectionist temporal classification-based (CTC) acoustic model, and a `decoder' which is partially initialized from a recurrent neural network language model trained on text data alone. The entire neural network is trained with the RNN-T loss and directly outputs the recognized transcript as a sequence of graphemes, thus performing end-to-end speech recognition. We find that performance can be improved further through the use of sub-word units (`wordpieces') which capture longer context and significantly reduce substitution errors. The best RNN-T system, a twelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000 wordpieces as output targets achieves a word error rate of 8.5\% on voice-search and 5.2\% on voice-dictation tasks and is comparable to a state-of-the-art baseline at 8.3\% on voice-search and 5.4\% voice-dictation.",2018-01-02T21:29:41Z,2018-01-02T21:29:41Z,http://arxiv.org/abs/1801.00841v1,http://arxiv.org/pdf/1801.00841v1,"cs.CL, cs.SD, eess.AS"
Deep Convolutional Neural Networks for Eigenvalue Problems in Mechanics,"David Finol, Yan Lu, Vijay Mahadevan, Ankit Srivastava","We show that deep convolutional neural networks (CNN) can massively outperform traditional densely-connected neural networks (both deep or shallow) in predicting eigenvalue problems in mechanics. In this sense, we strike out in a new direction in mechanics computations with strongly predictive NNs whose success depends not only on architectures being deep, but also being fundamentally different from the widely-used to date. We consider a model problem: predicting the eigenvalues of 1-D and 2-D phononic crystals. For the 1-D case, the optimal CNN architecture reaches $98\%$ accuracy level on unseen data when trained with just 20,000 samples, compared to $85\%$ accuracy even with $100,000$ samples for the typical network of choice in mechanics research. We show that, with relatively high data-efficiency, CNNs have the capability to generalize well and automatically learn deep symmetry operations, easily extending to higher dimensions and our 2D case. Most importantly, we show how CNNs can naturally represent mechanical material tensors, with its convolution kernels serving as local receptive fields, which is a natural representation of mechanical response. Strategies proposed are applicable to other mechanics' problems and may, in the future, be used to sidestep cumbersome algorithms with purely data-driven approaches based upon modern deep architectures.",2018-01-17T16:20:32Z,2018-07-17T20:29:59Z,http://arxiv.org/abs/1801.05733v3,http://arxiv.org/pdf/1801.05733v3,"physics.comp-ph, cond-mat.dis-nn"
