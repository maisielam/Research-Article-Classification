{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/daniellai/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from torchmetrics.classification import (MultilabelF1Score, MultilabelPrecision, \n",
    "                                         MultilabelRecall, MultilabelHammingDistance, \n",
    "                                         MultilabelJaccardIndex)\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>link</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>categories</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Physics Sick? [In Praise of Classical Physics]</td>\n",
       "      <td>Hisham Ghassib</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "      <td>2012-09-04T10:32:56Z</td>\n",
       "      <td>2012-09-04T10:32:56Z</td>\n",
       "      <td>http://arxiv.org/abs/1209.0592v1</td>\n",
       "      <td>http://arxiv.org/pdf/1209.0592v1</td>\n",
       "      <td>physics.gen-ph, physics.hist-ph</td>\n",
       "      <td>physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modern Mathematical Physics: what it should be?</td>\n",
       "      <td>Ludwig Faddeev</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "      <td>2000-02-08T13:13:00Z</td>\n",
       "      <td>2000-02-10T10:14:56Z</td>\n",
       "      <td>http://arxiv.org/abs/math-ph/0002018v2</td>\n",
       "      <td>http://arxiv.org/pdf/math-ph/0002018v2</td>\n",
       "      <td>math-ph, hep-th, math.MP</td>\n",
       "      <td>math-stats,physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology in Physics</td>\n",
       "      <td>R. Jackiw</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "      <td>2005-03-15T16:00:59Z</td>\n",
       "      <td>2005-03-15T16:00:59Z</td>\n",
       "      <td>http://arxiv.org/abs/math-ph/0503039v1</td>\n",
       "      <td>http://arxiv.org/pdf/math-ph/0503039v1</td>\n",
       "      <td>math-ph, cond-mat.mes-hall, math.MP, physics.c...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents of Physics Related E-Print Archives</td>\n",
       "      <td>E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "      <td>2003-08-28T13:12:57Z</td>\n",
       "      <td>2003-08-28T13:12:57Z</td>\n",
       "      <td>http://arxiv.org/abs/physics/0308107v1</td>\n",
       "      <td>http://arxiv.org/pdf/physics/0308107v1</td>\n",
       "      <td>physics.data-an</td>\n",
       "      <td>physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fundamental Dilemmas in Theoretical Physics</td>\n",
       "      <td>Hisham Ghassib</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "      <td>2014-05-22T07:49:09Z</td>\n",
       "      <td>2014-05-22T07:49:09Z</td>\n",
       "      <td>http://arxiv.org/abs/1405.5530v1</td>\n",
       "      <td>http://arxiv.org/pdf/1405.5530v1</td>\n",
       "      <td>physics.hist-ph</td>\n",
       "      <td>physic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
       "1    Modern Mathematical Physics: what it should be?   \n",
       "2                                Topology in Physics   \n",
       "3       Contents of Physics Related E-Print Archives   \n",
       "4        Fundamental Dilemmas in Theoretical Physics   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                     Hisham Ghassib   \n",
       "1                                     Ludwig Faddeev   \n",
       "2                                          R. Jackiw   \n",
       "3  E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...   \n",
       "4                                     Hisham Ghassib   \n",
       "\n",
       "                                             summary             published  \\\n",
       "0  In this paper, it is argued that theoretical p...  2012-09-04T10:32:56Z   \n",
       "1  Personal view of author on goals and content o...  2000-02-08T13:13:00Z   \n",
       "2  The phenomenon of quantum number fractionaliza...  2005-03-15T16:00:59Z   \n",
       "3  The frontiers of physics related e-print archi...  2003-08-28T13:12:57Z   \n",
       "4  In this paper, we argue that there are foundat...  2014-05-22T07:49:09Z   \n",
       "\n",
       "                updated                                    link  \\\n",
       "0  2012-09-04T10:32:56Z        http://arxiv.org/abs/1209.0592v1   \n",
       "1  2000-02-10T10:14:56Z  http://arxiv.org/abs/math-ph/0002018v2   \n",
       "2  2005-03-15T16:00:59Z  http://arxiv.org/abs/math-ph/0503039v1   \n",
       "3  2003-08-28T13:12:57Z  http://arxiv.org/abs/physics/0308107v1   \n",
       "4  2014-05-22T07:49:09Z        http://arxiv.org/abs/1405.5530v1   \n",
       "\n",
       "                                  pdf_url  \\\n",
       "0        http://arxiv.org/pdf/1209.0592v1   \n",
       "1  http://arxiv.org/pdf/math-ph/0002018v2   \n",
       "2  http://arxiv.org/pdf/math-ph/0503039v1   \n",
       "3  http://arxiv.org/pdf/physics/0308107v1   \n",
       "4        http://arxiv.org/pdf/1405.5530v1   \n",
       "\n",
       "                                          categories             target  \n",
       "0                    physics.gen-ph, physics.hist-ph             physic  \n",
       "1                           math-ph, hep-th, math.MP  math-stats,physic  \n",
       "2  math-ph, cond-mat.mes-hall, math.MP, physics.c...  math-stats,physic  \n",
       "3                                    physics.data-an             physic  \n",
       "4                                    physics.hist-ph             physic  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "# Load your data – adjust the path and method (read_csv or read_parquet) as needed.\n",
    "data_full = pd.read_parquet(\"/Users/daniellai/MSDS_2026/MSDS_2024_2026/Winter_2025/DSCI521/Project/data/data_final.parquet\")\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150171 entries, 0 to 150170\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   title       150171 non-null  object\n",
      " 1   authors     150171 non-null  object\n",
      " 2   summary     150171 non-null  object\n",
      " 3   published   150171 non-null  object\n",
      " 4   updated     150171 non-null  object\n",
      " 5   link        150171 non-null  object\n",
      " 6   pdf_url     150171 non-null  object\n",
      " 7   categories  150171 non-null  object\n",
      " 8   target      150171 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the text \n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "def normalized_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    tokens = wpt.tokenize(text)\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>link</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>categories</th>\n",
       "      <th>target</th>\n",
       "      <th>full_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Physics Sick? [In Praise of Classical Physics]</td>\n",
       "      <td>Hisham Ghassib</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "      <td>2012-09-04T10:32:56Z</td>\n",
       "      <td>2012-09-04T10:32:56Z</td>\n",
       "      <td>http://arxiv.org/abs/1209.0592v1</td>\n",
       "      <td>http://arxiv.org/pdf/1209.0592v1</td>\n",
       "      <td>physics.gen-ph, physics.hist-ph</td>\n",
       "      <td>physic</td>\n",
       "      <td>in this paper it is argued that theoretical ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modern Mathematical Physics: what it should be?</td>\n",
       "      <td>Ludwig Faddeev</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "      <td>2000-02-08T13:13:00Z</td>\n",
       "      <td>2000-02-10T10:14:56Z</td>\n",
       "      <td>http://arxiv.org/abs/math-ph/0002018v2</td>\n",
       "      <td>http://arxiv.org/pdf/math-ph/0002018v2</td>\n",
       "      <td>math-ph, hep-th, math.MP</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>personal view of author on goals and content o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology in Physics</td>\n",
       "      <td>R. Jackiw</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "      <td>2005-03-15T16:00:59Z</td>\n",
       "      <td>2005-03-15T16:00:59Z</td>\n",
       "      <td>http://arxiv.org/abs/math-ph/0503039v1</td>\n",
       "      <td>http://arxiv.org/pdf/math-ph/0503039v1</td>\n",
       "      <td>math-ph, cond-mat.mes-hall, math.MP, physics.c...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>the phenomenon of quantum number fractionaliza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents of Physics Related E-Print Archives</td>\n",
       "      <td>E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "      <td>2003-08-28T13:12:57Z</td>\n",
       "      <td>2003-08-28T13:12:57Z</td>\n",
       "      <td>http://arxiv.org/abs/physics/0308107v1</td>\n",
       "      <td>http://arxiv.org/pdf/physics/0308107v1</td>\n",
       "      <td>physics.data-an</td>\n",
       "      <td>physic</td>\n",
       "      <td>the frontiers of physics related e print archi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fundamental Dilemmas in Theoretical Physics</td>\n",
       "      <td>Hisham Ghassib</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "      <td>2014-05-22T07:49:09Z</td>\n",
       "      <td>2014-05-22T07:49:09Z</td>\n",
       "      <td>http://arxiv.org/abs/1405.5530v1</td>\n",
       "      <td>http://arxiv.org/pdf/1405.5530v1</td>\n",
       "      <td>physics.hist-ph</td>\n",
       "      <td>physic</td>\n",
       "      <td>in this paper we argue that there are foundati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
       "1    Modern Mathematical Physics: what it should be?   \n",
       "2                                Topology in Physics   \n",
       "3       Contents of Physics Related E-Print Archives   \n",
       "4        Fundamental Dilemmas in Theoretical Physics   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                     Hisham Ghassib   \n",
       "1                                     Ludwig Faddeev   \n",
       "2                                          R. Jackiw   \n",
       "3  E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...   \n",
       "4                                     Hisham Ghassib   \n",
       "\n",
       "                                             summary             published  \\\n",
       "0  In this paper, it is argued that theoretical p...  2012-09-04T10:32:56Z   \n",
       "1  Personal view of author on goals and content o...  2000-02-08T13:13:00Z   \n",
       "2  The phenomenon of quantum number fractionaliza...  2005-03-15T16:00:59Z   \n",
       "3  The frontiers of physics related e-print archi...  2003-08-28T13:12:57Z   \n",
       "4  In this paper, we argue that there are foundat...  2014-05-22T07:49:09Z   \n",
       "\n",
       "                updated                                    link  \\\n",
       "0  2012-09-04T10:32:56Z        http://arxiv.org/abs/1209.0592v1   \n",
       "1  2000-02-10T10:14:56Z  http://arxiv.org/abs/math-ph/0002018v2   \n",
       "2  2005-03-15T16:00:59Z  http://arxiv.org/abs/math-ph/0503039v1   \n",
       "3  2003-08-28T13:12:57Z  http://arxiv.org/abs/physics/0308107v1   \n",
       "4  2014-05-22T07:49:09Z        http://arxiv.org/abs/1405.5530v1   \n",
       "\n",
       "                                  pdf_url  \\\n",
       "0        http://arxiv.org/pdf/1209.0592v1   \n",
       "1  http://arxiv.org/pdf/math-ph/0002018v2   \n",
       "2  http://arxiv.org/pdf/math-ph/0503039v1   \n",
       "3  http://arxiv.org/pdf/physics/0308107v1   \n",
       "4        http://arxiv.org/pdf/1405.5530v1   \n",
       "\n",
       "                                          categories             target  \\\n",
       "0                    physics.gen-ph, physics.hist-ph             physic   \n",
       "1                           math-ph, hep-th, math.MP  math-stats,physic   \n",
       "2  math-ph, cond-mat.mes-hall, math.MP, physics.c...  math-stats,physic   \n",
       "3                                    physics.data-an             physic   \n",
       "4                                    physics.hist-ph             physic   \n",
       "\n",
       "                                          full_title  \n",
       "0  in this paper it is argued that theoretical ph...  \n",
       "1  personal view of author on goals and content o...  \n",
       "2  the phenomenon of quantum number fractionaliza...  \n",
       "3  the frontiers of physics related e print archi...  \n",
       "4  in this paper we argue that there are foundati...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full['full_title'] = data_full['summary'].apply(normalized_text)\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 10% validation, 10% test\n",
    "train_val_df, test_df = train_test_split(data_full, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_texts = train_df['full_title'].tolist()\n",
    "train_targets = train_df['target'].tolist()\n",
    "val_texts = val_df['full_title'].tolist()\n",
    "val_targets = val_df['target'].tolist()\n",
    "test_texts = test_df['full_title'].tolist()\n",
    "test_targets = test_df['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'bio': 0, 'cs': 1, 'econ-qfin': 2, 'eess': 3, 'math-stats': 4, 'physic': 5}\n"
     ]
    }
   ],
   "source": [
    "# Build label mapping from the entire dataset\n",
    "all_targets = data_full['target'].tolist()\n",
    "all_labels = set()\n",
    "for target in all_targets:\n",
    "    all_labels.update(target.split(','))\n",
    "label_to_idx = {label.strip(): idx for idx, label in enumerate(sorted(all_labels))}\n",
    "print(\"Label Mapping:\", label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBERTMultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, label_to_idx, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Remove the batch dimension.\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        \n",
    "        # Convert target string (e.g., \"cs,bio\") to a multi-hot vector.\n",
    "        target_str = self.targets[idx]\n",
    "        label_vec = torch.zeros(len(self.label_to_idx), dtype=torch.float)\n",
    "        for label in target_str.split(','):\n",
    "            label = label.strip()\n",
    "            if label in self.label_to_idx:\n",
    "                label_vec[self.label_to_idx[label]] = 1\n",
    "        item['labels'] = label_vec\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate datasets.\n",
    "train_dataset = SciBERTMultiLabelDataset(train_texts, train_targets, tokenizer, label_to_idx, max_length)\n",
    "val_dataset = SciBERTMultiLabelDataset(val_texts, val_targets, tokenizer, label_to_idx, max_length)\n",
    "test_dataset = SciBERTMultiLabelDataset(test_texts, test_targets, tokenizer, label_to_idx, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handle Imbalance with WeightedRandomSampler\n",
    "def compute_sample_weight(label_vec):\n",
    "    return 1.0 / (label_vec.sum().item() + 1e-6)\n",
    "\n",
    "sample_weights = [compute_sample_weight(train_dataset[i]['labels']) for i in range(len(train_dataset))]\n",
    "sample_weights = np.array(sample_weights)\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma= 3 , alpha=0.75, reduction='mean'):\n",
    "        \"\"\"\n",
    "        gamma: Focusing parameter.\n",
    "        alpha: Balancing parameter.\n",
    "        reduction: 'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Compute binary cross-entropy loss without reduction\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = targets * probs + (1 - targets) * (1 - probs)\n",
    "        # Compute modulating factor\n",
    "        focal_factor = (1 - p_t) ** self.gamma\n",
    "        loss = focal_factor * bce_loss\n",
    "        # Apply the alpha balancing factor\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                alpha_factor = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            else:\n",
    "                alpha_factor = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            loss = alpha_factor * loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBERTMultiLabel(pl.LightningModule):\n",
    "    def __init__(self, num_labels, learning_rate=0.001, test_threshold=0.45):\n",
    "        super(SciBERTMultiLabel, self).__init__()\n",
    "        \n",
    "        # Update the configuration to use dropout=0.6.\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            \"allenai/scibert_scivocab_uncased\", \n",
    "            num_labels=num_labels, \n",
    "            problem_type=\"multi_label_classification\",\n",
    "            hidden_dropout_prob=0.4,\n",
    "            attention_probs_dropout_prob=0.4\n",
    "        )\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"allenai/scibert_scivocab_uncased\", \n",
    "            config=config\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.test_threshold = test_threshold\n",
    "        \n",
    "        # Use Focal Loss.\n",
    "        self.criterion = FocalLoss(gamma=3, alpha=0.75, reduction='mean')\n",
    "        self.f1 = MultilabelF1Score(num_labels=num_labels, average='macro')\n",
    "        self.precision = MultilabelPrecision(num_labels=num_labels, average='macro')\n",
    "        self.recall = MultilabelRecall(num_labels=num_labels, average='macro')\n",
    "        self.hamming = MultilabelHammingDistance(num_labels=num_labels)\n",
    "        self.jaccard = MultilabelJaccardIndex(num_labels=num_labels)\n",
    "        \n",
    "        # Initialize lists to store predictions and labels for classification report.\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        \n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > self.test_threshold).float()\n",
    "        \n",
    "        # Update torchmetrics.\n",
    "        self.f1.update(preds, labels)\n",
    "        self.precision.update(preds, labels)\n",
    "        self.recall.update(preds, labels)\n",
    "        self.hamming.update(preds, labels)\n",
    "        self.jaccard.update(preds, labels)\n",
    "        \n",
    "        # Save predictions and labels for the classification report.\n",
    "        self.test_preds.append(preds.detach().cpu())\n",
    "        self.test_labels.append(labels.detach().cpu())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        f1 = self.f1.compute()\n",
    "        precision = self.precision.compute()\n",
    "        recall = self.recall.compute()\n",
    "        hamming = self.hamming.compute()\n",
    "        jaccard = self.jaccard.compute()\n",
    "        \n",
    "        print(\"\\n=== Test Metrics ===\")\n",
    "        print(f\"Test F1 Score: {f1:.4f}\")\n",
    "        print(f\"Test Precision: {precision:.4f}\")\n",
    "        print(f\"Test Recall: {recall:.4f}\")\n",
    "        print(f\"Test Hamming Loss: {hamming:.4f}\")\n",
    "        print(f\"Test Jaccard Score: {jaccard:.4f}\")\n",
    "        \n",
    "        # Reset metrics.\n",
    "        self.f1.reset()\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.hamming.reset()\n",
    "        self.jaccard.reset()\n",
    "        \n",
    "        # Concatenate all predictions and labels.\n",
    "        all_preds = torch.cat(self.test_preds, dim=0).numpy()\n",
    "        all_labels = torch.cat(self.test_labels, dim=0).numpy()\n",
    "        \n",
    "        # Generate and print a detailed classification report.\n",
    "        report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "        print(\"\\n=== Classification Report ===\")\n",
    "        for label, metrics in report.items():\n",
    "            print(f\"{label}: {metrics}\")\n",
    "        \n",
    "        # Clear stored predictions and labels.\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "csv_logger = CSVLogger(\"logs\", name=\"scibert_multilabel\")\n",
    "tb_logger = TensorBoardLogger(\"logs\", name=\"scibert_multilabel_tensorboard\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.0,\n",
    "    patience=1,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=4,\n",
    "    accelerator=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[early_stop_callback, lr_monitor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  | Name      | Type                          | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 109 M  | eval \n",
      "1 | criterion | FocalLoss                     | 0      | train\n",
      "2 | f1        | MultilabelF1Score             | 0      | train\n",
      "3 | precision | MultilabelPrecision           | 0      | train\n",
      "4 | recall    | MultilabelRecall              | 0      | train\n",
      "5 | hamming   | MultilabelHammingDistance     | 0      | train\n",
      "6 | jaccard   | MultilabelJaccardIndex        | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "439.692   Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "231       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21269aee60974425aec7a564477d6460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438d9678ea294dd9a955f68649f78702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc91c82e636420cb10af6f367db44f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72240a6761b344d2baf6feeefb1512e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48a0398b7bc43589440289390fa08a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 1 records. Best score: 0.013. Signaling Trainer to stop.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2ecad214e44f528b07d8a474674c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Metrics ===\n",
      "Test F1 Score: 0.7891\n",
      "Test Precision: 0.8165\n",
      "Test Recall: 0.7660\n",
      "Test Hamming Loss: 0.0709\n",
      "Test Jaccard Score: 0.6603\n",
      "\n",
      "=== Classification Report ===\n",
      "0: {'precision': 0.7837552742616034, 'recall': 0.7234664070107109, 'f1-score': 0.7524050632911392, 'support': 1027.0}\n",
      "1: {'precision': 0.9090399701158013, 'recall': 0.8363980065303317, 'f1-score': 0.8712073749216862, 'support': 5819.0}\n",
      "2: {'precision': 0.8871745419479267, 'recall': 0.7165109034267912, 'f1-score': 0.7927617406290393, 'support': 1284.0}\n",
      "3: {'precision': 0.6366704161979753, 'recall': 0.5895833333333333, 'f1-score': 0.6122228231476474, 'support': 960.0}\n",
      "4: {'precision': 0.8048951048951049, 'recall': 0.8435324294613411, 'f1-score': 0.8237609590266595, 'support': 5458.0}\n",
      "5: {'precision': 0.877602752127467, 'recall': 0.8864301389904902, 'f1-score': 0.8819943590210172, 'support': 5468.0}\n",
      "micro avg: {'precision': 0.8498279492578706, 'recall': 0.8266886490807354, 'f1-score': 0.8380986147339631, 'support': 20016.0}\n",
      "macro avg: {'precision': 0.816523009924313, 'recall': 0.7659868697921662, 'f1-score': 0.7890587200061981, 'support': 20016.0}\n",
      "weighted avg: {'precision': 0.8511593372967717, 'recall': 0.8266886490807354, 'f1-score': 0.8376672746479392, 'support': 20016.0}\n",
      "samples avg: {'precision': 0.8855595507613085, 'recall': 0.873731522173392, 'f1-score': 0.8591106545161679, 'support': 20016.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.013523343950510025    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.013523343950510025   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.013523343950510025}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SciBERTMultiLabel(num_labels=len(label_to_idx), learning_rate=0.001, test_threshold=0.45)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sci_bert(sentence, tokenizer, label_to_idx, model, threshold=0.45, max_length=256):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > threshold).float().squeeze(0)\n",
    "        predicted_labels = [label for label, idx in label_to_idx.items() if preds[idx] == 1]\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    \"Attention is all you need The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\",\n",
    "    \"Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.\",\n",
    "    \"Classification can be performed using either a discriminative or a generative learning approach. Discriminative learning consists of constructing the conditional probability of the outputs given the inputs, while generative learning consists of constructing the joint probability density of the inputs and outputs. Although most classical and quantum methods are discriminative, there are some advantages of the generative learning approach. For instance, it can be applied to unsupervised learning, statistical inference, uncertainty estimation, and synthetic data generation. In this article, we present a quantum generative multiclass classification strategy, called quantum generative classification (QGC). This model uses a variational quantum algorithm to estimate the joint probability density function of features and labels of a data set by means of a mixed quantum state. We also introduce a quantum map called quantum-enhanced Fourier features (QEFF), which leverages quantum superposition to prepare high-dimensional data samples in quantum hardware using a small number of qubits. We show that the quantum generative classification algorithm can be viewed as a Gaussian mixture that reproduces a kernel Hilbert space of the training data. In addition, we developed a hybrid quantum-classical neural network that shows that it is possible to perform generative classification on high-dimensional data sets. The method was tested on various low- and high-dimensional data sets including the 10-class MNIST and Fashion-MNIST data sets, illustrating that the generative classification strategy is competitive against other previous quantum models.\",\n",
    "    \"Research on human skin anatomy reveals its complex multi-scale, multi-phase nature, with up to 70% of its composition being bounded and free water. Fluid movement plays a key role in the skin's mechanical and biological responses, influencing its time-dependent behavior and nutrient transport.Poroelastic modeling is a promising approach for studying skin dynamics across scales by integrating multi-physics processes. This paper introduces a hierarchical two-compartment model capturing fluid distribution in the interstitium and micro-circulation. A theoretical framework is developed with a biphasic interstitium -- distinguishing interstitial fluid and non-structural cells -- and analyzed through a one-dimensional consolidation test of a column. This biphasic approach allows separate modeling of cell and fluid motion, considering their differing characteristic times. An appendix discusses extending the model to include biological exchanges like oxygen transport. Preliminary results indicate that cell viscosity introduces a second characteristic time, and at high viscosity and short time scales, cells behave similarly to solids.A simplified model was used to replicate an experimental campaign on short time scales. Local pressure (up to 31 kPa) was applied to dorsal finger skin using a laser Doppler probe PF801 (Perimed Sweden), following a setup described in Fromy Brain Res (1998). The model qualitatively captured ischemia and post-occlusive reactive hyperemia, aligning with experimental data.All numerical simulations used the open-source software FEniCSx v0.9.0. To ensure transparency and reproducibility, anonymized experimental data and finite element codes are publicly available on GitHub.\",\n",
    "    \"Currency arbitrage capitalizes on price discrepancies in currency exchange rates between markets to produce profits with minimal risk. By employing a combinatorial optimization problem, one can ascertain optimal paths within directed graphs, thereby facilitating the efficient identification of profitable trading routes. This research investigates the methodologies of quantum annealing and gate-based quantum computing in relation to the currency arbitrage problem. In this study, we implement the Quantum Approximate Optimization Algorithm (QAOA) utilizing Qiskit version 1.2. In order to optimize the parameters of QAOA, we perform simulations utilizing the AerSimulator and carry out experiments in simulation. Furthermore, we present an NchooseK-based methodology utilizing D-Wave's Ocean suite. This methodology enables a comparison of the effectiveness of quantum techniques in identifying optimal arbitrage paths. The results of our study enhance the existing literature on the application of quantum computing in financial optimization challenges, emphasizing both the prospective benefits and the present limitations of these developing technologies in real-world scenarios.\",\n",
    "    \"Despite advances in methods to interrogate tumor biology, the observational and population-based approach of classical cancer research and clinical oncology does not enable anticipation of tumor outcomes to hasten the discovery of cancer mechanisms and personalize disease management. To address these limitations, individualized cancer forecasts have been shown to predict tumor growth and therapeutic response, inform treatment optimization, and guide experimental efforts. These predictions are obtained via computer simulations of mathematical models that are constrained with data from a patient's cancer and experiments. This book chapter addresses the validation of these mathematical models to forecast tumor growth and treatment response. We start with an overview of mathematical modeling frameworks, model selection techniques, and fundamental metrics. We then describe the usual strategies employed to validate cancer forecasts in preclinical and clinical scenarios. Finally, we discuss existing barriers in validating these predictions along with potential strategies to address them.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SciBERT Back-Testing Results ===\n",
      "Sentence: Attention is all you need The dominant sequence transduction models are based on...\n",
      "Predicted Labels: ['cs']\n",
      "\n",
      "Sentence: Vector autogressions (VARs) are widely applied when it comes to modeling and for...\n",
      "Predicted Labels: ['math-stats']\n",
      "\n",
      "Sentence: Classification can be performed using either a discriminative or a generative le...\n",
      "Predicted Labels: ['cs', 'math-stats', 'physic']\n",
      "\n",
      "Sentence: Research on human skin anatomy reveals its complex multi-scale, multi-phase natu...\n",
      "Predicted Labels: ['bio', 'physic']\n",
      "\n",
      "Sentence: Currency arbitrage capitalizes on price discrepancies in currency exchange rates...\n",
      "Predicted Labels: ['econ-qfin', 'physic']\n",
      "\n",
      "Sentence: Despite advances in methods to interrogate tumor biology, the observational and ...\n",
      "Predicted Labels: ['bio', 'math-stats']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SciBERT Back-Testing Results ===\")\n",
    "for sentence in example_sentences:\n",
    "    predicted_labels = predict_sci_bert(sentence, tokenizer, label_to_idx, model, threshold=0.45)\n",
    "    print(f\"Sentence: {sentence[:80]}...\")\n",
    "    print(f\"Predicted Labels: {predicted_labels}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
