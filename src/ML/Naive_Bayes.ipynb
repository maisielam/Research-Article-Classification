{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
      "1    Modern Mathematical Physics: what it should be?   \n",
      "2                                Topology in Physics   \n",
      "3       Contents of Physics Related E-Print Archives   \n",
      "4        Fundamental Dilemmas in Theoretical Physics   \n",
      "\n",
      "                                             authors  \\\n",
      "0                                     Hisham Ghassib   \n",
      "1                                     Ludwig Faddeev   \n",
      "2                                          R. Jackiw   \n",
      "3  E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...   \n",
      "4                                     Hisham Ghassib   \n",
      "\n",
      "                                             summary             published  \\\n",
      "0  In this paper, it is argued that theoretical p...  2012-09-04T10:32:56Z   \n",
      "1  Personal view of author on goals and content o...  2000-02-08T13:13:00Z   \n",
      "2  The phenomenon of quantum number fractionaliza...  2005-03-15T16:00:59Z   \n",
      "3  The frontiers of physics related e-print archi...  2003-08-28T13:12:57Z   \n",
      "4  In this paper, we argue that there are foundat...  2014-05-22T07:49:09Z   \n",
      "\n",
      "                updated                                    link  \\\n",
      "0  2012-09-04T10:32:56Z        http://arxiv.org/abs/1209.0592v1   \n",
      "1  2000-02-10T10:14:56Z  http://arxiv.org/abs/math-ph/0002018v2   \n",
      "2  2005-03-15T16:00:59Z  http://arxiv.org/abs/math-ph/0503039v1   \n",
      "3  2003-08-28T13:12:57Z  http://arxiv.org/abs/physics/0308107v1   \n",
      "4  2014-05-22T07:49:09Z        http://arxiv.org/abs/1405.5530v1   \n",
      "\n",
      "                                  pdf_url  \\\n",
      "0        http://arxiv.org/pdf/1209.0592v1   \n",
      "1  http://arxiv.org/pdf/math-ph/0002018v2   \n",
      "2  http://arxiv.org/pdf/math-ph/0503039v1   \n",
      "3  http://arxiv.org/pdf/physics/0308107v1   \n",
      "4        http://arxiv.org/pdf/1405.5530v1   \n",
      "\n",
      "                                          categories             target  \n",
      "0                    physics.gen-ph, physics.hist-ph             physic  \n",
      "1                           math-ph, hep-th, math.MP  math-stats,physic  \n",
      "2  math-ph, cond-mat.mes-hall, math.MP, physics.c...  math-stats,physic  \n",
      "3                                    physics.data-an             physic  \n",
      "4                                    physics.hist-ph             physic  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./data/data_final.parquet')\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Preprocessing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tranminhanh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tranminhanh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the tokenizer, lemmatizer, stemmer\n",
    "wpt = WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Create a stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalized_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    tokens = wpt.tokenize(text)\n",
    "    \n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    lemma_stem_tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in filtered_tokens]\n",
    "    \n",
    "    cleaned_text = ' '.join(lemma_stem_tokens)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['summary'].apply(lambda x: normalized_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>link</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>categories</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Physics Sick? [In Praise of Classical Physics]</td>\n",
       "      <td>Hisham Ghassib</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "      <td>2012-09-04T10:32:56Z</td>\n",
       "      <td>2012-09-04T10:32:56Z</td>\n",
       "      <td>http://arxiv.org/abs/1209.0592v1</td>\n",
       "      <td>http://arxiv.org/pdf/1209.0592v1</td>\n",
       "      <td>physics.gen-ph, physics.hist-ph</td>\n",
       "      <td>physic</td>\n",
       "      <td>paper argu theoret physic akin organ rigid str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modern Mathematical Physics: what it should be?</td>\n",
       "      <td>Ludwig Faddeev</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "      <td>2000-02-08T13:13:00Z</td>\n",
       "      <td>2000-02-10T10:14:56Z</td>\n",
       "      <td>http://arxiv.org/abs/math-ph/0002018v2</td>\n",
       "      <td>http://arxiv.org/pdf/math-ph/0002018v2</td>\n",
       "      <td>math-ph, hep-th, math.MP</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>person view author goal content mathemat physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology in Physics</td>\n",
       "      <td>R. Jackiw</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "      <td>2005-03-15T16:00:59Z</td>\n",
       "      <td>2005-03-15T16:00:59Z</td>\n",
       "      <td>http://arxiv.org/abs/math-ph/0503039v1</td>\n",
       "      <td>http://arxiv.org/pdf/math-ph/0503039v1</td>\n",
       "      <td>math-ph, cond-mat.mes-hall, math.MP, physics.c...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>phenomenon quantum number fraction explain rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents of Physics Related E-Print Archives</td>\n",
       "      <td>E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "      <td>2003-08-28T13:12:57Z</td>\n",
       "      <td>2003-08-28T13:12:57Z</td>\n",
       "      <td>http://arxiv.org/abs/physics/0308107v1</td>\n",
       "      <td>http://arxiv.org/pdf/physics/0308107v1</td>\n",
       "      <td>physics.data-an</td>\n",
       "      <td>physic</td>\n",
       "      <td>frontier physic relat e print archiv web servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fundamental Dilemmas in Theoretical Physics</td>\n",
       "      <td>Hisham Ghassib</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "      <td>2014-05-22T07:49:09Z</td>\n",
       "      <td>2014-05-22T07:49:09Z</td>\n",
       "      <td>http://arxiv.org/abs/1405.5530v1</td>\n",
       "      <td>http://arxiv.org/pdf/1405.5530v1</td>\n",
       "      <td>physics.hist-ph</td>\n",
       "      <td>physic</td>\n",
       "      <td>paper argu foundat dilemma theoret physic rela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
       "1    Modern Mathematical Physics: what it should be?   \n",
       "2                                Topology in Physics   \n",
       "3       Contents of Physics Related E-Print Archives   \n",
       "4        Fundamental Dilemmas in Theoretical Physics   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                     Hisham Ghassib   \n",
       "1                                     Ludwig Faddeev   \n",
       "2                                          R. Jackiw   \n",
       "3  E. R. Prakasan, Anil Kumar, Anil Sagar, Lalit ...   \n",
       "4                                     Hisham Ghassib   \n",
       "\n",
       "                                             summary             published  \\\n",
       "0  In this paper, it is argued that theoretical p...  2012-09-04T10:32:56Z   \n",
       "1  Personal view of author on goals and content o...  2000-02-08T13:13:00Z   \n",
       "2  The phenomenon of quantum number fractionaliza...  2005-03-15T16:00:59Z   \n",
       "3  The frontiers of physics related e-print archi...  2003-08-28T13:12:57Z   \n",
       "4  In this paper, we argue that there are foundat...  2014-05-22T07:49:09Z   \n",
       "\n",
       "                updated                                    link  \\\n",
       "0  2012-09-04T10:32:56Z        http://arxiv.org/abs/1209.0592v1   \n",
       "1  2000-02-10T10:14:56Z  http://arxiv.org/abs/math-ph/0002018v2   \n",
       "2  2005-03-15T16:00:59Z  http://arxiv.org/abs/math-ph/0503039v1   \n",
       "3  2003-08-28T13:12:57Z  http://arxiv.org/abs/physics/0308107v1   \n",
       "4  2014-05-22T07:49:09Z        http://arxiv.org/abs/1405.5530v1   \n",
       "\n",
       "                                  pdf_url  \\\n",
       "0        http://arxiv.org/pdf/1209.0592v1   \n",
       "1  http://arxiv.org/pdf/math-ph/0002018v2   \n",
       "2  http://arxiv.org/pdf/math-ph/0503039v1   \n",
       "3  http://arxiv.org/pdf/physics/0308107v1   \n",
       "4        http://arxiv.org/pdf/1405.5530v1   \n",
       "\n",
       "                                          categories             target  \\\n",
       "0                    physics.gen-ph, physics.hist-ph             physic   \n",
       "1                           math-ph, hep-th, math.MP  math-stats,physic   \n",
       "2  math-ph, cond-mat.mes-hall, math.MP, physics.c...  math-stats,physic   \n",
       "3                                    physics.data-an             physic   \n",
       "4                                    physics.hist-ph             physic   \n",
       "\n",
       "                                             cleaned  \n",
       "0  paper argu theoret physic akin organ rigid str...  \n",
       "1    person view author goal content mathemat physic  \n",
       "2  phenomenon quantum number fraction explain rel...  \n",
       "3  frontier physic relat e print archiv web servi...  \n",
       "4  paper argu foundat dilemma theoret physic rela...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['bio' 'cs' 'econ-qfin' 'eess' 'math-stats' 'physic']\n",
      "Best alpha parameter: 2.0\n",
      "\n",
      "Test Set Metrics:\n",
      "Test F1 Score: 0.7466\n",
      "Test Precision: 0.7013\n",
      "Test Recall: 0.8138\n",
      "Test Hamming Loss: 0.0963\n",
      "Test Jaccard Score: 0.6063\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bio       0.70      0.77      0.73      1074\n",
      "          cs       0.81      0.89      0.85      5758\n",
      "   econ-qfin       0.67      0.88      0.76      1240\n",
      "        eess       0.41      0.73      0.53      1007\n",
      "  math-stats       0.76      0.80      0.78      5432\n",
      "      physic       0.86      0.81      0.83      5550\n",
      "\n",
      "   micro avg       0.76      0.83      0.79     20061\n",
      "   macro avg       0.70      0.81      0.75     20061\n",
      "weighted avg       0.78      0.83      0.80     20061\n",
      " samples avg       0.81      0.87      0.81     20061\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, jaccard_score, classification_report\n",
    "\n",
    "X = df['cleaned']  # Preprocessed text\n",
    "y = df['target'].apply(lambda x: x.split(\",\"))  # Convert to list if comma-separated\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_bin = mlb.fit_transform(y)\n",
    "\n",
    "print(\"Labels:\", mlb.classes_)  # Print encoded class labels\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X_vectorized, Y_bin, test_size=0.2, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "class_weights = []\n",
    "for i in range(Y_train.shape[1]):\n",
    "    class_counts = Counter(Y_train[:, i])\n",
    "    total_samples = len(Y_train)\n",
    "    weights = {label: total_samples / count for label, count in class_counts.items()}\n",
    "    class_weights.append(weights)\n",
    "\n",
    "nb_model = MultiOutputClassifier(MultinomialNB())\n",
    "\n",
    "param_grid = {'estimator__alpha': [0.1, 0.5, 1.0, 2.0, 3.0]}\n",
    "grid_search = GridSearchCV(nb_model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_alpha = grid_search.best_params_['estimator__alpha']\n",
    "print(f\"Best alpha parameter: {best_alpha}\")\n",
    "\n",
    "best_nb_model = MultiOutputClassifier(MultinomialNB(alpha=best_alpha))\n",
    "best_nb_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_val_pred = best_nb_model.predict(X_val)\n",
    "\n",
    "Y_test_pred = best_nb_model.predict(X_test)\n",
    "\n",
    "test_f1 = f1_score(Y_test, Y_test_pred, average='macro')\n",
    "test_precision = precision_score(Y_test, Y_test_pred, average='macro', zero_division=0)\n",
    "test_recall = recall_score(Y_test, Y_test_pred, average='macro', zero_division=0)\n",
    "test_hamming = hamming_loss(Y_test, Y_test_pred)\n",
    "test_jaccard = jaccard_score(Y_test, Y_test_pred, average='macro')\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test Hamming Loss: {test_hamming:.4f}\")\n",
    "print(f\"Test Jaccard Score: {test_jaccard:.4f}\")\n",
    "\n",
    "# Classification Reports\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(Y_test, Y_test_pred, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Backtesting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 **Generated Test Data Point 1**: Quantum entanglement and Bell's theorem challenge classical interpretations of physics.\n",
      "📌 **Predicted Categories**: ('math-stats', 'physic')\n",
      "\n",
      "🔹 **Generated Test Data Point 2**: Statistical methods in probability theory play a fundamental role in mathematical modeling.\n",
      "📌 **Predicted Categories**: ('bio', 'econ-qfin', 'physic')\n",
      "\n",
      "🔹 **Generated Test Data Point 3**: New deep learning architectures are transforming natural language processing applications.\n",
      "📌 **Predicted Categories**: ('cs',)\n",
      "\n",
      "🔹 **Generated Test Data Point 4**: Cryptography relies on number theory and complex mathematical algorithms.\n",
      "📌 **Predicted Categories**: ('math-stats',)\n",
      "\n",
      "🔹 **Generated Test Data Point 5**: Bayesian inference is widely used in statistical decision-making and machine learning.\n",
      "📌 **Predicted Categories**: ('cs', 'math-stats')\n",
      "\n",
      "🔹 **Generated Test Data Point 6**: Attention is all you need The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "📌 **Predicted Categories**: ('cs', 'eess')\n",
      "\n",
      "🔹 **Generated Test Data Point 7**: Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.\n",
      "📌 **Predicted Categories**: ('cs', 'econ-qfin', 'math-stats')\n",
      "\n",
      "🔹 **Generated Test Data Point 8**: Classification can be performed using either a discriminative or a generative learning approach. Discriminative learning consists of constructing the conditional probability of the outputs given the inputs, while generative learning consists of constructing the joint probability density of the inputs and outputs. Although most classical and quantum methods are discriminative, there are some advantages of the generative learning approach. For instance, it can be applied to unsupervised learning, statistical inference, uncertainty estimation, and synthetic data generation. In this article, we present a quantum generative multiclass classification strategy, called quantum generative classification (QGC). This model uses a variational quantum algorithm to estimate the joint probability density function of features and labels of a data set by means of a mixed quantum state. We also introduce a quantum map called quantum-enhanced Fourier features (QEFF), which leverages quantum superposition to prepare high-dimensional data samples in quantum hardware using a small number of qubits. We show that the quantum generative classification algorithm can be viewed as a Gaussian mixture that reproduces a kernel Hilbert space of the training data. In addition, we developed a hybrid quantum-classical neural network that shows that it is possible to perform generative classification on high-dimensional data sets. The method was tested on various low- and high-dimensional data sets including the 10-class MNIST and Fashion-MNIST data sets, illustrating that the generative classification strategy is competitive against other previous quantum models.\n",
      "📌 **Predicted Categories**: ('cs', 'math-stats', 'physic')\n",
      "\n",
      "🔹 **Generated Test Data Point 9**: Research on human skin anatomy reveals its complex multi-scale, multi-phase nature, with up to 70% of its composition being bounded and free water. Fluid movement plays a key role in the skin's mechanical and biological responses, influencing its time-dependent behavior and nutrient transport.Poroelastic modeling is a promising approach for studying skin dynamics across scales by integrating multi-physics processes. This paper introduces a biology hierarchical two-compartment model capturing fluid distribution in the interstitium and micro-circulation. A theoretical framework is developed with a biphasic interstitium -- distinguishing interstitial fluid and non-structural cells -- and analyzed through a one-dimensional consolidation test of a column. This biphasic approach allows separate modeling of cell and fluid motion, considering their differing characteristic times. An appendix discusses extending the model to include biological exchanges like oxygen transport. Preliminary results indicate that cell viscosity introduces a second characteristic time, and at high viscosity and short time scales, cells behave similarly to solids.A simplified model was used to replicate an experimental campaign on short time scales. Local pressure (up to 31 kPa) was applied to dorsal finger skin using a laser Doppler probe PF801 (Perimed Sweden), following a setup described in Fromy Brain Res (1998). The model qualitatively captured ischemia and post-occlusive reactive hyperemia, aligning with experimental data.All numerical simulations used the open-source software FEniCSx v0.9.0. To ensure transparency and reproducibility, anonymized experimental data and finite element codes are publicly available on GitHub.\n",
      "📌 **Predicted Categories**: ('bio', 'physic')\n",
      "\n",
      "🔹 **Generated Test Data Point 10**: Currency arbitrage capitalizes on price discrepancies in currency exchange rates between markets to produce profits with minimal risk. By employing a combinatorial optimization problem, one can ascertain optimal paths within directed graphs, thereby facilitating the efficient identification of profitable trading routes. This research investigates the methodologies of quantum annealing and gate-based quantum computing in relation to the currency arbitrage problem. In this study, we implement the Quantum Approximate Optimization Algorithm (QAOA) utilizing Qiskit version 1.2. In order to optimize the parameters of QAOA, we perform simulations utilizing the AerSimulator and carry out experiments in simulation. Furthermore, we present an NchooseK-based methodology utilizing D-Wave's Ocean suite. This methodology enables a comparison of the effectiveness of quantum techniques in identifying optimal arbitrage paths. The results of our study enhance the existing literature on the application of quantum computing in financial optimization challenges, emphasizing both the prospective benefits and the present limitations of these developing technologies in real-world scenarios.\n",
      "📌 **Predicted Categories**: ('physic',)\n",
      "\n",
      "🔹 **Generated Test Data Point 11**: Despite advances in methods to interrogate tumor biology, the observational and population-based approach of classical cancer research and clinical oncology does not enable anticipation of tumor outcomes to hasten the discovery of cancer mechanisms and personalize disease management. To address these limitations, individualized cancer forecasts have been shown to predict tumor growth and therapeutic response, inform treatment optimization, and guide experimental efforts. These predictions are obtained via computer simulations of mathematical models that are constrained with data from a patient's cancer and experiments. This book chapter addresses the validation of these mathematical models to forecast tumor growth and treatment response. We start with an overview of mathematical modeling frameworks, model selection techniques, and fundamental metrics. We then describe the usual strategies employed to validate cancer forecasts in preclinical and clinical scenarios. Finally, we discuss existing barriers in validating these predictions along with potential strategies to address them.\n",
      "📌 **Predicted Categories**: ('bio',)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "new_data_samples = [\n",
    "    \"Quantum entanglement and Bell's theorem challenge classical interpretations of physics.\",\n",
    "    \"Statistical methods in probability theory play a fundamental role in mathematical modeling.\",\n",
    "    \"New deep learning architectures are transforming natural language processing applications.\",\n",
    "    \"Cryptography relies on number theory and complex mathematical algorithms.\",\n",
    "    \"Bayesian inference is widely used in statistical decision-making and machine learning.\",\n",
    "    \"Attention is all you need The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\",\n",
    "    \"Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.\",\n",
    "    \"Classification can be performed using either a discriminative or a generative learning approach. Discriminative learning consists of constructing the conditional probability of the outputs given the inputs, while generative learning consists of constructing the joint probability density of the inputs and outputs. Although most classical and quantum methods are discriminative, there are some advantages of the generative learning approach. For instance, it can be applied to unsupervised learning, statistical inference, uncertainty estimation, and synthetic data generation. In this article, we present a quantum generative multiclass classification strategy, called quantum generative classification (QGC). This model uses a variational quantum algorithm to estimate the joint probability density function of features and labels of a data set by means of a mixed quantum state. We also introduce a quantum map called quantum-enhanced Fourier features (QEFF), which leverages quantum superposition to prepare high-dimensional data samples in quantum hardware using a small number of qubits. We show that the quantum generative classification algorithm can be viewed as a Gaussian mixture that reproduces a kernel Hilbert space of the training data. In addition, we developed a hybrid quantum-classical neural network that shows that it is possible to perform generative classification on high-dimensional data sets. The method was tested on various low- and high-dimensional data sets including the 10-class MNIST and Fashion-MNIST data sets, illustrating that the generative classification strategy is competitive against other previous quantum models.\",\n",
    "    \"Research on human skin anatomy reveals its complex multi-scale, multi-phase nature, with up to 70% of its composition being bounded and free water. Fluid movement plays a key role in the skin's mechanical and biological responses, influencing its time-dependent behavior and nutrient transport.Poroelastic modeling is a promising approach for studying skin dynamics across scales by integrating multi-physics processes. This paper introduces a biology hierarchical two-compartment model capturing fluid distribution in the interstitium and micro-circulation. A theoretical framework is developed with a biphasic interstitium -- distinguishing interstitial fluid and non-structural cells -- and analyzed through a one-dimensional consolidation test of a column. This biphasic approach allows separate modeling of cell and fluid motion, considering their differing characteristic times. An appendix discusses extending the model to include biological exchanges like oxygen transport. Preliminary results indicate that cell viscosity introduces a second characteristic time, and at high viscosity and short time scales, cells behave similarly to solids.A simplified model was used to replicate an experimental campaign on short time scales. Local pressure (up to 31 kPa) was applied to dorsal finger skin using a laser Doppler probe PF801 (Perimed Sweden), following a setup described in Fromy Brain Res (1998). The model qualitatively captured ischemia and post-occlusive reactive hyperemia, aligning with experimental data.All numerical simulations used the open-source software FEniCSx v0.9.0. To ensure transparency and reproducibility, anonymized experimental data and finite element codes are publicly available on GitHub.\",\n",
    "    \"Currency arbitrage capitalizes on price discrepancies in currency exchange rates between markets to produce profits with minimal risk. By employing a combinatorial optimization problem, one can ascertain optimal paths within directed graphs, thereby facilitating the efficient identification of profitable trading routes. This research investigates the methodologies of quantum annealing and gate-based quantum computing in relation to the currency arbitrage problem. In this study, we implement the Quantum Approximate Optimization Algorithm (QAOA) utilizing Qiskit version 1.2. In order to optimize the parameters of QAOA, we perform simulations utilizing the AerSimulator and carry out experiments in simulation. Furthermore, we present an NchooseK-based methodology utilizing D-Wave's Ocean suite. This methodology enables a comparison of the effectiveness of quantum techniques in identifying optimal arbitrage paths. The results of our study enhance the existing literature on the application of quantum computing in financial optimization challenges, emphasizing both the prospective benefits and the present limitations of these developing technologies in real-world scenarios.\",\n",
    "    \"Despite advances in methods to interrogate tumor biology, the observational and population-based approach of classical cancer research and clinical oncology does not enable anticipation of tumor outcomes to hasten the discovery of cancer mechanisms and personalize disease management. To address these limitations, individualized cancer forecasts have been shown to predict tumor growth and therapeutic response, inform treatment optimization, and guide experimental efforts. These predictions are obtained via computer simulations of mathematical models that are constrained with data from a patient's cancer and experiments. This book chapter addresses the validation of these mathematical models to forecast tumor growth and treatment response. We start with an overview of mathematical modeling frameworks, model selection techniques, and fundamental metrics. We then describe the usual strategies employed to validate cancer forecasts in preclinical and clinical scenarios. Finally, we discuss existing barriers in validating these predictions along with potential strategies to address them.\"\n",
    "]\n",
    "\n",
    "new_data_vectorized = vectorizer.transform(new_data_samples)\n",
    "\n",
    "new_predictions = best_nb_model.predict(new_data_vectorized)\n",
    "\n",
    "new_predictions = np.array(new_predictions)  # Convert list of arrays to a 2D NumPy array\n",
    "predicted_labels = mlb.inverse_transform(new_predictions)  # Now, this works correctly\n",
    "\n",
    "for i, text in enumerate(new_data_samples):\n",
    "    print(f\"\\n🔹 **Generated Test Data Point {i+1}**: {text}\")\n",
    "    print(f\"📌 **Predicted Categories**: {predicted_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
