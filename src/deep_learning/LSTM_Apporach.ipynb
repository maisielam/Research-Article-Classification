{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/daniellai/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import (MultilabelF1Score, MultilabelPrecision, \n",
    "                                         MultilabelRecall, MultilabelHammingDistance, \n",
    "                                         MultilabelJaccardIndex)\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from sklearn.metrics import classification_report\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files Parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150171 entries, 0 to 150170\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   title       150171 non-null  object\n",
      " 1   authors     150171 non-null  object\n",
      " 2   summary     150171 non-null  object\n",
      " 3   published   150171 non-null  object\n",
      " 4   updated     150171 non-null  object\n",
      " 5   link        150171 non-null  object\n",
      " 6   pdf_url     150171 non-null  object\n",
      " 7   categories  150171 non-null  object\n",
      " 8   target      150171 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/daniellai/MSDS_2026/MSDS_2024_2026/Winter_2025/DSCI521/Project/data/data_final.parquet\"\n",
    "data = pd.read_parquet(path)\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Physics Sick? [In Praise of Classical Physics]</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "      <td>physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modern Mathematical Physics: what it should be?</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology in Physics</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents of Physics Related E-Print Archives</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "      <td>physic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fundamental Dilemmas in Theoretical Physics</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "      <td>physic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
       "1    Modern Mathematical Physics: what it should be?   \n",
       "2                                Topology in Physics   \n",
       "3       Contents of Physics Related E-Print Archives   \n",
       "4        Fundamental Dilemmas in Theoretical Physics   \n",
       "\n",
       "                                             summary             target  \n",
       "0  In this paper, it is argued that theoretical p...             physic  \n",
       "1  Personal view of author on goals and content o...  math-stats,physic  \n",
       "2  The phenomenon of quantum number fractionaliza...  math-stats,physic  \n",
       "3  The frontiers of physics related e-print archi...             physic  \n",
       "4  In this paper, we argue that there are foundat...             physic  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full = data[['title','summary','target']]\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/nr4dmfxx2j37_0_y0w4r7_dw0000gn/T/ipykernel_89416/2540402291.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_full['full_title'] = data_full['summary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>target</th>\n",
       "      <th>full_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Physics Sick? [In Praise of Classical Physics]</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "      <td>physic</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modern Mathematical Physics: what it should be?</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology in Physics</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents of Physics Related E-Print Archives</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "      <td>physic</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fundamental Dilemmas in Theoretical Physics</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "      <td>physic</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
       "1    Modern Mathematical Physics: what it should be?   \n",
       "2                                Topology in Physics   \n",
       "3       Contents of Physics Related E-Print Archives   \n",
       "4        Fundamental Dilemmas in Theoretical Physics   \n",
       "\n",
       "                                             summary             target  \\\n",
       "0  In this paper, it is argued that theoretical p...             physic   \n",
       "1  Personal view of author on goals and content o...  math-stats,physic   \n",
       "2  The phenomenon of quantum number fractionaliza...  math-stats,physic   \n",
       "3  The frontiers of physics related e-print archi...             physic   \n",
       "4  In this paper, we argue that there are foundat...             physic   \n",
       "\n",
       "                                          full_title  \n",
       "0  In this paper, it is argued that theoretical p...  \n",
       "1  Personal view of author on goals and content o...  \n",
       "2  The phenomenon of quantum number fractionaliza...  \n",
       "3  The frontiers of physics related e-print archi...  \n",
       "4  In this paper, we argue that there are foundat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full['full_title'] = data_full['summary']\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this paper, it is argued that theoretical physics is more akin to an organism than to a rigid structure.It is in this sense that the epithet, \"sick\", applies to it. It is argued that classical physics is a model of a healthy science, and the degree of sickness of modern physics is measured accordingly. The malady is located in the relationship between mathematics and physical meaning in physical theory.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full['full_title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words_init = nltk.corpus.stopwords.words('english')\n",
    "stop_words = [i for i in stop_words_init if i not in ('not','and','for')]\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the text \n",
    "def normalized_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    tokens = wpt.tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/nr4dmfxx2j37_0_y0w4r7_dw0000gn/T/ipykernel_89416/3359930248.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_full['full_title'] = data_full['full_title'].apply(lambda x: normalized_text(x))\n"
     ]
    }
   ],
   "source": [
    "data_full['full_title'] = data_full['full_title'].apply(lambda x: normalized_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>target</th>\n",
       "      <th>full_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Physics Sick? [In Praise of Classical Physics]</td>\n",
       "      <td>In this paper, it is argued that theoretical p...</td>\n",
       "      <td>physic</td>\n",
       "      <td>paper argued theoretical physics akin organism...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Modern Mathematical Physics: what it should be?</td>\n",
       "      <td>Personal view of author on goals and content o...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>personal view author goals and content mathema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topology in Physics</td>\n",
       "      <td>The phenomenon of quantum number fractionaliza...</td>\n",
       "      <td>math-stats,physic</td>\n",
       "      <td>phenomenon quantum number fractionalization ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contents of Physics Related E-Print Archives</td>\n",
       "      <td>The frontiers of physics related e-print archi...</td>\n",
       "      <td>physic</td>\n",
       "      <td>frontiers physics related e print archives web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fundamental Dilemmas in Theoretical Physics</td>\n",
       "      <td>In this paper, we argue that there are foundat...</td>\n",
       "      <td>physic</td>\n",
       "      <td>paper argue foundational dilemmas theoretical ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is Physics Sick? [In Praise of Classical Physics]   \n",
       "1    Modern Mathematical Physics: what it should be?   \n",
       "2                                Topology in Physics   \n",
       "3       Contents of Physics Related E-Print Archives   \n",
       "4        Fundamental Dilemmas in Theoretical Physics   \n",
       "\n",
       "                                             summary             target  \\\n",
       "0  In this paper, it is argued that theoretical p...             physic   \n",
       "1  Personal view of author on goals and content o...  math-stats,physic   \n",
       "2  The phenomenon of quantum number fractionaliza...  math-stats,physic   \n",
       "3  The frontiers of physics related e-print archi...             physic   \n",
       "4  In this paper, we argue that there are foundat...             physic   \n",
       "\n",
       "                                          full_title  \n",
       "0  paper argued theoretical physics akin organism...  \n",
       "1  personal view author goals and content mathema...  \n",
       "2  phenomenon quantum number fractionalization ex...  \n",
       "3  frontiers physics related e print archives web...  \n",
       "4  paper argue foundational dilemmas theoretical ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-val-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(data_full, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['full_title'].tolist()\n",
    "train_targets = train_df['target'].tolist()\n",
    "val_texts = val_df['full_title'].tolist()\n",
    "val_targets = val_df['target'].tolist()\n",
    "test_texts = test_df['full_title'].tolist()\n",
    "test_targets = test_df['target'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple tokenizer (splitting on whitespace)\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "# Build vocabulary from training texts\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'bio': 0, 'cs': 1, 'econ-qfin': 2, 'eess': 3, 'math-stats': 4, 'physic': 5}\n"
     ]
    }
   ],
   "source": [
    "all_targets = data_full['target'].tolist()\n",
    "all_labels = set()\n",
    "for target in all_targets:\n",
    "    all_labels.update(target.split(','))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "print(\"Label Mapping:\", label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load GloVe Embeddings using TorchText\n",
    "# -------------------------------\n",
    "# This will download GloVe if not already available.\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "# Create an embedding matrix for our vocabulary\n",
    "embedding_dim = 100\n",
    "vocab_size = len(vocab)\n",
    "embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
    "# Optionally, set the embedding for the padding token (index 0) to zeros\n",
    "embedding_matrix[0] = torch.zeros(embedding_dim)\n",
    "# Build the embedding matrix: for each word in our vocab, if it exists in GloVe, use its vector.\n",
    "for word, idx in vocab.get_stoi().items():\n",
    "    if word in glove.stoi:\n",
    "        embedding_matrix[idx] = glove.vectors[glove.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset with Target Transformation\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, vocab, label_to_idx):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert text to token indices\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer(text)\n",
    "        indices = [self.vocab[token] for token in tokens if token in self.vocab]\n",
    "        if not indices:\n",
    "            indices = [self.vocab['<unk>']]\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        labels = self.targets[idx].split(',')\n",
    "        label_vec = torch.zeros(len(self.label_to_idx))\n",
    "        for label in labels:\n",
    "            if label in self.label_to_idx:\n",
    "                label_vec[self.label_to_idx[label]] = 1\n",
    "        return indices, label_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiLabelDataset(train_texts, train_targets, tokenizer, vocab, label_to_idx)\n",
    "val_dataset = MultiLabelDataset(val_texts, val_targets, tokenizer, vocab, label_to_idx)\n",
    "test_dataset = MultiLabelDataset(test_texts, test_targets, tokenizer, vocab, label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return sequences_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handle Imbalance with WeightedRandomSampler\n",
    "def compute_sample_weight(label_vec):\n",
    "    return 1.0 / (label_vec.sum().item() + 1e-4)\n",
    "\n",
    "sample_weights = [compute_sample_weight(train_dataset[i][1]) for i in range(len(train_dataset))]\n",
    "sample_weights = np.array(sample_weights)\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Count occurrences of each class\\nclass_counts = torch.zeros(len(label_to_idx))\\nfor _, labels in train_dataset:\\n    class_counts += labels  # Sum multi-hot vectors\\n\\n# Avoid division by zero\\nclass_weights = 1.0 / (class_counts + 1e-4) \\n\\n\\ndef compute_sample_weight(label_vec):\\n    return sum(class_weights[label_vec == 1]).item()\\n\\nsample_weights = [\\n    compute_sample_weight(train_dataset[i][1])\\n    for i in range(len(train_dataset))\\n]\\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''''''''''''''\n",
    "# Count occurrences of each class\n",
    "class_counts = torch.zeros(len(label_to_idx))\n",
    "for _, labels in train_dataset:\n",
    "    class_counts += labels  # Sum multi-hot vectors\n",
    "\n",
    "# Avoid division by zero\n",
    "class_weights = 1.0 / (class_counts + 1e-4) \n",
    "\n",
    "\n",
    "def compute_sample_weight(label_vec):\n",
    "    return sum(class_weights[label_vec == 1]).item()\n",
    "\n",
    "sample_weights = [\n",
    "    compute_sample_weight(train_dataset[i][1])\n",
    "    for i in range(len(train_dataset))\n",
    "]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "'''''''''''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    # Could change the gamma and alpha parameters ( gamma is focus on majority class, alpha is focus on minority class try consider alpha = 0.5)\n",
    "    def __init__(self, gamma= 2, alpha=0.75, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = targets * probs + (1 - targets) * (1 - probs)\n",
    "        focal_factor = (1 - p_t) ** self.gamma\n",
    "        loss = focal_factor * bce_loss\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                alpha_factor = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            else:\n",
    "                alpha_factor = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            loss = alpha_factor * loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM Classifier Model Using GloVe Embeddings\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, embedding_matrix, num_layers=2, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)          # (batch, seq_len, embed_dim)\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        last_hidden = h_n[-1, :, :]           # (batch, hidden_dim)\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        output = self.fc(dropped)             # (batch, num_labels)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelLSTM(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, embedding_matrix, num_layers=2, learning_rate=0.0005, dropout=0.5):\n",
    "        super(MultiLabelLSTM, self).__init__()\n",
    "        self.model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_labels, embedding_matrix, num_layers, dropout)\n",
    "        self.criterion = FocalLoss(gamma=3, alpha=0.5, reduction='mean')\n",
    "        self.f1 = MultilabelF1Score(num_labels=num_labels, average='macro')\n",
    "        self.precision = MultilabelPrecision(num_labels=num_labels, average='macro')\n",
    "        self.recall = MultilabelRecall(num_labels=num_labels, average='macro')\n",
    "        self.hamming = MultilabelHammingDistance(num_labels=num_labels)\n",
    "        self.jaccard = MultilabelJaccardIndex(num_labels=num_labels)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_losses = []  # Accumulate training losses per epoch\n",
    "        self.val_losses = []    # Accumulate validation losses per epoch\n",
    "        self.test_losses = []   # Accumulate test losses per epoch\n",
    "        \n",
    "        # Lists to store all predictions and labels for the classification report\n",
    "        self.all_preds = []\n",
    "        self.all_labels = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.train_losses.append(loss.detach())\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.train_losses).mean()\n",
    "        print(f\"Epoch {self.current_epoch}: Training Loss: {avg_loss.item():.4f}\")\n",
    "        self.train_losses.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.val_losses.append(loss.detach())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = torch.stack(self.val_losses).mean()\n",
    "        print(f\"Epoch {self.current_epoch}: Validation Loss: {avg_val_loss.item():.4f}\")\n",
    "        self.val_losses.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.test_losses.append(loss.detach())\n",
    "        threshold = 0.45 # Adjust threshold as needed\n",
    "        preds = (torch.sigmoid(logits) > threshold).float()  # Apply sigmoid to logits\n",
    "        \n",
    "        # Update metrics\n",
    "        self.f1.update(preds, y)\n",
    "        self.precision.update(preds, y)\n",
    "        self.recall.update(preds, y)\n",
    "        self.hamming.update(preds, y)\n",
    "        self.jaccard.update(preds, y)\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        \n",
    "        # Accumulate predictions and labels for classification report\n",
    "        self.all_preds.append(preds.detach().cpu().numpy())\n",
    "        self.all_labels.append(y.detach().cpu().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_test_loss = torch.stack(self.test_losses).mean()\n",
    "        print(f\"\\nTest Loss: {avg_test_loss.item():.4f}\")\n",
    "        self.test_losses.clear()\n",
    "        \n",
    "        # Compute and print test metrics\n",
    "        f1 = self.f1.compute()\n",
    "        precision = self.precision.compute()\n",
    "        recall = self.recall.compute()\n",
    "        hamming = self.hamming.compute()\n",
    "        jaccard = self.jaccard.compute()\n",
    "        print(\"\\n=== Classification Report ===\")\n",
    "        print(f\"Test F1-Score: {f1:.4f}\")\n",
    "        print(f\"Test Precision: {precision:.4f}\")\n",
    "        print(f\"Test Recall: {recall:.4f}\")\n",
    "        print(f\"Test Hamming Loss: {hamming:.4f}\")\n",
    "        print(f\"Test Jaccard Score: {jaccard:.4f}\")\n",
    "        self.log('test_f1', f1)\n",
    "        self.log('test_precision', precision)\n",
    "        self.log('test_recall', recall)\n",
    "        self.log('test_hamming', hamming)\n",
    "        self.log('test_jaccard', jaccard)\n",
    "        \n",
    "        # Reset metrics for the next test epoch\n",
    "        self.f1.reset()\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.hamming.reset()\n",
    "        self.jaccard.reset()\n",
    "        \n",
    "        # Concatenate accumulated predictions and labels\n",
    "        all_preds = np.concatenate(self.all_preds, axis=0)\n",
    "        all_labels = np.concatenate(self.all_labels, axis=0)\n",
    "        \n",
    "        # Generate and print a detailed classification report using scikit-learn\n",
    "        # Here we use string representations for target names (e.g., \"0\", \"1\", ..., \"num_labels-1\")\n",
    "        report = classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=[str(i) for i in range(all_labels.shape[1])],\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(\"\\nDetailed Classification Report:\\n\", report)\n",
    "        \n",
    "        # Clear stored predictions and labels for the next test run\n",
    "        self.all_preds.clear()\n",
    "        self.all_labels.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "csv_logger = CSVLogger(\"logs\", name=\"multilabel_model\")\n",
    "tb_logger = TensorBoardLogger(\"logs\", name=\"multilabel_model_tensorboard\")\n",
    "\n",
    "# EarlyStopping callback: monitor 'val_loss'\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=1,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=8,\n",
    "    accelerator='mps' if torch.backends.mps.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[early_stop_callback, lr_monitor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                      | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | model     | LSTMClassifier            | 14.0 M | train\n",
      "1 | criterion | FocalLoss                 | 0      | train\n",
      "2 | f1        | MultilabelF1Score         | 0      | train\n",
      "3 | precision | MultilabelPrecision       | 0      | train\n",
      "4 | recall    | MultilabelRecall          | 0      | train\n",
      "5 | hamming   | MultilabelHammingDistance | 0      | train\n",
      "6 | jaccard   | MultilabelJaccardIndex    | 0      | train\n",
      "----------------------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "55.890    Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e45b69074854eccbbcfa30e54b46a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434e6d14ac144190ab3205efe4445e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6720b3c13aa4c3a953845260a9b64fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 0.0224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e31dcc2d6443348fdb316845b1280f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 0.0142\n",
      "Epoch 1: Training Loss: 0.0125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3573d29a30144d1a57832027118b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 0.0138\n",
      "Epoch 2: Training Loss: 0.0100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab29512fcab4b6abb4fb1afaa429d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Loss: 0.0134\n",
      "Epoch 3: Training Loss: 0.0087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cb21041deb492ebd6fcf625920a758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 1 records. Best score: 0.013. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Loss: 0.0145\n",
      "Epoch 4: Training Loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed7da708dd94d6eb22ff79aaece4936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.0146\n",
      "\n",
      "=== Classification Report ===\n",
      "Test F1-Score: 0.7834\n",
      "Test Precision: 0.7988\n",
      "Test Recall: 0.7704\n",
      "Test Hamming Loss: 0.0741\n",
      "Test Jaccard Score: 0.6528\n",
      "\n",
      "Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.74      1027\n",
      "           1       0.87      0.89      0.88      5819\n",
      "           2       0.79      0.81      0.80      1284\n",
      "           3       0.68      0.56      0.61       960\n",
      "           4       0.81      0.78      0.80      5458\n",
      "           5       0.90      0.85      0.87      5468\n",
      "\n",
      "   micro avg       0.84      0.82      0.83     20016\n",
      "   macro avg       0.80      0.77      0.78     20016\n",
      "weighted avg       0.84      0.82      0.83     20016\n",
      " samples avg       0.89      0.87      0.86     20016\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7834431529045105     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_hamming        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.0741332620382309     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_jaccard        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6527856588363647     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.014599100686609745    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.798846423625946     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7704260945320129     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7834431529045105    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_hamming       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.0741332620382309    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_jaccard       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6527856588363647    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.014599100686609745   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.798846423625946    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7704260945320129    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.014599100686609745,\n",
       "  'test_f1': 0.7834431529045105,\n",
       "  'test_precision': 0.798846423625946,\n",
       "  'test_recall': 0.7704260945320129,\n",
       "  'test_hamming': 0.0741332620382309,\n",
       "  'test_jaccard': 0.6527856588363647}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiLabelLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=512,\n",
    "    num_labels=len(label_to_idx),\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    num_layers=2,\n",
    "    learning_rate=0.0005,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Back-Test with Example Sentences\n",
    "def predict(sentence, tokenizer, vocab, label_to_idx, model, threshold=0.45):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(sentence)\n",
    "        indices = [vocab[token] for token in tokens if token in vocab]\n",
    "        if not indices:\n",
    "            indices = [vocab['<unk>']]\n",
    "        indices = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "        logits = model(indices)\n",
    "        # Apply sigmoid to convert logits to probabilities and then threshold\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > threshold).float().squeeze(0)\n",
    "        predicted_labels = [label for label, idx in label_to_idx.items() if preds[idx] == 1]\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    \"Attention is all you need The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\",\n",
    "    \"Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.\",\n",
    "    \"Classification can be performed using either a discriminative or a generative learning approach. Discriminative learning consists of constructing the conditional probability of the outputs given the inputs, while generative learning consists of constructing the joint probability density of the inputs and outputs. Although most classical and quantum methods are discriminative, there are some advantages of the generative learning approach. For instance, it can be applied to unsupervised learning, statistical inference, uncertainty estimation, and synthetic data generation. In this article, we present a quantum generative multiclass classification strategy, called quantum generative classification (QGC). This model uses a variational quantum algorithm to estimate the joint probability density function of features and labels of a data set by means of a mixed quantum state. We also introduce a quantum map called quantum-enhanced Fourier features (QEFF), which leverages quantum superposition to prepare high-dimensional data samples in quantum hardware using a small number of qubits. We show that the quantum generative classification algorithm can be viewed as a Gaussian mixture that reproduces a kernel Hilbert space of the training data. In addition, we developed a hybrid quantum-classical neural network that shows that it is possible to perform generative classification on high-dimensional data sets. The method was tested on various low- and high-dimensional data sets including the 10-class MNIST and Fashion-MNIST data sets, illustrating that the generative classification strategy is competitive against other previous quantum models.\",\n",
    "    \"Research on human skin anatomy reveals its complex multi-scale, multi-phase nature, with up to 70% of its composition being bounded and free water. Fluid movement plays a key role in the skin's mechanical and biological responses, influencing its time-dependent behavior and nutrient transport.Poroelastic modeling is a promising approach for studying skin dynamics across scales by integrating multi-physics processes. This paper introduces a biology hierarchical two-compartment model capturing fluid distribution in the interstitium and micro-circulation. A theoretical framework is developed with a biphasic interstitium -- distinguishing interstitial fluid and non-structural cells -- and analyzed through a one-dimensional consolidation test of a column. This biphasic approach allows separate modeling of cell and fluid motion, considering their differing characteristic times. An appendix discusses extending the model to include biological exchanges like oxygen transport. Preliminary results indicate that cell viscosity introduces a second characteristic time, and at high viscosity and short time scales, cells behave similarly to solids.A simplified model was used to replicate an experimental campaign on short time scales. Local pressure (up to 31 kPa) was applied to dorsal finger skin using a laser Doppler probe PF801 (Perimed Sweden), following a setup described in Fromy Brain Res (1998). The model qualitatively captured ischemia and post-occlusive reactive hyperemia, aligning with experimental data.All numerical simulations used the open-source software FEniCSx v0.9.0. To ensure transparency and reproducibility, anonymized experimental data and finite element codes are publicly available on GitHub.\",\n",
    "    \"Currency arbitrage capitalizes on price discrepancies in currency exchange rates between markets to produce profits with minimal risk. By employing a combinatorial optimization problem, one can ascertain optimal paths within directed graphs, thereby facilitating the efficient identification of profitable trading routes. This research investigates the methodologies of quantum annealing and gate-based quantum computing in relation to the currency arbitrage problem. In this study, we implement the Quantum Approximate Optimization Algorithm (QAOA) utilizing Qiskit version 1.2. In order to optimize the parameters of QAOA, we perform simulations utilizing the AerSimulator and carry out experiments in simulation. Furthermore, we present an NchooseK-based methodology utilizing D-Wave's Ocean suite. This methodology enables a comparison of the effectiveness of quantum techniques in identifying optimal arbitrage paths. The results of our study enhance the existing literature on the application of quantum computing in financial optimization challenges, emphasizing both the prospective benefits and the present limitations of these developing technologies in real-world scenarios.\",\n",
    "    \"Despite advances in methods to interrogate tumor biology, the observational and population-based approach of classical cancer research and clinical oncology does not enable anticipation of tumor outcomes to hasten the discovery of cancer mechanisms and personalize disease management. To address these limitations, individualized cancer forecasts have been shown to predict tumor growth and therapeutic response, inform treatment optimization, and guide experimental efforts. These predictions are obtained via computer simulations of mathematical models that are constrained with data from a patient's cancer and experiments. This book chapter addresses the validation of these mathematical models to forecast tumor growth and treatment response. We start with an overview of mathematical modeling frameworks, model selection techniques, and fundamental metrics. We then describe the usual strategies employed to validate cancer forecasts in preclinical and clinical scenarios. Finally, we discuss existing barriers in validating these predictions along with potential strategies to address them.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Back-Testing Results ===\n",
      "Sentence: Attention is all you need The dominant sequence transduction models are based on...\n",
      "Predicted Labels: ['cs']\n",
      "\n",
      "Sentence: Vector autogressions (VARs) are widely applied when it comes to modeling and for...\n",
      "Predicted Labels: ['econ-qfin']\n",
      "\n",
      "Sentence: Classification can be performed using either a discriminative or a generative le...\n",
      "Predicted Labels: ['cs', 'physic']\n",
      "\n",
      "Sentence: Research on human skin anatomy reveals its complex multi-scale, multi-phase natu...\n",
      "Predicted Labels: ['bio', 'physic']\n",
      "\n",
      "Sentence: Currency arbitrage capitalizes on price discrepancies in currency exchange rates...\n",
      "Predicted Labels: ['econ-qfin', 'math-stats']\n",
      "\n",
      "Sentence: Despite advances in methods to interrogate tumor biology, the observational and ...\n",
      "Predicted Labels: ['bio']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Back-Testing Results ===\")\n",
    "for sentence in example_sentences:\n",
    "    predicted_labels = predict(sentence, tokenizer, vocab, label_to_idx, model)\n",
    "    print(f\"Sentence: {sentence[:80]}...\")\n",
    "    print(f\"Predicted Labels: {predicted_labels}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
